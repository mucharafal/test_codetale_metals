[{
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "scalastyle: class parameters also get 4 space indent https://github.com/databricks/scala-style-guide#spacing-and-indentation",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-25T10:54:33Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Right, ideally style check should catch this. Let me check my IJ settings again.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-25T19:02:00Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "it still deletes based on finishTimestamp. It should delete based on state being FAILED, CANCELED or CLOSED.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-25T16:14:10Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  sparkConf: SparkConf,\n+  server: Option[HiveServer2],\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerOperationStart => onOperationStart(e)\n+      case e: SparkListenerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerOperationError => onOperationError(e)\n+      case e: SparkListenerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())\n+    }\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0",
    "line": 234
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Thanks for the clarification. Updated.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-25T19:02:31Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  sparkConf: SparkConf,\n+  server: Option[HiveServer2],\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerOperationStart => onOperationStart(e)\n+      case e: SparkListenerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerOperationError => onOperationError(e)\n+      case e: SparkListenerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())\n+    }\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0",
    "line": 234
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "@gengliangwang could you also take a look at `updateLiveStore`, `update` and `flush` compared to what their counterparts in `SQLAppStatusListener` and `AppStatusListener` is doing?",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-25T16:15:38Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  sparkConf: SparkConf,\n+  server: Option[HiveServer2],\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerOperationStart => onOperationStart(e)\n+      case e: SparkListenerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerOperationError => onOperationError(e)\n+      case e: SparkListenerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "you're using this function only from `kvStore.onFlush`. Could inline it there?",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T09:59:19Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }",
    "line": 202
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "I have slightly refactored the store update logic.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T22:39:45Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }",
    "line": 202
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "could you change session -> entity? You use it for all entities, and when reviewing it I got confused that this function is for sessions, and `update` above is for other objects, but in fact `update` is only used in flush.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T10:00:51Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Thanks. Done.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T22:40:01Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "should this have `checkTriggers = force`?",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T10:01:05Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Yes, now I have modified the update method.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T22:40:21Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "Could you explain a little bit more about this? \r\nIn the original code:\r\n```\r\noverride def onJobStart(jobStart: SparkListenerJobStart): Unit = synchronized {\t\r\n      for {\t\r\n        props <- Option(jobStart.properties)\t\r\n        groupId <- Option(props.getProperty(SparkContext.SPARK_JOB_GROUP_ID))\t\r\n        (_, info) <- executionList if info.groupId == groupId\t\r\n      } {\t\r\n        info.jobId += jobStart.jobId.toString\t\r\n        info.groupId = groupId\t\r\n      }\t\r\n    }\t\r\n```\r\n\r\nSeems there is new logic here.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T21:22:35Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event."
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Actually there was a similar issue, which was rarely occur in SQLAppStatusListener, (see https://github.com/apache/spark/pull/23939). I couldn't reproduce it in HiveThriftserver case, but just to prevent that I added the logic here. I think, it seems redundant may be I can remove the logic from here.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T22:48:32Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event."
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I see. I think we can keep it. Please put more comments to explain it, thanks :)",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T23:08:54Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event."
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Sure, I will put comment.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T23:21:59Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event."
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Added comment",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-27T01:01:01Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event."
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "For live UI, we should check if \r\n```\r\nnow - lastWriteTime > LIVE_ENTITY_UPDATE_PERIOD\r\n```",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T22:16:35Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Thanks, updated",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-26T22:41:54Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "It seems that all the usages of this method is with trigger enabled.\r\nShould we change it to \r\n```\r\ndef updateStoreWithTriggerEnabled(entity: LiveEntity): Unit\r\n```",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-27T00:30:57Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.internal.config.Status.LIVE_ENTITY_UPDATE_PERIOD\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // How often to update live entities. -1 means \"never update\" when replaying applications,\n+  // meaning only the last write will happen. For live applications, this avoids a few\n+  // operations that we can live without when rapidly processing incoming task events.\n+  private val liveUpdatePeriodNs = if (live) sparkConf.get(LIVE_ENTITY_UPDATE_PERIOD) else -1L\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      flush(updateStore(_, trigger = true))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateStore(liveExec, trigger = true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateStore(session, trigger = true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId))\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateStore(executionList.get(e.id), trigger = true)\n+    executionList.remove(e.id)\n+  }\n+\n+  // Update both live and history stores. If trigger is enabled, it will cleanup\n+  // entity which exceeds the threshold.\n+  def updateStore(entity: LiveEntity, trigger: Boolean = false): Unit = {"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Updated",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-27T01:00:24Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.internal.config.Status.LIVE_ENTITY_UPDATE_PERIOD\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // How often to update live entities. -1 means \"never update\" when replaying applications,\n+  // meaning only the last write will happen. For live applications, this avoids a few\n+  // operations that we can live without when rapidly processing incoming task events.\n+  private val liveUpdatePeriodNs = if (live) sparkConf.get(LIVE_ENTITY_UPDATE_PERIOD) else -1L\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      flush(updateStore(_, trigger = true))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateStore(liveExec, trigger = true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateStore(session, trigger = true)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId))\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateStore(executionList.get(e.id), trigger = true)\n+    executionList.remove(e.id)\n+  }\n+\n+  // Update both live and history stores. If trigger is enabled, it will cleanup\n+  // entity which exceeds the threshold.\n+  def updateStore(entity: LiveEntity, trigger: Boolean = false): Unit = {"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "Nit: remove empty line.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-27T01:19:11Z",
    "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.internal.config.Status.LIVE_ENTITY_UPDATE_PERIOD\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // How often to update live entities. -1 means \"never update\" when replaying applications,\n+  // meaning only the last write will happen. For live applications, this avoids a few\n+  // operations that we can live without when rapidly processing incoming task events.\n+  private val liveUpdatePeriodNs = if (live) sparkConf.get(LIVE_ENTITY_UPDATE_PERIOD) else -1L\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      flush((entity: LiveEntity) => updateStoreWithTriggerEnabled(entity))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // It may possible that event reordering happens such a way that JobStart event come after\n+      // Execution end event (Refer SPARK-27019). To handle that situation, if occurs in\n+      // Thriftserver, following code will take care. Here will come only if JobStart event comes\n+      // after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateStoreWithTriggerEnabled(liveExec)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateStoreWithTriggerEnabled(session)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId))\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateStoreWithTriggerEnabled(executionList.get(e.id))\n+    executionList.remove(e.id)\n+  }\n+\n+  // Update both live and history stores. Trigger is enabled by default, hence\n+  // it will cleanup the entity which exceeds the threshold.\n+  def updateStoreWithTriggerEnabled(entity: LiveEntity): Unit = {\n+    entity.write(kvstore, System.nanoTime(), checkTriggers = true)\n+  }\n+\n+  // Update only live stores. If trigger is enabled, it will cleanup entity\n+  // which exceeds the threshold.\n+  def updateLiveStore(entity: LiveEntity, trigger: Boolean = false): Unit = {\n+    val now = System.nanoTime()\n+    if (live && liveUpdatePeriodNs >= 0 && now - entity.lastWriteTime > liveUpdatePeriodNs) {\n+      entity.write(kvstore, now, checkTriggers = trigger)\n+    }\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0\n+    }\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.execId) }\n+  }\n+\n+  private def cleanupSession(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedSessions)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[SessionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0L\n+    }\n+\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.sessionId) }\n+  }\n+\n+  /**\n+   * Remove at least (retainedSize / 10) items to reduce friction. Because tracking may be done\n+   * asynchronously, this method may return 0 in case enough items have been deleted already.\n+   */\n+  private def calculateNumberToRemove(dataSize: Long, retainedSize: Long): Long = {\n+    if (dataSize > retainedSize) {\n+      math.max(retainedSize / 10L, dataSize - retainedSize)\n+    } else {\n+      0L\n+    }\n+  }\n+}\n+\n+private[thriftserver] class LiveExecutionData(\n+    val execId: String,\n+    val statement: String,\n+    val sessionId: String,\n+    val startTimestamp: Long,\n+    val userName: String) extends LiveEntity {\n+\n+    var finishTimestamp: Long = 0L\n+    var closeTimestamp: Long = 0L\n+    var executePlan: String = \"\"\n+    var detail: String = \"\"\n+    var state: ExecutionState.Value = ExecutionState.STARTED\n+    val jobId: ArrayBuffer[String] = ArrayBuffer[String]()\n+    var groupId: String = \"\"\n+\n+  override protected def doUpdate(): Any = {\n+    new ExecutionInfo(\n+      execId,\n+      statement,\n+      sessionId,\n+      startTimestamp,\n+      userName,\n+      finishTimestamp,\n+      closeTimestamp,\n+      executePlan,\n+      detail,\n+      state,\n+      jobId,\n+      groupId)\n+  }\n+}\n+",
    "line": 296
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Done",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-27T01:42:19Z",
    "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver.ui\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hive.service.server.HiveServer2\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.internal.config.Status.LIVE_ENTITY_UPDATE_PERIOD\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+    kvstore: ElementTrackingStore,\n+    sparkConf: SparkConf,\n+    server: Option[HiveServer2],\n+    live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    (sparkConf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+      sparkConf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+  }\n+\n+  // How often to update live entities. -1 means \"never update\" when replaying applications,\n+  // meaning only the last write will happen. For live applications, this avoids a few\n+  // operations that we can live without when rapidly processing incoming task events.\n+  private val liveUpdatePeriodNs = if (live) sparkConf.get(LIVE_ENTITY_UPDATE_PERIOD) else -1L\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      flush((entity: LiveEntity) => updateStoreWithTriggerEnabled(entity))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // It may possible that event reordering happens such a way that JobStart event come after\n+      // Execution end event (Refer SPARK-27019). To handle that situation, if occurs in\n+      // Thriftserver, following code will take care. Here will come only if JobStart event comes\n+      // after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        updateStoreWithTriggerEnabled(liveExec)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerThriftServerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerThriftServerOperationStart => onOperationStart(e)\n+      case e: SparkListenerThriftServerOperationParsed => onOperationParsed(e)\n+      case e: SparkListenerThriftServerOperationCanceled => onOperationCanceled(e)\n+      case e: SparkListenerThriftServerOperationError => onOperationError(e)\n+      case e: SparkListenerThriftServerOperationFinish => onOperationFinished(e)\n+      case e: SparkListenerThriftServerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def onSessionCreated(e: SparkListenerThriftServerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session)\n+  }\n+\n+  private def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateStoreWithTriggerEnabled(session)\n+    sessionList.remove(e.sessionId)\n+  }\n+\n+  private def onOperationStart(e: SparkListenerThriftServerOperationStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId))\n+  }\n+\n+  private def onOperationParsed(e: SparkListenerThriftServerOperationParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationCanceled(e: SparkListenerThriftServerOperationCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationError(e: SparkListenerThriftServerOperationError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationFinished(e: SparkListenerThriftServerOperationFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def onOperationClosed(e: SparkListenerThriftServerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateStoreWithTriggerEnabled(executionList.get(e.id))\n+    executionList.remove(e.id)\n+  }\n+\n+  // Update both live and history stores. Trigger is enabled by default, hence\n+  // it will cleanup the entity which exceeds the threshold.\n+  def updateStoreWithTriggerEnabled(entity: LiveEntity): Unit = {\n+    entity.write(kvstore, System.nanoTime(), checkTriggers = true)\n+  }\n+\n+  // Update only live stores. If trigger is enabled, it will cleanup entity\n+  // which exceeds the threshold.\n+  def updateLiveStore(entity: LiveEntity, trigger: Boolean = false): Unit = {\n+    val now = System.nanoTime()\n+    if (live && liveUpdatePeriodNs >= 0 && now - entity.lastWriteTime > liveUpdatePeriodNs) {\n+      entity.write(kvstore, now, checkTriggers = trigger)\n+    }\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0\n+    }\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.execId) }\n+  }\n+\n+  private def cleanupSession(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedSessions)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[SessionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0L\n+    }\n+\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.sessionId) }\n+  }\n+\n+  /**\n+   * Remove at least (retainedSize / 10) items to reduce friction. Because tracking may be done\n+   * asynchronously, this method may return 0 in case enough items have been deleted already.\n+   */\n+  private def calculateNumberToRemove(dataSize: Long, retainedSize: Long): Long = {\n+    if (dataSize > retainedSize) {\n+      math.max(retainedSize / 10L, dataSize - retainedSize)\n+    } else {\n+      0L\n+    }\n+  }\n+}\n+\n+private[thriftserver] class LiveExecutionData(\n+    val execId: String,\n+    val statement: String,\n+    val sessionId: String,\n+    val startTimestamp: Long,\n+    val userName: String) extends LiveEntity {\n+\n+    var finishTimestamp: Long = 0L\n+    var closeTimestamp: Long = 0L\n+    var executePlan: String = \"\"\n+    var detail: String = \"\"\n+    var state: ExecutionState.Value = ExecutionState.STARTED\n+    val jobId: ArrayBuffer[String] = ArrayBuffer[String]()\n+    var groupId: String = \"\"\n+\n+  override protected def doUpdate(): Any = {\n+    new ExecutionInfo(\n+      execId,\n+      statement,\n+      sessionId,\n+      startTimestamp,\n+      userName,\n+      finishTimestamp,\n+      closeTimestamp,\n+      executePlan,\n+      detail,\n+      state,\n+      jobId,\n+      groupId)\n+  }\n+}\n+",
    "line": 296
  }],
  "prId": 26378
}]