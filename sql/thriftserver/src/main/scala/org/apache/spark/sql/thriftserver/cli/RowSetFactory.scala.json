[{
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "In Hive the handling of this is inside ColumnaBased set. I think it should be put there to keep this factory class simple.\r\n\r\nAlso, I think Spark will never use it server side, and this code is only for the client reading it from a server that used it, so Spark could also maybe drop it...",
    "commit": "d0de49f814a896bbfa4f2c1fbd074e7ac5e354ec",
    "createdAt": "2019-11-01T12:24:04Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.thriftserver.cli\n+\n+import java.io.ByteArrayInputStream\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.thrift.TException\n+import org.apache.thrift.protocol.TCompactProtocol\n+import org.apache.thrift.transport.TIOStreamTransport\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.thriftserver.cli.thrift.{TColumn, TProtocolVersion, TRowSet}\n+import org.apache.spark.sql.thriftserver.cli.thrift.TProtocolVersion._\n+import org.apache.spark.sql.types.StructType\n+\n+private[thriftserver] object RowSetFactory extends Logging {\n+  def create(types: StructType, rows: Seq[Row], version: TProtocolVersion): RowSet = {\n+    if (version.getValue >= HIVE_CLI_SERVICE_PROTOCOL_V6.getValue) {\n+      ColumnBasedSet(types, ArrayBuffer(rows: _*), 0)\n+    } else {\n+      RowBasedSet(types, ArrayBuffer(rows: _*), 0)\n+    }\n+  }\n+\n+  def create(types: StructType, version: TProtocolVersion): RowSet = {\n+    if (version.getValue >= HIVE_CLI_SERVICE_PROTOCOL_V6.getValue) {\n+      ColumnBasedSet(types, new ArrayBuffer[Row](), 0)\n+    } else {\n+      RowBasedSet(types, new ArrayBuffer[Row](), 0)\n+    }\n+  }\n+\n+  def create(tRowSet: TRowSet, version: TProtocolVersion): RowSet = {\n+    val rows = new ArrayBuffer[Row]()\n+    if (version.getValue >= HIVE_CLI_SERVICE_PROTOCOL_V6.getValue) {\n+      if (tRowSet.isSetBinaryColumns) {"
  }, {
    "author": {
      "login": "AngersZhuuuu"
    },
    "body": "> In Hive the handling of this is inside ColumnaBased set. I think it should be put there to keep this factory class simple.\r\n> \r\n> Also, I think Spark will never use it server side, and this code is only for the client reading it from a server that used it, so Spark could also maybe drop it...\r\n\r\nIf we want to build spark's own jdbc client, we need these code.",
    "commit": "d0de49f814a896bbfa4f2c1fbd074e7ac5e354ec",
    "createdAt": "2019-11-01T13:32:50Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.thriftserver.cli\n+\n+import java.io.ByteArrayInputStream\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.thrift.TException\n+import org.apache.thrift.protocol.TCompactProtocol\n+import org.apache.thrift.transport.TIOStreamTransport\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.thriftserver.cli.thrift.{TColumn, TProtocolVersion, TRowSet}\n+import org.apache.spark.sql.thriftserver.cli.thrift.TProtocolVersion._\n+import org.apache.spark.sql.types.StructType\n+\n+private[thriftserver] object RowSetFactory extends Logging {\n+  def create(types: StructType, rows: Seq[Row], version: TProtocolVersion): RowSet = {\n+    if (version.getValue >= HIVE_CLI_SERVICE_PROTOCOL_V6.getValue) {\n+      ColumnBasedSet(types, ArrayBuffer(rows: _*), 0)\n+    } else {\n+      RowBasedSet(types, ArrayBuffer(rows: _*), 0)\n+    }\n+  }\n+\n+  def create(types: StructType, version: TProtocolVersion): RowSet = {\n+    if (version.getValue >= HIVE_CLI_SERVICE_PROTOCOL_V6.getValue) {\n+      ColumnBasedSet(types, new ArrayBuffer[Row](), 0)\n+    } else {\n+      RowBasedSet(types, new ArrayBuffer[Row](), 0)\n+    }\n+  }\n+\n+  def create(tRowSet: TRowSet, version: TProtocolVersion): RowSet = {\n+    val rows = new ArrayBuffer[Row]()\n+    if (version.getValue >= HIVE_CLI_SERVICE_PROTOCOL_V6.getValue) {\n+      if (tRowSet.isSetBinaryColumns) {"
  }],
  "prId": 26340
}]