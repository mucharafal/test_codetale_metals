[{
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "maybe move SparkThriftServerListener to it's own file...",
    "commit": "d0de49f814a896bbfa4f2c1fbd074e7ac5e354ec",
    "createdAt": "2019-10-31T11:51:06Z",
    "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.thriftserver\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.hive.conf.HiveConf\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.UI.UI_ENABLED\n+import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd, SparkListenerJobStart}\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.hive.HiveUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.thriftserver.server.SparkThriftServer\n+import org.apache.spark.sql.thriftserver.ui.ThriftServerTab\n+import org.apache.spark.util.{ShutdownHookManager, Utils}\n+\n+\n+\n+/**\n+ * The main entry point for the Spark SQL port of HiveServer2.  Starts up a `SparkSQLContext` and a\n+ * `HiveThriftServer2` thrift server.\n+ */\n+object SparkThriftServer2 extends Logging {\n+  var uiTab: Option[ThriftServerTab] = None\n+  var listener: SparkThriftServerListener = _\n+\n+  def hiveConfForExecution(sparkContext: SparkContext): HiveConf = {\n+    val extraConfig = HiveUtils.newTemporaryConfiguration(true)\n+    val hiveConf: HiveConf = new HiveConf()\n+    (sparkContext.hadoopConfiguration\n+      .iterator().asScala.map(kv => kv.getKey -> kv.getValue)\n+      ++ sparkContext.conf.getAll.toMap ++ extraConfig).toMap\n+      .foreach { case (k, v) => hiveConf.set(k, v) }\n+    hiveConf\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Starts a new thrift server with the given context.\n+   */\n+  @DeveloperApi\n+  def startWithContext(sqlContext: SQLContext): SparkThriftServer = {\n+    val server = new SparkThriftServer(sqlContext)\n+\n+    server.init(hiveConfForExecution(sqlContext.sparkContext))\n+    server.start()\n+    listener = new SparkThriftServerListener(server, sqlContext.conf)\n+    sqlContext.sparkContext.addSparkListener(listener)\n+    uiTab = if (sqlContext.sparkContext.getConf.get(UI_ENABLED)) {\n+      Some(new ThriftServerTab(sqlContext.sparkContext))\n+    } else {\n+      None\n+    }\n+    server\n+  }\n+\n+  def main(args: Array[String]) {\n+    // If the arguments contains \"-h\" or \"--help\", print out the usage and exit.\n+    if (args.contains(\"-h\") || args.contains(\"--help\")) {\n+      SparkThriftServer.main(args)\n+      // The following code should not be reachable. It is added to ensure the main function exits.\n+      return\n+    }\n+\n+    Utils.initDaemon(log)\n+    val optionsProcessor = new  SparkThriftServer.ServerOptionsProcessor(\"HiveThriftServer2\")\n+    optionsProcessor.parse(args)\n+\n+    logInfo(\"Starting SparkContext\")\n+    SparkSQLEnv.init()\n+\n+    ShutdownHookManager.addShutdownHook { () =>\n+      SparkSQLEnv.stop()\n+      uiTab.foreach(_.detach())\n+    }\n+\n+    try {\n+      val server = new SparkThriftServer(SparkSQLEnv.sqlContext)\n+      server.init(hiveConfForExecution(SparkSQLEnv.sparkContext))\n+      server.start()\n+      logInfo(\"HiveThriftServer2 started\")\n+      listener = new SparkThriftServerListener(server, SparkSQLEnv.sqlContext.conf)\n+      SparkSQLEnv.sparkContext.addSparkListener(listener)\n+      uiTab = if (SparkSQLEnv.sparkContext.getConf.get(UI_ENABLED)) {\n+        Some(new ThriftServerTab(SparkSQLEnv.sparkContext))\n+      } else {\n+\n+        None\n+      }\n+      // If application was killed before HiveThriftServer2 start successfully then SparkSubmit\n+      // process can not exit, so check whether if SparkContext was stopped.\n+      if (SparkSQLEnv.sparkContext.stopped.get()) {\n+        logError(\"SparkContext has stopped even if HiveServer2 has started, so exit\")\n+        System.exit(-1)\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Error starting HiveThriftServer2\", e)\n+        System.exit(-1)\n+    }\n+  }\n+\n+  private[thriftserver] class SessionInfo(\n+                                           val sessionId: String,\n+                                           val startTimestamp: Long,\n+                                           val ip: String,\n+                                           val userName: String) {\n+    var finishTimestamp: Long = 0L\n+    var totalExecution: Int = 0\n+\n+    def totalTime: Long = {\n+      if (finishTimestamp == 0L) {\n+        System.currentTimeMillis - startTimestamp\n+      } else {\n+        finishTimestamp - startTimestamp\n+      }\n+    }\n+  }\n+\n+  private[thriftserver] object ExecutionState extends Enumeration {\n+    val STARTED, CANCELED, COMPILED, FAILED, FINISHED, CLOSED = Value\n+    type ExecutionState = Value\n+  }\n+\n+  private[thriftserver] class ExecutionInfo(\n+                                             val statement: String,\n+                                             val sessionId: String,\n+                                             val startTimestamp: Long,\n+                                             val userName: String) {\n+    var finishTimestamp: Long = 0L\n+    var closeTimestamp: Long = 0L\n+    var executePlan: String = \"\"\n+    var detail: String = \"\"\n+    var state: ExecutionState.Value = ExecutionState.STARTED\n+    val jobId: ArrayBuffer[String] = ArrayBuffer[String]()\n+    var groupId: String = \"\"\n+\n+    def totalTime(endTime: Long): Long = {\n+      if (endTime == 0L) {\n+        System.currentTimeMillis - startTimestamp\n+      } else {\n+        endTime - startTimestamp\n+      }\n+    }\n+  }\n+\n+\n+  /**\n+   * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+   */\n+  private[thriftserver] class SparkThriftServerListener(val server: SparkThriftServer,"
  }],
  "prId": 26340
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "Could this become `SparkThriftServer`, a companion object to the `SparkThriftServer` class in the server package?",
    "commit": "d0de49f814a896bbfa4f2c1fbd074e7ac5e354ec",
    "createdAt": "2019-10-31T11:54:29Z",
    "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.thriftserver\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.hive.conf.HiveConf\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.UI.UI_ENABLED\n+import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd, SparkListenerJobStart}\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.hive.HiveUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.thriftserver.server.SparkThriftServer\n+import org.apache.spark.sql.thriftserver.ui.ThriftServerTab\n+import org.apache.spark.util.{ShutdownHookManager, Utils}\n+\n+\n+\n+/**\n+ * The main entry point for the Spark SQL port of HiveServer2.  Starts up a `SparkSQLContext` and a\n+ * `HiveThriftServer2` thrift server.\n+ */\n+object SparkThriftServer2 extends Logging {"
  }, {
    "author": {
      "login": "AngersZhuuuu"
    },
    "body": "> Could this become `SparkThriftServer`, a companion object to the `SparkThriftServer` class in the server package?\r\n\r\nYes, can move to SparkThriftSerever",
    "commit": "d0de49f814a896bbfa4f2c1fbd074e7ac5e354ec",
    "createdAt": "2019-10-31T14:48:33Z",
    "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.thriftserver\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.hive.conf.HiveConf\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.UI.UI_ENABLED\n+import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd, SparkListenerJobStart}\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.hive.HiveUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.thriftserver.server.SparkThriftServer\n+import org.apache.spark.sql.thriftserver.ui.ThriftServerTab\n+import org.apache.spark.util.{ShutdownHookManager, Utils}\n+\n+\n+\n+/**\n+ * The main entry point for the Spark SQL port of HiveServer2.  Starts up a `SparkSQLContext` and a\n+ * `HiveThriftServer2` thrift server.\n+ */\n+object SparkThriftServer2 extends Logging {"
  }],
  "prId": 26340
}]