[{
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "Compared to https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L83\r\nI think this is missing `_: UserDefinedType[_]` here, and also a match for `CalendarIntervalType`, and I think also `NullType`.\r\nSame in ColumnBasedSet.",
    "commit": "d0de49f814a896bbfa4f2c1fbd074e7ac5e354ec",
    "createdAt": "2019-11-01T09:35:04Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.thriftserver.cli\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.HiveResult\n+import org.apache.spark.sql.thriftserver.cli.thrift._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A result set of Spark's [[Row]]s with its [[StructType]] as its schema,\n+ * with the ability of\n+ * transform to [[TRowSet]].\n+ */\n+private[thriftserver] case class RowBasedSet(types: StructType,\n+                       rows: ArrayBuffer[Row], initStartOffset: Long)\n+  extends RowSet {\n+\n+  var startOffset: Long = initStartOffset\n+\n+  override def toTRowSet: TRowSet = new TRowSet(startOffset, toTRows.asJava)\n+\n+  private[this] def toTRows: Seq[TRow] = {\n+    if (rows != null) {\n+      if (types == null || (rows.nonEmpty && rows.head.size != types.size)) {\n+        throw new IllegalArgumentException(\"The given schema does't match the given row\")\n+      }\n+      rows.map(toTRow)\n+    } else {\n+      Nil\n+    }\n+  }\n+\n+  private[this] def toTRow(row: Row): TRow = {\n+    val tRow = new TRow()\n+    (0 until row.length).map(i => toTColumnValue(i, row)).foreach(tRow.addToColVals)\n+    tRow\n+  }\n+\n+  private[this] def toTColumnValue(ordinal: Int, row: Row): TColumnValue =\n+    types(ordinal).dataType match {\n+      case BooleanType =>\n+        val boolValue = new TBoolValue\n+        if (!row.isNullAt(ordinal)) boolValue.setValue(row.getBoolean(ordinal))\n+        TColumnValue.boolVal(boolValue)\n+\n+      case ByteType =>\n+        val byteValue = new TByteValue\n+        if (!row.isNullAt(ordinal)) byteValue.setValue(row.getByte(ordinal))\n+        TColumnValue.byteVal(byteValue)\n+\n+      case ShortType =>\n+        val tI16Value = new TI16Value\n+        if (!row.isNullAt(ordinal)) tI16Value.setValue(row.getShort(ordinal))\n+        TColumnValue.i16Val(tI16Value)\n+\n+      case IntegerType =>\n+        val tI32Value = new TI32Value\n+        if (!row.isNullAt(ordinal)) tI32Value.setValue(row.getInt(ordinal))\n+        TColumnValue.i32Val(tI32Value)\n+\n+      case LongType =>\n+        val tI64Value = new TI64Value\n+        if (!row.isNullAt(ordinal)) tI64Value.setValue(row.getLong(ordinal))\n+        TColumnValue.i64Val(tI64Value)\n+\n+      case FloatType =>\n+        val tDoubleValue = new TDoubleValue\n+        if (!row.isNullAt(ordinal)) tDoubleValue.setValue(row.getFloat(ordinal))\n+        TColumnValue.doubleVal(tDoubleValue)\n+\n+      case DoubleType =>\n+        val tDoubleValue = new TDoubleValue\n+        if (!row.isNullAt(ordinal)) tDoubleValue.setValue(row.getDouble(ordinal))\n+        TColumnValue.doubleVal(tDoubleValue)\n+\n+      case StringType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStringValue.setValue(row.getString(ordinal))\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case DecimalType() =>\n+        val tStrValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStrValue.setValue(row.getDecimal(ordinal).toString)\n+        TColumnValue.stringVal(tStrValue)\n+\n+      case DateType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStringValue.setValue(row.get(ordinal).toString)\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case TimestampType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStringValue.setValue(row.get(ordinal).toString)\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case BinaryType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) {\n+          val bytes = row.getAs[Array[Byte]](ordinal)\n+          tStringValue.setValue(HiveResult.toHiveString((bytes, types(ordinal).dataType)))\n+        }\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case _: ArrayType | _: StructType | _: MapType =>"
  }, {
    "author": {
      "login": "AngersZhuuuu"
    },
    "body": "> Compared to https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L83\r\n> I think this is missing `_: UserDefinedType[_]` here, and also a match for `CalendarIntervalType`, and I think also `NullType`.\r\n> Same in ColumnBasedSet.\r\n\r\nYeah, update RowBasedSet and ColumnBasedSet",
    "commit": "d0de49f814a896bbfa4f2c1fbd074e7ac5e354ec",
    "createdAt": "2019-11-01T10:33:40Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.thriftserver.cli\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.HiveResult\n+import org.apache.spark.sql.thriftserver.cli.thrift._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A result set of Spark's [[Row]]s with its [[StructType]] as its schema,\n+ * with the ability of\n+ * transform to [[TRowSet]].\n+ */\n+private[thriftserver] case class RowBasedSet(types: StructType,\n+                       rows: ArrayBuffer[Row], initStartOffset: Long)\n+  extends RowSet {\n+\n+  var startOffset: Long = initStartOffset\n+\n+  override def toTRowSet: TRowSet = new TRowSet(startOffset, toTRows.asJava)\n+\n+  private[this] def toTRows: Seq[TRow] = {\n+    if (rows != null) {\n+      if (types == null || (rows.nonEmpty && rows.head.size != types.size)) {\n+        throw new IllegalArgumentException(\"The given schema does't match the given row\")\n+      }\n+      rows.map(toTRow)\n+    } else {\n+      Nil\n+    }\n+  }\n+\n+  private[this] def toTRow(row: Row): TRow = {\n+    val tRow = new TRow()\n+    (0 until row.length).map(i => toTColumnValue(i, row)).foreach(tRow.addToColVals)\n+    tRow\n+  }\n+\n+  private[this] def toTColumnValue(ordinal: Int, row: Row): TColumnValue =\n+    types(ordinal).dataType match {\n+      case BooleanType =>\n+        val boolValue = new TBoolValue\n+        if (!row.isNullAt(ordinal)) boolValue.setValue(row.getBoolean(ordinal))\n+        TColumnValue.boolVal(boolValue)\n+\n+      case ByteType =>\n+        val byteValue = new TByteValue\n+        if (!row.isNullAt(ordinal)) byteValue.setValue(row.getByte(ordinal))\n+        TColumnValue.byteVal(byteValue)\n+\n+      case ShortType =>\n+        val tI16Value = new TI16Value\n+        if (!row.isNullAt(ordinal)) tI16Value.setValue(row.getShort(ordinal))\n+        TColumnValue.i16Val(tI16Value)\n+\n+      case IntegerType =>\n+        val tI32Value = new TI32Value\n+        if (!row.isNullAt(ordinal)) tI32Value.setValue(row.getInt(ordinal))\n+        TColumnValue.i32Val(tI32Value)\n+\n+      case LongType =>\n+        val tI64Value = new TI64Value\n+        if (!row.isNullAt(ordinal)) tI64Value.setValue(row.getLong(ordinal))\n+        TColumnValue.i64Val(tI64Value)\n+\n+      case FloatType =>\n+        val tDoubleValue = new TDoubleValue\n+        if (!row.isNullAt(ordinal)) tDoubleValue.setValue(row.getFloat(ordinal))\n+        TColumnValue.doubleVal(tDoubleValue)\n+\n+      case DoubleType =>\n+        val tDoubleValue = new TDoubleValue\n+        if (!row.isNullAt(ordinal)) tDoubleValue.setValue(row.getDouble(ordinal))\n+        TColumnValue.doubleVal(tDoubleValue)\n+\n+      case StringType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStringValue.setValue(row.getString(ordinal))\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case DecimalType() =>\n+        val tStrValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStrValue.setValue(row.getDecimal(ordinal).toString)\n+        TColumnValue.stringVal(tStrValue)\n+\n+      case DateType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStringValue.setValue(row.get(ordinal).toString)\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case TimestampType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) tStringValue.setValue(row.get(ordinal).toString)\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case BinaryType =>\n+        val tStringValue = new TStringValue\n+        if (!row.isNullAt(ordinal)) {\n+          val bytes = row.getAs[Array[Byte]](ordinal)\n+          tStringValue.setValue(HiveResult.toHiveString((bytes, types(ordinal).dataType)))\n+        }\n+        TColumnValue.stringVal(tStringValue)\n+\n+      case _: ArrayType | _: StructType | _: MapType =>"
  }],
  "prId": 26340
}]