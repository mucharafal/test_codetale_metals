[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Could you also verify the logic of `normalizeFuncName` in this test case?",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-05-21T01:08:43Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val analyzer = {\n+      val conf = new SQLConf()\n+      val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Also add another one for the function that triggers `isRegisteredFunction`?",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-05-21T01:10:19Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val analyzer = {\n+      val conf = new SQLConf()\n+      val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)"
  }, {
    "author": {
      "login": "kevinyu98"
    },
    "body": "sorry for the delay. I will do that.",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-05-21T19:25:22Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val analyzer = {\n+      val conf = new SQLConf()\n+      val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)"
  }],
  "prId": 20795
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "nit: call3?",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-05-21T02:55:07Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val analyzer = {\n+      val conf = new SQLConf()\n+      val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedFunc, \"call1\")(), Alias(unresolvedFunc, \"call2\")(),\n+        Alias(unresolvedFunc, \"call1\")()),"
  }, {
    "author": {
      "login": "kevinyu98"
    },
    "body": "changed, thanks.",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-12T06:28:44Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val analyzer = {\n+      val conf = new SQLConf()\n+      val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedFunc, \"call1\")(), Alias(unresolvedFunc, \"call2\")(),\n+        Alias(unresolvedFunc, \"call1\")()),"
  }],
  "prId": 20795
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "I mean adding another test case to check whether `LookupFunctions` does not resolve the registeredFunction more than once. ",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-11T16:23:19Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val conf = new SQLConf()\n+    val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedFunc, \"call1\")(), Alias(unresolvedFunc, \"call2\")(),\n+        Alias(unresolvedFunc, \"call1\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+    assert(externalCatalog.getFunctionExistsCalledTimes == 1)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+      (unresolvedFunc.name).database == Some(\"default\"))\n+    assert(catalog.isRegisteredFunction(unresolvedFunc.name) == false)\n+    assert(catalog.isRegisteredFunction(FunctionIdentifier(\"max\")) == true)"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "We do not need to add assert. ",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-11T16:23:52Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val conf = new SQLConf()\n+    val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedFunc, \"call1\")(), Alias(unresolvedFunc, \"call2\")(),\n+        Alias(unresolvedFunc, \"call1\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+    assert(externalCatalog.getFunctionExistsCalledTimes == 1)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+      (unresolvedFunc.name).database == Some(\"default\"))\n+    assert(catalog.isRegisteredFunction(unresolvedFunc.name) == false)\n+    assert(catalog.isRegisteredFunction(FunctionIdentifier(\"max\")) == true)"
  }, {
    "author": {
      "login": "kevinyu98"
    },
    "body": "I see, I add the test case, can you verify ? thanks a lot.",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-12T06:28:28Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: LookupFunctions should not check the same function name more than once\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val conf = new SQLConf()\n+    val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedFunc, \"call1\")(), Alias(unresolvedFunc, \"call2\")(),\n+        Alias(unresolvedFunc, \"call1\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+    assert(externalCatalog.getFunctionExistsCalledTimes == 1)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+      (unresolvedFunc.name).database == Some(\"default\"))\n+    assert(catalog.isRegisteredFunction(unresolvedFunc.name) == false)\n+    assert(catalog.isRegisteredFunction(FunctionIdentifier(\"max\")) == true)"
  }],
  "prId": 20795
}, {
  "comments": [{
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@kevinyu98 Instead of extending FunctionRegistry and Catalog, what do think of extending SessionCatalog and overriding isRegisteredFunction and isPersistentFunction. So after a invocation of LookupFunction we get a count of how many times isRegisteredFunction was called and how many times isPersistentFunction was called ? We can just create an instance of analyzer with a extended Session catalog that we can use in more than one test ? Would that be simpler ?",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-12T18:10:33Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: the functionExists for the Persistent function check\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val conf = new SQLConf()\n+    val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedPersistentFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedPersistentFunc, \"call1\")(), Alias(unresolvedPersistentFunc, \"call2\")(),\n+        Alias(unresolvedPersistentFunc, \"call3\")(), Alias(unresolvedRegisteredFunc, \"call4\")(),\n+        Alias(unresolvedRegisteredFunc, \"call5\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+   assert(externalCatalog.getFunctionExistsCalledTimes == 1)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+      (unresolvedPersistentFunc.name).database == Some(\"default\"))\n+  }\n+\n+  test(\"SPARK-23486: the functionExists for the Registered function check\") {\n+\n+    val externalCatalog = new InMemoryCatalog\n+    val conf = new SQLConf()\n+    val customerFunctionReg = new CustomerFunctionRegistry\n+    val catalog = new SessionCatalog(externalCatalog, customerFunctionReg, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedRegisteredFunc, \"call1\")(), Alias(unresolvedRegisteredFunc, \"call2\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+    assert(customerFunctionReg.getIsRegisteredFunctionCalledTimes == 2)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+    (unresolvedRegisteredFunc.name).database == Some(\"default\"))\n+\n+  }\n+}\n+\n+class CustomerFunctionRegistry extends SimpleFunctionRegistry {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Either is fine to me. The major goal of these test cases is to count the number of invocation of `functionExists`. That is why the current way is more straightforward to reviewers.",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-12T20:52:48Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: the functionExists for the Persistent function check\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val conf = new SQLConf()\n+    val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedPersistentFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedPersistentFunc, \"call1\")(), Alias(unresolvedPersistentFunc, \"call2\")(),\n+        Alias(unresolvedPersistentFunc, \"call3\")(), Alias(unresolvedRegisteredFunc, \"call4\")(),\n+        Alias(unresolvedRegisteredFunc, \"call5\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+   assert(externalCatalog.getFunctionExistsCalledTimes == 1)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+      (unresolvedPersistentFunc.name).database == Some(\"default\"))\n+  }\n+\n+  test(\"SPARK-23486: the functionExists for the Registered function check\") {\n+\n+    val externalCatalog = new InMemoryCatalog\n+    val conf = new SQLConf()\n+    val customerFunctionReg = new CustomerFunctionRegistry\n+    val catalog = new SessionCatalog(externalCatalog, customerFunctionReg, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedRegisteredFunc, \"call1\")(), Alias(unresolvedRegisteredFunc, \"call2\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+    assert(customerFunctionReg.getIsRegisteredFunctionCalledTimes == 2)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+    (unresolvedRegisteredFunc.name).database == Some(\"default\"))\n+\n+  }\n+}\n+\n+class CustomerFunctionRegistry extends SimpleFunctionRegistry {"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@gatorsmile Sure Sean. ",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-12T20:59:55Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: the functionExists for the Persistent function check\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val conf = new SQLConf()\n+    val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedPersistentFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedPersistentFunc, \"call1\")(), Alias(unresolvedPersistentFunc, \"call2\")(),\n+        Alias(unresolvedPersistentFunc, \"call3\")(), Alias(unresolvedRegisteredFunc, \"call4\")(),\n+        Alias(unresolvedRegisteredFunc, \"call5\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+   assert(externalCatalog.getFunctionExistsCalledTimes == 1)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+      (unresolvedPersistentFunc.name).database == Some(\"default\"))\n+  }\n+\n+  test(\"SPARK-23486: the functionExists for the Registered function check\") {\n+\n+    val externalCatalog = new InMemoryCatalog\n+    val conf = new SQLConf()\n+    val customerFunctionReg = new CustomerFunctionRegistry\n+    val catalog = new SessionCatalog(externalCatalog, customerFunctionReg, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedRegisteredFunc, \"call1\")(), Alias(unresolvedRegisteredFunc, \"call2\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+    assert(customerFunctionReg.getIsRegisteredFunctionCalledTimes == 2)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+    (unresolvedRegisteredFunc.name).database == Some(\"default\"))\n+\n+  }\n+}\n+\n+class CustomerFunctionRegistry extends SimpleFunctionRegistry {"
  }, {
    "author": {
      "login": "kevinyu98"
    },
    "body": "thanks",
    "commit": "26f2f540d30f2e87405489513220468e7708742b",
    "createdAt": "2018-07-12T23:21:46Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import java.net.URI\n+\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, InMemoryCatalog, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+class LookupFunctionsSuite extends PlanTest {\n+\n+  test(\"SPARK-23486: the functionExists for the Persistent function check\") {\n+    val externalCatalog = new CustomInMemoryCatalog\n+    val conf = new SQLConf()\n+    val catalog = new SessionCatalog(externalCatalog, FunctionRegistry.builtin, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedPersistentFunc = UnresolvedFunction(\"func\", Seq.empty, false)\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedPersistentFunc, \"call1\")(), Alias(unresolvedPersistentFunc, \"call2\")(),\n+        Alias(unresolvedPersistentFunc, \"call3\")(), Alias(unresolvedRegisteredFunc, \"call4\")(),\n+        Alias(unresolvedRegisteredFunc, \"call5\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+   assert(externalCatalog.getFunctionExistsCalledTimes == 1)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+      (unresolvedPersistentFunc.name).database == Some(\"default\"))\n+  }\n+\n+  test(\"SPARK-23486: the functionExists for the Registered function check\") {\n+\n+    val externalCatalog = new InMemoryCatalog\n+    val conf = new SQLConf()\n+    val customerFunctionReg = new CustomerFunctionRegistry\n+    val catalog = new SessionCatalog(externalCatalog, customerFunctionReg, conf)\n+    val analyzer = {\n+      catalog.createDatabase(\n+        CatalogDatabase(\"default\", \"\", new URI(\"loc\"), Map.empty),\n+        ignoreIfExists = false)\n+      new Analyzer(catalog, conf)\n+    }\n+\n+    def table(ref: String): LogicalPlan = UnresolvedRelation(TableIdentifier(ref))\n+    val unresolvedRegisteredFunc = UnresolvedFunction(\"max\", Seq.empty, false)\n+    val plan = Project(\n+      Seq(Alias(unresolvedRegisteredFunc, \"call1\")(), Alias(unresolvedRegisteredFunc, \"call2\")()),\n+      table(\"TaBlE\"))\n+    analyzer.LookupFunctions.apply(plan)\n+    assert(customerFunctionReg.getIsRegisteredFunctionCalledTimes == 2)\n+\n+    assert(analyzer.LookupFunctions.normalizeFuncName\n+    (unresolvedRegisteredFunc.name).database == Some(\"default\"))\n+\n+  }\n+}\n+\n+class CustomerFunctionRegistry extends SimpleFunctionRegistry {"
  }],
  "prId": 20795
}]