[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "typo?",
    "commit": "31381569d08d079a12fa195533125d7237fa219d",
    "createdAt": "2017-09-19T05:34:04Z",
    "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BoundReference, CreateArray, Literal, SpecificInternalRow}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class ApproxCountDistinctForIntervalsSuite extends SparkFunSuite {\n+\n+  test(\"fails analysis if parameters are invalid\") {\n+    def assertEqual[T](left: T, right: T): Unit = {\n+      assert(left == right)\n+    }\n+\n+    val wrongColumnTypes = Seq(BinaryType, BooleanType, StringType, ArrayType(IntegerType),\n+      MapType(IntegerType, IntegerType), StructType(Seq(StructField(\"s\", IntegerType))))\n+    wrongColumnTypes.foreach { dataType =>\n+      val wrongColumn = new ApproxCountDistinctForIntervals(\n+        AttributeReference(\"a\", dataType)(),\n+        endpointsExpression = CreateArray(Seq(1, 10).map(Literal(_))))\n+      assert(\n+        wrongColumn.checkInputDataTypes() match {\n+          case TypeCheckFailure(msg)\n+            if msg.contains(\"requires (numeric or timestamp or date) type\") => true\n+          case _ => false\n+        })\n+    }\n+\n+    var wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = Literal(0.5d))\n+    assert(\n+      wrongEndpoints.checkInputDataTypes() match {\n+        case TypeCheckFailure(msg) if msg.contains(\"requires array type\") => true\n+        case _ => false\n+      })\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Seq(AttributeReference(\"b\", DoubleType)())))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The intervals provided must be constant literals\"))\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Array(10L).map(Literal(_))))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The number of endpoints must be >= 2 to construct intervals\"))\n+  }\n+\n+  /** Create an ApproxCountDistinctForIntervals instance and an input and output buffer. */\n+  private def createEstimator(\n+      endpoints: Array[Double],\n+      rsd: Double = 0.05,\n+      dt: DataType = IntegerType): (ApproxCountDistinctForIntervals, InternalRow, InternalRow) = {\n+    val input = new SpecificInternalRow(Seq(dt))\n+    val aggFunc = ApproxCountDistinctForIntervals(\n+      BoundReference(0, dt, nullable = true), CreateArray(endpoints.map(Literal(_))), rsd)\n+    val buffer = createBuffer(aggFunc)\n+    (aggFunc, input, buffer)\n+  }\n+\n+  private def createBuffer(aggFunc: ApproxCountDistinctForIntervals): InternalRow = {\n+    val buffer = new SpecificInternalRow(aggFunc.aggBufferAttributes.map(_.dataType))\n+    aggFunc.initialize(buffer)\n+    buffer\n+  }\n+\n+  test(\"merging ApproxCountDistinctForIntervals instances\") {\n+    val (aggFunc, input, buffer1a) = createEstimator(Array[Double](0, 10, 2000, 345678, 1000000))\n+    val buffer1b = createBuffer(aggFunc)\n+    val buffer2 = createBuffer(aggFunc)\n+\n+    // Create the"
  }],
  "prId": 15544
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "are `Array(7, 7, 7, 9)` valid end points?",
    "commit": "31381569d08d079a12fa195533125d7237fa219d",
    "createdAt": "2017-09-19T05:36:14Z",
    "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BoundReference, CreateArray, Literal, SpecificInternalRow}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class ApproxCountDistinctForIntervalsSuite extends SparkFunSuite {\n+\n+  test(\"fails analysis if parameters are invalid\") {\n+    def assertEqual[T](left: T, right: T): Unit = {\n+      assert(left == right)\n+    }\n+\n+    val wrongColumnTypes = Seq(BinaryType, BooleanType, StringType, ArrayType(IntegerType),\n+      MapType(IntegerType, IntegerType), StructType(Seq(StructField(\"s\", IntegerType))))\n+    wrongColumnTypes.foreach { dataType =>\n+      val wrongColumn = new ApproxCountDistinctForIntervals(\n+        AttributeReference(\"a\", dataType)(),\n+        endpointsExpression = CreateArray(Seq(1, 10).map(Literal(_))))\n+      assert(\n+        wrongColumn.checkInputDataTypes() match {\n+          case TypeCheckFailure(msg)\n+            if msg.contains(\"requires (numeric or timestamp or date) type\") => true\n+          case _ => false\n+        })\n+    }\n+\n+    var wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = Literal(0.5d))\n+    assert(\n+      wrongEndpoints.checkInputDataTypes() match {\n+        case TypeCheckFailure(msg) if msg.contains(\"requires array type\") => true\n+        case _ => false\n+      })\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Seq(AttributeReference(\"b\", DoubleType)())))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The intervals provided must be constant literals\"))\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Array(10L).map(Literal(_))))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The number of endpoints must be >= 2 to construct intervals\"))\n+  }\n+\n+  /** Create an ApproxCountDistinctForIntervals instance and an input and output buffer. */\n+  private def createEstimator(\n+      endpoints: Array[Double],\n+      rsd: Double = 0.05,\n+      dt: DataType = IntegerType): (ApproxCountDistinctForIntervals, InternalRow, InternalRow) = {\n+    val input = new SpecificInternalRow(Seq(dt))\n+    val aggFunc = ApproxCountDistinctForIntervals(\n+      BoundReference(0, dt, nullable = true), CreateArray(endpoints.map(Literal(_))), rsd)\n+    val buffer = createBuffer(aggFunc)\n+    (aggFunc, input, buffer)\n+  }\n+\n+  private def createBuffer(aggFunc: ApproxCountDistinctForIntervals): InternalRow = {\n+    val buffer = new SpecificInternalRow(aggFunc.aggBufferAttributes.map(_.dataType))\n+    aggFunc.initialize(buffer)\n+    buffer\n+  }\n+\n+  test(\"merging ApproxCountDistinctForIntervals instances\") {\n+    val (aggFunc, input, buffer1a) = createEstimator(Array[Double](0, 10, 2000, 345678, 1000000))\n+    val buffer1b = createBuffer(aggFunc)\n+    val buffer2 = createBuffer(aggFunc)\n+\n+    // Create the\n+    // Add the lower half\n+    var i = 0\n+    while (i < 500000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1a, input)\n+      i += 1\n+    }\n+\n+    // Add the upper half\n+    i = 500000\n+    while (i < 1000000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1b, input)\n+      i += 1\n+    }\n+\n+    // Merge the lower and upper halves.\n+    aggFunc.merge(buffer1a, buffer1b)\n+\n+    // Create the other buffer in reverse\n+    i = 999999\n+    while (i >= 0) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer2, input)\n+      i -= 1\n+    }\n+\n+    // Check if the buffers are equal.\n+    assert(buffer2 == buffer1a, \"Buffers should be equal\")\n+  }\n+\n+  test(\"test findHllppIndex(value) for values in the range\") {\n+    def checkHllppIndex(\n+        endpoints: Array[Double],\n+        value: Double,\n+        expectedIntervalIndex: Int): Unit = {\n+      val aggFunc = ApproxCountDistinctForIntervals(\n+        BoundReference(0, DoubleType, nullable = true), CreateArray(endpoints.map(Literal(_))))\n+      assert(aggFunc.findHllppIndex(value) == expectedIntervalIndex)\n+    }\n+    val endpoints = Array[Double](0, 3, 6, 10)\n+    // value is found (value is an interval boundary)\n+    checkHllppIndex(endpoints = endpoints, value = 0, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 3, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 6, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 10, expectedIntervalIndex = 2)\n+    // value is not found\n+    checkHllppIndex(endpoints = endpoints, value = 2, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 4, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 8, expectedIntervalIndex = 2)\n+\n+    // value is the same as multiple boundaries\n+    checkHllppIndex(endpoints = Array(7, 7, 7, 9), value = 7, expectedIntervalIndex = 0)",
    "line": 150
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "yes, it means 7 is the min value and it occupies 2 buckets from 3 buckets.",
    "commit": "31381569d08d079a12fa195533125d7237fa219d",
    "createdAt": "2017-09-19T10:58:21Z",
    "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BoundReference, CreateArray, Literal, SpecificInternalRow}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class ApproxCountDistinctForIntervalsSuite extends SparkFunSuite {\n+\n+  test(\"fails analysis if parameters are invalid\") {\n+    def assertEqual[T](left: T, right: T): Unit = {\n+      assert(left == right)\n+    }\n+\n+    val wrongColumnTypes = Seq(BinaryType, BooleanType, StringType, ArrayType(IntegerType),\n+      MapType(IntegerType, IntegerType), StructType(Seq(StructField(\"s\", IntegerType))))\n+    wrongColumnTypes.foreach { dataType =>\n+      val wrongColumn = new ApproxCountDistinctForIntervals(\n+        AttributeReference(\"a\", dataType)(),\n+        endpointsExpression = CreateArray(Seq(1, 10).map(Literal(_))))\n+      assert(\n+        wrongColumn.checkInputDataTypes() match {\n+          case TypeCheckFailure(msg)\n+            if msg.contains(\"requires (numeric or timestamp or date) type\") => true\n+          case _ => false\n+        })\n+    }\n+\n+    var wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = Literal(0.5d))\n+    assert(\n+      wrongEndpoints.checkInputDataTypes() match {\n+        case TypeCheckFailure(msg) if msg.contains(\"requires array type\") => true\n+        case _ => false\n+      })\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Seq(AttributeReference(\"b\", DoubleType)())))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The intervals provided must be constant literals\"))\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Array(10L).map(Literal(_))))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The number of endpoints must be >= 2 to construct intervals\"))\n+  }\n+\n+  /** Create an ApproxCountDistinctForIntervals instance and an input and output buffer. */\n+  private def createEstimator(\n+      endpoints: Array[Double],\n+      rsd: Double = 0.05,\n+      dt: DataType = IntegerType): (ApproxCountDistinctForIntervals, InternalRow, InternalRow) = {\n+    val input = new SpecificInternalRow(Seq(dt))\n+    val aggFunc = ApproxCountDistinctForIntervals(\n+      BoundReference(0, dt, nullable = true), CreateArray(endpoints.map(Literal(_))), rsd)\n+    val buffer = createBuffer(aggFunc)\n+    (aggFunc, input, buffer)\n+  }\n+\n+  private def createBuffer(aggFunc: ApproxCountDistinctForIntervals): InternalRow = {\n+    val buffer = new SpecificInternalRow(aggFunc.aggBufferAttributes.map(_.dataType))\n+    aggFunc.initialize(buffer)\n+    buffer\n+  }\n+\n+  test(\"merging ApproxCountDistinctForIntervals instances\") {\n+    val (aggFunc, input, buffer1a) = createEstimator(Array[Double](0, 10, 2000, 345678, 1000000))\n+    val buffer1b = createBuffer(aggFunc)\n+    val buffer2 = createBuffer(aggFunc)\n+\n+    // Create the\n+    // Add the lower half\n+    var i = 0\n+    while (i < 500000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1a, input)\n+      i += 1\n+    }\n+\n+    // Add the upper half\n+    i = 500000\n+    while (i < 1000000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1b, input)\n+      i += 1\n+    }\n+\n+    // Merge the lower and upper halves.\n+    aggFunc.merge(buffer1a, buffer1b)\n+\n+    // Create the other buffer in reverse\n+    i = 999999\n+    while (i >= 0) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer2, input)\n+      i -= 1\n+    }\n+\n+    // Check if the buffers are equal.\n+    assert(buffer2 == buffer1a, \"Buffers should be equal\")\n+  }\n+\n+  test(\"test findHllppIndex(value) for values in the range\") {\n+    def checkHllppIndex(\n+        endpoints: Array[Double],\n+        value: Double,\n+        expectedIntervalIndex: Int): Unit = {\n+      val aggFunc = ApproxCountDistinctForIntervals(\n+        BoundReference(0, DoubleType, nullable = true), CreateArray(endpoints.map(Literal(_))))\n+      assert(aggFunc.findHllppIndex(value) == expectedIntervalIndex)\n+    }\n+    val endpoints = Array[Double](0, 3, 6, 10)\n+    // value is found (value is an interval boundary)\n+    checkHllppIndex(endpoints = endpoints, value = 0, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 3, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 6, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 10, expectedIntervalIndex = 2)\n+    // value is not found\n+    checkHllppIndex(endpoints = endpoints, value = 2, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 4, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 8, expectedIntervalIndex = 2)\n+\n+    // value is the same as multiple boundaries\n+    checkHllppIndex(endpoints = Array(7, 7, 7, 9), value = 7, expectedIntervalIndex = 0)",
    "line": 150
  }],
  "prId": 15544
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why do we need this?",
    "commit": "31381569d08d079a12fa195533125d7237fa219d",
    "createdAt": "2017-09-19T14:33:05Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BoundReference, CreateArray, Literal, SpecificInternalRow}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class ApproxCountDistinctForIntervalsSuite extends SparkFunSuite {\n+\n+  test(\"fails analysis if parameters are invalid\") {\n+    def assertEqual[T](left: T, right: T): Unit = {"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "oh, I'll remove this. Previously I put some other logic here, but we should remove it now.",
    "commit": "31381569d08d079a12fa195533125d7237fa219d",
    "createdAt": "2017-09-20T01:49:28Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BoundReference, CreateArray, Literal, SpecificInternalRow}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class ApproxCountDistinctForIntervalsSuite extends SparkFunSuite {\n+\n+  test(\"fails analysis if parameters are invalid\") {\n+    def assertEqual[T](left: T, right: T): Unit = {"
  }],
  "prId": 15544
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this can be inlined",
    "commit": "31381569d08d079a12fa195533125d7237fa219d",
    "createdAt": "2017-09-19T14:40:20Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BoundReference, CreateArray, Literal, SpecificInternalRow}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class ApproxCountDistinctForIntervalsSuite extends SparkFunSuite {\n+\n+  test(\"fails analysis if parameters are invalid\") {\n+    def assertEqual[T](left: T, right: T): Unit = {\n+      assert(left == right)\n+    }\n+\n+    val wrongColumnTypes = Seq(BinaryType, BooleanType, StringType, ArrayType(IntegerType),\n+      MapType(IntegerType, IntegerType), StructType(Seq(StructField(\"s\", IntegerType))))\n+    wrongColumnTypes.foreach { dataType =>\n+      val wrongColumn = new ApproxCountDistinctForIntervals(\n+        AttributeReference(\"a\", dataType)(),\n+        endpointsExpression = CreateArray(Seq(1, 10).map(Literal(_))))\n+      assert(\n+        wrongColumn.checkInputDataTypes() match {\n+          case TypeCheckFailure(msg)\n+            if msg.contains(\"requires (numeric or timestamp or date) type\") => true\n+          case _ => false\n+        })\n+    }\n+\n+    var wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = Literal(0.5d))\n+    assert(\n+      wrongEndpoints.checkInputDataTypes() match {\n+        case TypeCheckFailure(msg) if msg.contains(\"requires array type\") => true\n+        case _ => false\n+      })\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Seq(AttributeReference(\"b\", DoubleType)())))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The intervals provided must be constant literals\"))\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Array(10L).map(Literal(_))))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The number of endpoints must be >= 2 to construct intervals\"))\n+  }\n+\n+  /** Create an ApproxCountDistinctForIntervals instance and an input and output buffer. */\n+  private def createEstimator(\n+      endpoints: Array[Double],\n+      rsd: Double = 0.05,\n+      dt: DataType = IntegerType): (ApproxCountDistinctForIntervals, InternalRow, InternalRow) = {\n+    val input = new SpecificInternalRow(Seq(dt))\n+    val aggFunc = ApproxCountDistinctForIntervals(\n+      BoundReference(0, dt, nullable = true), CreateArray(endpoints.map(Literal(_))), rsd)\n+    val buffer = createBuffer(aggFunc)\n+    (aggFunc, input, buffer)\n+  }\n+\n+  private def createBuffer(aggFunc: ApproxCountDistinctForIntervals): InternalRow = {\n+    val buffer = new SpecificInternalRow(aggFunc.aggBufferAttributes.map(_.dataType))\n+    aggFunc.initialize(buffer)\n+    buffer\n+  }\n+\n+  test(\"merging ApproxCountDistinctForIntervals instances\") {\n+    val (aggFunc, input, buffer1a) = createEstimator(Array[Double](0, 10, 2000, 345678, 1000000))\n+    val buffer1b = createBuffer(aggFunc)\n+    val buffer2 = createBuffer(aggFunc)\n+\n+    // Add the lower half to `buffer1a`.\n+    var i = 0\n+    while (i < 500000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1a, input)\n+      i += 1\n+    }\n+\n+    // Add the upper half to `buffer1b`.\n+    i = 500000\n+    while (i < 1000000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1b, input)\n+      i += 1\n+    }\n+\n+    // Merge the lower and upper halves to `buffer1a`.\n+    aggFunc.merge(buffer1a, buffer1b)\n+\n+    // Create the other buffer in reverse.\n+    i = 999999\n+    while (i >= 0) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer2, input)\n+      i -= 1\n+    }\n+\n+    // Check if the buffers are equal.\n+    assert(buffer2 == buffer1a, \"Buffers should be equal\")\n+  }\n+\n+  test(\"test findHllppIndex(value) for values in the range\") {\n+    def checkHllppIndex(\n+        endpoints: Array[Double],\n+        value: Double,\n+        expectedIntervalIndex: Int): Unit = {\n+      val aggFunc = ApproxCountDistinctForIntervals(\n+        BoundReference(0, DoubleType, nullable = true), CreateArray(endpoints.map(Literal(_))))\n+      assert(aggFunc.findHllppIndex(value) == expectedIntervalIndex)\n+    }\n+    val endpoints = Array[Double](0, 3, 6, 10)\n+    // value is found (value is an interval boundary)\n+    checkHllppIndex(endpoints = endpoints, value = 0, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 3, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 6, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 10, expectedIntervalIndex = 2)\n+    // value is not found\n+    checkHllppIndex(endpoints = endpoints, value = 2, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 4, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 8, expectedIntervalIndex = 2)\n+\n+    // value is the same as multiple boundaries\n+    checkHllppIndex(endpoints = Array(7, 7, 7, 9), value = 7, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = Array(3, 5, 7, 7, 7), value = 7, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = Array(1, 3, 5, 7, 7, 9), value = 7, expectedIntervalIndex = 2)\n+  }\n+\n+  test(\"basic operations: update, merge, eval...\") {\n+    val endpoints = Array[Double](0, 0.33, 0.6, 0.6, 0.6, 1.0)\n+    val data: Seq[Double] = Seq(0, 0.6, 0.3, 1, 0.6, 0.5, 0.6, 0.33)\n+    val expectedNdvs = Array[Long](3, 2, 1, 1, 1)"
  }],
  "prId": 15544
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "the input is double type, shall we do `input.setDouble(0, 2.0)` here?",
    "commit": "31381569d08d079a12fa195533125d7237fa219d",
    "createdAt": "2017-09-19T14:42:10Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BoundReference, CreateArray, Literal, SpecificInternalRow}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class ApproxCountDistinctForIntervalsSuite extends SparkFunSuite {\n+\n+  test(\"fails analysis if parameters are invalid\") {\n+    def assertEqual[T](left: T, right: T): Unit = {\n+      assert(left == right)\n+    }\n+\n+    val wrongColumnTypes = Seq(BinaryType, BooleanType, StringType, ArrayType(IntegerType),\n+      MapType(IntegerType, IntegerType), StructType(Seq(StructField(\"s\", IntegerType))))\n+    wrongColumnTypes.foreach { dataType =>\n+      val wrongColumn = new ApproxCountDistinctForIntervals(\n+        AttributeReference(\"a\", dataType)(),\n+        endpointsExpression = CreateArray(Seq(1, 10).map(Literal(_))))\n+      assert(\n+        wrongColumn.checkInputDataTypes() match {\n+          case TypeCheckFailure(msg)\n+            if msg.contains(\"requires (numeric or timestamp or date) type\") => true\n+          case _ => false\n+        })\n+    }\n+\n+    var wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = Literal(0.5d))\n+    assert(\n+      wrongEndpoints.checkInputDataTypes() match {\n+        case TypeCheckFailure(msg) if msg.contains(\"requires array type\") => true\n+        case _ => false\n+      })\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Seq(AttributeReference(\"b\", DoubleType)())))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The intervals provided must be constant literals\"))\n+\n+    wrongEndpoints = new ApproxCountDistinctForIntervals(\n+      AttributeReference(\"a\", DoubleType)(),\n+      endpointsExpression = CreateArray(Array(10L).map(Literal(_))))\n+    assertEqual(\n+      wrongEndpoints.checkInputDataTypes(),\n+      TypeCheckFailure(\"The number of endpoints must be >= 2 to construct intervals\"))\n+  }\n+\n+  /** Create an ApproxCountDistinctForIntervals instance and an input and output buffer. */\n+  private def createEstimator(\n+      endpoints: Array[Double],\n+      rsd: Double = 0.05,\n+      dt: DataType = IntegerType): (ApproxCountDistinctForIntervals, InternalRow, InternalRow) = {\n+    val input = new SpecificInternalRow(Seq(dt))\n+    val aggFunc = ApproxCountDistinctForIntervals(\n+      BoundReference(0, dt, nullable = true), CreateArray(endpoints.map(Literal(_))), rsd)\n+    val buffer = createBuffer(aggFunc)\n+    (aggFunc, input, buffer)\n+  }\n+\n+  private def createBuffer(aggFunc: ApproxCountDistinctForIntervals): InternalRow = {\n+    val buffer = new SpecificInternalRow(aggFunc.aggBufferAttributes.map(_.dataType))\n+    aggFunc.initialize(buffer)\n+    buffer\n+  }\n+\n+  test(\"merging ApproxCountDistinctForIntervals instances\") {\n+    val (aggFunc, input, buffer1a) = createEstimator(Array[Double](0, 10, 2000, 345678, 1000000))\n+    val buffer1b = createBuffer(aggFunc)\n+    val buffer2 = createBuffer(aggFunc)\n+\n+    // Add the lower half to `buffer1a`.\n+    var i = 0\n+    while (i < 500000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1a, input)\n+      i += 1\n+    }\n+\n+    // Add the upper half to `buffer1b`.\n+    i = 500000\n+    while (i < 1000000) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer1b, input)\n+      i += 1\n+    }\n+\n+    // Merge the lower and upper halves to `buffer1a`.\n+    aggFunc.merge(buffer1a, buffer1b)\n+\n+    // Create the other buffer in reverse.\n+    i = 999999\n+    while (i >= 0) {\n+      input.setInt(0, i)\n+      aggFunc.update(buffer2, input)\n+      i -= 1\n+    }\n+\n+    // Check if the buffers are equal.\n+    assert(buffer2 == buffer1a, \"Buffers should be equal\")\n+  }\n+\n+  test(\"test findHllppIndex(value) for values in the range\") {\n+    def checkHllppIndex(\n+        endpoints: Array[Double],\n+        value: Double,\n+        expectedIntervalIndex: Int): Unit = {\n+      val aggFunc = ApproxCountDistinctForIntervals(\n+        BoundReference(0, DoubleType, nullable = true), CreateArray(endpoints.map(Literal(_))))\n+      assert(aggFunc.findHllppIndex(value) == expectedIntervalIndex)\n+    }\n+    val endpoints = Array[Double](0, 3, 6, 10)\n+    // value is found (value is an interval boundary)\n+    checkHllppIndex(endpoints = endpoints, value = 0, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 3, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 6, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 10, expectedIntervalIndex = 2)\n+    // value is not found\n+    checkHllppIndex(endpoints = endpoints, value = 2, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = endpoints, value = 4, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = endpoints, value = 8, expectedIntervalIndex = 2)\n+\n+    // value is the same as multiple boundaries\n+    checkHllppIndex(endpoints = Array(7, 7, 7, 9), value = 7, expectedIntervalIndex = 0)\n+    checkHllppIndex(endpoints = Array(3, 5, 7, 7, 7), value = 7, expectedIntervalIndex = 1)\n+    checkHllppIndex(endpoints = Array(1, 3, 5, 7, 7, 9), value = 7, expectedIntervalIndex = 2)\n+  }\n+\n+  test(\"basic operations: update, merge, eval...\") {\n+    val endpoints = Array[Double](0, 0.33, 0.6, 0.6, 0.6, 1.0)\n+    val data: Seq[Double] = Seq(0, 0.6, 0.3, 1, 0.6, 0.5, 0.6, 0.33)\n+    val expectedNdvs = Array[Long](3, 2, 1, 1, 1)\n+\n+    Seq(0.01, 0.05, 0.1).foreach { relativeSD =>\n+      val (aggFunc, input, buffer) = createEstimator(endpoints, relativeSD)\n+\n+      data.grouped(4).foreach { group =>\n+        val (partialAggFunc, partialInput, partialBuffer) =\n+          createEstimator(endpoints, relativeSD, DoubleType)\n+        group.foreach { x =>\n+          partialInput.setDouble(0, x)\n+          partialAggFunc.update(partialBuffer, partialInput)\n+        }\n+        aggFunc.merge(buffer, partialBuffer)\n+      }\n+      // before eval(), for intervals with the same endpoints, only the first interval counts the\n+      // value\n+      checkNDVs(\n+        ndvs = aggFunc.hllppResults(buffer),\n+        expectedNdvs = Array(3, 2, 0, 0, 1),\n+        rsd = relativeSD)\n+\n+      // A value out of the whole range will not change the buffer\n+      input.setInt(0, 2)"
  }],
  "prId": 15544
}]