[{
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "super Nit: too much empty lines",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-03T20:32:21Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,\n+        ReplaceIntersectWithSemiJoin,\n+        OptimizeLimitZero,\n+        PropagateEmptyRelation) :: Nil\n+  }\n+\n+  val testRelation1 = LocalRelation.fromExternalRows(Seq('a.int), data = Seq(Row(1)))\n+  val testRelation2 = LocalRelation.fromExternalRows(Seq('b.int), data = Seq(Row(1)))\n+\n+"
  }, {
    "author": {
      "login": "aayushmaanjain"
    },
    "body": "fixed",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-04T09:34:23Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,\n+        ReplaceIntersectWithSemiJoin,\n+        OptimizeLimitZero,\n+        PropagateEmptyRelation) :: Nil\n+  }\n+\n+  val testRelation1 = LocalRelation.fromExternalRows(Seq('a.int), data = Seq(Row(1)))\n+  val testRelation2 = LocalRelation.fromExternalRows(Seq('b.int), data = Seq(Row(1)))\n+\n+"
  }],
  "prId": 24271
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "You know you can create easily separated test for each join types, like this:\r\n\r\n```scala\r\nSeq(\r\n    (Inner, LocalRelation('a.int, 'b.int)),\r\n    (LeftOuter, Project(Seq('a, Literal(null).cast(IntegerType).as('b)), testRelation1).analyze),\r\n    (RightOuter, LocalRelation('a.int, 'b.int)),\r\n    (FullOuter, Project(Seq('a, Literal(null).cast(IntegerType).as('b)), testRelation1).analyze)\r\n  ).foreach { case (jt, answer) =>\r\n    test(s\"Limit 0: for join type $jt\") {\r\n        val query = testRelation1\r\n          .join(testRelation2.limit(0), jt, condition = Some('a.attr == 'b.attr))\r\n\r\n        val optimized = Optimize.execute(query.analyze)\r\n        val correctAnswer = answer\r\n\r\n        comparePlans(optimized, correctAnswer)\r\n      }\r\n    }\r\n```\r\n\r\nSo this way the suite execution will contain sth like:\r\n\r\n```\r\n[info] - Limit 0: for join type Inner (30 milliseconds)\r\n[info] - Limit 0: for join type LeftOuter (21 milliseconds)\r\n[info] - Limit 0: for join type RightOuter (13 milliseconds)\r\n[info] - Limit 0: for join type FullOuter (12 milliseconds)\r\n```\r\n\r\nIt has only one little advantage if more than one would fail execution would not stop at the first assert.\r\nBut your current solution is also very fine.\r\n",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-03T20:59:23Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,\n+        ReplaceIntersectWithSemiJoin,\n+        OptimizeLimitZero,\n+        PropagateEmptyRelation) :: Nil\n+  }\n+\n+  val testRelation1 = LocalRelation.fromExternalRows(Seq('a.int), data = Seq(Row(1)))\n+  val testRelation2 = LocalRelation.fromExternalRows(Seq('b.int), data = Seq(Row(1)))\n+\n+\n+  test(\"Limit 0: return empty local relation\") {\n+    val query = testRelation1.limit(0)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual LocalLimit 0 node\") {\n+    val query = LocalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual GlobalLimit 0 node\") {\n+    val query = GlobalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: Joins\") {"
  }, {
    "author": {
      "login": "aayushmaanjain"
    },
    "body": "@attilapiros thanks for the input. I made the required changes. ",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-04T06:43:01Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,\n+        ReplaceIntersectWithSemiJoin,\n+        OptimizeLimitZero,\n+        PropagateEmptyRelation) :: Nil\n+  }\n+\n+  val testRelation1 = LocalRelation.fromExternalRows(Seq('a.int), data = Seq(Row(1)))\n+  val testRelation2 = LocalRelation.fromExternalRows(Seq('b.int), data = Seq(Row(1)))\n+\n+\n+  test(\"Limit 0: return empty local relation\") {\n+    val query = testRelation1.limit(0)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual LocalLimit 0 node\") {\n+    val query = LocalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual GlobalLimit 0 node\") {\n+    val query = GlobalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: Joins\") {"
  }],
  "prId": 24271
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "I think we can have two batches here, just the same as `Optimizer`.",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-04T07:04:13Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,",
    "line": 33
  }, {
    "author": {
      "login": "aayushmaanjain"
    },
    "body": "@gengliangwang Is there a specific reason for having it that way? I modelled the test suite based on `PropagateEmptyRelation`, wherein also they have all rules as part of the same batch..",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-04T09:01:49Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,",
    "line": 33
  }],
  "prId": 24271
}, {
  "comments": [{
    "author": {
      "login": "dilipbiswal"
    },
    "body": "redundant var ?",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-04T08:24:36Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,\n+        ReplaceIntersectWithSemiJoin,\n+        OptimizeLimitZero,\n+        PropagateEmptyRelation) :: Nil\n+  }\n+\n+  val testRelation1 = LocalRelation.fromExternalRows(Seq('a.int), data = Seq(Row(1)))\n+  val testRelation2 = LocalRelation.fromExternalRows(Seq('b.int), data = Seq(Row(1)))\n+\n+  test(\"Limit 0: return empty local relation\") {\n+    val query = testRelation1.limit(0)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual LocalLimit 0 node\") {\n+    val query = LocalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual GlobalLimit 0 node\") {\n+    val query = GlobalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  Seq(\n+    (Inner, LocalRelation('a.int, 'b.int)),\n+    (LeftOuter, Project(Seq('a, Literal(null).cast(IntegerType).as('b)), testRelation1).analyze),\n+    (RightOuter, LocalRelation('a.int, 'b.int)),\n+    (FullOuter, Project(Seq('a, Literal(null).cast(IntegerType).as('b)), testRelation1).analyze)\n+  ).foreach { case (jt, answer) =>\n+      test(s\"Limit 0: for join type $jt\") {\n+        val query = testRelation1\n+          .join(testRelation2.limit(0), joinType = jt, condition = Some('a.attr == 'b.attr))\n+\n+        val optimized = Optimize.execute(query.analyze)\n+        val correctAnswer = answer"
  }, {
    "author": {
      "login": "aayushmaanjain"
    },
    "body": "fixed",
    "commit": "1936ec6cd7a6ec95c29177b5c3f02019b1729d25",
    "createdAt": "2019-04-04T09:28:05Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, GlobalLimit, LocalLimit, LocalRelation, LogicalPlan, Project}\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+import org.apache.spark.sql.types.IntegerType\n+\n+// Test class to verify correct functioning of OptimizeLimitZero rule in various scenarios\n+class OptimizeLimitZeroSuite extends PlanTest {\n+  object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"OptimizeLimitZero\", Once,\n+        ReplaceIntersectWithSemiJoin,\n+        OptimizeLimitZero,\n+        PropagateEmptyRelation) :: Nil\n+  }\n+\n+  val testRelation1 = LocalRelation.fromExternalRows(Seq('a.int), data = Seq(Row(1)))\n+  val testRelation2 = LocalRelation.fromExternalRows(Seq('b.int), data = Seq(Row(1)))\n+\n+  test(\"Limit 0: return empty local relation\") {\n+    val query = testRelation1.limit(0)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual LocalLimit 0 node\") {\n+    val query = LocalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  test(\"Limit 0: individual GlobalLimit 0 node\") {\n+    val query = GlobalLimit(0, testRelation1)\n+\n+    val optimized = Optimize.execute(query.analyze)\n+    val correctAnswer = LocalRelation('a.int)\n+\n+    comparePlans(optimized, correctAnswer)\n+  }\n+\n+  Seq(\n+    (Inner, LocalRelation('a.int, 'b.int)),\n+    (LeftOuter, Project(Seq('a, Literal(null).cast(IntegerType).as('b)), testRelation1).analyze),\n+    (RightOuter, LocalRelation('a.int, 'b.int)),\n+    (FullOuter, Project(Seq('a, Literal(null).cast(IntegerType).as('b)), testRelation1).analyze)\n+  ).foreach { case (jt, answer) =>\n+      test(s\"Limit 0: for join type $jt\") {\n+        val query = testRelation1\n+          .join(testRelation2.limit(0), joinType = jt, condition = Some('a.attr == 'b.attr))\n+\n+        val optimized = Optimize.execute(query.analyze)\n+        val correctAnswer = answer"
  }],
  "prId": 24271
}]