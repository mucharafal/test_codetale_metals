[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "should have a test case that tests a plan that cannot be eliminated?\n",
    "commit": "c34aacfeeb44c372cdc6385277c2511a1cd69270",
    "createdAt": "2016-01-14T08:20:17Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.reflect.runtime.universe.TypeTag\n+\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions.NewInstance\n+import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan, MapPartitions}\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+\n+case class OtherTuple(_1: Int, _2: Int)\n+\n+class EliminateSerializationSuite extends PlanTest {\n+  private object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"Serialization\", FixedPoint(100),\n+        EliminateSerialization) :: Nil\n+  }\n+\n+  implicit private def productEncoder[T <: Product : TypeTag] = ExpressionEncoder[T]()\n+  private val func = identity[Iterator[(Int, Int)]] _\n+  private val func2 = identity[Iterator[OtherTuple]] _\n+\n+  def assertObjectCreations(count: Int, plan: LogicalPlan): Unit = {\n+    val newInstances = plan.flatMap(_.expressions.collect {\n+      case n: NewInstance => n\n+    })\n+\n+    if (newInstances.size != count) {\n+      fail(\n+        s\"\"\"\n+           |Wrong number of object creations in plan: ${newInstances.size} != $count\n+           |$plan\n+         \"\"\".stripMargin)\n+    }\n+  }\n+\n+  test(\"back to back MapPartitions\") {\n+    val input = LocalRelation('_1.int, '_2.int)\n+    val plan =\n+      MapPartitions(func,",
    "line": 60
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Oh yeah, I guess I forgot to push it.\n",
    "commit": "c34aacfeeb44c372cdc6385277c2511a1cd69270",
    "createdAt": "2016-01-14T16:01:53Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.reflect.runtime.universe.TypeTag\n+\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions.NewInstance\n+import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan, MapPartitions}\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+\n+case class OtherTuple(_1: Int, _2: Int)\n+\n+class EliminateSerializationSuite extends PlanTest {\n+  private object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"Serialization\", FixedPoint(100),\n+        EliminateSerialization) :: Nil\n+  }\n+\n+  implicit private def productEncoder[T <: Product : TypeTag] = ExpressionEncoder[T]()\n+  private val func = identity[Iterator[(Int, Int)]] _\n+  private val func2 = identity[Iterator[OtherTuple]] _\n+\n+  def assertObjectCreations(count: Int, plan: LogicalPlan): Unit = {\n+    val newInstances = plan.flatMap(_.expressions.collect {\n+      case n: NewInstance => n\n+    })\n+\n+    if (newInstances.size != count) {\n+      fail(\n+        s\"\"\"\n+           |Wrong number of object creations in plan: ${newInstances.size} != $count\n+           |$plan\n+         \"\"\".stripMargin)\n+    }\n+  }\n+\n+  test(\"back to back MapPartitions\") {\n+    val input = LocalRelation('_1.int, '_2.int)\n+    val plan =\n+      MapPartitions(func,",
    "line": 60
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "There's a test here and and in end-to-end one in DatasetSuite now.\n",
    "commit": "c34aacfeeb44c372cdc6385277c2511a1cd69270",
    "createdAt": "2016-01-14T18:38:39Z",
    "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.reflect.runtime.universe.TypeTag\n+\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions.NewInstance\n+import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan, MapPartitions}\n+import org.apache.spark.sql.catalyst.plans.PlanTest\n+import org.apache.spark.sql.catalyst.rules.RuleExecutor\n+\n+case class OtherTuple(_1: Int, _2: Int)\n+\n+class EliminateSerializationSuite extends PlanTest {\n+  private object Optimize extends RuleExecutor[LogicalPlan] {\n+    val batches =\n+      Batch(\"Serialization\", FixedPoint(100),\n+        EliminateSerialization) :: Nil\n+  }\n+\n+  implicit private def productEncoder[T <: Product : TypeTag] = ExpressionEncoder[T]()\n+  private val func = identity[Iterator[(Int, Int)]] _\n+  private val func2 = identity[Iterator[OtherTuple]] _\n+\n+  def assertObjectCreations(count: Int, plan: LogicalPlan): Unit = {\n+    val newInstances = plan.flatMap(_.expressions.collect {\n+      case n: NewInstance => n\n+    })\n+\n+    if (newInstances.size != count) {\n+      fail(\n+        s\"\"\"\n+           |Wrong number of object creations in plan: ${newInstances.size} != $count\n+           |$plan\n+         \"\"\".stripMargin)\n+    }\n+  }\n+\n+  test(\"back to back MapPartitions\") {\n+    val input = LocalRelation('_1.int, '_2.int)\n+    val plan =\n+      MapPartitions(func,",
    "line": 60
  }],
  "prId": 10747
}]