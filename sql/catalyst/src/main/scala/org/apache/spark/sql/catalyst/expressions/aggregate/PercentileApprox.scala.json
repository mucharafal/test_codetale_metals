[{
  "comments": [{
    "author": {
      "login": "thunterdb"
    },
    "body": "why did you need to move the implementation around?\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-07-21T16:33:55Z",
    "diffHunk": "@@ -0,0 +1,456 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either an single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.\n+  override def supportsPartial: Boolean = false\n+\n+  override def aggBufferSchema: StructType = StructType.fromAttributes(aggBufferAttributes)\n+\n+  override val aggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  override val inputAggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  private var summary: QuantileSummaries = null\n+\n+  override def initialize(buffer: MutableRow): Unit = {\n+    // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+    // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+    // Please refer to SPARK-16283 for details.\n+    val relativeError = PercentileApprox.bToRelativeError(B)\n+    summary = new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, relativeError)\n+  }\n+\n+  override def update(buffer: MutableRow, input: InternalRow): Unit = {\n+    val value = child.eval(input) match {\n+      case o: Byte => o.toDouble\n+      case o: Short => o.toDouble\n+      case o: Int => o.toDouble\n+      case o: Long => o.toDouble\n+      case o: Float => o.toDouble\n+      case o: Decimal => o.toDouble\n+      case o: Double => o\n+    }\n+\n+    summary.insert(value)\n+  }\n+\n+  override def merge(buffer: MutableRow, inputBuffer: InternalRow): Unit = {\n+    sys.error(\"PercentileApprox does not support partial aggregation.\")\n+  }\n+\n+  override def eval(buffer: InternalRow): Any = {\n+    // summary must be compressed before being queried\n+    summary = summary.compress()\n+\n+    if (resultAsArray) {\n+      // return the result as an array of doubles, or return null for empty inputs\n+      if (summary.count > 0) new GenericArrayData(percentiles.map { summary.query(_) }) else null\n+    }\n+    else {\n+      // return the result as a double, or return null for empty inputs\n+      if (summary.count > 0) summary.query(percentiles.head) else null\n+    }\n+  }\n+\n+  private def childrenSQL: String =\n+    if (bExpr.isDefined) {\n+      s\"${child.sql}, ${percentilesExpr.toString}, ${bExpr.get.toString}\"\n+    }\n+    else {\n+      s\"${child.sql}, ${percentilesExpr.toString}\"\n+    }\n+\n+  override def sql: String = s\"$prettyName($childrenSQL)\"\n+\n+  override def sql(isDistinct: Boolean): String = {\n+    val distinct = if (isDistinct) \"DISTINCT \" else \"\"\n+    s\"$prettyName($distinct$childrenSQL)\"\n+  }\n+}\n+\n+object PercentileApprox {\n+\n+  // Using 1000 as a default achieves a relativeError of 0.001\n+  private[sql] val B_DEFAULT = 1000\n+\n+  // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+  // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+  // Please refer to SPARK-16283 for details.\n+  private[sql] def bToRelativeError(B: Int): Double = Math.max(1.0d / B, 0.001)\n+\n+  /**\n+   * Validates the percentile(s) expression and extract the percentile(s).\n+   * Returns the extracted percentile(s) and an indicator of whether it's an array.\n+   */\n+  private def validatePercentilesLiteral(exp: Expression): (Seq[Double], Boolean) = {\n+    def withinRange(v: Double): Boolean = 0.0 <= v && v <= 1.0\n+    exp match {\n+      case Literal(f: Float, FloatType) if withinRange(f) => (Seq(f.toDouble), false)\n+      case Literal(d: Double, DoubleType) if withinRange(d) => (Seq(d), false)\n+      case Literal(dec: Decimal, _) if withinRange(dec.toDouble) => (Seq(dec.toDouble), false)\n+\n+      case CreateArray(children: Seq[Expression]) if (children.length > 0) =>\n+        (children.map(_ match {\n+          case Literal(f: Float, FloatType) if withinRange(f) => f.toDouble\n+          case Literal(d: Double, DoubleType) if withinRange(d) => d\n+          case Literal(dec: Decimal, _) if withinRange(dec.toDouble) => dec.toDouble\n+          case _ =>\n+            throw new AnalysisException(\n+              \"The second argument should be a double literal or an array of doubles, and should \" +\n+                \"be within range [0.0, 1.0]\")\n+        }), true)\n+\n+      case _ =>\n+        throw new AnalysisException(\n+          \"The second argument should be a double literal or an array of doubles, and should \" +\n+            \"be within range [0.0, 1.0]\")\n+    }\n+  }\n+\n+  /** Validates the B expression and extract its value. */\n+  private def validateBLiteral(exp: Expression): Int = exp match {\n+    case Literal(i: Int, IntegerType) if i > 0 => i\n+\n+    case _ =>\n+      throw new AnalysisException(\"The third argument should be a positive integer literal\")\n+  }\n+}\n+\n+/**\n+ * Helper class to compute approximate quantile summary.\n+ * This implementation is based on the algorithm proposed in the paper:",
    "line": 225
  }, {
    "author": {
      "login": "lw-lin"
    },
    "body": "thanks!\n\nmodule `sql/core` depends on module `sql/catalyst`, but module `sql/catalyst` does not depend on module `sql/core`; so in order to enable expressions like `PercentileApprox` (which lives in `sql/catalyst`) to use of the G-K algorithm, the algorithm should probably live in `sql/catalyst`. As a reference, algorithms like HyperLogLog also live in `sql/catalyst`(please refer to [HyperLogLogPlusPlus](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.scala)).\n\n@thunterdb does this make sense?\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-07-22T01:26:54Z",
    "diffHunk": "@@ -0,0 +1,456 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either an single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.\n+  override def supportsPartial: Boolean = false\n+\n+  override def aggBufferSchema: StructType = StructType.fromAttributes(aggBufferAttributes)\n+\n+  override val aggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  override val inputAggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  private var summary: QuantileSummaries = null\n+\n+  override def initialize(buffer: MutableRow): Unit = {\n+    // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+    // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+    // Please refer to SPARK-16283 for details.\n+    val relativeError = PercentileApprox.bToRelativeError(B)\n+    summary = new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, relativeError)\n+  }\n+\n+  override def update(buffer: MutableRow, input: InternalRow): Unit = {\n+    val value = child.eval(input) match {\n+      case o: Byte => o.toDouble\n+      case o: Short => o.toDouble\n+      case o: Int => o.toDouble\n+      case o: Long => o.toDouble\n+      case o: Float => o.toDouble\n+      case o: Decimal => o.toDouble\n+      case o: Double => o\n+    }\n+\n+    summary.insert(value)\n+  }\n+\n+  override def merge(buffer: MutableRow, inputBuffer: InternalRow): Unit = {\n+    sys.error(\"PercentileApprox does not support partial aggregation.\")\n+  }\n+\n+  override def eval(buffer: InternalRow): Any = {\n+    // summary must be compressed before being queried\n+    summary = summary.compress()\n+\n+    if (resultAsArray) {\n+      // return the result as an array of doubles, or return null for empty inputs\n+      if (summary.count > 0) new GenericArrayData(percentiles.map { summary.query(_) }) else null\n+    }\n+    else {\n+      // return the result as a double, or return null for empty inputs\n+      if (summary.count > 0) summary.query(percentiles.head) else null\n+    }\n+  }\n+\n+  private def childrenSQL: String =\n+    if (bExpr.isDefined) {\n+      s\"${child.sql}, ${percentilesExpr.toString}, ${bExpr.get.toString}\"\n+    }\n+    else {\n+      s\"${child.sql}, ${percentilesExpr.toString}\"\n+    }\n+\n+  override def sql: String = s\"$prettyName($childrenSQL)\"\n+\n+  override def sql(isDistinct: Boolean): String = {\n+    val distinct = if (isDistinct) \"DISTINCT \" else \"\"\n+    s\"$prettyName($distinct$childrenSQL)\"\n+  }\n+}\n+\n+object PercentileApprox {\n+\n+  // Using 1000 as a default achieves a relativeError of 0.001\n+  private[sql] val B_DEFAULT = 1000\n+\n+  // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+  // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+  // Please refer to SPARK-16283 for details.\n+  private[sql] def bToRelativeError(B: Int): Double = Math.max(1.0d / B, 0.001)\n+\n+  /**\n+   * Validates the percentile(s) expression and extract the percentile(s).\n+   * Returns the extracted percentile(s) and an indicator of whether it's an array.\n+   */\n+  private def validatePercentilesLiteral(exp: Expression): (Seq[Double], Boolean) = {\n+    def withinRange(v: Double): Boolean = 0.0 <= v && v <= 1.0\n+    exp match {\n+      case Literal(f: Float, FloatType) if withinRange(f) => (Seq(f.toDouble), false)\n+      case Literal(d: Double, DoubleType) if withinRange(d) => (Seq(d), false)\n+      case Literal(dec: Decimal, _) if withinRange(dec.toDouble) => (Seq(dec.toDouble), false)\n+\n+      case CreateArray(children: Seq[Expression]) if (children.length > 0) =>\n+        (children.map(_ match {\n+          case Literal(f: Float, FloatType) if withinRange(f) => f.toDouble\n+          case Literal(d: Double, DoubleType) if withinRange(d) => d\n+          case Literal(dec: Decimal, _) if withinRange(dec.toDouble) => dec.toDouble\n+          case _ =>\n+            throw new AnalysisException(\n+              \"The second argument should be a double literal or an array of doubles, and should \" +\n+                \"be within range [0.0, 1.0]\")\n+        }), true)\n+\n+      case _ =>\n+        throw new AnalysisException(\n+          \"The second argument should be a double literal or an array of doubles, and should \" +\n+            \"be within range [0.0, 1.0]\")\n+    }\n+  }\n+\n+  /** Validates the B expression and extract its value. */\n+  private def validateBLiteral(exp: Expression): Int = exp match {\n+    case Literal(i: Int, IntegerType) if i > 0 => i\n+\n+    case _ =>\n+      throw new AnalysisException(\"The third argument should be a positive integer literal\")\n+  }\n+}\n+\n+/**\n+ * Helper class to compute approximate quantile summary.\n+ * This implementation is based on the algorithm proposed in the paper:",
    "line": 225
  }, {
    "author": {
      "login": "thunterdb"
    },
    "body": "Yes, this makes sense, thanks.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-07-25T20:27:47Z",
    "diffHunk": "@@ -0,0 +1,456 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either an single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.\n+  override def supportsPartial: Boolean = false\n+\n+  override def aggBufferSchema: StructType = StructType.fromAttributes(aggBufferAttributes)\n+\n+  override val aggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  override val inputAggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  private var summary: QuantileSummaries = null\n+\n+  override def initialize(buffer: MutableRow): Unit = {\n+    // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+    // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+    // Please refer to SPARK-16283 for details.\n+    val relativeError = PercentileApprox.bToRelativeError(B)\n+    summary = new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, relativeError)\n+  }\n+\n+  override def update(buffer: MutableRow, input: InternalRow): Unit = {\n+    val value = child.eval(input) match {\n+      case o: Byte => o.toDouble\n+      case o: Short => o.toDouble\n+      case o: Int => o.toDouble\n+      case o: Long => o.toDouble\n+      case o: Float => o.toDouble\n+      case o: Decimal => o.toDouble\n+      case o: Double => o\n+    }\n+\n+    summary.insert(value)\n+  }\n+\n+  override def merge(buffer: MutableRow, inputBuffer: InternalRow): Unit = {\n+    sys.error(\"PercentileApprox does not support partial aggregation.\")\n+  }\n+\n+  override def eval(buffer: InternalRow): Any = {\n+    // summary must be compressed before being queried\n+    summary = summary.compress()\n+\n+    if (resultAsArray) {\n+      // return the result as an array of doubles, or return null for empty inputs\n+      if (summary.count > 0) new GenericArrayData(percentiles.map { summary.query(_) }) else null\n+    }\n+    else {\n+      // return the result as a double, or return null for empty inputs\n+      if (summary.count > 0) summary.query(percentiles.head) else null\n+    }\n+  }\n+\n+  private def childrenSQL: String =\n+    if (bExpr.isDefined) {\n+      s\"${child.sql}, ${percentilesExpr.toString}, ${bExpr.get.toString}\"\n+    }\n+    else {\n+      s\"${child.sql}, ${percentilesExpr.toString}\"\n+    }\n+\n+  override def sql: String = s\"$prettyName($childrenSQL)\"\n+\n+  override def sql(isDistinct: Boolean): String = {\n+    val distinct = if (isDistinct) \"DISTINCT \" else \"\"\n+    s\"$prettyName($distinct$childrenSQL)\"\n+  }\n+}\n+\n+object PercentileApprox {\n+\n+  // Using 1000 as a default achieves a relativeError of 0.001\n+  private[sql] val B_DEFAULT = 1000\n+\n+  // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+  // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+  // Please refer to SPARK-16283 for details.\n+  private[sql] def bToRelativeError(B: Int): Double = Math.max(1.0d / B, 0.001)\n+\n+  /**\n+   * Validates the percentile(s) expression and extract the percentile(s).\n+   * Returns the extracted percentile(s) and an indicator of whether it's an array.\n+   */\n+  private def validatePercentilesLiteral(exp: Expression): (Seq[Double], Boolean) = {\n+    def withinRange(v: Double): Boolean = 0.0 <= v && v <= 1.0\n+    exp match {\n+      case Literal(f: Float, FloatType) if withinRange(f) => (Seq(f.toDouble), false)\n+      case Literal(d: Double, DoubleType) if withinRange(d) => (Seq(d), false)\n+      case Literal(dec: Decimal, _) if withinRange(dec.toDouble) => (Seq(dec.toDouble), false)\n+\n+      case CreateArray(children: Seq[Expression]) if (children.length > 0) =>\n+        (children.map(_ match {\n+          case Literal(f: Float, FloatType) if withinRange(f) => f.toDouble\n+          case Literal(d: Double, DoubleType) if withinRange(d) => d\n+          case Literal(dec: Decimal, _) if withinRange(dec.toDouble) => dec.toDouble\n+          case _ =>\n+            throw new AnalysisException(\n+              \"The second argument should be a double literal or an array of doubles, and should \" +\n+                \"be within range [0.0, 1.0]\")\n+        }), true)\n+\n+      case _ =>\n+        throw new AnalysisException(\n+          \"The second argument should be a double literal or an array of doubles, and should \" +\n+            \"be within range [0.0, 1.0]\")\n+    }\n+  }\n+\n+  /** Validates the B expression and extract its value. */\n+  private def validateBLiteral(exp: Expression): Int = exp match {\n+    case Literal(i: Int, IntegerType) if i > 0 => i\n+\n+    case _ =>\n+      throw new AnalysisException(\"The third argument should be a positive integer literal\")\n+  }\n+}\n+\n+/**\n+ * Helper class to compute approximate quantile summary.\n+ * This implementation is based on the algorithm proposed in the paper:",
    "line": 225
  }],
  "prId": 14298
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "You could also pattern match on the result of the `exp.eval()`. That would be way easier.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-07-26T10:55:10Z",
    "diffHunk": "@@ -0,0 +1,456 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either an single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.\n+  override def supportsPartial: Boolean = false\n+\n+  override def aggBufferSchema: StructType = StructType.fromAttributes(aggBufferAttributes)\n+\n+  override val aggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  override val inputAggBufferAttributes: Seq[AttributeReference] = Seq()\n+\n+  private var summary: QuantileSummaries = null\n+\n+  override def initialize(buffer: MutableRow): Unit = {\n+    // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+    // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+    // Please refer to SPARK-16283 for details.\n+    val relativeError = PercentileApprox.bToRelativeError(B)\n+    summary = new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, relativeError)\n+  }\n+\n+  override def update(buffer: MutableRow, input: InternalRow): Unit = {\n+    val value = child.eval(input) match {\n+      case o: Byte => o.toDouble\n+      case o: Short => o.toDouble\n+      case o: Int => o.toDouble\n+      case o: Long => o.toDouble\n+      case o: Float => o.toDouble\n+      case o: Decimal => o.toDouble\n+      case o: Double => o\n+    }\n+\n+    summary.insert(value)\n+  }\n+\n+  override def merge(buffer: MutableRow, inputBuffer: InternalRow): Unit = {\n+    sys.error(\"PercentileApprox does not support partial aggregation.\")\n+  }\n+\n+  override def eval(buffer: InternalRow): Any = {\n+    // summary must be compressed before being queried\n+    summary = summary.compress()\n+\n+    if (resultAsArray) {\n+      // return the result as an array of doubles, or return null for empty inputs\n+      if (summary.count > 0) new GenericArrayData(percentiles.map { summary.query(_) }) else null\n+    }\n+    else {\n+      // return the result as a double, or return null for empty inputs\n+      if (summary.count > 0) summary.query(percentiles.head) else null\n+    }\n+  }\n+\n+  private def childrenSQL: String =\n+    if (bExpr.isDefined) {\n+      s\"${child.sql}, ${percentilesExpr.toString}, ${bExpr.get.toString}\"\n+    }\n+    else {\n+      s\"${child.sql}, ${percentilesExpr.toString}\"\n+    }\n+\n+  override def sql: String = s\"$prettyName($childrenSQL)\"\n+\n+  override def sql(isDistinct: Boolean): String = {\n+    val distinct = if (isDistinct) \"DISTINCT \" else \"\"\n+    s\"$prettyName($distinct$childrenSQL)\"\n+  }\n+}\n+\n+object PercentileApprox {\n+\n+  // Using 1000 as a default achieves a relativeError of 0.001\n+  private[sql] val B_DEFAULT = 1000\n+\n+  // Our `PercentileApprox` function takes a `B` parameter, but the underlying GK algorithm takes\n+  // a `relativeError` parameter, so we need to convert `B` to `relativeError`.\n+  // Please refer to SPARK-16283 for details.\n+  private[sql] def bToRelativeError(B: Int): Double = Math.max(1.0d / B, 0.001)\n+\n+  /**\n+   * Validates the percentile(s) expression and extract the percentile(s).\n+   * Returns the extracted percentile(s) and an indicator of whether it's an array.\n+   */\n+  private def validatePercentilesLiteral(exp: Expression): (Seq[Double], Boolean) = {\n+    def withinRange(v: Double): Boolean = 0.0 <= v && v <= 1.0\n+    exp match {"
  }],
  "prId": 14298
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "minor: `an single` -> `a single`\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-07-26T17:14:45Z",
    "diffHunk": "@@ -0,0 +1,456 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either an single double value or an"
  }],
  "prId": 14298
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @lw-lin .\nI know the reason why you define this as a capatal 'B', but I'm just wondering it's consistent with Spark naming rule.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-07-26T17:17:12Z",
    "diffHunk": "@@ -0,0 +1,456 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either an single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B",
    "line": 50
  }, {
    "author": {
      "login": "lw-lin"
    },
    "body": "thanks. I don't have strong preference here -- let's see what reviewers say.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-02T06:44:14Z",
    "diffHunk": "@@ -0,0 +1,456 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either an single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B",
    "line": 50
  }],
  "prId": 14298
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can you explain a bit more about this? AFAIK, hive supports partial aggregate for `percentile_approx`, and it looks to me that your implementation keeps the buffer data(`QuantileSummaries`) in this aggregate function object, instead of letting aggregate operator manage it, that's the main reason why we can't support partial aggregate for `percentile_approx` I think.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-08T07:29:32Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }, {
    "author": {
      "login": "lw-lin"
    },
    "body": "> hive supports partial aggregate for percentile_approx\n\nHive's implementation computes approximate percentile values from a histogram, thus Hive supports partial aggregation (but makes no approximation guarantees).\n\n> ... QuantileSummaries in this aggregate function object, instead of letting aggregate operator manage it, that's the main reason why we can't support partial aggregate\n\nYes that's quite right. `QuantileSummaries` has been implemented and well tested prior to this patch, so it'd be great if we can reuse that and put a `QuantileSummaries` instance directly into the aggregation buffer (in order to support partial aggregation). @cloud-fan any pointer on how to do that please?\n\nThanks!\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-09T02:48:52Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "to be clear, are you saying that our current algorithm to compute `percentile_approx` can't support partial aggregation fundamentally?\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-09T02:59:34Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }, {
    "author": {
      "login": "lw-lin"
    },
    "body": "Fundamentally the `QuantileSummaries` implementation does support RDD-style partial aggregation -- `QuantileSummaries` itself is the agg buffer of a partition's data at mappers, and multiple `QuantileSummaries`s will be merged at reducers (please refer to [StatFunctions.multipleApproxQuantiles](https://github.com/apache/spark/blob/5a3533e779d8e43ce0980203dfd3cbe343cc7d0a/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala#L93)).\n\nFundamentally it should also support the SparkSQL-style partial aggregation. I'm trying to reuse `QuantileSummaries` here; if there's no easy way to reuse `QuantileSummaries`, I'm afraid we'll have to re-write this GK algorithm totally to support our SparkSQL-style partial aggregation.\n\nSo any way we can just directly put a `QuantileSummaries` instance into SparkSQL's agg buffer? Or do we have to break a `QuantileSummaries` instance up, say into `an int + a double + a double[] + an int[] + an int[]` which SparkSQL's agg buffer can manage?\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-09T03:26:51Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think putting buffer object in agg buffer row is better, but that need to be well designed.  Can you hold this PR for a while? We are discussing about it internally.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-09T03:40:00Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }, {
    "author": {
      "login": "lw-lin"
    },
    "body": "Sure, this can wait. Thanks for the information!\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-09T04:05:31Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }, {
    "author": {
      "login": "clockfly"
    },
    "body": "Some updates, https://github.com/apache/spark/pull/14753\nis created to support putting generic object in aggregation buffer.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-22T16:22:54Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }, {
    "author": {
      "login": "lw-lin"
    },
    "body": "@clockfly  cool!\nnow that https://github.com/apache/spark/pull/14754 has been merged, looks like I should update this patch once https://github.com/apache/spark/pull/14753 gets merged.\n",
    "commit": "c0acf1697ad369302068aeaecad59e812038d14a",
    "createdAt": "2016-08-23T09:26:20Z",
    "diffHunk": "@@ -0,0 +1,462 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate.QuantileSummaries.Stats\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Computes an approximate percentile (quantile) using the G-K algorithm (see below), for very\n+ * large numbers of rows where the regular percentile() UDAF might run out of memory.\n+ *\n+ * The input is a single double value or an array of double values representing the percentiles\n+ * requested. The output, corresponding to the input, is either a single double value or an\n+ * array of doubles that are the percentile values.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"_FUNC_(col, p [, B]) - Returns an approximate pth percentile of a numeric column in the\n+     group. The B parameter, which defaults to 1000, controls approximation accuracy at the cost of\n+     memory; higher values yield better approximations.\n+    _FUNC_(col, array(p1 [, p2]...) [, B]) - Same as above, but accepts and returns an array of\n+     percentile values instead of a single one.\n+    \"\"\")\n+case class PercentileApprox(\n+    child: Expression,\n+    percentilesExpr: Expression,\n+    bExpr: Option[Expression],\n+    percentiles: Seq[Double],  // the extracted percentiles\n+    B: Int,                    // the extracted B\n+    resultAsArray: Boolean,    // whether to return the result as an array\n+    mutableAggBufferOffset: Int = 0,\n+    inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  private def this(child: Expression, percentilesExpr: Expression, bExpr: Option[Expression]) = {\n+    this(\n+      child = child,\n+      percentilesExpr = percentilesExpr,\n+      bExpr = bExpr,\n+      // validate and extract percentiles\n+      percentiles = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._1,\n+      // validate and extract B\n+      B = bExpr.map(PercentileApprox.validateBLiteral(_)).getOrElse(PercentileApprox.B_DEFAULT),\n+      // validate and mark whether we should return results as array of double or not\n+      resultAsArray = PercentileApprox.validatePercentilesLiteral(percentilesExpr)._2)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p) / _FUNC_(col, array(p1, ...))\" form\n+  def this(child: Expression, percentilesExpr: Expression) = {\n+    this(child, percentilesExpr, None)\n+  }\n+\n+  // Constructor for the \"_FUNC_(col, p, B) / _FUNC_(col, array(p1, ...), B)\" form\n+  def this(child: Expression, percentilesExpr: Expression, bExpr: Expression) = {\n+    this(child, percentilesExpr, Some(bExpr))\n+  }\n+\n+  override def prettyName: String = \"percentile_approx\"\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def children: Seq[Expression] =\n+    bExpr.map(child :: percentilesExpr :: _ :: Nil).getOrElse(child :: percentilesExpr :: Nil)\n+\n+  // we would return null for empty inputs\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = if (resultAsArray) ArrayType(DoubleType) else DoubleType\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(NumericType, AnyDataType, IntegralType)\n+\n+  override def checkInputDataTypes(): TypeCheckResult =\n+    TypeUtils.checkForNumericExpr(child.dataType, \"function percentile_approx\")\n+\n+  // The number of intermediate outputs is highly relative to the actual data-set (an upper bound is\n+  // (11/2e)log(2en), where e is the relativeError parameter, n is the number of items in the\n+  // dataset) -- thus it's hard to allocate agg buffer in advance without knowing the size of\n+  // inputs. Due to this reason, currently we don't support partial mode.",
    "line": 102
  }],
  "prId": 14298
}]