[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think a better way is to change this `if` to `if seq.nonEmpty && seq.toSet.subsetOf(children.toSet)`\n",
    "commit": "18091f732b2ecb72e4037baf0cf114ecbad70763",
    "createdAt": "2016-01-11T04:52:57Z",
    "diffHunk": "@@ -398,6 +395,7 @@ abstract class TreeNode[BaseType <: TreeNode[BaseType]] extends Product {\n   def argString: String = productIterator.flatMap {\n     case tn: TreeNode[_] if containsChild(tn) => Nil\n     case tn: TreeNode[_] => s\"${tn.simpleString}\" :: Nil\n+    case seq: Seq[BaseType] if seq.isEmpty => \"[]\" :: Nil\n     case seq: Seq[BaseType] if seq.toSet.subsetOf(children.toSet) => Nil"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Thank you! Will do! \n",
    "commit": "18091f732b2ecb72e4037baf0cf114ecbad70763",
    "createdAt": "2016-01-11T04:55:02Z",
    "diffHunk": "@@ -398,6 +395,7 @@ abstract class TreeNode[BaseType <: TreeNode[BaseType]] extends Product {\n   def argString: String = productIterator.flatMap {\n     case tn: TreeNode[_] if containsChild(tn) => Nil\n     case tn: TreeNode[_] => s\"${tn.simpleString}\" :: Nil\n+    case seq: Seq[BaseType] if seq.isEmpty => \"[]\" :: Nil\n     case seq: Seq[BaseType] if seq.toSet.subsetOf(children.toSet) => Nil"
  }],
  "prId": 10646
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "these import changes are irrelevantï¼Ÿ\n",
    "commit": "18091f732b2ecb72e4037baf0cf114ecbad70763",
    "createdAt": "2016-01-11T17:47:05Z",
    "diffHunk": "@@ -26,17 +26,14 @@ import org.json4s.JsonAST._\n import org.json4s.JsonDSL._\n import org.json4s.jackson.JsonMethods._\n \n-import org.apache.spark.SparkContext\n import org.apache.spark.rdd.{EmptyRDD, RDD}\n-import org.apache.spark.sql.catalyst.{ScalaReflectionLock, TableIdentifier}\n+import org.apache.spark.SparkContext\n import org.apache.spark.sql.catalyst.ScalaReflection._\n+import org.apache.spark.sql.catalyst.ScalaReflectionLock\n import org.apache.spark.sql.catalyst.errors._\n import org.apache.spark.sql.catalyst.expressions._\n-import org.apache.spark.sql.catalyst.plans.logical.Statistics\n-import org.apache.spark.sql.catalyst.util.DateTimeUtils\n import org.apache.spark.sql.types._\n import org.apache.spark.storage.StorageLevel\n-import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}",
    "line": 16
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Yeah, I just saw these useless imports in this file. Do you want me to keep them unchanged?\n",
    "commit": "18091f732b2ecb72e4037baf0cf114ecbad70763",
    "createdAt": "2016-01-11T17:53:39Z",
    "diffHunk": "@@ -26,17 +26,14 @@ import org.json4s.JsonAST._\n import org.json4s.JsonDSL._\n import org.json4s.jackson.JsonMethods._\n \n-import org.apache.spark.SparkContext\n import org.apache.spark.rdd.{EmptyRDD, RDD}\n-import org.apache.spark.sql.catalyst.{ScalaReflectionLock, TableIdentifier}\n+import org.apache.spark.SparkContext\n import org.apache.spark.sql.catalyst.ScalaReflection._\n+import org.apache.spark.sql.catalyst.ScalaReflectionLock\n import org.apache.spark.sql.catalyst.errors._\n import org.apache.spark.sql.catalyst.expressions._\n-import org.apache.spark.sql.catalyst.plans.logical.Statistics\n-import org.apache.spark.sql.catalyst.util.DateTimeUtils\n import org.apache.spark.sql.types._\n import org.apache.spark.storage.StorageLevel\n-import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}",
    "line": 16
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "oh, it's fine to remove useless imports.\n",
    "commit": "18091f732b2ecb72e4037baf0cf114ecbad70763",
    "createdAt": "2016-01-11T18:37:26Z",
    "diffHunk": "@@ -26,17 +26,14 @@ import org.json4s.JsonAST._\n import org.json4s.JsonDSL._\n import org.json4s.jackson.JsonMethods._\n \n-import org.apache.spark.SparkContext\n import org.apache.spark.rdd.{EmptyRDD, RDD}\n-import org.apache.spark.sql.catalyst.{ScalaReflectionLock, TableIdentifier}\n+import org.apache.spark.SparkContext\n import org.apache.spark.sql.catalyst.ScalaReflection._\n+import org.apache.spark.sql.catalyst.ScalaReflectionLock\n import org.apache.spark.sql.catalyst.errors._\n import org.apache.spark.sql.catalyst.expressions._\n-import org.apache.spark.sql.catalyst.plans.logical.Statistics\n-import org.apache.spark.sql.catalyst.util.DateTimeUtils\n import org.apache.spark.sql.types._\n import org.apache.spark.storage.StorageLevel\n-import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}",
    "line": 16
  }],
  "prId": 10646
}]