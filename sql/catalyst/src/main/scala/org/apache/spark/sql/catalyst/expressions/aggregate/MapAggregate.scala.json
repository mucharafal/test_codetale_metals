[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Here, we need to document `distinct non-null values`. This function excludes NULL.\n\nHowever, I am not sure whether this is expected. NULL is also a value, right?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-29T18:15:41Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "We only keep non-null values in histogram, number of nulls is already stored in ColumnStat.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-30T02:37:01Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "I see. Thanks!\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-30T02:52:52Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified"
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`invalid` is confusing. How about `isInvalid`?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-29T18:45:57Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified\n+      maximum number of bins. Otherwise, it returns an empty map.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $numBins)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (buffer.invalid) {\n+      return\n+    }\n+    val evaluated = child.eval(input)\n+    if (evaluated != null) {\n+      buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (buffer.invalid) return\n+    if (other.invalid) {\n+      buffer.invalid = true\n+      buffer.clear()\n+      return\n+    }\n+    buffer.merge(other, numBins)\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.invalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer match {\n+      case stringDigest: StringMapDigest => StringMapDigest.serialize(stringDigest)\n+      case numericDigest: NumericMapDigest => NumericMapDigest.serialize(numericDigest)\n+    }\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest.deserialize(bytes)\n+      case _ => NumericMapDigest.deserialize(bytes)\n+    }\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = false\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var invalid: Boolean = false"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "yeah, that's better.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-30T11:09:17Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified\n+      maximum number of bins. Otherwise, it returns an empty map.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $numBins)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (buffer.invalid) {\n+      return\n+    }\n+    val evaluated = child.eval(input)\n+    if (evaluated != null) {\n+      buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (buffer.invalid) return\n+    if (other.invalid) {\n+      buffer.invalid = true\n+      buffer.clear()\n+      return\n+    }\n+    buffer.merge(other, numBins)\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.invalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer match {\n+      case stringDigest: StringMapDigest => StringMapDigest.serialize(stringDigest)\n+      case numericDigest: NumericMapDigest => NumericMapDigest.serialize(numericDigest)\n+    }\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest.deserialize(bytes)\n+      case _ => NumericMapDigest.deserialize(bytes)\n+    }\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = false\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var invalid: Boolean = false"
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`asInstanceOf[Int]` is risky. When `eval()` returns null, it will be casted to 0.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-29T18:58:33Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified\n+      maximum number of bins. Otherwise, it returns an empty map.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]",
    "line": 71
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "How about this change?\n\n``` Scala\n  private lazy val numBins: Int = numBinsExpression.eval() match {\n    case o: Int => o\n    case x => throw new AnalysisException(\n      s\"The maximum number of bins provided must be a foldable integer expression: $x\")\n  }\n```\n\nUpdated: let us move it into `checkInputDataTypes`\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-29T19:21:10Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified\n+      maximum number of bins. Otherwise, it returns an empty map.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]",
    "line": 71
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "null is not int type, it will not pass type check in `checkInputDataTypes()`, so it is safe here.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-30T11:07:29Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified\n+      maximum number of bins. Otherwise, it returns an empty map.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]",
    "line": 71
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Try to use `null` as the value of numBins. You will understand what I mean. \n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-31T19:20:45Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified\n+      maximum number of bins. Otherwise, it returns an empty map.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]",
    "line": 71
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Oh, I see, thanks!\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T14:01:26Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns bins - (distinct value, frequency) pairs\n+ * of equi-width histogram when the number of distinct values is less than or equal to the\n+ * specified maximum number of bins. Otherwise, it returns an empty map.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns bins - (distinct value, frequency) pairs of equi-width\n+      histogram when the number of distinct values is less than or equal to the specified\n+      maximum number of bins. Otherwise, it returns an empty map.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]",
    "line": 71
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "How about `null if all values are null`? \n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-31T19:16:35Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists."
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Now if the input is empty, it also returns null, so that empty result is returned only when ndv exceeds numBins\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T13:44:48Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists."
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "I see. Then, we would better to explicitly explain it. `Returns null if the result set is empty or all values are null`. \n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T21:40:51Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists."
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Describe the general description of this function at first, and then explains the return values?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-31T19:17:34Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise."
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "ok\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T13:49:40Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise."
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Any reason why `Boolean` and `Binary` types are not supported? \n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-31T19:19:19Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)",
    "line": 74
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Boolean data only has two values, and they are already in `numTrues` and `numFalses` in `ColumnStat`.\nBinary data usually represents big objects, and usually it won't be used in filter conditions or join conditions, so we don't compute its ndv and histogram.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T13:49:37Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)",
    "line": 74
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "In the use cases of CBO, YES. However, this function becomes a general one. It could be also used/called by external users. Then, it might not make sense for this limit. \n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T21:33:53Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)",
    "line": 74
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "In today's software development environment, speed is an important factor.  One of the agile development methodology principles is to avoid over design.  The over design can slow down the development and divert an engineer's attention on the unimportant cases.  Since we develop MapAggregate.scala for CBO, let's focus on CBO's need.  There is no need to develop NDV and histogram for binary data type.  For Boolean data type, we already have number of trues and number of falses defined in ColumnStat.  I think wxhfy's code is doing fine here.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-03T18:35:05Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)",
    "line": 74
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Yea I think it's fine for now to not support those for now.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:52:45Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)",
    "line": 74
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "A general comment about the impl. You can update the codes in the other functions.\n\nHere, I think we should avoid `return` if possible. For example, we can re-write it like\n\n``` Scala\n    if (!buffer.isInvalid) {\n      val evaluated = child.eval(input)\n      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n    }\n```\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-31T19:28:54Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $numBins)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (buffer.isInvalid) {\n+      return\n+    }\n+    val evaluated = child.eval(input)\n+    if (evaluated != null) {\n+      buffer.update(child.dataType, evaluated, numBins)\n+    }"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Yeah, that's better, thanks!\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T14:09:39Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $numBins)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (buffer.isInvalid) {\n+      return\n+    }\n+    val evaluated = child.eval(input)\n+    if (evaluated != null) {\n+      buffer.update(child.dataType, evaluated, numBins)\n+    }"
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "srinathshankar"
    },
    "body": "Couldn't you leave this type promotion to the engine ?\nThat is, for numeric types instead of calling\n`mapaggregate(numericcol, bins)`\ncall\n`mapaggregate(cast(numericcol as double), bins)`\n\nThat way you only have to deal with doubles in your aggregate.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-01T18:27:41Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $numBins)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (buffer.isInvalid) {\n+      return\n+    }\n+    val evaluated = child.eval(input)\n+    if (evaluated != null) {\n+      buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (buffer.isInvalid) return\n+    if (other.isInvalid) {\n+      buffer.isInvalid = true\n+      buffer.clear()\n+      return\n+    }\n+    buffer.merge(other, numBins)\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer match {\n+      case stringDigest: StringMapDigest => StringMapDigest.serialize(stringDigest)\n+      case numericDigest: NumericMapDigest => NumericMapDigest.serialize(numericDigest)\n+    }\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest.deserialize(bytes)\n+      case _ => NumericMapDigest.deserialize(bytes)\n+    }\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean = false\n+\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+\n+  // Update baseMap and clear it when its size exceeds numBins.\n+  def updateMap[T](baseMap: mutable.HashMap[T, Long], value: T, numBins: Int): Unit = {\n+    mergeMap(baseMap, mutable.HashMap(value -> 1L), numBins)\n+  }\n+\n+  // Merge two maps and clear baseMap when its size exceeds numBins.\n+  def mergeMap[T](\n+      baseMap: mutable.HashMap[T, Long],\n+      otherMap: mutable.HashMap[T, Long],\n+      numBins: Int): Unit = {\n+    otherMap.foreach { case (key, value) =>\n+      if (baseMap.contains(key)) {\n+        baseMap.update(key, baseMap(key) + value)\n+      } else {\n+        if (baseMap.size >= numBins) {\n+          isInvalid = true\n+          baseMap.clear()\n+          return\n+        } else {\n+          baseMap.put(key, value)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+// Digest class for column of string type.\n+case class StringMapDigest(\n+    bins: mutable.HashMap[UTF8String, Long] = mutable.HashMap.empty) extends MapDigest {\n+  def this(bins: mutable.HashMap[UTF8String, Long], isInvalid: Boolean) = {\n+    this(bins)\n+    this.isInvalid = isInvalid\n+  }\n+\n+  override def update(dataType: DataType, value: Any, numBins: Int): Unit = {\n+    updateMap(bins, value.asInstanceOf[UTF8String], numBins)\n+  }\n+\n+  override def merge(otherDigest: MapDigest, numBins: Int): Unit = {\n+    mergeMap(baseMap = bins, otherMap = otherDigest.asInstanceOf[StringMapDigest].bins, numBins)\n+  }\n+\n+  override def clear(): Unit = bins.clear()\n+}\n+\n+object StringMapDigest {\n+\n+  private final def length(obj: StringMapDigest): Int = {\n+    // isInvalid, size of bins\n+    var len: Int = Ints.BYTES + Ints.BYTES\n+    obj.bins.foreach { case (key, value) =>\n+      // length of key, key, value\n+      len += Ints.BYTES + key.getBytes.length + Longs.BYTES\n+    }\n+    len\n+  }\n+\n+  final def serialize(obj: StringMapDigest): Array[Byte] = {\n+    val buffer = ByteBuffer.wrap(new Array(length(obj)))\n+    buffer.putInt(if (obj.isInvalid) 0 else 1)\n+    buffer.putInt(obj.bins.size)\n+    obj.bins.foreach { case (key, value) =>\n+      val bytes = key.getBytes\n+      buffer.putInt(bytes.length)\n+      buffer.put(bytes)\n+      buffer.putLong(value)\n+    }\n+    buffer.array()\n+  }\n+\n+  final def deserialize(bytes: Array[Byte]): StringMapDigest = {\n+    val buffer = ByteBuffer.wrap(bytes)\n+    val isInvalid = if (buffer.getInt == 0) true else false\n+    val size = buffer.getInt\n+    val bins = new mutable.HashMap[UTF8String, Long]\n+    var i = 0\n+    while (i < size) {\n+      val keyLength = buffer.getInt\n+      var j = 0\n+      val keyBytes = new Array[Byte](keyLength)\n+      while (j < keyLength) {\n+        keyBytes(j) = buffer.get()\n+        j += 1\n+      }\n+      val value = buffer.getLong\n+      bins.put(UTF8String.fromBytes(keyBytes), value)\n+      i += 1\n+    }\n+    new StringMapDigest(bins, isInvalid)\n+  }\n+}\n+\n+// Digest class for column of numeric type.\n+case class NumericMapDigest(\n+    bins: mutable.HashMap[Double, Long] = mutable.HashMap.empty) extends MapDigest {\n+  def this(bins: mutable.HashMap[Double, Long], isInvalid: Boolean) = {\n+    this(bins)\n+    this.isInvalid = isInvalid\n+  }\n+\n+  override def update(dataType: DataType, value: Any, numBins: Int): Unit = {\n+    // Use Double to represent endpoints (in histograms) for simplicity\n+    val doubleValue = dataType match {"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "In sql, Date and Timestamp will be casted to UTF8String instead of Double, right?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T07:11:46Z",
    "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function for a column returns:\n+ * 1. null if no non-null value exists.\n+ * 2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+ * distinct non-null values is less than or equal to the specified maximum number of bins.\n+ * 3. an empty map otherwise.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of bins.\n+ */\n+@ExpressionDescription(\n+  usage =\n+    \"\"\"\n+      _FUNC_(col, numBins) - Returns 1. null if no non-null value exists.\n+      2. (distinct non-null value, frequency) pairs of equi-width histogram when the number of\n+      distinct non-null values is less than or equal to the specified maximum number of bins.\n+      3. an empty map otherwise.\n+    \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $numBins)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (buffer.isInvalid) {\n+      return\n+    }\n+    val evaluated = child.eval(input)\n+    if (evaluated != null) {\n+      buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (buffer.isInvalid) return\n+    if (other.isInvalid) {\n+      buffer.isInvalid = true\n+      buffer.clear()\n+      return\n+    }\n+    buffer.merge(other, numBins)\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer match {\n+      case stringDigest: StringMapDigest => StringMapDigest.serialize(stringDigest)\n+      case numericDigest: NumericMapDigest => NumericMapDigest.serialize(numericDigest)\n+    }\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest.deserialize(bytes)\n+      case _ => NumericMapDigest.deserialize(bytes)\n+    }\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean = false\n+\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+\n+  // Update baseMap and clear it when its size exceeds numBins.\n+  def updateMap[T](baseMap: mutable.HashMap[T, Long], value: T, numBins: Int): Unit = {\n+    mergeMap(baseMap, mutable.HashMap(value -> 1L), numBins)\n+  }\n+\n+  // Merge two maps and clear baseMap when its size exceeds numBins.\n+  def mergeMap[T](\n+      baseMap: mutable.HashMap[T, Long],\n+      otherMap: mutable.HashMap[T, Long],\n+      numBins: Int): Unit = {\n+    otherMap.foreach { case (key, value) =>\n+      if (baseMap.contains(key)) {\n+        baseMap.update(key, baseMap(key) + value)\n+      } else {\n+        if (baseMap.size >= numBins) {\n+          isInvalid = true\n+          baseMap.clear()\n+          return\n+        } else {\n+          baseMap.put(key, value)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+// Digest class for column of string type.\n+case class StringMapDigest(\n+    bins: mutable.HashMap[UTF8String, Long] = mutable.HashMap.empty) extends MapDigest {\n+  def this(bins: mutable.HashMap[UTF8String, Long], isInvalid: Boolean) = {\n+    this(bins)\n+    this.isInvalid = isInvalid\n+  }\n+\n+  override def update(dataType: DataType, value: Any, numBins: Int): Unit = {\n+    updateMap(bins, value.asInstanceOf[UTF8String], numBins)\n+  }\n+\n+  override def merge(otherDigest: MapDigest, numBins: Int): Unit = {\n+    mergeMap(baseMap = bins, otherMap = otherDigest.asInstanceOf[StringMapDigest].bins, numBins)\n+  }\n+\n+  override def clear(): Unit = bins.clear()\n+}\n+\n+object StringMapDigest {\n+\n+  private final def length(obj: StringMapDigest): Int = {\n+    // isInvalid, size of bins\n+    var len: Int = Ints.BYTES + Ints.BYTES\n+    obj.bins.foreach { case (key, value) =>\n+      // length of key, key, value\n+      len += Ints.BYTES + key.getBytes.length + Longs.BYTES\n+    }\n+    len\n+  }\n+\n+  final def serialize(obj: StringMapDigest): Array[Byte] = {\n+    val buffer = ByteBuffer.wrap(new Array(length(obj)))\n+    buffer.putInt(if (obj.isInvalid) 0 else 1)\n+    buffer.putInt(obj.bins.size)\n+    obj.bins.foreach { case (key, value) =>\n+      val bytes = key.getBytes\n+      buffer.putInt(bytes.length)\n+      buffer.put(bytes)\n+      buffer.putLong(value)\n+    }\n+    buffer.array()\n+  }\n+\n+  final def deserialize(bytes: Array[Byte]): StringMapDigest = {\n+    val buffer = ByteBuffer.wrap(bytes)\n+    val isInvalid = if (buffer.getInt == 0) true else false\n+    val size = buffer.getInt\n+    val bins = new mutable.HashMap[UTF8String, Long]\n+    var i = 0\n+    while (i < size) {\n+      val keyLength = buffer.getInt\n+      var j = 0\n+      val keyBytes = new Array[Byte](keyLength)\n+      while (j < keyLength) {\n+        keyBytes(j) = buffer.get()\n+        j += 1\n+      }\n+      val value = buffer.getLong\n+      bins.put(UTF8String.fromBytes(keyBytes), value)\n+      i += 1\n+    }\n+    new StringMapDigest(bins, isInvalid)\n+  }\n+}\n+\n+// Digest class for column of numeric type.\n+case class NumericMapDigest(\n+    bins: mutable.HashMap[Double, Long] = mutable.HashMap.empty) extends MapDigest {\n+  def this(bins: mutable.HashMap[Double, Long], isInvalid: Boolean) = {\n+    this(bins)\n+    this.isInvalid = isInvalid\n+  }\n+\n+  override def update(dataType: DataType, value: Any, numBins: Int): Unit = {\n+    // Use Double to represent endpoints (in histograms) for simplicity\n+    val doubleValue = dataType match {"
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "\"must be a literal or constant foldable\"\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:36:18Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")",
    "line": 82
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can you improve the documentation for numBins? I don't really understand without reading the code or design doc what you mean by \"max number of pairs\"\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:37:53Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.",
    "line": 43
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "note - a basic thing: please use `/**  ...  */` for commenting member variables or methods, rather than `//`\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:40:07Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins",
    "line": 168
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "add some classdoc explaining what this is. \n\nalso why is this called \"Map\"?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:40:26Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {",
    "line": 167
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "don't use return within a foreach https://github.com/databricks/scala-style-guide#return\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:41:24Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]\n+\n+  // Update bins and clear it when its size exceeds numBins.\n+  def updateMap(value: T, numBins: Int): Unit = {\n+    mergeMap(mutable.HashMap(value -> 1L), numBins)\n+  }\n+\n+  // Merge two maps and clear bins when its size exceeds numBins.\n+  override def merge(otherDigest: MapDigest, numBins: Int): Unit = {\n+    val otherMap = otherDigest.asInstanceOf[MapDigestBase[T]].bins\n+    mergeMap(otherMap, numBins)\n+  }\n+\n+  def mergeMap(otherMap: mutable.HashMap[T, Long], numBins: Int): Unit = {\n+    otherMap.foreach { case (key, value) =>\n+      if (bins.contains(key)) {\n+        bins.update(key, bins(key) + value)\n+      } else {\n+        if (bins.size >= numBins) {\n+          isInvalid = true\n+          bins.clear()\n+          return",
    "line": 198
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "use java.util.HashMap for better performance\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:41:59Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]",
    "line": 177
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "also you need to document what T is, and what the Long is.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:42:24Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]",
    "line": 177
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "should we just serialize using Kryo rather than writing our own map serializer here?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:43:16Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]\n+\n+  // Update bins and clear it when its size exceeds numBins.\n+  def updateMap(value: T, numBins: Int): Unit = {\n+    mergeMap(mutable.HashMap(value -> 1L), numBins)\n+  }\n+\n+  // Merge two maps and clear bins when its size exceeds numBins.\n+  override def merge(otherDigest: MapDigest, numBins: Int): Unit = {\n+    val otherMap = otherDigest.asInstanceOf[MapDigestBase[T]].bins\n+    mergeMap(otherMap, numBins)\n+  }\n+\n+  def mergeMap(otherMap: mutable.HashMap[T, Long], numBins: Int): Unit = {\n+    otherMap.foreach { case (key, value) =>\n+      if (bins.contains(key)) {\n+        bins.update(key, bins(key) + value)\n+      } else {\n+        if (bins.size >= numBins) {\n+          isInvalid = true\n+          bins.clear()\n+          return\n+        } else {\n+          bins.put(key, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def clear(): Unit = bins.clear()\n+\n+  override def serialize(): Array[Byte] = {",
    "line": 208
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "maybe it's fine, but if you are writing our own binary protocol, I'd document it here.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:43:51Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]\n+\n+  // Update bins and clear it when its size exceeds numBins.\n+  def updateMap(value: T, numBins: Int): Unit = {\n+    mergeMap(mutable.HashMap(value -> 1L), numBins)\n+  }\n+\n+  // Merge two maps and clear bins when its size exceeds numBins.\n+  override def merge(otherDigest: MapDigest, numBins: Int): Unit = {\n+    val otherMap = otherDigest.asInstanceOf[MapDigestBase[T]].bins\n+    mergeMap(otherMap, numBins)\n+  }\n+\n+  def mergeMap(otherMap: mutable.HashMap[T, Long], numBins: Int): Unit = {\n+    otherMap.foreach { case (key, value) =>\n+      if (bins.contains(key)) {\n+        bins.update(key, bins(key) + value)\n+      } else {\n+        if (bins.size >= numBins) {\n+          isInvalid = true\n+          bins.clear()\n+          return\n+        } else {\n+          bins.put(key, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def clear(): Unit = bins.clear()\n+\n+  override def serialize(): Array[Byte] = {",
    "line": 208
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "actually if you just use kryo maybe you don't need any of the keyLen, putKey, getKey functions and it will look a lot simpler.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:48:24Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]\n+\n+  // Update bins and clear it when its size exceeds numBins.\n+  def updateMap(value: T, numBins: Int): Unit = {\n+    mergeMap(mutable.HashMap(value -> 1L), numBins)\n+  }\n+\n+  // Merge two maps and clear bins when its size exceeds numBins.\n+  override def merge(otherDigest: MapDigest, numBins: Int): Unit = {\n+    val otherMap = otherDigest.asInstanceOf[MapDigestBase[T]].bins\n+    mergeMap(otherMap, numBins)\n+  }\n+\n+  def mergeMap(otherMap: mutable.HashMap[T, Long], numBins: Int): Unit = {\n+    otherMap.foreach { case (key, value) =>\n+      if (bins.contains(key)) {\n+        bins.update(key, bins(key) + value)\n+      } else {\n+        if (bins.size >= numBins) {\n+          isInvalid = true\n+          bins.clear()\n+          return\n+        } else {\n+          bins.put(key, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def clear(): Unit = bins.clear()\n+\n+  override def serialize(): Array[Byte] = {",
    "line": 208
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "I think why we are writing our own serializer here (and also in `ApproximatePercentile`) is because `SerializerInstance` is not thread-safe, right?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T16:15:30Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]\n+\n+  // Update bins and clear it when its size exceeds numBins.\n+  def updateMap(value: T, numBins: Int): Unit = {\n+    mergeMap(mutable.HashMap(value -> 1L), numBins)\n+  }\n+\n+  // Merge two maps and clear bins when its size exceeds numBins.\n+  override def merge(otherDigest: MapDigest, numBins: Int): Unit = {\n+    val otherMap = otherDigest.asInstanceOf[MapDigestBase[T]].bins\n+    mergeMap(otherMap, numBins)\n+  }\n+\n+  def mergeMap(otherMap: mutable.HashMap[T, Long], numBins: Int): Unit = {\n+    otherMap.foreach { case (key, value) =>\n+      if (bins.contains(key)) {\n+        bins.update(key, bins(key) + value)\n+      } else {\n+        if (bins.size >= numBins) {\n+          isInvalid = true\n+          bins.clear()\n+          return\n+        } else {\n+          bins.put(key, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def clear(): Unit = bins.clear()\n+\n+  override def serialize(): Array[Byte] = {",
    "line": 208
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i think you are doing this backwards by calling mergeMap in updateMap. If anything, it should be the other way around, i.e. mergeMap has a loop that calls updateMap. That way you will not need to create a whole new hash map for each record, or loop over some hash map of one entry for every record.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:46:19Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]\n+\n+  // Update bins and clear it when its size exceeds numBins.\n+  def updateMap(value: T, numBins: Int): Unit = {\n+    mergeMap(mutable.HashMap(value -> 1L), numBins)",
    "line": 181
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "once you do that you can also just put mergeMap into merge.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:47:03Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values\n+      is less than or equal to the specified threshold `numBins`.\n+      3. an empty result if the number of distinct non-null values exceeds `numBins`.\n+  \"\"\",\n+  extended = \"\"\"\n+    Examples:\n+      > SELECT map_aggregate(col, 3) FROM tbl;\n+       1. null - if `tbl` is empty or values of `col` are all nulls\n+       2. Map((10, 2), (20, 1)) - if values of `col` are (10, 20, 10)\n+       3. Map.empty - if values of `col` are (1, 2, 3, 4)\n+  \"\"\")\n+case class MapAggregate(\n+    child: Expression,\n+    numBinsExpression: Expression,\n+    override val mutableAggBufferOffset: Int,\n+    override val inputAggBufferOffset: Int) extends TypedImperativeAggregate[MapDigest] {\n+\n+  def this(child: Expression, numBinsExpression: Expression) = {\n+    this(child, numBinsExpression, 0, 0)\n+  }\n+\n+  // Mark as lazy so that numBinsExpression is not evaluated during tree transformation.\n+  private lazy val numBins: Int = numBinsExpression.eval().asInstanceOf[Int]\n+\n+  override def inputTypes: Seq[AbstractDataType] = {\n+    Seq(TypeCollection(NumericType, TimestampType, DateType, StringType), IntegerType)\n+  }\n+\n+  override def checkInputDataTypes(): TypeCheckResult = {\n+    val defaultCheck = super.checkInputDataTypes()\n+    if (defaultCheck.isFailure) {\n+      defaultCheck\n+    } else if (!numBinsExpression.foldable) {\n+      TypeCheckFailure(\"The maximum number of bins provided must be a constant literal\")\n+    } else if (numBins < 2) {\n+      val currentValue = if (numBinsExpression.eval() == null) null else numBins\n+      TypeCheckFailure(\n+        \"The maximum number of bins provided must be a positive integer literal >= 2 \" +\n+          s\"(current value = $currentValue)\")\n+    } else {\n+      TypeCheckSuccess\n+    }\n+  }\n+\n+  override def update(buffer: MapDigest, input: InternalRow): Unit = {\n+    if (!buffer.isInvalid) {\n+      val evaluated = child.eval(input)\n+      if (evaluated != null) buffer.update(child.dataType, evaluated, numBins)\n+    }\n+  }\n+\n+  override def merge(buffer: MapDigest, other: MapDigest): Unit = {\n+    if (!buffer.isInvalid) {\n+      if (other.isInvalid) {\n+        buffer.isInvalid = true\n+        buffer.clear()\n+      } else {\n+        buffer.merge(other, numBins)\n+      }\n+    }\n+  }\n+\n+  override def eval(buffer: MapDigest): Any = {\n+    if (buffer.isInvalid) {\n+      // return empty map\n+      ArrayBasedMapData(Map.empty)\n+    } else {\n+      // sort the result to make it more readable\n+      val sorted = buffer match {\n+        case stringDigest: StringMapDigest => TreeMap[UTF8String, Long](stringDigest.bins.toSeq: _*)\n+        case numericDigest: NumericMapDigest => TreeMap[Double, Long](numericDigest.bins.toSeq: _*)\n+      }\n+      if (sorted.isEmpty) {\n+        // don't have non-null values\n+        null\n+      } else {\n+        ArrayBasedMapData(sorted.keys.toArray, sorted.values.toArray)\n+      }\n+    }\n+  }\n+\n+  override def serialize(buffer: MapDigest): Array[Byte] = {\n+    buffer.serialize()\n+  }\n+\n+  override def deserialize(bytes: Array[Byte]): MapDigest = {\n+    MapDigest.deserialize(child.dataType, bytes)\n+  }\n+\n+  override def createAggregationBuffer(): MapDigest = {\n+    child.dataType match {\n+      case StringType => StringMapDigest()\n+      case _ => NumericMapDigest()\n+    }\n+  }\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): MapAggregate = {\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): MapAggregate = {\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+  }\n+\n+  override def nullable: Boolean = true\n+\n+  override def dataType: DataType = {\n+    child.dataType match {\n+      case StringType => MapType(StringType, LongType)\n+      case _ => MapType(DoubleType, LongType)\n+    }\n+  }\n+\n+  override def children: Seq[Expression] = Seq(child, numBinsExpression)\n+\n+  override def prettyName: String = \"map_aggregate\"\n+}\n+\n+trait MapDigest {\n+  // Mark this MapDigest invalid when the size of the hashmap (ndv of the column) exceeds numBins\n+  var isInvalid: Boolean\n+  def update(dataType: DataType, value: Any, numBins: Int): Unit\n+  def merge(otherDigest: MapDigest, numBins: Int): Unit\n+  def clear(): Unit\n+  def serialize(): Array[Byte]\n+}\n+\n+abstract class MapDigestBase[T] extends MapDigest {\n+  val bins: mutable.HashMap[T, Long]\n+\n+  // Update bins and clear it when its size exceeds numBins.\n+  def updateMap(value: T, numBins: Int): Unit = {\n+    mergeMap(mutable.HashMap(value -> 1L), numBins)",
    "line": 181
  }],
  "prId": 15637
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "it's really not frequency, but just count?\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T08:52:22Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values",
    "line": 49
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "Frequency usually means the rate something occurs.  In statistics, frequency means the number of items occurring in a given category.  Here frequency actually is equivalent to count.  We use frequency and count interchangeably.\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-11-04T23:11:07Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.immutable.TreeMap\n+import scala.collection.mutable\n+\n+import com.google.common.primitives.{Doubles, Ints, Longs}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult\n+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription}\n+import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n+import org.apache.spark.sql.types.{DataType, _}\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * The MapAggregate function Computes frequency for each distinct non-null value of a column.\n+ * It returns: 1. null if the table is empty or all values of the column are null.\n+ * 2. (distinct non-null value, frequency) pairs if the number of distinct non-null values is\n+ * less than or equal to the specified threshold.\n+ * 3. an empty result if the number of distinct non-null values exceeds that threshold.\n+ *\n+ * @param child child expression that can produce column value with `child.eval(inputRow)`\n+ * @param numBinsExpression The maximum number of pairs.\n+ */\n+@ExpressionDescription(\n+  usage = \"\"\"\n+    _FUNC_(col, numBins) - Computes frequency for each distinct non-null value of column `col`.\n+      It returns: 1. null if the table is empty or all values of column `col` are null.\n+      2. (distinct non-null value, frequency) pairs if the number of distinct non-null values",
    "line": 49
  }],
  "prId": 15637
}]