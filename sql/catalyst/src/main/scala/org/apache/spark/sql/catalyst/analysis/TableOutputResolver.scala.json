[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why `DataType.canWrite` need to take the `useStrictRules` parameter? In this branch `useStrictRules` is true.",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-19T15:44:21Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    Project(resolved, query)\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    val useStrictRules = conf.storeAssignmentPolicy == StoreAssignmentPolicy.STRICT\n+\n+    if (!useStrictRules) {\n+      // Renaming is needed for handling the following cases like\n+      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2\n+      // 2) Target tables have column metadata\n+      Some(Alias(\n+        Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone)),\n+        tableAttr.name)(explicitMetadata = Option(tableAttr.metadata)))\n+    } else {\n+      // run the type check first to ensure type errors are present\n+      val canWrite = DataType.canWrite(\n+        queryExpr.dataType, tableAttr.dataType, byName, useStrictRules,"
  }],
  "prId": 25453
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`useStrictRules` -> `useStrictRule`",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-19T15:44:33Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    Project(resolved, query)\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    val useStrictRules = conf.storeAssignmentPolicy == StoreAssignmentPolicy.STRICT"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I would perfer \"rules\"",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-19T16:50:20Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    Project(resolved, query)\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    val useStrictRules = conf.storeAssignmentPolicy == StoreAssignmentPolicy.STRICT"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "If we will add another rule in follow-up activities, it'd be better to use pattern-matching here instead of `if`?",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-20T02:08:57Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    Project(resolved, query)\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    val useStrictRules = conf.storeAssignmentPolicy == StoreAssignmentPolicy.STRICT"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "what \"rules\" mean here? From the code we are applying the STRICT rule here. If you want to represent \"non-legacy rules\", I think it's better to write\r\n```\r\nval isLegacyMode = conf.storeAssignmentPolicy == StoreAssignmentPolicy.LEGACY\r\n...\r\n```",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-20T05:16:33Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    Project(resolved, query)\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    val useStrictRules = conf.storeAssignmentPolicy == StoreAssignmentPolicy.STRICT"
  }],
  "prId": 25453
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we use `Cast` here? The upcast logic is already checked in `DataType.canWrite`. Then we can remove https://github.com/apache/spark/pull/25453/files#diff-7690f56bde3f7a3dd76fab9c136c1494R181",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-19T15:47:51Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    Project(resolved, query)\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    val useStrictRules = conf.storeAssignmentPolicy == StoreAssignmentPolicy.STRICT\n+\n+    if (!useStrictRules) {\n+      // Renaming is needed for handling the following cases like\n+      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2\n+      // 2) Target tables have column metadata\n+      Some(Alias(\n+        Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone)),\n+        tableAttr.name)(explicitMetadata = Option(tableAttr.metadata)))\n+    } else {\n+      // run the type check first to ensure type errors are present\n+      val canWrite = DataType.canWrite(\n+        queryExpr.dataType, tableAttr.dataType, byName, useStrictRules,\n+        conf.resolver, tableAttr.name, addError)\n+      if (queryExpr.nullable && !tableAttr.nullable) {\n+        addError(s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")\n+        None\n+\n+      } else if (!canWrite) {\n+        None\n+\n+      } else {\n+        // always add an UpCast. it will be removed in the optimizer if it is unnecessary.\n+        Some(Alias(\n+          UpCast(queryExpr, tableAttr.dataType), tableAttr.name"
  }],
  "prId": 25453
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "This is just a check to confirm; you don't modify any logic in `resolveOutputColumns`?",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-20T02:05:25Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(",
    "line": 30
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Yes.",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-20T08:37:48Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression, UpCast}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(",
    "line": 30
  }],
  "prId": 25453
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about\r\n```\r\nif (policy == Legacy) {\r\n  ...\r\n} else if (policy == STRICT) {\r\n  ...\r\n} else {\r\n  throw exception (\"unsupported policy\")\r\n}\r\n```",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-20T07:28:43Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    Project(resolved, query)\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    if (!conf.useStrictStoreAssignmentPolicy) {"
  }],
  "prId": 25453
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "We can always use `Cast` here and let the optimization rule `SimplifyCasts` to remove unnecessary ones. But there is test case(E.g. `HiveQuerySuite.SPARK-7270: consider dynamic partition when comparing table output`) checking if there is no casting in the analyzed plan. To keep the behavior of V1, let's remove unnecessary `Cast` here. ",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-20T12:59:17Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolveQuoted(tableAttr.name, conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    if (resolved == query.output) {\n+      query\n+    } else {\n+      Project(resolved, query)\n+    }\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&\n+      tableAttr.name == queryExpr.name &&\n+      tableAttr.metadata == queryExpr.metadata) {",
    "line": 96
  }],
  "prId": 25453
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "We need this? It seems we already have checked if the mode is valid? https://github.com/apache/spark/pull/25453/files#diff-9a6b543db706f1a90f790783d6930a13R1661",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-21T00:24:14Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolve(Seq(tableAttr.name), conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    if (resolved == query.output) {\n+      query\n+    } else {\n+      Project(resolved, query)\n+    }\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&\n+      tableAttr.name == queryExpr.name &&\n+      tableAttr.metadata == queryExpr.metadata) {\n+      Some(queryExpr)\n+    } else {\n+      // Renaming is needed for handling the following cases like\n+      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2\n+      // 2) Target tables have column metadata\n+      Some(Alias(\n+        Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone)),\n+        tableAttr.name)(explicitMetadata = Option(tableAttr.metadata)))\n+    }\n+\n+    conf.storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.LEGACY =>\n+        outputField\n+\n+      case StoreAssignmentPolicy.STRICT =>\n+        // run the type check first to ensure type errors are present\n+        val canWrite = DataType.canWrite(\n+          queryExpr.dataType, tableAttr.dataType, byName, conf.resolver, tableAttr.name, addError)\n+        if (queryExpr.nullable && !tableAttr.nullable) {\n+          addError(s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")\n+          None\n+\n+        } else if (!canWrite) {\n+          None\n+\n+        } else {\n+          outputField\n+        }\n+\n+      case other =>\n+        throw new AnalysisException(s\"Unsupported store assignment policy: $other\")"
  }],
  "prId": 25453
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "```\r\n        if (queryExpr.nullable && !tableAttr.nullable) {\r\n          addError(s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")\r\n          None\r\n        } else {\r\n          // run the type check first to ensure type errors are present\r\n          val canWrite = DataType.canWrite(\r\n            queryExpr.dataType, tableAttr.dataType, byName, conf.resolver, tableAttr.name, addError)\r\n          if (canWrite) {\r\n            outputField\r\n          } else {\r\n            None\r\n          }\r\n        }\r\n```\r\n?",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-21T00:35:41Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolve(Seq(tableAttr.name), conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    if (resolved == query.output) {\n+      query\n+    } else {\n+      Project(resolved, query)\n+    }\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&\n+      tableAttr.name == queryExpr.name &&\n+      tableAttr.metadata == queryExpr.metadata) {\n+      Some(queryExpr)\n+    } else {\n+      // Renaming is needed for handling the following cases like\n+      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2\n+      // 2) Target tables have column metadata\n+      Some(Alias(\n+        Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone)),\n+        tableAttr.name)(explicitMetadata = Option(tableAttr.metadata)))\n+    }\n+\n+    conf.storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.LEGACY =>\n+        outputField\n+\n+      case StoreAssignmentPolicy.STRICT =>\n+        // run the type check first to ensure type errors are present",
    "line": 112
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "btw, we don't need the check `queryExpr.nullable && !tableAttr.nullable` in the other modes?",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-21T00:36:03Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolve(Seq(tableAttr.name), conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    if (resolved == query.output) {\n+      query\n+    } else {\n+      Project(resolved, query)\n+    }\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&\n+      tableAttr.name == queryExpr.name &&\n+      tableAttr.metadata == queryExpr.metadata) {\n+      Some(queryExpr)\n+    } else {\n+      // Renaming is needed for handling the following cases like\n+      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2\n+      // 2) Target tables have column metadata\n+      Some(Alias(\n+        Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone)),\n+        tableAttr.name)(explicitMetadata = Option(tableAttr.metadata)))\n+    }\n+\n+    conf.storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.LEGACY =>\n+        outputField\n+\n+      case StoreAssignmentPolicy.STRICT =>\n+        // run the type check first to ensure type errors are present",
    "line": 112
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I think this is on purpose in the original code. Running `DataType.canWrite` can expose more errors.",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-21T05:22:50Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolve(Seq(tableAttr.name), conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    if (resolved == query.output) {\n+      query\n+    } else {\n+      Project(resolved, query)\n+    }\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&\n+      tableAttr.name == queryExpr.name &&\n+      tableAttr.metadata == queryExpr.metadata) {\n+      Some(queryExpr)\n+    } else {\n+      // Renaming is needed for handling the following cases like\n+      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2\n+      // 2) Target tables have column metadata\n+      Some(Alias(\n+        Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone)),\n+        tableAttr.name)(explicitMetadata = Option(tableAttr.metadata)))\n+    }\n+\n+    conf.storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.LEGACY =>\n+        outputField\n+\n+      case StoreAssignmentPolicy.STRICT =>\n+        // run the type check first to ensure type errors are present",
    "line": 112
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "> btw, we don't need the check queryExpr.nullable && !tableAttr.nullable in the other modes?\r\n\r\nIIRC there is no such check in Spark 2.x",
    "commit": "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "createdAt": "2019-08-21T05:23:39Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.DataType\n+\n+object TableOutputResolver {\n+  def resolveOutputColumns(\n+      tableName: String,\n+      expected: Seq[Attribute],\n+      query: LogicalPlan,\n+      byName: Boolean,\n+      conf: SQLConf): LogicalPlan = {\n+\n+    if (expected.size < query.output.size) {\n+      throw new AnalysisException(\n+        s\"\"\"Cannot write to '$tableName', too many data columns:\n+           |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+           |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\".stripMargin)\n+    }\n+\n+    val errors = new mutable.ArrayBuffer[String]()\n+    val resolved: Seq[NamedExpression] = if (byName) {\n+      expected.flatMap { tableAttr =>\n+        query.resolve(Seq(tableAttr.name), conf.resolver) match {\n+          case Some(queryExpr) =>\n+            checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+          case None =>\n+            errors += s\"Cannot find data for output column '${tableAttr.name}'\"\n+            None\n+        }\n+      }\n+\n+    } else {\n+      if (expected.size > query.output.size) {\n+        throw new AnalysisException(\n+          s\"\"\"Cannot write to '$tableName', not enough data columns:\n+             |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")}\n+             |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\"\n+            .stripMargin)\n+      }\n+\n+      query.output.zip(expected).flatMap {\n+        case (queryExpr, tableAttr) =>\n+          checkField(tableAttr, queryExpr, byName, conf, err => errors += err)\n+      }\n+    }\n+\n+    if (errors.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\")\n+    }\n+\n+    if (resolved == query.output) {\n+      query\n+    } else {\n+      Project(resolved, query)\n+    }\n+  }\n+\n+  private def checkField(\n+      tableAttr: Attribute,\n+      queryExpr: NamedExpression,\n+      byName: Boolean,\n+      conf: SQLConf,\n+      addError: String => Unit): Option[NamedExpression] = {\n+\n+    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&\n+      tableAttr.name == queryExpr.name &&\n+      tableAttr.metadata == queryExpr.metadata) {\n+      Some(queryExpr)\n+    } else {\n+      // Renaming is needed for handling the following cases like\n+      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2\n+      // 2) Target tables have column metadata\n+      Some(Alias(\n+        Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone)),\n+        tableAttr.name)(explicitMetadata = Option(tableAttr.metadata)))\n+    }\n+\n+    conf.storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.LEGACY =>\n+        outputField\n+\n+      case StoreAssignmentPolicy.STRICT =>\n+        // run the type check first to ensure type errors are present",
    "line": 112
  }],
  "prId": 25453
}]