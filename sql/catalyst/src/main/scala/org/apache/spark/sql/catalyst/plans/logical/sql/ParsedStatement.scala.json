[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Is this what you want?\r\n```\r\n    case mapArg: Map[String, String] => conf.redactOptions(mapArg)\r\n    case other => other\r\n```",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-05T05:33:15Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "but, my question is should we still redact the string key when the value is not string, or redact the string value when the key is not string? ",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-05T05:34:23Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "jzhuge"
    },
    "body": "It does not work due to type erasure. As a matter of fact, compiler gives this warning:\r\n```\r\nWarning:(41, 18) non-variable type argument String in type pattern\r\nscala.collection.immutable.Map[String,String] (the underlying of Map[String,String]) is\r\nunchecked since it is eliminated by erasure\r\n           case mapArg: Map[String, String] =>\r\n```",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-05T06:03:14Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "jzhuge"
    },
    "body": "Currently only `options` and `properties` need to be redacted, they are `Map[String, String]`.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-05T06:07:35Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "ParsedStatement is an abstract class, we might not be able to know how it will be used in the future. We should try our best to avoid adding any assumption here. Could you try it? I think it is not a hard problem. ",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-05T06:27:10Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I think the `Try` option is reasonable here. If any map is `Map[String, String]` then it will be redacted. Otherwise, it is passed on. Redaction only works on `Map[String, String]` so there is nothing to do for other maps.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-05T17:37:04Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "jzhuge"
    },
    "body": "@gatorsmile `SQLConf.redactOptions` calls `Utils.redact` that only supports Seq[(String, String)]. We could enhance these Utils redact methods to support non-String keys or values, but that feels like a separate PR.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-05T17:52:29Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "In Spark source code, we always try to avoid rely on the exception handling if we can possibly avoid it. \r\n\r\nAlso, we try our best to avoid making a hidden assumption in the utility class. \r\n\r\nI think we can enhance these Utils redact methods in this PR.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-07T05:39:22Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Another solution: let `redactOptions` take a map of any key and value type. Only evaluate and update keys/values that are actually strings. This would maybe make it more flexible and handle, say, a `Map[SomeEnum,String]` later.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-07T13:05:31Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }, {
    "author": {
      "login": "jzhuge"
    },
    "body": "@dongjoon-hyun @gatorsmile @srowen Please review the new commit.\r\n\r\nPlease note the commit does not redact Map[String, non-String] because the replacement text is string. For non-String, e.g, Int, if we use -1 as replacement, it may be confusing.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-08T04:37:03Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {\n-    case mapArg: Map[_, _] => conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])\n-    case other => other\n+    case mapArg: Map[_, _] =>\n+      // May match any Map type, e.g. Map[String, Int], due to type erasure\n+      Try(conf.redactOptions(mapArg.asInstanceOf[Map[String, String]])).getOrElse(mapArg)"
  }],
  "prId": 24800
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "why don't we add like:\r\n\r\n```scala\r\nprotected def options: Map[String, String] = { Map.empty }\r\nprotected def properties: Map[String, String] = { Map.empty }\r\n```\r\n\r\nand, \r\n\r\n```diff\r\n-    options: Map[String, String],\r\n+    override val options: Map[String, String],\r\n```\r\n\r\nat implementation of this classes? Seems like currently we'll check every maps whatever it is.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-07T03:28:11Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {",
    "line": 3
  }, {
    "author": {
      "login": "jzhuge"
    },
    "body": "This is not flexible enough to automatically redact other field names. Please check out the solution in my new commit.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-08T04:38:44Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {",
    "line": 3
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "We can just redact `options` and `properties` and remove `productIterator` override. Wasn't that the purpose of doing this?\r\n\r\nI am currently unable to follow why we need to it for every map argument in implemented case classes. How about array?",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-08T05:38:18Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {",
    "line": 3
  }, {
    "author": {
      "login": "jzhuge"
    },
    "body": "My guess the intention is to auto-redact sensitive info for any Map field in subclasses. More subclasses will be added and sensitive fields may not be called `options` or `properties`.",
    "commit": "1055a220f5eb867d596f708f67a1c9f1c0385e2e",
    "createdAt": "2019-06-10T02:27:16Z",
    "diffHunk": "@@ -36,8 +38,11 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n private[sql] abstract class ParsedStatement extends LogicalPlan {\n   // Redact properties and options when parsed nodes are used by generic methods like toString\n   override def productIterator: Iterator[Any] = super.productIterator.map {",
    "line": 3
  }],
  "prId": 24800
}]