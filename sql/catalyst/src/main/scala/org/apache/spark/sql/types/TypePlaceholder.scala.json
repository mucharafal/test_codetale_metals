[{
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "I just added this weird type for this prototype, so we need to consider more here. I don't have better idea now, so I'd like to have any suggestion about this.",
    "commit": "4544433760bd70cff41aa8e8bb718e6de0e3b877",
    "createdAt": "2018-03-31T00:37:01Z",
    "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.types\n+\n+/**\n+ * An internal type that is a not yet available and will be replaced by an actual type later.\n+ */\n+case object TypePlaceholder extends StringType"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Is it necessary to introduce a new DataType? Would it be the same if we use `NullType`? With the flag on, at the end of schema inference, `NullType`, `ArrayType(NullType)`, etc should be dropped instead of using StringType as fallback. Basically, during schema inference, we keep the one that reveals more details, for example:\r\n\r\n```\r\n(NullType, ArrayType(NullType)) => ArrayType(NullType)\r\n(ArrayType(NullType), ArrayType(StructType(Field(\"a\", NullType)))) => ArrayType(StructType(Field(\"a\", NullType))))\r\n```\r\n\r\nAt the end, we implement a util method that determine whether a field is all null and drop them if true. It should be done recursively. I have an internal implementation that implements a similar logic, but on the JSON record itself. You might want to apply it to data types.\r\n\r\n```scala\r\n  /**\r\n   * Removes null fields recursively from the input JSON record.\r\n   * An array is null if all its elements are null.\r\n   * An object is null if all its values are null.\r\n   */\r\n  def removeNullRecursively(jsonStr: String): String = {\r\n    val json = parse(jsonStr)\r\n    val cleaned = doRemoveNullRecursively(json)\r\n    compact(render(cleaned)) // should handle null correctly\r\n  }\r\n\r\n  private def doRemoveNullRecursively(value: JValue): JValue = {\r\n    value match {\r\n      case null =>\r\n        null\r\n\r\n      case JNull =>\r\n        null\r\n\r\n      case JArray(values) =>\r\n        val cleaned = values.map(doRemoveNullRecursively)\r\n        if (cleaned.exists(_ != null)) {\r\n          JArray(cleaned)\r\n        } else {\r\n          null\r\n        }\r\n\r\n      case JObject(pairs) =>\r\n        val cleaned = pairs.flatMap { case (k, v) =>\r\n          val cv = doRemoveNullRecursively(v)\r\n          if (cv != null) {\r\n            Some((k, cv))\r\n          } else {\r\n            None\r\n          }\r\n        }\r\n        if (cleaned.nonEmpty) {\r\n          JObject(cleaned)\r\n        } else {\r\n          null\r\n        }\r\n\r\n      // all other types are non-null\r\n      case _ =>\r\n        value\r\n    }\r\n  }\r\n```",
    "commit": "4544433760bd70cff41aa8e8bb718e6de0e3b877",
    "createdAt": "2018-04-10T03:59:19Z",
    "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.types\n+\n+/**\n+ * An internal type that is a not yet available and will be replaced by an actual type later.\n+ */\n+case object TypePlaceholder extends StringType"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "In the first attempt, I used the new type instead of `NullType` because some `Sink`s (`FileStreamSink`) could not handle `NullType`;\r\n```\r\n// parquet\r\njava.lang.RuntimeException: Unsupported data type NullType.\r\n        at scala.sys.package$.error(package.scala:27)\r\n        at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(ParquetWriteSupport.scala:206)\r\n        at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$init$2.apply(ParquetWriteSupport.scala:93)\r\n        at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$init$2.apply(ParquetWriteSupport.scala:93)\r\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n// orc\r\njava.lang.IllegalArgumentException: Can't parse category at 'struct<c0:bigint,c1:null^,c2:array<null>>'\r\n        at org.apache.orc.TypeDescription.parseCategory(TypeDescription.java:223)\r\n        at org.apache.orc.TypeDescription.parseType(TypeDescription.java:332)\r\n        at org.apache.orc.TypeDescription.parseStruct(TypeDescription.java:327)\r\n        at org.apache.orc.TypeDescription.parseType(TypeDescription.java:385)\r\n        at org.apache.orc.TypeDescription.fromString(TypeDescription.java:406)\r\n\r\n// csv\r\njava.lang.UnsupportedOperationException: CSV data source does not support null data type.\r\n        at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.org$apache$spark$sql$execution$datasources$csv$CSVUtils$$verifyType$1(CSVUtils.scala:130)\r\n        at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:134)\r\n        at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:134)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\r\n```\r\nSo, in the previous fix, I tried to add `PlaceholderType` inherited from `StringType` and this type could be correctly handled in all the `Sink`, but too tricky. \r\n\r\nIn the suggested, `NullType, ArrayType(NullType), etc should be dropped` means that we need to handle an inferred schema as follows? e.g.,\r\n```\r\nInferred schema: \"StructType<IntegerType, NullType, ArrayType(NullType)>\" -> Schema used in FileStreamSource: \"StructType<IntegerType>\"\r\n```\r\nIs this right?",
    "commit": "4544433760bd70cff41aa8e8bb718e6de0e3b877",
    "createdAt": "2018-05-07T23:57:03Z",
    "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.types\n+\n+/**\n+ * An internal type that is a not yet available and will be replaced by an actual type later.\n+ */\n+case object TypePlaceholder extends StringType"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "I think `removeNullRecursively` seems to be a little time-consuming (json parsing twice) and why can't we do the same thing directly in `JacksonParser`? e.g.,\r\nhttps://github.com/apache/spark/pull/20929/files#diff-635e02b2d1ce4ad1675b0350ccac0c10R334",
    "commit": "4544433760bd70cff41aa8e8bb718e6de0e3b877",
    "createdAt": "2018-05-08T00:31:41Z",
    "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.types\n+\n+/**\n+ * An internal type that is a not yet available and will be replaced by an actual type later.\n+ */\n+case object TypePlaceholder extends StringType"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "In the latest fix, I used `NullType` for unresolved types and `FileStreamSource` would resolve the types when we see non-null values. As suggested in `SPARK-12436`, I think we might need to fallback into `StringType` just before `Sink`s, e.g., https://github.com/apache/spark/blob/76ecd095024a658bf68e5db658e4416565b30c17/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala#L492",
    "commit": "4544433760bd70cff41aa8e8bb718e6de0e3b877",
    "createdAt": "2018-05-08T01:03:47Z",
    "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.types\n+\n+/**\n+ * An internal type that is a not yet available and will be replaced by an actual type later.\n+ */\n+case object TypePlaceholder extends StringType"
  }],
  "prId": 20929
}]