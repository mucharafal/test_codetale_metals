[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@jzhuge Could you print out more information of `CatalogNotFoundException` without the full stack trace?\r\nWithout that, the previous error message will be better for users because it gives the exact reason `Cannot find catalog plugin class for catalog 'session': xxx`.\r\n```scala\r\n$ bin/spark-shell --conf spark.sql.catalog.session=xxx\r\nscala> spark.sessionState.analyzer.sessionCatalog\r\n19/08/10 18:17:29 ERROR HiveSessionStateBuilder$$anon$1: Cannot load v2 session catalog\r\norg.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'session': xxx\r\n```",
    "commit": "12282811bf7c9ed5c5d31481abbeb71d1eae4f7b",
    "createdAt": "2019-08-11T01:20:30Z",
    "diffHunk": "@@ -62,6 +62,9 @@ trait LookupCatalog extends Logging {\n     try {\n       Some(lookupCatalog(SESSION_CATALOG_NAME))\n     } catch {\n+      case _: CatalogNotFoundException =>\n+        logWarning(\"Session catalog is not defined\")\n+        None"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "BTW, @jzhuge . Is this all places to give a warning instead of the stacks traces? \r\nWith this PR, I still see the previous behavior.",
    "commit": "12282811bf7c9ed5c5d31481abbeb71d1eae4f7b",
    "createdAt": "2019-08-11T01:30:49Z",
    "diffHunk": "@@ -62,6 +62,9 @@ trait LookupCatalog extends Logging {\n     try {\n       Some(lookupCatalog(SESSION_CATALOG_NAME))\n     } catch {\n+      case _: CatalogNotFoundException =>\n+        logWarning(\"Session catalog is not defined\")\n+        None"
  }, {
    "author": {
      "login": "jzhuge"
    },
    "body": "@dongjoon-hyun Thanks for the review. Your command line is not the case I tried to fix in the PR. In your case, the stack trace is helpful.\r\n\r\nIt seems that the current master has session catalog defined by default, so here is the command line to reproduce my case:\r\n```\r\n$ bin/spark-shell --master 'local[*]' --conf spark.sql.catalog.session=\r\n...\r\nSpark context available as 'sc' (master = local[*], app id = local-1565588237201).\r\nSpark session available as 'spark'.\r\n...\r\nscala> spark.sessionState.analyzer.sessionCatalog\r\n...\r\n2019-08-11 22:37:24,216 ERROR [main] hive.HiveSessionStateBuilder$$anon$1 (Logging.scala:logError(94)) - Cannot load v2 session catalog\r\norg.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'session':\r\n\tat org.apache.spark.sql.catalog.v2.Catalogs.load(Catalogs.java:81)\r\n...\r\nres0: Option[org.apache.spark.sql.catalog.v2.CatalogPlugin] = None\r\n```\r\nHere the stack trace does not add more information. And I am concerned that if any rule uses session catalog, we will see this long stack trace again and again.",
    "commit": "12282811bf7c9ed5c5d31481abbeb71d1eae4f7b",
    "createdAt": "2019-08-12T05:45:25Z",
    "diffHunk": "@@ -62,6 +62,9 @@ trait LookupCatalog extends Logging {\n     try {\n       Some(lookupCatalog(SESSION_CATALOG_NAME))\n     } catch {\n+      case _: CatalogNotFoundException =>\n+        logWarning(\"Session catalog is not defined\")\n+        None"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Could you fix both cases in this PR? I believe `typo` cases will be more frequent than the empty configuration cases.",
    "commit": "12282811bf7c9ed5c5d31481abbeb71d1eae4f7b",
    "createdAt": "2019-08-12T06:10:06Z",
    "diffHunk": "@@ -62,6 +62,9 @@ trait LookupCatalog extends Logging {\n     try {\n       Some(lookupCatalog(SESSION_CATALOG_NAME))\n     } catch {\n+      case _: CatalogNotFoundException =>\n+        logWarning(\"Session catalog is not defined\")\n+        None"
  }],
  "prId": 25372
}]