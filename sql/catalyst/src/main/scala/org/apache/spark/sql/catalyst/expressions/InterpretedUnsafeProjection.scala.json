[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why it's not a problem for the codegen version?",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-14T19:58:02Z",
    "diffHunk": "@@ -0,0 +1,372 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful non deterministic expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: StatefulNondeterministic => s.freshCopy()"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "In codegen the state is put in the generated class, if you happen to visit the same expression twice the state is added twice and is not shared during evaluation. In interpreted mode the Expression will be the same, and the same state will modified twice during evaluation.",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-14T20:07:31Z",
    "diffHunk": "@@ -0,0 +1,372 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful non deterministic expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: StatefulNondeterministic => s.freshCopy()"
  }],
  "prId": 20750
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We can add `UnsafeWriter#getBufferHolder`, so that we don't need to pass 2 parameters.",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-15T17:50:45Z",
    "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: Stateful => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,",
    "line": 115
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Yeah we could do that. I think we need to refactor the writers a little bit anyway, but I would like to do that in a follow-up.",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-16T11:47:56Z",
    "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: Stateful => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,",
    "line": 115
  }],
  "prId": 20750
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "null type will hit this branch, can we add a test to make sure it works?",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-15T18:41:42Z",
    "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: Stateful => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,\n+      fields: Array[StructField]): InternalRow => Unit = {\n+    val numFields = fields.length\n+\n+    // Create field writers.\n+    val fieldWriters = fields.map { field =>\n+      generateFieldWriter(bufferHolder, rowWriter, field.dataType, field.nullable)\n+    }\n+    // Create basic writer.\n+    row => {\n+      var i = 0\n+      while (i < numFields) {\n+        fieldWriters(i).apply(row, i)\n+        i += 1\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Generate a writer function for a struct field, array element, map key or map value. The\n+   * generated function writes the element at an index in a [[SpecializedGetters]] object (row\n+   * or array) to the given buffer using the given [[UnsafeWriter]].\n+   */\n+  private def generateFieldWriter(\n+      bufferHolder: BufferHolder,\n+      writer: UnsafeWriter,\n+      dt: DataType,\n+      nullable: Boolean): (SpecializedGetters, Int) => Unit = {\n+\n+    // Create the the basic writer.\n+    val unsafeWriter: (SpecializedGetters, Int) => Unit = dt match {\n+      case BooleanType =>\n+        (v, i) => writer.write(i, v.getBoolean(i))\n+\n+      case ByteType =>\n+        (v, i) => writer.write(i, v.getByte(i))\n+\n+      case ShortType =>\n+        (v, i) => writer.write(i, v.getShort(i))\n+\n+      case IntegerType | DateType =>\n+        (v, i) => writer.write(i, v.getInt(i))\n+\n+      case LongType | TimestampType =>\n+        (v, i) => writer.write(i, v.getLong(i))\n+\n+      case FloatType =>\n+        (v, i) => writer.write(i, v.getFloat(i))\n+\n+      case DoubleType =>\n+        (v, i) => writer.write(i, v.getDouble(i))\n+\n+      case DecimalType.Fixed(precision, scale) =>\n+        (v, i) => writer.write(i, v.getDecimal(i, precision, scale), precision, scale)\n+\n+      case CalendarIntervalType =>\n+        (v, i) => writer.write(i, v.getInterval(i))\n+\n+      case BinaryType =>\n+        (v, i) => writer.write(i, v.getBinary(i))\n+\n+      case StringType =>\n+        (v, i) => writer.write(i, v.getUTF8String(i))\n+\n+      case StructType(fields) =>\n+        val numFields = fields.length\n+        val rowWriter = new UnsafeRowWriter(bufferHolder, numFields)\n+        val structWriter = generateStructWriter(bufferHolder, rowWriter, fields)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getStruct(i, fields.length) match {\n+            case row: UnsafeRow =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                row.getBaseObject,\n+                row.getBaseOffset,\n+                row.getSizeInBytes)\n+            case row =>\n+              // Nested struct. We don't know where this will start because a row can be\n+              // variable length, so we need to update the offsets and zero out the bit mask.\n+              rowWriter.reset()\n+              structWriter.apply(row)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case ArrayType(elementType, containsNull) =>\n+        val arrayWriter = new UnsafeArrayWriter\n+        val elementSize = getElementSize(elementType)\n+        val elementWriter = generateFieldWriter(\n+          bufferHolder,\n+          arrayWriter,\n+          elementType,\n+          containsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          writeArray(bufferHolder, arrayWriter, elementWriter, v.getArray(i), elementSize)\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case MapType(keyType, valueType, valueContainsNull) =>\n+        val keyArrayWriter = new UnsafeArrayWriter\n+        val keySize = getElementSize(keyType)\n+        val keyWriter = generateFieldWriter(\n+          bufferHolder,\n+          keyArrayWriter,\n+          keyType,\n+          nullable = false)\n+        val valueArrayWriter = new UnsafeArrayWriter\n+        val valueSize = getElementSize(valueType)\n+        val valueWriter = generateFieldWriter(\n+          bufferHolder,\n+          valueArrayWriter,\n+          valueType,\n+          valueContainsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getMap(i) match {\n+            case map: UnsafeMapData =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                map.getBaseObject,\n+                map.getBaseOffset,\n+                map.getSizeInBytes)\n+            case map =>\n+              // preserve 8 bytes to write the key array numBytes later.\n+              bufferHolder.grow(8)\n+              bufferHolder.cursor += 8\n+\n+              // Write the keys and write the numBytes of key array into the first 8 bytes.\n+              writeArray(bufferHolder, keyArrayWriter, keyWriter, map.keyArray(), keySize)\n+              Platform.putLong(bufferHolder.buffer, tmpCursor, bufferHolder.cursor - tmpCursor - 8)\n+\n+              // Write the values.\n+              writeArray(bufferHolder, valueArrayWriter, valueWriter, map.valueArray(), valueSize)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case udt: UserDefinedType[_] =>\n+        generateFieldWriter(bufferHolder, writer, udt.sqlType, nullable)\n+\n+      case NullType =>\n+        (_, _) => {}\n+\n+      case _ =>\n+        throw new SparkException(s\"Unsupported data type $dt\")\n+    }\n+\n+    // Always wrap the writer with a null safe version.\n+    dt match {\n+      case _: UserDefinedType[_] =>\n+        // The null wrapper depends on the sql type and not on the UDT.\n+        unsafeWriter\n+      case DecimalType.Fixed(precision, _) if precision > Decimal.MAX_LONG_DIGITS =>\n+        // We can't call setNullAt() for DecimalType with precision larger than 18, we call write\n+        // directly. We can use the unwrapped writer directly.\n+        unsafeWriter\n+      case BooleanType | ByteType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull1Bytes(i)\n+          }\n+        }\n+      case ShortType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull2Bytes(i)\n+          }\n+        }\n+      case IntegerType | DateType | FloatType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull4Bytes(i)\n+          }\n+        }\n+      case _ =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull8Bytes(i)",
    "line": 302
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah it's consistent with the codegen version. Maybe we should fix it later.",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-15T18:47:47Z",
    "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: Stateful => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,\n+      fields: Array[StructField]): InternalRow => Unit = {\n+    val numFields = fields.length\n+\n+    // Create field writers.\n+    val fieldWriters = fields.map { field =>\n+      generateFieldWriter(bufferHolder, rowWriter, field.dataType, field.nullable)\n+    }\n+    // Create basic writer.\n+    row => {\n+      var i = 0\n+      while (i < numFields) {\n+        fieldWriters(i).apply(row, i)\n+        i += 1\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Generate a writer function for a struct field, array element, map key or map value. The\n+   * generated function writes the element at an index in a [[SpecializedGetters]] object (row\n+   * or array) to the given buffer using the given [[UnsafeWriter]].\n+   */\n+  private def generateFieldWriter(\n+      bufferHolder: BufferHolder,\n+      writer: UnsafeWriter,\n+      dt: DataType,\n+      nullable: Boolean): (SpecializedGetters, Int) => Unit = {\n+\n+    // Create the the basic writer.\n+    val unsafeWriter: (SpecializedGetters, Int) => Unit = dt match {\n+      case BooleanType =>\n+        (v, i) => writer.write(i, v.getBoolean(i))\n+\n+      case ByteType =>\n+        (v, i) => writer.write(i, v.getByte(i))\n+\n+      case ShortType =>\n+        (v, i) => writer.write(i, v.getShort(i))\n+\n+      case IntegerType | DateType =>\n+        (v, i) => writer.write(i, v.getInt(i))\n+\n+      case LongType | TimestampType =>\n+        (v, i) => writer.write(i, v.getLong(i))\n+\n+      case FloatType =>\n+        (v, i) => writer.write(i, v.getFloat(i))\n+\n+      case DoubleType =>\n+        (v, i) => writer.write(i, v.getDouble(i))\n+\n+      case DecimalType.Fixed(precision, scale) =>\n+        (v, i) => writer.write(i, v.getDecimal(i, precision, scale), precision, scale)\n+\n+      case CalendarIntervalType =>\n+        (v, i) => writer.write(i, v.getInterval(i))\n+\n+      case BinaryType =>\n+        (v, i) => writer.write(i, v.getBinary(i))\n+\n+      case StringType =>\n+        (v, i) => writer.write(i, v.getUTF8String(i))\n+\n+      case StructType(fields) =>\n+        val numFields = fields.length\n+        val rowWriter = new UnsafeRowWriter(bufferHolder, numFields)\n+        val structWriter = generateStructWriter(bufferHolder, rowWriter, fields)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getStruct(i, fields.length) match {\n+            case row: UnsafeRow =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                row.getBaseObject,\n+                row.getBaseOffset,\n+                row.getSizeInBytes)\n+            case row =>\n+              // Nested struct. We don't know where this will start because a row can be\n+              // variable length, so we need to update the offsets and zero out the bit mask.\n+              rowWriter.reset()\n+              structWriter.apply(row)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case ArrayType(elementType, containsNull) =>\n+        val arrayWriter = new UnsafeArrayWriter\n+        val elementSize = getElementSize(elementType)\n+        val elementWriter = generateFieldWriter(\n+          bufferHolder,\n+          arrayWriter,\n+          elementType,\n+          containsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          writeArray(bufferHolder, arrayWriter, elementWriter, v.getArray(i), elementSize)\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case MapType(keyType, valueType, valueContainsNull) =>\n+        val keyArrayWriter = new UnsafeArrayWriter\n+        val keySize = getElementSize(keyType)\n+        val keyWriter = generateFieldWriter(\n+          bufferHolder,\n+          keyArrayWriter,\n+          keyType,\n+          nullable = false)\n+        val valueArrayWriter = new UnsafeArrayWriter\n+        val valueSize = getElementSize(valueType)\n+        val valueWriter = generateFieldWriter(\n+          bufferHolder,\n+          valueArrayWriter,\n+          valueType,\n+          valueContainsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getMap(i) match {\n+            case map: UnsafeMapData =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                map.getBaseObject,\n+                map.getBaseOffset,\n+                map.getSizeInBytes)\n+            case map =>\n+              // preserve 8 bytes to write the key array numBytes later.\n+              bufferHolder.grow(8)\n+              bufferHolder.cursor += 8\n+\n+              // Write the keys and write the numBytes of key array into the first 8 bytes.\n+              writeArray(bufferHolder, keyArrayWriter, keyWriter, map.keyArray(), keySize)\n+              Platform.putLong(bufferHolder.buffer, tmpCursor, bufferHolder.cursor - tmpCursor - 8)\n+\n+              // Write the values.\n+              writeArray(bufferHolder, valueArrayWriter, valueWriter, map.valueArray(), valueSize)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case udt: UserDefinedType[_] =>\n+        generateFieldWriter(bufferHolder, writer, udt.sqlType, nullable)\n+\n+      case NullType =>\n+        (_, _) => {}\n+\n+      case _ =>\n+        throw new SparkException(s\"Unsupported data type $dt\")\n+    }\n+\n+    // Always wrap the writer with a null safe version.\n+    dt match {\n+      case _: UserDefinedType[_] =>\n+        // The null wrapper depends on the sql type and not on the UDT.\n+        unsafeWriter\n+      case DecimalType.Fixed(precision, _) if precision > Decimal.MAX_LONG_DIGITS =>\n+        // We can't call setNullAt() for DecimalType with precision larger than 18, we call write\n+        // directly. We can use the unwrapped writer directly.\n+        unsafeWriter\n+      case BooleanType | ByteType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull1Bytes(i)\n+          }\n+        }\n+      case ShortType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull2Bytes(i)\n+          }\n+        }\n+      case IntegerType | DateType | FloatType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull4Bytes(i)\n+          }\n+        }\n+      case _ =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull8Bytes(i)",
    "line": 302
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "or just leave it, as array of null type doesn't make sense and maybe no one will do this.",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-15T18:48:39Z",
    "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: Stateful => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,\n+      fields: Array[StructField]): InternalRow => Unit = {\n+    val numFields = fields.length\n+\n+    // Create field writers.\n+    val fieldWriters = fields.map { field =>\n+      generateFieldWriter(bufferHolder, rowWriter, field.dataType, field.nullable)\n+    }\n+    // Create basic writer.\n+    row => {\n+      var i = 0\n+      while (i < numFields) {\n+        fieldWriters(i).apply(row, i)\n+        i += 1\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Generate a writer function for a struct field, array element, map key or map value. The\n+   * generated function writes the element at an index in a [[SpecializedGetters]] object (row\n+   * or array) to the given buffer using the given [[UnsafeWriter]].\n+   */\n+  private def generateFieldWriter(\n+      bufferHolder: BufferHolder,\n+      writer: UnsafeWriter,\n+      dt: DataType,\n+      nullable: Boolean): (SpecializedGetters, Int) => Unit = {\n+\n+    // Create the the basic writer.\n+    val unsafeWriter: (SpecializedGetters, Int) => Unit = dt match {\n+      case BooleanType =>\n+        (v, i) => writer.write(i, v.getBoolean(i))\n+\n+      case ByteType =>\n+        (v, i) => writer.write(i, v.getByte(i))\n+\n+      case ShortType =>\n+        (v, i) => writer.write(i, v.getShort(i))\n+\n+      case IntegerType | DateType =>\n+        (v, i) => writer.write(i, v.getInt(i))\n+\n+      case LongType | TimestampType =>\n+        (v, i) => writer.write(i, v.getLong(i))\n+\n+      case FloatType =>\n+        (v, i) => writer.write(i, v.getFloat(i))\n+\n+      case DoubleType =>\n+        (v, i) => writer.write(i, v.getDouble(i))\n+\n+      case DecimalType.Fixed(precision, scale) =>\n+        (v, i) => writer.write(i, v.getDecimal(i, precision, scale), precision, scale)\n+\n+      case CalendarIntervalType =>\n+        (v, i) => writer.write(i, v.getInterval(i))\n+\n+      case BinaryType =>\n+        (v, i) => writer.write(i, v.getBinary(i))\n+\n+      case StringType =>\n+        (v, i) => writer.write(i, v.getUTF8String(i))\n+\n+      case StructType(fields) =>\n+        val numFields = fields.length\n+        val rowWriter = new UnsafeRowWriter(bufferHolder, numFields)\n+        val structWriter = generateStructWriter(bufferHolder, rowWriter, fields)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getStruct(i, fields.length) match {\n+            case row: UnsafeRow =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                row.getBaseObject,\n+                row.getBaseOffset,\n+                row.getSizeInBytes)\n+            case row =>\n+              // Nested struct. We don't know where this will start because a row can be\n+              // variable length, so we need to update the offsets and zero out the bit mask.\n+              rowWriter.reset()\n+              structWriter.apply(row)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case ArrayType(elementType, containsNull) =>\n+        val arrayWriter = new UnsafeArrayWriter\n+        val elementSize = getElementSize(elementType)\n+        val elementWriter = generateFieldWriter(\n+          bufferHolder,\n+          arrayWriter,\n+          elementType,\n+          containsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          writeArray(bufferHolder, arrayWriter, elementWriter, v.getArray(i), elementSize)\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case MapType(keyType, valueType, valueContainsNull) =>\n+        val keyArrayWriter = new UnsafeArrayWriter\n+        val keySize = getElementSize(keyType)\n+        val keyWriter = generateFieldWriter(\n+          bufferHolder,\n+          keyArrayWriter,\n+          keyType,\n+          nullable = false)\n+        val valueArrayWriter = new UnsafeArrayWriter\n+        val valueSize = getElementSize(valueType)\n+        val valueWriter = generateFieldWriter(\n+          bufferHolder,\n+          valueArrayWriter,\n+          valueType,\n+          valueContainsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getMap(i) match {\n+            case map: UnsafeMapData =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                map.getBaseObject,\n+                map.getBaseOffset,\n+                map.getSizeInBytes)\n+            case map =>\n+              // preserve 8 bytes to write the key array numBytes later.\n+              bufferHolder.grow(8)\n+              bufferHolder.cursor += 8\n+\n+              // Write the keys and write the numBytes of key array into the first 8 bytes.\n+              writeArray(bufferHolder, keyArrayWriter, keyWriter, map.keyArray(), keySize)\n+              Platform.putLong(bufferHolder.buffer, tmpCursor, bufferHolder.cursor - tmpCursor - 8)\n+\n+              // Write the values.\n+              writeArray(bufferHolder, valueArrayWriter, valueWriter, map.valueArray(), valueSize)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case udt: UserDefinedType[_] =>\n+        generateFieldWriter(bufferHolder, writer, udt.sqlType, nullable)\n+\n+      case NullType =>\n+        (_, _) => {}\n+\n+      case _ =>\n+        throw new SparkException(s\"Unsupported data type $dt\")\n+    }\n+\n+    // Always wrap the writer with a null safe version.\n+    dt match {\n+      case _: UserDefinedType[_] =>\n+        // The null wrapper depends on the sql type and not on the UDT.\n+        unsafeWriter\n+      case DecimalType.Fixed(precision, _) if precision > Decimal.MAX_LONG_DIGITS =>\n+        // We can't call setNullAt() for DecimalType with precision larger than 18, we call write\n+        // directly. We can use the unwrapped writer directly.\n+        unsafeWriter\n+      case BooleanType | ByteType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull1Bytes(i)\n+          }\n+        }\n+      case ShortType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull2Bytes(i)\n+          }\n+        }\n+      case IntegerType | DateType | FloatType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull4Bytes(i)\n+          }\n+        }\n+      case _ =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull8Bytes(i)",
    "line": 302
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "There is a test that actually tests this for arrays: https://github.com/apache/spark/blob/master/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala#L80",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-16T13:06:18Z",
    "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: Stateful => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,\n+      fields: Array[StructField]): InternalRow => Unit = {\n+    val numFields = fields.length\n+\n+    // Create field writers.\n+    val fieldWriters = fields.map { field =>\n+      generateFieldWriter(bufferHolder, rowWriter, field.dataType, field.nullable)\n+    }\n+    // Create basic writer.\n+    row => {\n+      var i = 0\n+      while (i < numFields) {\n+        fieldWriters(i).apply(row, i)\n+        i += 1\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Generate a writer function for a struct field, array element, map key or map value. The\n+   * generated function writes the element at an index in a [[SpecializedGetters]] object (row\n+   * or array) to the given buffer using the given [[UnsafeWriter]].\n+   */\n+  private def generateFieldWriter(\n+      bufferHolder: BufferHolder,\n+      writer: UnsafeWriter,\n+      dt: DataType,\n+      nullable: Boolean): (SpecializedGetters, Int) => Unit = {\n+\n+    // Create the the basic writer.\n+    val unsafeWriter: (SpecializedGetters, Int) => Unit = dt match {\n+      case BooleanType =>\n+        (v, i) => writer.write(i, v.getBoolean(i))\n+\n+      case ByteType =>\n+        (v, i) => writer.write(i, v.getByte(i))\n+\n+      case ShortType =>\n+        (v, i) => writer.write(i, v.getShort(i))\n+\n+      case IntegerType | DateType =>\n+        (v, i) => writer.write(i, v.getInt(i))\n+\n+      case LongType | TimestampType =>\n+        (v, i) => writer.write(i, v.getLong(i))\n+\n+      case FloatType =>\n+        (v, i) => writer.write(i, v.getFloat(i))\n+\n+      case DoubleType =>\n+        (v, i) => writer.write(i, v.getDouble(i))\n+\n+      case DecimalType.Fixed(precision, scale) =>\n+        (v, i) => writer.write(i, v.getDecimal(i, precision, scale), precision, scale)\n+\n+      case CalendarIntervalType =>\n+        (v, i) => writer.write(i, v.getInterval(i))\n+\n+      case BinaryType =>\n+        (v, i) => writer.write(i, v.getBinary(i))\n+\n+      case StringType =>\n+        (v, i) => writer.write(i, v.getUTF8String(i))\n+\n+      case StructType(fields) =>\n+        val numFields = fields.length\n+        val rowWriter = new UnsafeRowWriter(bufferHolder, numFields)\n+        val structWriter = generateStructWriter(bufferHolder, rowWriter, fields)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getStruct(i, fields.length) match {\n+            case row: UnsafeRow =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                row.getBaseObject,\n+                row.getBaseOffset,\n+                row.getSizeInBytes)\n+            case row =>\n+              // Nested struct. We don't know where this will start because a row can be\n+              // variable length, so we need to update the offsets and zero out the bit mask.\n+              rowWriter.reset()\n+              structWriter.apply(row)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case ArrayType(elementType, containsNull) =>\n+        val arrayWriter = new UnsafeArrayWriter\n+        val elementSize = getElementSize(elementType)\n+        val elementWriter = generateFieldWriter(\n+          bufferHolder,\n+          arrayWriter,\n+          elementType,\n+          containsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          writeArray(bufferHolder, arrayWriter, elementWriter, v.getArray(i), elementSize)\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case MapType(keyType, valueType, valueContainsNull) =>\n+        val keyArrayWriter = new UnsafeArrayWriter\n+        val keySize = getElementSize(keyType)\n+        val keyWriter = generateFieldWriter(\n+          bufferHolder,\n+          keyArrayWriter,\n+          keyType,\n+          nullable = false)\n+        val valueArrayWriter = new UnsafeArrayWriter\n+        val valueSize = getElementSize(valueType)\n+        val valueWriter = generateFieldWriter(\n+          bufferHolder,\n+          valueArrayWriter,\n+          valueType,\n+          valueContainsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getMap(i) match {\n+            case map: UnsafeMapData =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                map.getBaseObject,\n+                map.getBaseOffset,\n+                map.getSizeInBytes)\n+            case map =>\n+              // preserve 8 bytes to write the key array numBytes later.\n+              bufferHolder.grow(8)\n+              bufferHolder.cursor += 8\n+\n+              // Write the keys and write the numBytes of key array into the first 8 bytes.\n+              writeArray(bufferHolder, keyArrayWriter, keyWriter, map.keyArray(), keySize)\n+              Platform.putLong(bufferHolder.buffer, tmpCursor, bufferHolder.cursor - tmpCursor - 8)\n+\n+              // Write the values.\n+              writeArray(bufferHolder, valueArrayWriter, valueWriter, map.valueArray(), valueSize)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case udt: UserDefinedType[_] =>\n+        generateFieldWriter(bufferHolder, writer, udt.sqlType, nullable)\n+\n+      case NullType =>\n+        (_, _) => {}\n+\n+      case _ =>\n+        throw new SparkException(s\"Unsupported data type $dt\")\n+    }\n+\n+    // Always wrap the writer with a null safe version.\n+    dt match {\n+      case _: UserDefinedType[_] =>\n+        // The null wrapper depends on the sql type and not on the UDT.\n+        unsafeWriter\n+      case DecimalType.Fixed(precision, _) if precision > Decimal.MAX_LONG_DIGITS =>\n+        // We can't call setNullAt() for DecimalType with precision larger than 18, we call write\n+        // directly. We can use the unwrapped writer directly.\n+        unsafeWriter\n+      case BooleanType | ByteType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull1Bytes(i)\n+          }\n+        }\n+      case ShortType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull2Bytes(i)\n+          }\n+        }\n+      case IntegerType | DateType | FloatType =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull4Bytes(i)\n+          }\n+        }\n+      case _ =>\n+        (v, i) => {\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.setNull8Bytes(i)",
    "line": 302
  }],
  "prId": 20750
}]