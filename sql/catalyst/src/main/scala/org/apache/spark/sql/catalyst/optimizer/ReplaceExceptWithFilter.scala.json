[{
  "comments": [{
    "author": {
      "login": "sathiyapk"
    },
    "body": "Following @rxin comment, i'm leaving it as `IllegalStateException `, please let me know if you have any other suggestions..",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-16T17:27:44Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, Except, Filter, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND a1 <> 5\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+      ).asInstanceOf[Filter].condition\n+\n+      val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+      val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+        attributeNameMap(a.name)\n+      }\n+\n+      Distinct(Filter(Not(transformedCondition), left))\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (left, right: Filter) => nonFilterChild(left).sameResult(nonFilterChild(right))\n+    case _ => false\n+  }\n+\n+  private def nonFilterChild(plan: LogicalPlan) = plan.find(!_.isInstanceOf[Filter]).getOrElse {\n+    throw new IllegalStateException(\"Leaf node is expected\")"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Could you update this?",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-16T23:46:13Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, Except, Filter, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND a1 <> 5"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "This is too restrictive. For example, the children of `EXCEPT` could be `PROJECT`\r\n\r\n> SELECT k, v FROM t1 EXCEPT SELECT k, v FROM t1 where v <> 1\r\n",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-17T18:35:40Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{Distinct, Except, Filter, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+      ).asInstanceOf[Filter].condition\n+\n+      val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+      val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+        attributeNameMap(a.name)\n+      }\n+\n+      Distinct(Filter(Not(transformedCondition), left))\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (left, right: Filter) => nonFilterChild(left).sameResult(nonFilterChild(right))"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Please add a SQLConf `spark.sql.optimizer.replaceExceptWithFilter`. \r\n\r\nThe logics in `apply` will be like\r\n```Scala\r\n    if (!plan.conf.replaceExceptWithFilter) {\r\n      return plan\r\n    }\r\n\r\n    plan.transform {\r\n     ...\r\n```",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T16:05:45Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "You mean to add something like below in the catalyst `org.apache.spark.sql.internal.SQLConf` ?\r\n\r\n```scala\r\nval REPLACE_EXCEPT_WITH_FILTER = buildConf(\"spark.sql.optimizer.replaceExceptWithFilter\")\r\n.doc(\"When true, the apply function of the rule verifies, whether the right node of the except operation is of type Project followed Filter or Filter. If yes, the rule further verifies 1) Excluding the filter operations from the right (as well as the left node, if any) on the top, whether both the nodes evaluates to a same result by using the same project list if the top node is of type project, otherwise node.output. Remember a project list may contain a SubqueryExpression, so it is necessary to check it using projection 2) The filter condition don't contains any SubqueryExpression. If all the conditions are met, the rule will replace the except operation with a Filter operation by flipping the filter condition of the right node.\")\r\n.booleanConf\r\n.createWithDefault(true)\r\n```\r\nand\r\n```scala\r\ndef replaceExceptWithFilter: Boolean = getConf(REPLACE_EXCEPT_WITH_FILTER)\r\n```",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T20:30:02Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "y",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T23:26:17Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Can we remove this case? Adding `Filter` above `Project` is fine. ",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T16:16:52Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "Are you sure? Because, by adding the `Filter` below the `Project`, the two adjacent filters (the flipped filter of the right node and any existing filters of the left node, if any) will be combined to one, further in the optimisation..",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T20:03:32Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Yes. We will have the other rules to do push down and filter combination in the subsequent batches",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T23:18:08Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "You can make a try. If the filters are not pushed down and combined, we can fix the rules.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T23:19:04Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "To filter out the cases of `SubqueryExpression `,  you can simply check \r\n```Scala\r\nSubqueryExpression.hasCorrelatedSubquery(e)\r\n```\r\n\r\n@dilipbiswal Could you take a look at this rule?",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T16:25:26Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "That would be perfect, i'll do that, thanks!",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T19:59:27Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "Also, is there a way to simplify the function something like `val filterConditions = plan.takeWhile(_.isInstanceOf[Filter])` ?",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T20:00:34Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@gatorsmile I have a question. Is the intention to skip Filter nodes containing all kinds of subquery expression (i.e correlated or otherwise) ? If so , then we can't call the hasCorrelatedSubquery method as it only detects the correlated subquery expressions. Can we add a utility method  for this in SubqueryExpression ? We may have future usage for it.\r\n```\r\ndef hasSubquery(e: Expression): Boolean = {\r\n    e.find {\r\n      case _: SubqueryExpression => true\r\n      case _ => false\r\n    }.isDefined\r\n  }```",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T23:08:24Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "I am fine if we do not support any subquery expression for such a rule. \r\n\r\nMaybe you can review the whole rule?",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T23:17:30Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@gatorsmile yeah.. going through it..",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-23T23:23:53Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@sathiyapk I suspect there may be a regression in the following query. I am using the same schema as the one used in except.sql.\r\n```\r\nselect t1.* from t1, t2 where t1.k = t2.k except select t1.* from t1, t2 where t1.k = t2.k and t1.k != 'one'\r\n```\r\nWe are producing more rows using this new rule as we are not applying the distinct on the correct set of columns.\r\nAlso for this test case, in transformCondition function, we are encountering collision in attribute names as both side of join producing (k#,v#, k#, v#).  For this case, can we please check if logic to replace the attribute references is correct ?\r\n@gatorsmile fyi.. please help double check.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T06:50:14Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@gatorsmile @sathiyapk Do you think we  should explicitly specify the return types for all the private defs in the rule ? I just find it more readable. ",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T06:56:53Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "@dilipbiswal well found! the reason for the more rows is in the case 1 of the rule, we put the Filter below the project. As @gatorsmile suggested, we should remove the first case then the Filter will be added above the project, then it will be resolved..",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T15:39:56Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "Regarding the collision in the attribute names, the transformCondition function found the right attributes in this specific example, but i agree we should test it with different corner cases. One more conservative solution to this problem could be to filter out the cases where a projectList contains SubqueryExpression like we do for the Filter condition. In any case, if the function didn't find the right attributes, spark will throw an error..",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T15:48:09Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "And I don't have any problem in explicitly specifying the return types for all the private defs if @gatorsmile is ok with that.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T15:49:56Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Yeah, please follow what @dilipbiswal suggested if the return type can help code readability.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T16:19:09Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "@dilipbiswal One quick update regarding the attribute collision case, actually we didn't encounter any collision in the previous example, because we projected only t1.*. Also it looks like we will never encounter this problem..",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T16:26:38Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@sathiyapk Ok.. glad that its not an issue. Can you please check the content of left.output ? I remember seeing collision.  \r\n```\r\nleft.output.map(x => (x.name, x)).toMap\r\n```\r\nthe toMap operation was removing the collision if i remember it correctly. I just wanted to make sure we are replacing the attributes correctly.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T18:25:52Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "@dilipbiswal You are right! I first removed the first case of the rule and then tested for collision, that's why i didn't find any collision. Without removing the first case of the rule, yes there is a collision. Although it looks sufficient now, in order to be careful about any unanticipated cases, i will also add an additional check `left.output.size == left.output.distinct.size` in the verifyCondition function.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-24T20:55:51Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left: Project, right) if isEligible(left, right) =>\n+      Project(left.projectList,\n+        Distinct(Filter(Not(transformCondition(left.child, skipProject(right))), left.child)))\n+\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan) = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan) = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan) = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+      Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+        Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def verifyFilterCondition(plan: LogicalPlan) = {"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "How about this?\r\n\r\n```Scala\r\n    left.output.size == left.output.distinct.size &&\r\n      left.expressions.forall(!SubqueryExpression.hasSubquery(_)) &&\r\n      right.expressions.forall(!SubqueryExpression.hasSubquery(_)) &&\r\n      Project(projectList(right), nonFilterChild(skipProject(left))).sameResult(\r\n        Project(projectList(right), nonFilterChild(skipProject(right))))\r\n```",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T06:20:36Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Then, `verifyFilterCondition ` and `collectAllExpressions ` can be removed.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T06:21:21Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "For the rule it is enough to test SubqueryExpressions for the top Filter node(s) conditions in the tree. I think `node.expressions` will return all the expressions of the entire tree, which is actually not needed. Correct me if i'm wrong..",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T16:16:42Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "It would be nice if there is a function like `def takeWhile(p: A => Boolean)` in functional programming to substitute this logic in the verifyFilterCondition function:\r\n```scala\r\n    var i = 0\r\n    val filterConditions = new collection.mutable.ArrayBuffer[Expression]\r\n    while (plan.p(i).isInstanceOf[Filter]) {\r\n      val condition = plan.p(i).asInstanceOf[Filter].condition\r\n      filterConditions.insertAll(i, collectAllExpressions(condition))\r\n      i += 1\r\n    }\r\n```  ",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T16:23:27Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "As a second thought, i think it's possible to remove the `collectAllExpressions` function by modifying the `verifyFilterCondition` function like below. But assuming there could be another filter follows, we need to iterate down, so we still need `verifyFilterCondition `.\r\n```scala\r\nprivate def verifyFilterCondition(plan: LogicalPlan): Boolean = {\r\n    var i = 0\r\n    val filterConditions = new collection.mutable.ArrayBuffer[Expression]\r\n    while (plan.p(i).isInstanceOf[Filter]) {\r\n      filterConditions.insertAll(i, plan.p(i).expressions)\r\n      i += 1\r\n    }\r\n    filterConditions.forall(!SubqueryExpression.hasSubquery(_))\r\n  }\r\n```",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T20:12:23Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "And i didn't notice, the `left.output.size == left.output.distinct.size` in the verifyConditions function has to be changed something like \r\n`left.output.size == left.output.map(_.name).distinct.size`",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T20:18:34Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "I missed it. `left.expressions` only returns the expressions of the left operator. It does not include the expressions of the children.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T20:46:16Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Does `left.find(_.expressions.exists(SubqueryExpression.hasSubquery)).isEmpty` work?",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T20:52:30Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "The point here is to avoid the extra complexity. ",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-25T20:54:20Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "@gatorsmile doing `left.find(_.expressions.exists(SubqueryExpression.hasSubquery)).isEmpty` will check for the `SubqueryExpression` for all the children of the left node. But it's enough to do the verification only for the successive Filter node(s). ",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-26T16:07:21Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Let us check all the children of the left node. This will not be an issue for 99% cases and also simplify the codes.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-26T17:14:33Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "That makes sense.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-26T21:12:30Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case Except(left, right) if isEligible(left, right) =>\n+      Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.distinct.size &&\n+      verifyFilterCondition(skipProject(left)) && verifyFilterCondition(skipProject(right)) &&\n+        Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+          Project(rightProjectList, nonFilterChild(skipProject(right))))"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "This can be further extended in the future. Let us leave a TODO mark here.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-27T20:17:55Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!plan.conf.replaceExceptWithFilter) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case Except(left, right) if isEligible(left, right) =>\n+        Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+    }\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "```Scala\r\nfilterCondition.transform { case a: AttributeReference => attributeNameMap(a.name) }\r\n```",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-27T20:19:30Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!plan.conf.replaceExceptWithFilter) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case Except(left, right) if isEligible(left, right) =>\n+        Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+    }\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "```Scala\r\n    val filterCondition =\r\n      InferFiltersFromConstraints(combineFilters(right)).asInstanceOf[Filter].condition\r\n```",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-27T20:20:00Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!plan.conf.replaceExceptWithFilter) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case Except(left, right) if isEligible(left, right) =>\n+        Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+    }\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Also need to leave a comment to explain why we need to call the rule `InferFiltersFromConstraints `",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-27T20:24:14Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!plan.conf.replaceExceptWithFilter) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case Except(left, right) if isEligible(left, right) =>\n+        Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+    }\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "There is a note in the class doc regarding why do we need to call `InferFiltersFromConstraints ` rule. Comment is still needed?",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-27T21:16:53Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!plan.conf.replaceExceptWithFilter) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case Except(left, right) if isEligible(left, right) =>\n+        Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+    }\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition"
  }, {
    "author": {
      "login": "sathiyapk"
    },
    "body": "Better like this? `* 2. Apply InferFiltersFromConstraints rule (to take into account of NULL values in the condition).`",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-27T21:18:27Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!plan.conf.replaceExceptWithFilter) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case Except(left, right) if isEligible(left, right) =>\n+        Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+    }\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition"
  }],
  "prId": 19451
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Remove this empty line.",
    "commit": "40d8797b1750d8a8814dcdc113fa79a3ab086cf2",
    "createdAt": "2017-10-27T20:24:34Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import scala.annotation.tailrec\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+\n+/**\n+ * If one or both of the datasets in the logical [[Except]] operator are purely transformed using\n+ * [[Filter]], this rule will replace logical [[Except]] operator with a [[Filter]] operator by\n+ * flipping the filter condition of the right child.\n+ * {{{\n+ *   SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\n+ *   ==>  SELECT DISTINCT a1, a2 FROM Tab1 WHERE a2 = 12 AND (a1 is null OR a1 <> 5)\n+ * }}}\n+ *\n+ * Note:\n+ * Before flipping the filter condition of the right node, we should:\n+ * 1. Combine all it's [[Filter]].\n+ * 2. Apply InferFiltersFromConstraints rule (to support NULL values of the condition).\n+ */\n+object ReplaceExceptWithFilter extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!plan.conf.replaceExceptWithFilter) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case Except(left, right) if isEligible(left, right) =>\n+        Distinct(Filter(Not(transformCondition(left, skipProject(right))), left))\n+    }\n+  }\n+\n+  private def transformCondition(left: LogicalPlan, right: LogicalPlan): Expression = {\n+    val filterCondition = InferFiltersFromConstraints(combineFilters(right)\n+    ).asInstanceOf[Filter].condition\n+\n+    val attributeNameMap: Map[String, Attribute] = left.output.map(x => (x.name, x)).toMap\n+    val transformedCondition = filterCondition transform { case a : AttributeReference =>\n+      attributeNameMap(a.name)\n+    }\n+\n+    transformedCondition\n+  }\n+\n+  private def isEligible(left: LogicalPlan, right: LogicalPlan): Boolean = (left, right) match {\n+    case (_, right @ (Project(_, _: Filter) | Filter(_, _))) => verifyConditions(left, right)\n+    case _ => false\n+  }\n+\n+  private def verifyConditions(left: LogicalPlan, right: LogicalPlan): Boolean = {\n+    val leftProjectList = projectList(left)\n+    val rightProjectList = projectList(right)\n+\n+    left.output.size == left.output.map(_.name).distinct.size &&\n+      left.find(_.expressions.exists(SubqueryExpression.hasSubquery)).isEmpty &&\n+        right.find(_.expressions.exists(SubqueryExpression.hasSubquery)).isEmpty &&\n+          Project(leftProjectList, nonFilterChild(skipProject(left))).sameResult(\n+            Project(rightProjectList, nonFilterChild(skipProject(right))))\n+  }\n+\n+  private def projectList(node: LogicalPlan): Seq[NamedExpression] = node match {\n+    case p: Project => p.projectList\n+    case x => x.output\n+  }\n+\n+  private def skipProject(node: LogicalPlan): LogicalPlan = node match {\n+    case p: Project => p.child\n+    case x => x\n+  }\n+\n+  private def nonFilterChild(plan: LogicalPlan) = plan.find(!_.isInstanceOf[Filter]).getOrElse {\n+    throw new IllegalStateException(\"Leaf node is expected\")\n+  }\n+\n+  private def combineFilters(plan: LogicalPlan): LogicalPlan = {\n+    @tailrec\n+    def iterate(plan: LogicalPlan, acc: LogicalPlan): LogicalPlan = {\n+      if (acc.fastEquals(plan)) acc else iterate(acc, CombineFilters(acc))\n+    }\n+"
  }],
  "prId": 19451
}]