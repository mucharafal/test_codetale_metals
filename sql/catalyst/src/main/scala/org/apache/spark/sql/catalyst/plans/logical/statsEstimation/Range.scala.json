[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "please explain the 4 outputs in the doc ",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-09T13:25:21Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)\n+  }\n+\n+  def isIntersected(r1: Range, r2: Range): Boolean = (r1, r2) match {\n+    case (_, _: DefaultRange) | (_: DefaultRange, _) =>\n+      // Skip overlapping check for binary/string types\n+      true\n+    case (_, _: NullRange) | (_: NullRange, _) =>\n+      false\n+    case (n1: NumericRange, n2: NumericRange) =>\n+      n1.min.compareTo(n2.max) <= 0 && n1.max.compareTo(n2.min) >= 0\n+  }\n+\n+  /** This is only for two overlapped ranges. */"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "ok I'll improve the doc",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-11T05:06:53Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)\n+  }\n+\n+  def isIntersected(r1: Range, r2: Range): Boolean = (r1, r2) match {\n+    case (_, _: DefaultRange) | (_: DefaultRange, _) =>\n+      // Skip overlapping check for binary/string types\n+      true\n+    case (_, _: NullRange) | (_: NullRange, _) =>\n+      false\n+    case (n1: NumericRange, n2: NumericRange) =>\n+      n1.min.compareTo(n2.max) <= 0 && n1.max.compareTo(n2.min) >= 0\n+  }\n+\n+  /** This is only for two overlapped ranges. */"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we return false to skip overlapping check?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T19:48:20Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)\n+  }\n+\n+  def isIntersected(r1: Range, r2: Range): Boolean = (r1, r2) match {\n+    case (_, _: DefaultRange) | (_: DefaultRange, _) =>\n+      // Skip overlapping check for binary/string types\n+      true"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "let's update the comment to say: `The DefaultRange represents string/binary types which do not have max/min stats, we assume they are intersected to be conservative on estimation`",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T20:46:26Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)\n+  }\n+\n+  def isIntersected(r1: Range, r2: Range): Boolean = (r1, r2) match {\n+    case (_, _: DefaultRange) | (_: DefaultRange, _) =>\n+      // Skip overlapping check for binary/string types\n+      true"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This doesn't work for empty column stats you defined in https://github.com/apache/spark/pull/16228/files#diff-6387e7aaeb7d8e0cb1457b9d0fe5cd00R270\r\n\r\nthat's why I was worried about the empty stats, it breaks some assumptions, like numeric type stats must have max/min.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T21:25:51Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)",
    "line": 46
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "ok, let's remove empty column stats. When we know rowCount=0, we can derive the column stats is empty, we don't need to keep it. what do you think?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-14T01:05:21Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)",
    "line": 46
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "SGTM",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-14T01:11:35Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)",
    "line": 46
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can simplify this, we will only calculate intersection for same-type ranges.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-15T00:09:56Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import java.math.{BigDecimal => JDecimal}\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.types.{BooleanType, DateType, TimestampType, _}\n+\n+\n+/** Value range of a column. */\n+trait Range\n+\n+/** For simplicity we use decimal to unify operations of numeric ranges. */\n+case class NumericRange(min: JDecimal, max: JDecimal) extends Range\n+\n+/**\n+ * This version of Spark does not have min/max for binary/string types, we define their default\n+ * behaviors by this class.\n+ */\n+class DefaultRange extends Range\n+\n+/** This is for columns with only null values. */\n+class NullRange extends Range\n+\n+object Range {\n+  def apply(min: Option[Any], max: Option[Any], dataType: DataType): Range = dataType match {\n+    case StringType | BinaryType => new DefaultRange()\n+    case _ if min.isEmpty || max.isEmpty => new NullRange()\n+    case _ => toNumericRange(min.get, max.get, dataType)\n+  }\n+\n+  def isIntersected(r1: Range, r2: Range): Boolean = (r1, r2) match {\n+    case (_, _: DefaultRange) | (_: DefaultRange, _) =>\n+      // The DefaultRange represents string/binary types which do not have max/min stats,\n+      // we assume they are intersected to be conservative on estimation\n+      true\n+    case (_, _: NullRange) | (_: NullRange, _) =>\n+      false\n+    case (n1: NumericRange, n2: NumericRange) =>\n+      n1.min.compareTo(n2.max) <= 0 && n1.max.compareTo(n2.min) >= 0\n+  }\n+\n+  /**\n+   * Intersected results of two ranges. This is only for two overlapped ranges.\n+   * The outputs are the intersected min/max values of the two columns based on their data types.\n+   */\n+  def intersect(\n+      r1: Range,\n+      r2: Range,\n+      dt1: DataType,\n+      dt2: DataType): (Option[Any], Option[Any], Option[Any], Option[Any]) = {"
  }],
  "prId": 16228
}]