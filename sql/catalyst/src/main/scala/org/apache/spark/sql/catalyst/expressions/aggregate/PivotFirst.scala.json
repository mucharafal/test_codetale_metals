[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "How about we use `inputTypes` to ask the analyzer to do type casting. So, if there is a value column that has an invalid data type, the analyzer will complain.\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T01:45:40Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\"\n+    )\n+  }\n+}\n+\n+case class PivotFirst(pivotColumn: Expression,\n+                      valueColumn: Expression,\n+                      pivotColumnValues: Seq[Any],\n+                      mutableAggBufferOffset: Int = 0,\n+                      inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  val pivotIndex = HashMap(pivotColumnValues.zipWithIndex: _*)\n+\n+  val valueDataType = valueColumn.dataType\n+\n+  val indexSize = pivotIndex.size\n+\n+  private val updateRow: (MutableRow, Int, Any) => Unit = PivotFirst.updateFunction(valueDataType)\n+\n+  override def update(mutableAggBuffer: MutableRow, inputRow: InternalRow): Unit = {\n+    val pivotColValue = pivotColumn.eval(inputRow)\n+    if (pivotColValue != null) {\n+      val index = pivotIndex.getOrElse(pivotColValue, -1)\n+      if (index >= 0) {\n+        val value = valueColumn.eval(inputRow)\n+        if (value != null) {\n+          updateRow(mutableAggBuffer, mutableAggBufferOffset + index, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def merge(mutableAggBuffer: MutableRow, inputAggBuffer: InternalRow): Unit = {\n+    for (i <- 0 until indexSize) {\n+      if (!inputAggBuffer.isNullAt(inputAggBufferOffset + i)) {\n+        val value = inputAggBuffer.get(inputAggBufferOffset + i, valueDataType)\n+        updateRow(mutableAggBuffer, mutableAggBufferOffset + i, value)\n+      }\n+    }\n+  }\n+\n+  override def initialize(mutableAggBuffer: MutableRow): Unit = valueDataType match {\n+    case d: DecimalType =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setDecimal(mutableAggBufferOffset + i, null, d.precision)\n+      }\n+    case _ =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setNullAt(mutableAggBufferOffset + i)\n+      }\n+  }\n+\n+  override def eval(input: InternalRow): Any = {\n+    val result = new Array[Any](indexSize)\n+    for (i <- 0 until indexSize) {\n+      result(i) = input.get(mutableAggBufferOffset + i, valueDataType)\n+    }\n+    new GenericArrayData(result)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+\n+  override lazy val aggBufferAttributes: Seq[AttributeReference] =\n+    pivotIndex.toList.sortBy(_._2).map(kv => AttributeReference(kv._1.toString, valueDataType)())\n+\n+  override lazy val aggBufferSchema: StructType = StructType.fromAttributes(aggBufferAttributes)\n+\n+  override lazy val inputAggBufferAttributes: Seq[AttributeReference] =\n+    aggBufferAttributes.map(_.newInstance())\n+\n+  override lazy val inputTypes: Seq[AbstractDataType] = children.map(_.dataType)"
  }, {
    "author": {
      "login": "aray"
    },
    "body": "I'm not sure what you mean by this, but no casting is needed.\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-04-18T17:31:26Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\"\n+    )\n+  }\n+}\n+\n+case class PivotFirst(pivotColumn: Expression,\n+                      valueColumn: Expression,\n+                      pivotColumnValues: Seq[Any],\n+                      mutableAggBufferOffset: Int = 0,\n+                      inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  val pivotIndex = HashMap(pivotColumnValues.zipWithIndex: _*)\n+\n+  val valueDataType = valueColumn.dataType\n+\n+  val indexSize = pivotIndex.size\n+\n+  private val updateRow: (MutableRow, Int, Any) => Unit = PivotFirst.updateFunction(valueDataType)\n+\n+  override def update(mutableAggBuffer: MutableRow, inputRow: InternalRow): Unit = {\n+    val pivotColValue = pivotColumn.eval(inputRow)\n+    if (pivotColValue != null) {\n+      val index = pivotIndex.getOrElse(pivotColValue, -1)\n+      if (index >= 0) {\n+        val value = valueColumn.eval(inputRow)\n+        if (value != null) {\n+          updateRow(mutableAggBuffer, mutableAggBufferOffset + index, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def merge(mutableAggBuffer: MutableRow, inputAggBuffer: InternalRow): Unit = {\n+    for (i <- 0 until indexSize) {\n+      if (!inputAggBuffer.isNullAt(inputAggBufferOffset + i)) {\n+        val value = inputAggBuffer.get(inputAggBufferOffset + i, valueDataType)\n+        updateRow(mutableAggBuffer, mutableAggBufferOffset + i, value)\n+      }\n+    }\n+  }\n+\n+  override def initialize(mutableAggBuffer: MutableRow): Unit = valueDataType match {\n+    case d: DecimalType =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setDecimal(mutableAggBufferOffset + i, null, d.precision)\n+      }\n+    case _ =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setNullAt(mutableAggBufferOffset + i)\n+      }\n+  }\n+\n+  override def eval(input: InternalRow): Any = {\n+    val result = new Array[Any](indexSize)\n+    for (i <- 0 until indexSize) {\n+      result(i) = input.get(mutableAggBufferOffset + i, valueDataType)\n+    }\n+    new GenericArrayData(result)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+\n+  override lazy val aggBufferAttributes: Seq[AttributeReference] =\n+    pivotIndex.toList.sortBy(_._2).map(kv => AttributeReference(kv._1.toString, valueDataType)())\n+\n+  override lazy val aggBufferSchema: StructType = StructType.fromAttributes(aggBufferAttributes)\n+\n+  override lazy val inputAggBufferAttributes: Seq[AttributeReference] =\n+    aggBufferAttributes.map(_.newInstance())\n+\n+  override lazy val inputTypes: Seq[AbstractDataType] = children.map(_.dataType)"
  }],
  "prId": 11583
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Is there is any data type that works with existing pivot but will not work with this new version?\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T01:46:10Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\""
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Oh, i see. If we have an unsupported data type, we will fall back to the previous code path.\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T01:52:45Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\""
  }],
  "prId": 11583
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Let's have scala doc to explain this function.\n\nAlso, for the format, we can use\n\n```\ncase class PivotFirst(\n  pivotColumn: Expression,\n  valueColumn: Expression,\n  ...)\n```\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T01:46:52Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\"\n+    )\n+  }\n+}\n+\n+case class PivotFirst(pivotColumn: Expression,"
  }],
  "prId": 11583
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "I guess it is better to avoid of try/catch to determine if a data type is supported.\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T01:52:16Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }"
  }],
  "prId": 11583
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Can we add a comment to explain when `index` will be `-1`?\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T02:44:32Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\"\n+    )\n+  }\n+}\n+\n+case class PivotFirst(pivotColumn: Expression,\n+                      valueColumn: Expression,\n+                      pivotColumnValues: Seq[Any],\n+                      mutableAggBufferOffset: Int = 0,\n+                      inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  val pivotIndex = HashMap(pivotColumnValues.zipWithIndex: _*)\n+\n+  val valueDataType = valueColumn.dataType\n+\n+  val indexSize = pivotIndex.size\n+\n+  private val updateRow: (MutableRow, Int, Any) => Unit = PivotFirst.updateFunction(valueDataType)\n+\n+  override def update(mutableAggBuffer: MutableRow, inputRow: InternalRow): Unit = {\n+    val pivotColValue = pivotColumn.eval(inputRow)\n+    if (pivotColValue != null) {\n+      val index = pivotIndex.getOrElse(pivotColValue, -1)\n+      if (index >= 0) {"
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Also, for two different inputRows, we should not get the same index, right?\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T02:45:43Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\"\n+    )\n+  }\n+}\n+\n+case class PivotFirst(pivotColumn: Expression,\n+                      valueColumn: Expression,\n+                      pivotColumnValues: Seq[Any],\n+                      mutableAggBufferOffset: Int = 0,\n+                      inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  val pivotIndex = HashMap(pivotColumnValues.zipWithIndex: _*)\n+\n+  val valueDataType = valueColumn.dataType\n+\n+  val indexSize = pivotIndex.size\n+\n+  private val updateRow: (MutableRow, Int, Any) => Unit = PivotFirst.updateFunction(valueDataType)\n+\n+  override def update(mutableAggBuffer: MutableRow, inputRow: InternalRow): Unit = {\n+    val pivotColValue = pivotColumn.eval(inputRow)\n+    if (pivotColValue != null) {\n+      val index = pivotIndex.getOrElse(pivotColValue, -1)\n+      if (index >= 0) {"
  }],
  "prId": 11583
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Let's add a comment to explain why we need a special care for `DecimalType`.\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T02:46:46Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\"\n+    )\n+  }\n+}\n+\n+case class PivotFirst(pivotColumn: Expression,\n+                      valueColumn: Expression,\n+                      pivotColumnValues: Seq[Any],\n+                      mutableAggBufferOffset: Int = 0,\n+                      inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  val pivotIndex = HashMap(pivotColumnValues.zipWithIndex: _*)\n+\n+  val valueDataType = valueColumn.dataType\n+\n+  val indexSize = pivotIndex.size\n+\n+  private val updateRow: (MutableRow, Int, Any) => Unit = PivotFirst.updateFunction(valueDataType)\n+\n+  override def update(mutableAggBuffer: MutableRow, inputRow: InternalRow): Unit = {\n+    val pivotColValue = pivotColumn.eval(inputRow)\n+    if (pivotColValue != null) {\n+      val index = pivotIndex.getOrElse(pivotColValue, -1)\n+      if (index >= 0) {\n+        val value = valueColumn.eval(inputRow)\n+        if (value != null) {\n+          updateRow(mutableAggBuffer, mutableAggBufferOffset + index, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def merge(mutableAggBuffer: MutableRow, inputAggBuffer: InternalRow): Unit = {\n+    for (i <- 0 until indexSize) {\n+      if (!inputAggBuffer.isNullAt(inputAggBufferOffset + i)) {\n+        val value = inputAggBuffer.get(inputAggBufferOffset + i, valueDataType)\n+        updateRow(mutableAggBuffer, mutableAggBufferOffset + i, value)\n+      }\n+    }\n+  }\n+\n+  override def initialize(mutableAggBuffer: MutableRow): Unit = valueDataType match {\n+    case d: DecimalType =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setDecimal(mutableAggBufferOffset + i, null, d.precision)\n+      }"
  }],
  "prId": 11583
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "(I feel it will be better for readers if we can put `inputTypes`, `nullable`, `dataType`, and `children` at the beginning o the class body. )\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-03-28T02:58:22Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = {\n+    try {\n+      updateFunction(dataType)\n+      true\n+    } catch {\n+      case _: UnsupportedOperationException => false\n+    }\n+  }\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private def updateFunction(dataType: DataType): (MutableRow, Int, Any) => Unit = dataType match {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Unsupported datatype ($dataType) used in PivotFirst, this is a bug.\"\n+    )\n+  }\n+}\n+\n+case class PivotFirst(pivotColumn: Expression,\n+                      valueColumn: Expression,\n+                      pivotColumnValues: Seq[Any],\n+                      mutableAggBufferOffset: Int = 0,\n+                      inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  val pivotIndex = HashMap(pivotColumnValues.zipWithIndex: _*)\n+\n+  val valueDataType = valueColumn.dataType\n+\n+  val indexSize = pivotIndex.size\n+\n+  private val updateRow: (MutableRow, Int, Any) => Unit = PivotFirst.updateFunction(valueDataType)\n+\n+  override def update(mutableAggBuffer: MutableRow, inputRow: InternalRow): Unit = {\n+    val pivotColValue = pivotColumn.eval(inputRow)\n+    if (pivotColValue != null) {\n+      val index = pivotIndex.getOrElse(pivotColValue, -1)\n+      if (index >= 0) {\n+        val value = valueColumn.eval(inputRow)\n+        if (value != null) {\n+          updateRow(mutableAggBuffer, mutableAggBufferOffset + index, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def merge(mutableAggBuffer: MutableRow, inputAggBuffer: InternalRow): Unit = {\n+    for (i <- 0 until indexSize) {\n+      if (!inputAggBuffer.isNullAt(inputAggBufferOffset + i)) {\n+        val value = inputAggBuffer.get(inputAggBufferOffset + i, valueDataType)\n+        updateRow(mutableAggBuffer, mutableAggBufferOffset + i, value)\n+      }\n+    }\n+  }\n+\n+  override def initialize(mutableAggBuffer: MutableRow): Unit = valueDataType match {\n+    case d: DecimalType =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setDecimal(mutableAggBufferOffset + i, null, d.precision)\n+      }\n+    case _ =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setNullAt(mutableAggBufferOffset + i)\n+      }\n+  }\n+\n+  override def eval(input: InternalRow): Any = {\n+    val result = new Array[Any](indexSize)\n+    for (i <- 0 until indexSize) {\n+      result(i) = input.get(mutableAggBufferOffset + i, valueDataType)\n+    }\n+    new GenericArrayData(result)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+\n+  override lazy val aggBufferAttributes: Seq[AttributeReference] =\n+    pivotIndex.toList.sortBy(_._2).map(kv => AttributeReference(kv._1.toString, valueDataType)())\n+\n+  override lazy val aggBufferSchema: StructType = StructType.fromAttributes(aggBufferAttributes)\n+\n+  override lazy val inputAggBufferAttributes: Seq[AttributeReference] =\n+    aggBufferAttributes.map(_.newInstance())\n+\n+  override lazy val inputTypes: Seq[AbstractDataType] = children.map(_.dataType)\n+\n+  override val nullable: Boolean = false\n+\n+  override val dataType: DataType = ArrayType(valueDataType)\n+\n+  override val children: Seq[Expression] = pivotColumn :: valueColumn :: Nil"
  }],
  "prId": 11583
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "How about we avoid of using lazy val for `aggBufferAttributes`, `aggBufferSchema`, and `inputAggBufferAttributes`?\n",
    "commit": "1723046b83a51ba1132992daed807b372300b4e5",
    "createdAt": "2016-05-02T16:38:28Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions.aggregate\n+\n+import scala.collection.immutable.HashMap\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+object PivotFirst {\n+\n+  def supportsDataType(dataType: DataType): Boolean = updateFunction.isDefinedAt(dataType)\n+\n+  // Currently UnsafeRow does not support the generic update method (throws\n+  // UnsupportedOperationException), so we need to explicitly support each DataType.\n+  private val updateFunction: PartialFunction[DataType, (MutableRow, Int, Any) => Unit] = {\n+    case DoubleType =>\n+      (row, offset, value) => row.setDouble(offset, value.asInstanceOf[Double])\n+    case IntegerType =>\n+      (row, offset, value) => row.setInt(offset, value.asInstanceOf[Int])\n+    case LongType =>\n+      (row, offset, value) => row.setLong(offset, value.asInstanceOf[Long])\n+    case FloatType =>\n+      (row, offset, value) => row.setFloat(offset, value.asInstanceOf[Float])\n+    case BooleanType =>\n+      (row, offset, value) => row.setBoolean(offset, value.asInstanceOf[Boolean])\n+    case ShortType =>\n+      (row, offset, value) => row.setShort(offset, value.asInstanceOf[Short])\n+    case ByteType =>\n+      (row, offset, value) => row.setByte(offset, value.asInstanceOf[Byte])\n+    case d: DecimalType =>\n+      (row, offset, value) => row.setDecimal(offset, value.asInstanceOf[Decimal], d.precision)\n+  }\n+}\n+\n+/**\n+ * PivotFirst is a aggregate function used in the second phase of a two phase pivot to do the\n+ * required rearrangement of values into pivoted form.\n+ *\n+ * For example on an input of\n+ * A | B\n+ * --+--\n+ * x | 1\n+ * y | 2\n+ * z | 3\n+ *\n+ * with pivotColumn=A, valueColumn=B, and pivotColumnValues=[z,y] the output is [3,2].\n+ *\n+ * @param pivotColumn column that determines which output position to put valueColumn in.\n+ * @param valueColumn the column that is being rearranged.\n+ * @param pivotColumnValues the list of pivotColumn values in the order of desired output. Values\n+ *                          not listed here will be ignored.\n+ */\n+case class PivotFirst(\n+  pivotColumn: Expression,\n+  valueColumn: Expression,\n+  pivotColumnValues: Seq[Any],\n+  mutableAggBufferOffset: Int = 0,\n+  inputAggBufferOffset: Int = 0) extends ImperativeAggregate {\n+\n+  override val children: Seq[Expression] = pivotColumn :: valueColumn :: Nil\n+\n+  override lazy val inputTypes: Seq[AbstractDataType] = children.map(_.dataType)\n+\n+  override val nullable: Boolean = false\n+\n+  val valueDataType = valueColumn.dataType\n+\n+  override val dataType: DataType = ArrayType(valueDataType)\n+\n+  val pivotIndex = HashMap(pivotColumnValues.zipWithIndex: _*)\n+\n+  val indexSize = pivotIndex.size\n+\n+  private val updateRow: (MutableRow, Int, Any) => Unit = PivotFirst.updateFunction(valueDataType)\n+\n+  override def update(mutableAggBuffer: MutableRow, inputRow: InternalRow): Unit = {\n+    val pivotColValue = pivotColumn.eval(inputRow)\n+    if (pivotColValue != null) {\n+      // We ignore rows whose pivot column value is not in the list of pivot column values.\n+      val index = pivotIndex.getOrElse(pivotColValue, -1)\n+      if (index >= 0) {\n+        val value = valueColumn.eval(inputRow)\n+        if (value != null) {\n+          updateRow(mutableAggBuffer, mutableAggBufferOffset + index, value)\n+        }\n+      }\n+    }\n+  }\n+\n+  override def merge(mutableAggBuffer: MutableRow, inputAggBuffer: InternalRow): Unit = {\n+    for (i <- 0 until indexSize) {\n+      if (!inputAggBuffer.isNullAt(inputAggBufferOffset + i)) {\n+        val value = inputAggBuffer.get(inputAggBufferOffset + i, valueDataType)\n+        updateRow(mutableAggBuffer, mutableAggBufferOffset + i, value)\n+      }\n+    }\n+  }\n+\n+  override def initialize(mutableAggBuffer: MutableRow): Unit = valueDataType match {\n+    case d: DecimalType =>\n+      // Per doc of setDecimal we need to do this instead of setNullAt for DecimalType.\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setDecimal(mutableAggBufferOffset + i, null, d.precision)\n+      }\n+    case _ =>\n+      for (i <- 0 until indexSize) {\n+        mutableAggBuffer.setNullAt(mutableAggBufferOffset + i)\n+      }\n+  }\n+\n+  override def eval(input: InternalRow): Any = {\n+    val result = new Array[Any](indexSize)\n+    for (i <- 0 until indexSize) {\n+      result(i) = input.get(mutableAggBufferOffset + i, valueDataType)\n+    }\n+    new GenericArrayData(result)\n+  }\n+\n+  override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): ImperativeAggregate =\n+    copy(inputAggBufferOffset = newInputAggBufferOffset)\n+\n+  override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): ImperativeAggregate =\n+    copy(mutableAggBufferOffset = newMutableAggBufferOffset)\n+\n+\n+  override lazy val aggBufferAttributes: Seq[AttributeReference] =\n+    pivotIndex.toList.sortBy(_._2).map(kv => AttributeReference(kv._1.toString, valueDataType)())",
    "line": 145
  }],
  "prId": 11583
}]