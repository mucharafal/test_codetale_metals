[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I'd prefer to move the `ResolveCatalogAndTables` cases into this one and create `UnresovledRelation` instances.",
    "commit": "9ad516ea9a0ec7525308c9ca64d36d4069531ae0",
    "createdAt": "2019-09-25T18:53:30Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{DropTable, LogicalPlan, ShowNamespaces, ShowTables}\n+import org.apache.spark.sql.catalyst.plans.logical.sql.{DropTableStatement, DropViewStatement, ShowNamespacesStatement, ShowTablesStatement}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, LookupCatalog}\n+\n+/**\n+ * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements\n+ * to the corresponding v2 commands if the resolved catalog is not the session catalog.\n+ */\n+class ResolveCatalogs(val catalogManager: CatalogManager)",
    "line": 30
  }],
  "prId": 25747
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: in other cases, SQL keywords use all caps to distinguish from text. Should be \"UPDATE TABLE is not ...\"",
    "commit": "9ad516ea9a0ec7525308c9ca64d36d4069531ae0",
    "createdAt": "2019-10-03T17:57:16Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.plans.logical.sql._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, TableChange}\n+\n+/**\n+ * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements\n+ * to the corresponding v2 commands if the resolved catalog is not the session catalog.\n+ */\n+class ResolveCatalogs(val catalogManager: CatalogManager)\n+  extends Rule[LogicalPlan] with LookupCatalog {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case AlterTableAddColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map { col =>\n+        TableChange.addColumn(col.name.toArray, col.dataType, true, col.comment.orNull)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableAlterColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), colName, dataType, comment) =>\n+      val typeChange = dataType.map { newDataType =>\n+        TableChange.updateColumnType(colName.toArray, newDataType, true)\n+      }\n+      val commentChange = comment.map { newComment =>\n+        TableChange.updateColumnComment(colName.toArray, newComment)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, typeChange.toSeq ++ commentChange)\n+\n+    case AlterTableRenameColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), col, newName) =>\n+      val changes = Seq(TableChange.renameColumn(col.toArray, newName))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableDropColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map(col => TableChange.deleteColumn(col.toArray))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), props) =>\n+      val changes = props.map { case (key, value) =>\n+        TableChange.setProperty(key, value)\n+      }.toSeq\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    // TODO: v2 `UNSET TBLPROPERTIES` should respect the ifExists flag.\n+    case AlterTableUnsetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), keys, _) =>\n+      val changes = keys.map(key => TableChange.removeProperty(key))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetLocationStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), newLoc) =>\n+      val changes = Seq(TableChange.setProperty(\"location\", newLoc))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterViewSetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), props) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case AlterViewUnsetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), keys, ifExists) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case DeleteFromStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), tableAlias, condition) =>\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      val aliased = tableAlias.map(SubqueryAlias(_, r)).getOrElse(r)\n+      DeleteFromTable(aliased, condition)\n+\n+    case update: UpdateTableStatement =>\n+      throw new AnalysisException(s\"Update table is not supported temporarily.\")"
  }],
  "prId": 25747
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: should we use the full keyword instead of an abbreviation? \"DESCRIBE TABLE does not ...\"",
    "commit": "9ad516ea9a0ec7525308c9ca64d36d4069531ae0",
    "createdAt": "2019-10-03T18:00:13Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.plans.logical.sql._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, TableChange}\n+\n+/**\n+ * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements\n+ * to the corresponding v2 commands if the resolved catalog is not the session catalog.\n+ */\n+class ResolveCatalogs(val catalogManager: CatalogManager)\n+  extends Rule[LogicalPlan] with LookupCatalog {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case AlterTableAddColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map { col =>\n+        TableChange.addColumn(col.name.toArray, col.dataType, true, col.comment.orNull)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableAlterColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), colName, dataType, comment) =>\n+      val typeChange = dataType.map { newDataType =>\n+        TableChange.updateColumnType(colName.toArray, newDataType, true)\n+      }\n+      val commentChange = comment.map { newComment =>\n+        TableChange.updateColumnComment(colName.toArray, newComment)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, typeChange.toSeq ++ commentChange)\n+\n+    case AlterTableRenameColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), col, newName) =>\n+      val changes = Seq(TableChange.renameColumn(col.toArray, newName))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableDropColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map(col => TableChange.deleteColumn(col.toArray))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), props) =>\n+      val changes = props.map { case (key, value) =>\n+        TableChange.setProperty(key, value)\n+      }.toSeq\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    // TODO: v2 `UNSET TBLPROPERTIES` should respect the ifExists flag.\n+    case AlterTableUnsetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), keys, _) =>\n+      val changes = keys.map(key => TableChange.removeProperty(key))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetLocationStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), newLoc) =>\n+      val changes = Seq(TableChange.setProperty(\"location\", newLoc))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterViewSetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), props) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case AlterViewUnsetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), keys, ifExists) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case DeleteFromStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), tableAlias, condition) =>\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      val aliased = tableAlias.map(SubqueryAlias(_, r)).getOrElse(r)\n+      DeleteFromTable(aliased, condition)\n+\n+    case update: UpdateTableStatement =>\n+      throw new AnalysisException(s\"Update table is not supported temporarily.\")\n+\n+    case DescribeTableStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), partitionSpec, isExtended) =>\n+      if (partitionSpec.nonEmpty) {\n+        throw new AnalysisException(\"DESC TABLE does not support partition for v2 tables.\")"
  }],
  "prId": 25747
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: purge isn't used so it should be `_`.",
    "commit": "9ad516ea9a0ec7525308c9ca64d36d4069531ae0",
    "createdAt": "2019-10-03T22:20:04Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.plans.logical.sql._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, TableChange}\n+\n+/**\n+ * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements\n+ * to the corresponding v2 commands if the resolved catalog is not the session catalog.\n+ */\n+class ResolveCatalogs(val catalogManager: CatalogManager)\n+  extends Rule[LogicalPlan] with LookupCatalog {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case AlterTableAddColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map { col =>\n+        TableChange.addColumn(col.name.toArray, col.dataType, true, col.comment.orNull)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableAlterColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), colName, dataType, comment) =>\n+      val typeChange = dataType.map { newDataType =>\n+        TableChange.updateColumnType(colName.toArray, newDataType, true)\n+      }\n+      val commentChange = comment.map { newComment =>\n+        TableChange.updateColumnComment(colName.toArray, newComment)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, typeChange.toSeq ++ commentChange)\n+\n+    case AlterTableRenameColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), col, newName) =>\n+      val changes = Seq(TableChange.renameColumn(col.toArray, newName))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableDropColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map(col => TableChange.deleteColumn(col.toArray))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), props) =>\n+      val changes = props.map { case (key, value) =>\n+        TableChange.setProperty(key, value)\n+      }.toSeq\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    // TODO: v2 `UNSET TBLPROPERTIES` should respect the ifExists flag.\n+    case AlterTableUnsetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), keys, _) =>\n+      val changes = keys.map(key => TableChange.removeProperty(key))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetLocationStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), newLoc) =>\n+      val changes = Seq(TableChange.setProperty(\"location\", newLoc))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterViewSetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), props) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case AlterViewUnsetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), keys, ifExists) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case DeleteFromStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), tableAlias, condition) =>\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      val aliased = tableAlias.map(SubqueryAlias(_, r)).getOrElse(r)\n+      DeleteFromTable(aliased, condition)\n+\n+    case update: UpdateTableStatement =>\n+      throw new AnalysisException(s\"Update table is not supported temporarily.\")\n+\n+    case DescribeTableStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), partitionSpec, isExtended) =>\n+      if (partitionSpec.nonEmpty) {\n+        throw new AnalysisException(\"DESC TABLE does not support partition for v2 tables.\")\n+      }\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      DescribeTable(r, isExtended)\n+\n+    case DescribeColumnStatement(\n+         NonSessionCatalog(catalog, tableName), colNameParts, isExtended) =>\n+      throw new AnalysisException(\"Describing columns is not supported for v2 tables.\")\n+\n+    case c @ CreateTableStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      CreateV2Table(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        c.tableSchema,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        ignoreIfExists = c.ifNotExists)\n+\n+    case c @ CreateTableAsSelectStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      CreateTableAsSelect(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        c.asSelect,\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        writeOptions = c.options.filterKeys(_ != \"path\"),\n+        ignoreIfExists = c.ifNotExists)\n+\n+    case c @ ReplaceTableStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      ReplaceTable(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        c.tableSchema,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        orCreate = c.orCreate)\n+\n+    case c @ ReplaceTableAsSelectStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      ReplaceTableAsSelect(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        c.asSelect,\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        writeOptions = c.options.filterKeys(_ != \"path\"),\n+        orCreate = c.orCreate)\n+\n+    case DropTableStatement(NonSessionCatalog(catalog, tableName), ifExists, purge) =>"
  }],
  "prId": 25747
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "+1 for this. We may want to move this into the lookup methods and use it as a common extractor pattern. Looks like we probably need to clean those up and simplify after all the planned updates are done.",
    "commit": "9ad516ea9a0ec7525308c9ca64d36d4069531ae0",
    "createdAt": "2019-10-03T22:21:01Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.plans.logical.sql._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, TableChange}\n+\n+/**\n+ * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements\n+ * to the corresponding v2 commands if the resolved catalog is not the session catalog.\n+ */\n+class ResolveCatalogs(val catalogManager: CatalogManager)\n+  extends Rule[LogicalPlan] with LookupCatalog {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case AlterTableAddColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map { col =>\n+        TableChange.addColumn(col.name.toArray, col.dataType, true, col.comment.orNull)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableAlterColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), colName, dataType, comment) =>\n+      val typeChange = dataType.map { newDataType =>\n+        TableChange.updateColumnType(colName.toArray, newDataType, true)\n+      }\n+      val commentChange = comment.map { newComment =>\n+        TableChange.updateColumnComment(colName.toArray, newComment)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, typeChange.toSeq ++ commentChange)\n+\n+    case AlterTableRenameColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), col, newName) =>\n+      val changes = Seq(TableChange.renameColumn(col.toArray, newName))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableDropColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map(col => TableChange.deleteColumn(col.toArray))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), props) =>\n+      val changes = props.map { case (key, value) =>\n+        TableChange.setProperty(key, value)\n+      }.toSeq\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    // TODO: v2 `UNSET TBLPROPERTIES` should respect the ifExists flag.\n+    case AlterTableUnsetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), keys, _) =>\n+      val changes = keys.map(key => TableChange.removeProperty(key))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetLocationStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), newLoc) =>\n+      val changes = Seq(TableChange.setProperty(\"location\", newLoc))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterViewSetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), props) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case AlterViewUnsetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), keys, ifExists) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case DeleteFromStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), tableAlias, condition) =>\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      val aliased = tableAlias.map(SubqueryAlias(_, r)).getOrElse(r)\n+      DeleteFromTable(aliased, condition)\n+\n+    case update: UpdateTableStatement =>\n+      throw new AnalysisException(s\"Update table is not supported temporarily.\")\n+\n+    case DescribeTableStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), partitionSpec, isExtended) =>\n+      if (partitionSpec.nonEmpty) {\n+        throw new AnalysisException(\"DESC TABLE does not support partition for v2 tables.\")\n+      }\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      DescribeTable(r, isExtended)\n+\n+    case DescribeColumnStatement(\n+         NonSessionCatalog(catalog, tableName), colNameParts, isExtended) =>\n+      throw new AnalysisException(\"Describing columns is not supported for v2 tables.\")\n+\n+    case c @ CreateTableStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      CreateV2Table(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        c.tableSchema,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        ignoreIfExists = c.ifNotExists)\n+\n+    case c @ CreateTableAsSelectStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      CreateTableAsSelect(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        c.asSelect,\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        writeOptions = c.options.filterKeys(_ != \"path\"),\n+        ignoreIfExists = c.ifNotExists)\n+\n+    case c @ ReplaceTableStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      ReplaceTable(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        c.tableSchema,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        orCreate = c.orCreate)\n+\n+    case c @ ReplaceTableAsSelectStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      ReplaceTableAsSelect(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        c.asSelect,\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        writeOptions = c.options.filterKeys(_ != \"path\"),\n+        orCreate = c.orCreate)\n+\n+    case DropTableStatement(NonSessionCatalog(catalog, tableName), ifExists, purge) =>\n+      DropTable(catalog.asTableCatalog, tableName.asIdentifier, ifExists)\n+\n+    case DropViewStatement(NonSessionCatalog(catalog, viewName), _) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${viewName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case ShowNamespacesStatement(Some(NonSessionCatalog(catalog, nameParts)), pattern) =>\n+      val namespace = if (nameParts.isEmpty) None else Some(nameParts)\n+      ShowNamespaces(catalog.asNamespaceCatalog, namespace, pattern)\n+\n+    // TODO (SPARK-29014): we should check if the current catalog is not session catalog here.\n+    case ShowNamespacesStatement(None, pattern) if defaultCatalog.isDefined =>\n+      ShowNamespaces(defaultCatalog.get.asNamespaceCatalog, None, pattern)\n+\n+    case ShowTablesStatement(Some(NonSessionCatalog(catalog, nameParts)), pattern) =>\n+      ShowTables(catalog.asTableCatalog, nameParts, pattern)\n+\n+    // TODO (SPARK-29014): we should check if the current catalog is not session catalog here.\n+    case ShowTablesStatement(None, pattern) if defaultCatalog.isDefined =>\n+      ShowTables(defaultCatalog.get.asTableCatalog, catalogManager.currentNamespace, pattern)\n+\n+    case UseStatement(isNamespaceSet, nameParts) =>\n+      if (isNamespaceSet) {\n+        SetCatalogAndNamespace(catalogManager, None, Some(nameParts))\n+      } else {\n+        val CurrentCatalogAndNamespace(catalog, namespace) = nameParts\n+        val ns = if (namespace.isEmpty) { None } else { Some(namespace) }\n+        SetCatalogAndNamespace(catalogManager, Some(catalog.name()), ns)\n+      }\n+  }\n+\n+  object NonSessionCatalog {",
    "line": 193
  }],
  "prId": 25747
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Yes, this should use the current catalog. After `ShowNamespaces` and `ShowDatabases` are merged, we won't need the fallback.",
    "commit": "9ad516ea9a0ec7525308c9ca64d36d4069531ae0",
    "createdAt": "2019-10-03T22:22:23Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.plans.logical.sql._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, TableChange}\n+\n+/**\n+ * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements\n+ * to the corresponding v2 commands if the resolved catalog is not the session catalog.\n+ */\n+class ResolveCatalogs(val catalogManager: CatalogManager)\n+  extends Rule[LogicalPlan] with LookupCatalog {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case AlterTableAddColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map { col =>\n+        TableChange.addColumn(col.name.toArray, col.dataType, true, col.comment.orNull)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableAlterColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), colName, dataType, comment) =>\n+      val typeChange = dataType.map { newDataType =>\n+        TableChange.updateColumnType(colName.toArray, newDataType, true)\n+      }\n+      val commentChange = comment.map { newComment =>\n+        TableChange.updateColumnComment(colName.toArray, newComment)\n+      }\n+      createAlterTable(nameParts, catalog, tableName, typeChange.toSeq ++ commentChange)\n+\n+    case AlterTableRenameColumnStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), col, newName) =>\n+      val changes = Seq(TableChange.renameColumn(col.toArray, newName))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableDropColumnsStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), cols) =>\n+      val changes = cols.map(col => TableChange.deleteColumn(col.toArray))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), props) =>\n+      val changes = props.map { case (key, value) =>\n+        TableChange.setProperty(key, value)\n+      }.toSeq\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    // TODO: v2 `UNSET TBLPROPERTIES` should respect the ifExists flag.\n+    case AlterTableUnsetPropertiesStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), keys, _) =>\n+      val changes = keys.map(key => TableChange.removeProperty(key))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterTableSetLocationStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), newLoc) =>\n+      val changes = Seq(TableChange.setProperty(\"location\", newLoc))\n+      createAlterTable(nameParts, catalog, tableName, changes)\n+\n+    case AlterViewSetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), props) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case AlterViewUnsetPropertiesStatement(\n+         NonSessionCatalog(catalog, tableName), keys, ifExists) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${tableName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case DeleteFromStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), tableAlias, condition) =>\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      val aliased = tableAlias.map(SubqueryAlias(_, r)).getOrElse(r)\n+      DeleteFromTable(aliased, condition)\n+\n+    case update: UpdateTableStatement =>\n+      throw new AnalysisException(s\"Update table is not supported temporarily.\")\n+\n+    case DescribeTableStatement(\n+         nameParts @ NonSessionCatalog(catalog, tableName), partitionSpec, isExtended) =>\n+      if (partitionSpec.nonEmpty) {\n+        throw new AnalysisException(\"DESC TABLE does not support partition for v2 tables.\")\n+      }\n+      val r = UnresolvedV2Table(nameParts, catalog.asTableCatalog, tableName.asIdentifier)\n+      DescribeTable(r, isExtended)\n+\n+    case DescribeColumnStatement(\n+         NonSessionCatalog(catalog, tableName), colNameParts, isExtended) =>\n+      throw new AnalysisException(\"Describing columns is not supported for v2 tables.\")\n+\n+    case c @ CreateTableStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      CreateV2Table(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        c.tableSchema,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        ignoreIfExists = c.ifNotExists)\n+\n+    case c @ CreateTableAsSelectStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      CreateTableAsSelect(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        c.asSelect,\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        writeOptions = c.options.filterKeys(_ != \"path\"),\n+        ignoreIfExists = c.ifNotExists)\n+\n+    case c @ ReplaceTableStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      ReplaceTable(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        c.tableSchema,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        orCreate = c.orCreate)\n+\n+    case c @ ReplaceTableAsSelectStatement(\n+         NonSessionCatalog(catalog, tableName), _, _, _, _, _, _, _, _, _) =>\n+      ReplaceTableAsSelect(\n+        catalog.asTableCatalog,\n+        tableName.asIdentifier,\n+        // convert the bucket spec and add it as a transform\n+        c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+        c.asSelect,\n+        convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+        writeOptions = c.options.filterKeys(_ != \"path\"),\n+        orCreate = c.orCreate)\n+\n+    case DropTableStatement(NonSessionCatalog(catalog, tableName), ifExists, purge) =>\n+      DropTable(catalog.asTableCatalog, tableName.asIdentifier, ifExists)\n+\n+    case DropViewStatement(NonSessionCatalog(catalog, viewName), _) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${viewName.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case ShowNamespacesStatement(Some(NonSessionCatalog(catalog, nameParts)), pattern) =>\n+      val namespace = if (nameParts.isEmpty) None else Some(nameParts)\n+      ShowNamespaces(catalog.asNamespaceCatalog, namespace, pattern)\n+\n+    // TODO (SPARK-29014): we should check if the current catalog is not session catalog here.",
    "line": 172
  }],
  "prId": 25747
}]