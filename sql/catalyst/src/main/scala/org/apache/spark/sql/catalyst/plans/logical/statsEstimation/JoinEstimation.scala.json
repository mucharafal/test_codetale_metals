[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "what does `\\cup` means?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-09T13:06:07Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B."
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "It's a set operation. Please ignore this, because I'll update computing equation for FullOuter.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-11T04:57:37Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B."
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "what's the difference between no column stats and empty column stats? it `rowCount == 0`, we won't look at column stats at all, isn't it?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-09T13:16:28Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): BigDecimal = {\n+\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Get the largest ndv among pairs of join keys\n+          val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+          if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+        } else {\n+          // Set ndvDenom to zero to indicate that this join should have no output\n+          ndvDenom = 0\n+        }\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /** Update column stats for output attributes. */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): AttributeMap[ColumnStat] = {\n+    val outputAttrStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == 0) {\n+      // empty output\n+      attributes.foreach(a => outputAttrStats.put(a, emptyColumnStat(a.dataType)))"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "no column stats means we don't collect stats, while empty column stats means we have the stats and we know after estimation the column has no satisfying data.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-11T05:06:15Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): BigDecimal = {\n+\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Get the largest ndv among pairs of join keys\n+          val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+          if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+        } else {\n+          // Set ndvDenom to zero to indicate that this join should have no output\n+          ndvDenom = 0\n+        }\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /** Update column stats for output attributes. */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): AttributeMap[ColumnStat] = {\n+    val outputAttrStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == 0) {\n+      // empty output\n+      attributes.foreach(a => outputAttrStats.put(a, emptyColumnStat(a.dataType)))"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "in the case of `rowCount == 0`, I think column stats becomes useless, like we don't need column stats for an empty table. Do I miss something?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-11T23:27:30Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): BigDecimal = {\n+\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Get the largest ndv among pairs of join keys\n+          val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+          if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+        } else {\n+          // Set ndvDenom to zero to indicate that this join should have no output\n+          ndvDenom = 0\n+        }\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /** Update column stats for output attributes. */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): AttributeMap[ColumnStat] = {\n+    val outputAttrStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == 0) {\n+      // empty output\n+      attributes.foreach(a => outputAttrStats.put(a, emptyColumnStat(a.dataType)))"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "we may union this plan with the other plan, then empty column stats is useful. Now we don't support union estimation because it's difficult to estimate distinct count after union.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-12T02:43:28Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): BigDecimal = {\n+\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Get the largest ndv among pairs of join keys\n+          val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+          if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+        } else {\n+          // Set ndvDenom to zero to indicate that this join should have no output\n+          ndvDenom = 0\n+        }\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /** Update column stats for output attributes. */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): AttributeMap[ColumnStat] = {\n+    val outputAttrStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == 0) {\n+      // empty output\n+      attributes.foreach(a => outputAttrStats.put(a, emptyColumnStat(a.dataType)))"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "the `nullCount` will be inaccurate after this, right?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-09T13:20:59Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Yes. It's difficult to get a good nullCount estimation after outer joins currently. So in filter, we only use nullCount to estimate if the child is a table.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-11T05:00:01Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can you explain the heuristic used in this method in the doc?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-09T13:22:30Z",
    "diffHunk": "@@ -0,0 +1,314 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): BigDecimal = {\n+\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Get the largest ndv among pairs of join keys\n+          val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+          if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+        } else {\n+          // Set ndvDenom to zero to indicate that this join should have no output\n+          ndvDenom = 0\n+        }\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /** Update column stats for output attributes. */"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "use `collect` instead of `flatMap`",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T20:36:20Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): BigDecimal = {\n+\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Get the largest ndv among pairs of join keys\n+          val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+          if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+        } else {\n+          // Set ndvDenom to zero to indicate that this join should have no output\n+          ndvDenom = 0\n+        }\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /**\n+   * Update column stats for output attributes.\n+   * 1. For empty output, update all column stats to be empty.\n+   * 2. For cartesian product, all values are preserved, so there's no need to change column stats.\n+   * 3. For other cases, a) update max/min of join keys based on their intersected range. b) update\n+   * distinct count of other attributes based on output rows after join.\n+   */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): AttributeMap[ColumnStat] = {\n+    val outputAttrStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == 0) {\n+      // empty output\n+      attributes.foreach(a => outputAttrStats.put(a, emptyColumnStat(a.dataType)))\n+    } else if (outputRows == leftRows * rightRows) {\n+      // Cartesian product, just propagate the original column stats\n+      attributes.foreach(a => outputAttrStats.put(a, oldAttrStats(a)))\n+    } else {\n+      val leftRatio =\n+        if (leftRows != 0) BigDecimal(outputRows) / BigDecimal(leftRows) else BigDecimal(0)\n+      val rightRatio =\n+        if (rightRows != 0) BigDecimal(outputRows) / BigDecimal(rightRows) else BigDecimal(0)\n+      attributes.foreach { a =>\n+        // check if this attribute is a join key\n+        if (joinKeyStats.contains(a)) {\n+          outputAttrStats.put(a, joinKeyStats(a))\n+        } else {\n+          val oldCS = oldAttrStats(a)\n+          val oldNdv = oldCS.distinctCount\n+          // We only change (scale down) the number of distinct values if the number of rows\n+          // decreases after join, because join won't produce new values even if the number of\n+          // rows increases.\n+          val newNdv = if (join.left.outputSet.contains(a) && leftRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * leftRatio)\n+          } else if (join.right.outputSet.contains(a) && rightRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * rightRatio)\n+          } else {\n+            oldNdv\n+          }\n+          // TODO: support nullCount updates for specific outer joins\n+          outputAttrStats.put(a, oldCS.copy(distinctCount = newNdv))\n+        }\n+      }\n+    }\n+    AttributeMap(outputAttrStats.toSeq)\n+  }\n+\n+  /** Update intersected column stats for join keys. */\n+  private def updateIntersectedStats(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): AttributeMap[ColumnStat] = {\n+    val intersectedStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    joinKeyPairs.foreach { case (leftKey, rightKey) =>\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Update intersected column stats\n+          val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+          val (newMin1, newMax1, newMin2, newMax2) =\n+            Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+          intersectedStats.put(leftKey, intersectedColumnStat(leftKeyStats, minNdv,\n+            newMin1, newMax1))\n+          intersectedStats.put(rightKey, intersectedColumnStat(rightKeyStats, minNdv,\n+            newMin2, newMax2))\n+        }\n+      }\n+    }\n+    AttributeMap(intersectedStats.toSeq)\n+  }\n+\n+  private def emptyColumnStat(dataType: DataType): ColumnStat = {\n+    ColumnStat(distinctCount = 0, min = None, max = None, nullCount = 0,\n+      avgLen = dataType.defaultSize, maxLen = dataType.defaultSize)\n+  }\n+\n+  private def intersectedColumnStat(\n+      origin: ColumnStat,\n+      newDistinctCount: BigInt,\n+      newMin: Option[Any],\n+      newMax: Option[Any]): ColumnStat = {\n+    origin.copy(distinctCount = newDistinctCount, min = newMin, max = newMax, nullCount = 0)\n+  }\n+\n+  private def extractJoinKeys(\n+      leftKeys: Seq[Expression],\n+      rightKeys: Seq[Expression]): Seq[(AttributeReference, AttributeReference)] = {\n+    leftKeys.zip(rightKeys).flatMap {"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "outer join will also hit this branch, right?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T20:38:18Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:",
    "line": 159
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "This is only for estimating the inner joined part.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T23:55:25Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:",
    "line": 159
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "look at the code path: `JoinEstimation.estimate` -> `InnerOuterEstimation.doEstimate` -> `joinSelectivity`, outer join will hit this branch.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-14T01:02:59Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:",
    "line": 159
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "yes, I mean it computes the inner join part in the outer join.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-14T01:16:21Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:",
    "line": 159
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `joinedRows`",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T20:48:02Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "what's the difference between `selectivity == 0` and `outputRows == 0`? does it only matter for outer joins?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T21:00:26Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "for outer joins, if selectivity is 0, then the number of output rows is same as the number of left/right side rows. And the column stats should also be same as the left/right side columns, while the other side columns are all null.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T21:03:11Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "let's name it `joinKeyStats`",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T21:13:26Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "yea good point, thanks",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-14T01:03:32Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can check column stats existence here, so that we don't need to do `columnStatsExist((leftStats, leftKey), (rightStats, rightKey))` again and again later.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-13T21:10:02Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerRows) + rightRows.max(innerRows) - innerRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val intersectedStats = if (selectivity == 0) {\n+        AttributeMap[ColumnStat](Nil)\n+      } else {\n+        updateIntersectedStats(joinKeyPairs, leftStats, rightStats)\n+      }\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+      val outputStats: Map[Attribute, ColumnStat] = join.joinType match {\n+        case LeftOuter =>\n+          // Don't update column stats for attributes from left side.\n+          fromLeft.map(a => (a, inputAttrStats(a))).toMap ++\n+            updateAttrStats(outputRows, fromRight, inputAttrStats, intersectedStats)\n+        case RightOuter =>\n+          // Don't update column stats for attributes from right side.\n+          updateAttrStats(outputRows, fromLeft, inputAttrStats, intersectedStats) ++\n+            fromRight.map(a => (a, inputAttrStats(a))).toMap\n+        case FullOuter =>\n+          // Don't update column stats for attributes from both sides.\n+          attributesWithStat.map(a => (a, inputAttrStats(a))).toMap\n+        case _ =>\n+          // Update column stats from both sides for inner or cross join.\n+          updateAttrStats(outputRows, attributesWithStat, inputAttrStats, intersectedStats)\n+      }\n+      val outputAttrStats = AttributeMap(outputStats.toSeq)\n+\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): BigDecimal = {\n+\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Get the largest ndv among pairs of join keys\n+          val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+          if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+        } else {\n+          // Set ndvDenom to zero to indicate that this join should have no output\n+          ndvDenom = 0\n+        }\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /**\n+   * Update column stats for output attributes.\n+   * 1. For empty output, update all column stats to be empty.\n+   * 2. For cartesian product, all values are preserved, so there's no need to change column stats.\n+   * 3. For other cases, a) update max/min of join keys based on their intersected range. b) update\n+   * distinct count of other attributes based on output rows after join.\n+   */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): AttributeMap[ColumnStat] = {\n+    val outputAttrStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == 0) {\n+      // empty output\n+      attributes.foreach(a => outputAttrStats.put(a, emptyColumnStat(a.dataType)))\n+    } else if (outputRows == leftRows * rightRows) {\n+      // Cartesian product, just propagate the original column stats\n+      attributes.foreach(a => outputAttrStats.put(a, oldAttrStats(a)))\n+    } else {\n+      val leftRatio =\n+        if (leftRows != 0) BigDecimal(outputRows) / BigDecimal(leftRows) else BigDecimal(0)\n+      val rightRatio =\n+        if (rightRows != 0) BigDecimal(outputRows) / BigDecimal(rightRows) else BigDecimal(0)\n+      attributes.foreach { a =>\n+        // check if this attribute is a join key\n+        if (joinKeyStats.contains(a)) {\n+          outputAttrStats.put(a, joinKeyStats(a))\n+        } else {\n+          val oldCS = oldAttrStats(a)\n+          val oldNdv = oldCS.distinctCount\n+          // We only change (scale down) the number of distinct values if the number of rows\n+          // decreases after join, because join won't produce new values even if the number of\n+          // rows increases.\n+          val newNdv = if (join.left.outputSet.contains(a) && leftRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * leftRatio)\n+          } else if (join.right.outputSet.contains(a) && rightRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * rightRatio)\n+          } else {\n+            oldNdv\n+          }\n+          // TODO: support nullCount updates for specific outer joins\n+          outputAttrStats.put(a, oldCS.copy(distinctCount = newNdv))\n+        }\n+      }\n+    }\n+    AttributeMap(outputAttrStats.toSeq)\n+  }\n+\n+  /** Update intersected column stats for join keys. */\n+  private def updateIntersectedStats(\n+      joinKeyPairs: Seq[(AttributeReference, AttributeReference)],\n+      leftStats: Statistics,\n+      rightStats: Statistics): AttributeMap[ColumnStat] = {\n+    val intersectedStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    joinKeyPairs.foreach { case (leftKey, rightKey) =>\n+      // Do estimation if we have enough statistics\n+      if (columnStatsExist((leftStats, leftKey), (rightStats, rightKey))) {\n+        // Check if the two sides are disjoint\n+        val leftKeyStats = leftStats.attributeStats(leftKey)\n+        val rightKeyStats = rightStats.attributeStats(rightKey)\n+        val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+        val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+        if (Range.isIntersected(lRange, rRange)) {\n+          // Update intersected column stats\n+          val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+          val (newMin1, newMax1, newMin2, newMax2) =\n+            Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+          intersectedStats.put(leftKey, intersectedColumnStat(leftKeyStats, minNdv,\n+            newMin1, newMax1))\n+          intersectedStats.put(rightKey, intersectedColumnStat(rightKeyStats, minNdv,\n+            newMin2, newMax2))\n+        }\n+      }\n+    }\n+    AttributeMap(intersectedStats.toSeq)\n+  }\n+\n+  private def emptyColumnStat(dataType: DataType): ColumnStat = {\n+    ColumnStat(distinctCount = 0, min = None, max = None, nullCount = 0,\n+      avgLen = dataType.defaultSize, maxLen = dataType.defaultSize)\n+  }\n+\n+  private def intersectedColumnStat(\n+      origin: ColumnStat,\n+      newDistinctCount: BigInt,\n+      newMin: Option[Any],\n+      newMax: Option[Any]): ColumnStat = {\n+    origin.copy(distinctCount = newDistinctCount, min = newMin, max = newMax, nullCount = 0)\n+  }\n+\n+  private def extractJoinKeys(\n+      leftKeys: Seq[Expression],\n+      rightKeys: Seq[Expression]): Seq[(AttributeReference, AttributeReference)] = {\n+    leftKeys.zip(rightKeys).flatMap {\n+      case (lk: AttributeReference, rk: AttributeReference) => Some((lk, rk))"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "logically the join keys should have same column stats, we can write it more explicitly\r\n```\r\nassert(leftKey.dataType.sameType(rightKey.dataType))\r\nval stats = ColumnStats(minNdv, newMin, newMax, nullCount = 0) // and some more logic to update the avg/max length.\r\nintersectedStats.put(leftKey, stats)\r\nintersectedStats.put(rightKey, stats)\r\n```",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-15T00:09:07Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeysWithColStats(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerJoinedRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerJoinedRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerJoinedRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerJoinedRows) + rightRows.max(innerJoinedRows) - innerJoinedRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerJoinedRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+\n+      val outputStats: Seq[(Attribute, ColumnStat)] = if (innerJoinedRows == 0) {\n+        joinType match {\n+          // For outer joins, if the inner join part is empty, the number of output rows is the\n+          // same as that of the outer side. And column stats of join keys from the outer side\n+          // keep unchanged, while column stats of join keys from the other side should be updated\n+          // based on added null values.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              fromRight.map(a => (a, nullColumnStat(a.dataType, leftRows)))\n+          case RightOuter =>\n+            fromRight.map(a => (a, inputAttrStats(a))) ++\n+              fromLeft.map(a => (a, nullColumnStat(a.dataType, rightRows)))\n+          case FullOuter =>\n+            fromLeft.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + rightRows))\n+            } ++ fromRight.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + leftRows))\n+            }\n+          case _ =>\n+            // For inner join, since the output is empty, we don't need to keep column stats.\n+            Nil\n+        }\n+      } else {\n+        val joinKeyStats = getIntersectedStats(joinKeyPairs)\n+        join.joinType match {\n+          // For outer joins, don't update column stats from the outer side.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              updateAttrStats(outputRows, fromRight, inputAttrStats, joinKeyStats)\n+          case RightOuter =>\n+            updateAttrStats(outputRows, fromLeft, inputAttrStats, joinKeyStats) ++\n+              fromRight.map(a => (a, inputAttrStats(a)))\n+          case FullOuter =>\n+            attributesWithStat.map(a => (a, inputAttrStats(a)))\n+          case _ =>\n+            // Update column stats from both sides for inner or cross join.\n+            updateAttrStats(outputRows, attributesWithStat, inputAttrStats, joinKeyStats)\n+        }\n+      }\n+\n+      val outputAttrStats = AttributeMap(outputStats)\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(joinKeyPairs: Seq[(AttributeReference, AttributeReference)]): BigDecimal = {\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Check if the two sides are disjoint\n+      val leftKeyStats = leftStats.attributeStats(leftKey)\n+      val rightKeyStats = rightStats.attributeStats(rightKey)\n+      val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+      val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+      if (Range.isIntersected(lRange, rRange)) {\n+        // Get the largest ndv among pairs of join keys\n+        val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+        if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+      } else {\n+        // Set ndvDenom to zero to indicate that this join should have no output\n+        ndvDenom = 0\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /**\n+   * Propagate or update column stats for output attributes.\n+   * 1. For empty output, we don't need to keep any column stats.\n+   * 2. For cartesian product, all values are preserved, so there's no need to change column stats.\n+   * 3. For other cases, a) update max/min of join keys based on their intersected range. b) update\n+   * distinct count of other attributes based on output rows after join.\n+   */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): Seq[(Attribute, ColumnStat)] = {\n+    val outputAttrStats = new ArrayBuffer[(Attribute, ColumnStat)]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == leftRows * rightRows) {\n+      // Cartesian product, just propagate the original column stats\n+      attributes.foreach(a => outputAttrStats += a -> oldAttrStats(a))\n+    } else if (outputRows != 0) {\n+      val leftRatio =\n+        if (leftRows != 0) BigDecimal(outputRows) / BigDecimal(leftRows) else BigDecimal(0)\n+      val rightRatio =\n+        if (rightRows != 0) BigDecimal(outputRows) / BigDecimal(rightRows) else BigDecimal(0)\n+      attributes.foreach { a =>\n+        // check if this attribute is a join key\n+        if (joinKeyStats.contains(a)) {\n+          outputAttrStats += a -> joinKeyStats(a)\n+        } else {\n+          val oldCS = oldAttrStats(a)\n+          val oldNdv = oldCS.distinctCount\n+          // We only change (scale down) the number of distinct values if the number of rows\n+          // decreases after join, because join won't produce new values even if the number of\n+          // rows increases.\n+          val newNdv = if (join.left.outputSet.contains(a) && leftRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * leftRatio)\n+          } else if (join.right.outputSet.contains(a) && rightRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * rightRatio)\n+          } else {\n+            oldNdv\n+          }\n+          // TODO: support nullCount updates for specific outer joins\n+          outputAttrStats += a -> oldCS.copy(distinctCount = newNdv)\n+        }\n+      }\n+    }\n+    outputAttrStats\n+  }\n+\n+  /** Get intersected column stats for join keys. */\n+  private def getIntersectedStats(joinKeyPairs: Seq[(AttributeReference, AttributeReference)])\n+    : AttributeMap[ColumnStat] = {\n+\n+    val intersectedStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    joinKeyPairs.foreach { case (leftKey, rightKey) =>\n+      // Check if the two sides are disjoint\n+      val leftKeyStats = leftStats.attributeStats(leftKey)\n+      val rightKeyStats = rightStats.attributeStats(rightKey)\n+      val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+      val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+      if (Range.isIntersected(lRange, rRange)) {\n+        // Update intersected column stats\n+        val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+        val (newMin1, newMax1, newMin2, newMax2) =\n+          Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+        intersectedStats.put(leftKey, intersectedColumnStat(leftKeyStats, minNdv,"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this is just `inputAttrStats` right?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-15T00:11:24Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeysWithColStats(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerJoinedRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerJoinedRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerJoinedRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerJoinedRows) + rightRows.max(innerJoinedRows) - innerJoinedRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerJoinedRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+\n+      val outputStats: Seq[(Attribute, ColumnStat)] = if (innerJoinedRows == 0) {\n+        joinType match {\n+          // For outer joins, if the inner join part is empty, the number of output rows is the\n+          // same as that of the outer side. And column stats of join keys from the outer side\n+          // keep unchanged, while column stats of join keys from the other side should be updated\n+          // based on added null values.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              fromRight.map(a => (a, nullColumnStat(a.dataType, leftRows)))\n+          case RightOuter =>\n+            fromRight.map(a => (a, inputAttrStats(a))) ++\n+              fromLeft.map(a => (a, nullColumnStat(a.dataType, rightRows)))\n+          case FullOuter =>\n+            fromLeft.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + rightRows))\n+            } ++ fromRight.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + leftRows))\n+            }\n+          case _ =>\n+            // For inner join, since the output is empty, we don't need to keep column stats.\n+            Nil\n+        }\n+      } else {\n+        val joinKeyStats = getIntersectedStats(joinKeyPairs)\n+        join.joinType match {\n+          // For outer joins, don't update column stats from the outer side.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              updateAttrStats(outputRows, fromRight, inputAttrStats, joinKeyStats)\n+          case RightOuter =>\n+            updateAttrStats(outputRows, fromLeft, inputAttrStats, joinKeyStats) ++\n+              fromRight.map(a => (a, inputAttrStats(a)))\n+          case FullOuter =>\n+            attributesWithStat.map(a => (a, inputAttrStats(a)))"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "when we hit this method, the `outputRows` will never be 0 right?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-15T00:17:44Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeysWithColStats(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerJoinedRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerJoinedRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerJoinedRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerJoinedRows) + rightRows.max(innerJoinedRows) - innerJoinedRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerJoinedRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+\n+      val outputStats: Seq[(Attribute, ColumnStat)] = if (innerJoinedRows == 0) {\n+        joinType match {\n+          // For outer joins, if the inner join part is empty, the number of output rows is the\n+          // same as that of the outer side. And column stats of join keys from the outer side\n+          // keep unchanged, while column stats of join keys from the other side should be updated\n+          // based on added null values.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              fromRight.map(a => (a, nullColumnStat(a.dataType, leftRows)))\n+          case RightOuter =>\n+            fromRight.map(a => (a, inputAttrStats(a))) ++\n+              fromLeft.map(a => (a, nullColumnStat(a.dataType, rightRows)))\n+          case FullOuter =>\n+            fromLeft.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + rightRows))\n+            } ++ fromRight.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + leftRows))\n+            }\n+          case _ =>\n+            // For inner join, since the output is empty, we don't need to keep column stats.\n+            Nil\n+        }\n+      } else {\n+        val joinKeyStats = getIntersectedStats(joinKeyPairs)\n+        join.joinType match {\n+          // For outer joins, don't update column stats from the outer side.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              updateAttrStats(outputRows, fromRight, inputAttrStats, joinKeyStats)\n+          case RightOuter =>\n+            updateAttrStats(outputRows, fromLeft, inputAttrStats, joinKeyStats) ++\n+              fromRight.map(a => (a, inputAttrStats(a)))\n+          case FullOuter =>\n+            attributesWithStat.map(a => (a, inputAttrStats(a)))\n+          case _ =>\n+            // Update column stats from both sides for inner or cross join.\n+            updateAttrStats(outputRows, attributesWithStat, inputAttrStats, joinKeyStats)\n+        }\n+      }\n+\n+      val outputAttrStats = AttributeMap(outputStats)\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(joinKeyPairs: Seq[(AttributeReference, AttributeReference)]): BigDecimal = {\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Check if the two sides are disjoint\n+      val leftKeyStats = leftStats.attributeStats(leftKey)\n+      val rightKeyStats = rightStats.attributeStats(rightKey)\n+      val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+      val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+      if (Range.isIntersected(lRange, rRange)) {\n+        // Get the largest ndv among pairs of join keys\n+        val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+        if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+      } else {\n+        // Set ndvDenom to zero to indicate that this join should have no output\n+        ndvDenom = 0\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /**\n+   * Propagate or update column stats for output attributes.\n+   * 1. For empty output, we don't need to keep any column stats."
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `oldColumnStats`",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-15T00:20:24Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeysWithColStats(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerJoinedRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerJoinedRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerJoinedRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerJoinedRows) + rightRows.max(innerJoinedRows) - innerJoinedRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerJoinedRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+\n+      val outputStats: Seq[(Attribute, ColumnStat)] = if (innerJoinedRows == 0) {\n+        joinType match {\n+          // For outer joins, if the inner join part is empty, the number of output rows is the\n+          // same as that of the outer side. And column stats of join keys from the outer side\n+          // keep unchanged, while column stats of join keys from the other side should be updated\n+          // based on added null values.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              fromRight.map(a => (a, nullColumnStat(a.dataType, leftRows)))\n+          case RightOuter =>\n+            fromRight.map(a => (a, inputAttrStats(a))) ++\n+              fromLeft.map(a => (a, nullColumnStat(a.dataType, rightRows)))\n+          case FullOuter =>\n+            fromLeft.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + rightRows))\n+            } ++ fromRight.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + leftRows))\n+            }\n+          case _ =>\n+            // For inner join, since the output is empty, we don't need to keep column stats.\n+            Nil\n+        }\n+      } else {\n+        val joinKeyStats = getIntersectedStats(joinKeyPairs)\n+        join.joinType match {\n+          // For outer joins, don't update column stats from the outer side.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              updateAttrStats(outputRows, fromRight, inputAttrStats, joinKeyStats)\n+          case RightOuter =>\n+            updateAttrStats(outputRows, fromLeft, inputAttrStats, joinKeyStats) ++\n+              fromRight.map(a => (a, inputAttrStats(a)))\n+          case FullOuter =>\n+            attributesWithStat.map(a => (a, inputAttrStats(a)))\n+          case _ =>\n+            // Update column stats from both sides for inner or cross join.\n+            updateAttrStats(outputRows, attributesWithStat, inputAttrStats, joinKeyStats)\n+        }\n+      }\n+\n+      val outputAttrStats = AttributeMap(outputStats)\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(joinKeyPairs: Seq[(AttributeReference, AttributeReference)]): BigDecimal = {\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Check if the two sides are disjoint\n+      val leftKeyStats = leftStats.attributeStats(leftKey)\n+      val rightKeyStats = rightStats.attributeStats(rightKey)\n+      val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+      val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+      if (Range.isIntersected(lRange, rRange)) {\n+        // Get the largest ndv among pairs of join keys\n+        val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+        if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+      } else {\n+        // Set ndvDenom to zero to indicate that this join should have no output\n+        ndvDenom = 0\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /**\n+   * Propagate or update column stats for output attributes.\n+   * 1. For empty output, we don't need to keep any column stats.\n+   * 2. For cartesian product, all values are preserved, so there's no need to change column stats.\n+   * 3. For other cases, a) update max/min of join keys based on their intersected range. b) update\n+   * distinct count of other attributes based on output rows after join.\n+   */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): Seq[(Attribute, ColumnStat)] = {\n+    val outputAttrStats = new ArrayBuffer[(Attribute, ColumnStat)]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == leftRows * rightRows) {\n+      // Cartesian product, just propagate the original column stats\n+      attributes.foreach(a => outputAttrStats += a -> oldAttrStats(a))\n+    } else if (outputRows != 0) {\n+      val leftRatio =\n+        if (leftRows != 0) BigDecimal(outputRows) / BigDecimal(leftRows) else BigDecimal(0)\n+      val rightRatio =\n+        if (rightRows != 0) BigDecimal(outputRows) / BigDecimal(rightRows) else BigDecimal(0)\n+      attributes.foreach { a =>\n+        // check if this attribute is a join key\n+        if (joinKeyStats.contains(a)) {\n+          outputAttrStats += a -> joinKeyStats(a)\n+        } else {\n+          val oldCS = oldAttrStats(a)"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is it just `inputsAttrStats`?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-15T00:23:25Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeysWithColStats(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerJoinedRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerJoinedRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerJoinedRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerJoinedRows) + rightRows.max(innerJoinedRows) - innerJoinedRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerJoinedRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+\n+      val outputStats: Seq[(Attribute, ColumnStat)] = if (innerJoinedRows == 0) {\n+        joinType match {\n+          // For outer joins, if the inner join part is empty, the number of output rows is the\n+          // same as that of the outer side. And column stats of join keys from the outer side\n+          // keep unchanged, while column stats of join keys from the other side should be updated\n+          // based on added null values.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              fromRight.map(a => (a, nullColumnStat(a.dataType, leftRows)))\n+          case RightOuter =>\n+            fromRight.map(a => (a, inputAttrStats(a))) ++\n+              fromLeft.map(a => (a, nullColumnStat(a.dataType, rightRows)))\n+          case FullOuter =>\n+            fromLeft.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + rightRows))\n+            } ++ fromRight.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + leftRows))\n+            }\n+          case _ =>\n+            // For inner join, since the output is empty, we don't need to keep column stats.\n+            Nil\n+        }\n+      } else {\n+        val joinKeyStats = getIntersectedStats(joinKeyPairs)\n+        join.joinType match {\n+          // For outer joins, don't update column stats from the outer side.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              updateAttrStats(outputRows, fromRight, inputAttrStats, joinKeyStats)\n+          case RightOuter =>\n+            updateAttrStats(outputRows, fromLeft, inputAttrStats, joinKeyStats) ++\n+              fromRight.map(a => (a, inputAttrStats(a)))\n+          case FullOuter =>\n+            attributesWithStat.map(a => (a, inputAttrStats(a)))\n+          case _ =>\n+            // Update column stats from both sides for inner or cross join.\n+            updateAttrStats(outputRows, attributesWithStat, inputAttrStats, joinKeyStats)\n+        }\n+      }\n+\n+      val outputAttrStats = AttributeMap(outputStats)\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is it just `leftStats.attributeStats`?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2017-02-15T00:24:42Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.statsEstimation\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.CatalystConf\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils._\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(conf: CatalystConf, join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(conf, join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(conf, join).doEstimate()\n+      case _ =>\n+        logDebug(s\"[CBO] Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(conf: CatalystConf, join: Join) extends Logging {\n+\n+  private val leftStats = join.left.stats(conf)\n+  private val rightStats = join.right.stats(conf)\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !rowCountsExist(conf, join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val joinKeyPairs = extractJoinKeysWithColStats(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerJoinedRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerJoinedRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerJoinedRows)\n+        case FullOuter =>\n+          // T(A FOJ B) = T(A LOJ B) + T(A ROJ B) - T(A IJ B)\n+          leftRows.max(innerJoinedRows) + rightRows.max(innerJoinedRows) - innerJoinedRows\n+        case _ =>\n+          // Don't change for inner or cross join\n+          innerJoinedRows\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      val attributesWithStat = join.output.filter(a => inputAttrStats.contains(a))\n+      val (fromLeft, fromRight) = attributesWithStat.partition(join.left.outputSet.contains(_))\n+\n+      val outputStats: Seq[(Attribute, ColumnStat)] = if (innerJoinedRows == 0) {\n+        joinType match {\n+          // For outer joins, if the inner join part is empty, the number of output rows is the\n+          // same as that of the outer side. And column stats of join keys from the outer side\n+          // keep unchanged, while column stats of join keys from the other side should be updated\n+          // based on added null values.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              fromRight.map(a => (a, nullColumnStat(a.dataType, leftRows)))\n+          case RightOuter =>\n+            fromRight.map(a => (a, inputAttrStats(a))) ++\n+              fromLeft.map(a => (a, nullColumnStat(a.dataType, rightRows)))\n+          case FullOuter =>\n+            fromLeft.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + rightRows))\n+            } ++ fromRight.map { a =>\n+              val oriColStat = inputAttrStats(a)\n+              (a, oriColStat.copy(nullCount = oriColStat.nullCount + leftRows))\n+            }\n+          case _ =>\n+            // For inner join, since the output is empty, we don't need to keep column stats.\n+            Nil\n+        }\n+      } else {\n+        val joinKeyStats = getIntersectedStats(joinKeyPairs)\n+        join.joinType match {\n+          // For outer joins, don't update column stats from the outer side.\n+          case LeftOuter =>\n+            fromLeft.map(a => (a, inputAttrStats(a))) ++\n+              updateAttrStats(outputRows, fromRight, inputAttrStats, joinKeyStats)\n+          case RightOuter =>\n+            updateAttrStats(outputRows, fromLeft, inputAttrStats, joinKeyStats) ++\n+              fromRight.map(a => (a, inputAttrStats(a)))\n+          case FullOuter =>\n+            attributesWithStat.map(a => (a, inputAttrStats(a)))\n+          case _ =>\n+            // Update column stats from both sides for inner or cross join.\n+            updateAttrStats(outputRows, attributesWithStat, inputAttrStats, joinKeyStats)\n+        }\n+      }\n+\n+      val outputAttrStats = AttributeMap(outputStats)\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+\n+    case _ =>\n+      // When there is no equi-join condition, we do estimation like cartesian product.\n+      val inputAttrStats = AttributeMap(\n+        leftStats.attributeStats.toSeq ++ rightStats.attributeStats.toSeq)\n+      // Propagate the original column stats\n+      val outputAttrStats = getOutputMap(inputAttrStats, join.output)\n+      val outputRows = leftStats.rowCount.get * rightStats.rowCount.get\n+      Some(Statistics(\n+        sizeInBytes = getOutputSize(join.output, outputRows, outputAttrStats),\n+        rowCount = Some(outputRows),\n+        attributeStats = outputAttrStats,\n+        isBroadcastable = false))\n+  }\n+\n+  // scalastyle:off\n+  /**\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   */\n+  // scalastyle:on\n+  def joinSelectivity(joinKeyPairs: Seq[(AttributeReference, AttributeReference)]): BigDecimal = {\n+    var ndvDenom: BigInt = -1\n+    var i = 0\n+    while(i < joinKeyPairs.length && ndvDenom != 0) {\n+      val (leftKey, rightKey) = joinKeyPairs(i)\n+      // Check if the two sides are disjoint\n+      val leftKeyStats = leftStats.attributeStats(leftKey)\n+      val rightKeyStats = rightStats.attributeStats(rightKey)\n+      val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+      val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+      if (Range.isIntersected(lRange, rRange)) {\n+        // Get the largest ndv among pairs of join keys\n+        val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+        if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+      } else {\n+        // Set ndvDenom to zero to indicate that this join should have no output\n+        ndvDenom = 0\n+      }\n+      i += 1\n+    }\n+\n+    if (ndvDenom < 0) {\n+      // There isn't join keys or column stats for any of the join key pairs, we do estimation like\n+      // cartesian product.\n+      1\n+    } else if (ndvDenom == 0) {\n+      // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+      0\n+    } else {\n+      1 / BigDecimal(ndvDenom)\n+    }\n+  }\n+\n+  /**\n+   * Propagate or update column stats for output attributes.\n+   * 1. For empty output, we don't need to keep any column stats.\n+   * 2. For cartesian product, all values are preserved, so there's no need to change column stats.\n+   * 3. For other cases, a) update max/min of join keys based on their intersected range. b) update\n+   * distinct count of other attributes based on output rows after join.\n+   */\n+  private def updateAttrStats(\n+      outputRows: BigInt,\n+      attributes: Seq[Attribute],\n+      oldAttrStats: AttributeMap[ColumnStat],\n+      joinKeyStats: AttributeMap[ColumnStat]): Seq[(Attribute, ColumnStat)] = {\n+    val outputAttrStats = new ArrayBuffer[(Attribute, ColumnStat)]()\n+    val leftRows = leftStats.rowCount.get\n+    val rightRows = rightStats.rowCount.get\n+    if (outputRows == leftRows * rightRows) {\n+      // Cartesian product, just propagate the original column stats\n+      attributes.foreach(a => outputAttrStats += a -> oldAttrStats(a))\n+    } else if (outputRows != 0) {\n+      val leftRatio =\n+        if (leftRows != 0) BigDecimal(outputRows) / BigDecimal(leftRows) else BigDecimal(0)\n+      val rightRatio =\n+        if (rightRows != 0) BigDecimal(outputRows) / BigDecimal(rightRows) else BigDecimal(0)\n+      attributes.foreach { a =>\n+        // check if this attribute is a join key\n+        if (joinKeyStats.contains(a)) {\n+          outputAttrStats += a -> joinKeyStats(a)\n+        } else {\n+          val oldCS = oldAttrStats(a)\n+          val oldNdv = oldCS.distinctCount\n+          // We only change (scale down) the number of distinct values if the number of rows\n+          // decreases after join, because join won't produce new values even if the number of\n+          // rows increases.\n+          val newNdv = if (join.left.outputSet.contains(a) && leftRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * leftRatio)\n+          } else if (join.right.outputSet.contains(a) && rightRatio < 1) {\n+            ceil(BigDecimal(oldNdv) * rightRatio)\n+          } else {\n+            oldNdv\n+          }\n+          // TODO: support nullCount updates for specific outer joins\n+          outputAttrStats += a -> oldCS.copy(distinctCount = newNdv)\n+        }\n+      }\n+    }\n+    outputAttrStats\n+  }\n+\n+  /** Get intersected column stats for join keys. */\n+  private def getIntersectedStats(joinKeyPairs: Seq[(AttributeReference, AttributeReference)])\n+    : AttributeMap[ColumnStat] = {\n+\n+    val intersectedStats = new mutable.HashMap[Attribute, ColumnStat]()\n+    joinKeyPairs.foreach { case (leftKey, rightKey) =>\n+      // Check if the two sides are disjoint\n+      val leftKeyStats = leftStats.attributeStats(leftKey)\n+      val rightKeyStats = rightStats.attributeStats(rightKey)\n+      val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+      val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+      if (Range.isIntersected(lRange, rRange)) {\n+        // Update intersected column stats\n+        val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+        val (newMin1, newMax1, newMin2, newMax2) =\n+          Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+        intersectedStats.put(leftKey, intersectedColumnStat(leftKeyStats, minNdv,\n+          newMin1, newMax1))\n+        intersectedStats.put(rightKey, intersectedColumnStat(rightKeyStats, minNdv,\n+          newMin2, newMax2))\n+      }\n+    }\n+    AttributeMap(intersectedStats.toSeq)\n+  }\n+\n+  private def intersectedColumnStat(\n+      origin: ColumnStat,\n+      newDistinctCount: BigInt,\n+      newMin: Option[Any],\n+      newMax: Option[Any]): ColumnStat = {\n+    origin.copy(distinctCount = newDistinctCount, min = newMin, max = newMax, nullCount = 0)\n+  }\n+\n+  private def extractJoinKeysWithColStats(\n+      leftKeys: Seq[Expression],\n+      rightKeys: Seq[Expression]): Seq[(AttributeReference, AttributeReference)] = {\n+    leftKeys.zip(rightKeys).collect {\n+      // Currently we don't deal with equal joins like key1 = key2 + 5.\n+      // Note: join keys from EqualNullSafe also fall into this case (Coalesce), consider to\n+      // support it in the future by using `nullCount` in column stats.\n+      case (lk: AttributeReference, rk: AttributeReference)\n+        if columnStatsExist((leftStats, lk), (rightStats, rk)) => (lk, rk)\n+    }\n+  }\n+}\n+\n+case class LeftSemiAntiEstimation(conf: CatalystConf, join: Join) {\n+  def doEstimate(): Option[Statistics] = {\n+    // TODO: It's error-prone to estimate cardinalities for LeftSemi and LeftAnti based on basic\n+    // column stats. Now we just propagate the statistics from left side. We should do more\n+    // accurate estimation when advanced stats (e.g. histograms) are available.\n+    if (rowCountsExist(conf, join.left)) {\n+      val leftStats = join.left.stats(conf)\n+      // Propagate the original column stats for cartesian product\n+      val outputAttrStats = getOutputMap(leftStats.attributeStats, join.output)"
  }],
  "prId": 16228
}]