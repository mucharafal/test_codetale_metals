[{
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Note to self: Group By and Having clause should not be pipelined, a user is allowed to define a global aggregate and filter that...\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-03-07T10:29:20Z",
    "diffHunk": "@@ -0,0 +1,1128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ParseUtils\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.trees.CurrentOrigin\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import AstBuilder._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          notSupported(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(unquote(pattern.getText)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }.toMap\n+      With(query, ctes)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryOrganization).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryOrganization).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).toSeq.flatMap {\n+      _.partitionVal.asScala.map {\n+        pVal => (pVal.identifier.getText, Option(pVal.constant).map(c => unquote(c.getText)))\n+      }\n+    }.toMap\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan.\n+   */\n+  private def withQueryOrganization(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      notSupported(\"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // LIMIT\n+    val withLimit = withOrder.optional(limit) {\n+      Limit(typedVisit(limit), withOrder)\n+    }\n+\n+    // WINDOWS\n+    withLimit.optionalMap(windows)(withWindows)\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    val withFilter = relation.optional(where) {\n+      Filter(expression(where), relation)\n+    }\n+\n+    // Expressions.\n+    val expressions = namedExpression.asScala.map(visit).map {\n+      case e: Expression => UnresolvedAlias(e)\n+    }\n+\n+    // Create either a transform or a regular query.\n+    kind.getType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Create the attributes.\n+        val attributes = if (colTypeList != null) {\n+          // Typed return columns.\n+          visitColTypeList(colTypeList).toAttributes\n+        } else if (columnAliasList != null) {\n+          // Untyped return columns.\n+          visitColumnAliasList(columnAliasList).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          unquote(script.getText),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, outRowFormat, outRecordReader))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(withFilter)(withGenerate)\n+\n+        // Add aggregation with having or a project.\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, expressions, withLateralView).optionalMap(having) {\n+            case (h, p) => Filter(expression(h), p)"
  }],
  "prId": 11557
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "We should flatten OR/AND's here, see: https://github.com/apache/spark/pull/11501\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-03-07T10:35:06Z",
    "diffHunk": "@@ -0,0 +1,1128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ParseUtils\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.trees.CurrentOrigin\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import AstBuilder._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          notSupported(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(unquote(pattern.getText)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }.toMap\n+      With(query, ctes)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryOrganization).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryOrganization).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).toSeq.flatMap {\n+      _.partitionVal.asScala.map {\n+        pVal => (pVal.identifier.getText, Option(pVal.constant).map(c => unquote(c.getText)))\n+      }\n+    }.toMap\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan.\n+   */\n+  private def withQueryOrganization(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      notSupported(\"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // LIMIT\n+    val withLimit = withOrder.optional(limit) {\n+      Limit(typedVisit(limit), withOrder)\n+    }\n+\n+    // WINDOWS\n+    withLimit.optionalMap(windows)(withWindows)\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    val withFilter = relation.optional(where) {\n+      Filter(expression(where), relation)\n+    }\n+\n+    // Expressions.\n+    val expressions = namedExpression.asScala.map(visit).map {\n+      case e: Expression => UnresolvedAlias(e)\n+    }\n+\n+    // Create either a transform or a regular query.\n+    kind.getType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Create the attributes.\n+        val attributes = if (colTypeList != null) {\n+          // Typed return columns.\n+          visitColTypeList(colTypeList).toAttributes\n+        } else if (columnAliasList != null) {\n+          // Untyped return columns.\n+          visitColumnAliasList(columnAliasList).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          unquote(script.getText),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, outRowFormat, outRecordReader))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(withFilter)(withGenerate)\n+\n+        // Add aggregation with having or a project.\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, expressions, withLateralView).optionalMap(having) {\n+            case (h, p) => Filter(expression(h), p)\n+          }\n+        } else {\n+          Project(expressions, withLateralView)\n+        }\n+\n+        // Distinct\n+        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n+          Distinct(withProject)\n+        } else {\n+          withProject\n+        }\n+\n+        // Window\n+        withDistinct.optionalMap(windows)(withWindows)\n+    }\n+  }\n+\n+  /**\n+   * Create a (Hive based) [[ScriptInputOutputSchema]].\n+   */\n+  protected def withScriptIOSchema(\n+      inRowFormat: RowFormatContext,\n+      outRowFormat: RowFormatContext,\n+      outRecordReader: Token): ScriptInputOutputSchema = null\n+\n+  /**\n+   * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma\n+   * separated) relations here, these get converted into a single plan by condition-less inner join.\n+   */\n+  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n+    ctx.relation.asScala.map(plan).reduceLeft(Join(_, _, Inner, None))\n+  }\n+\n+  /**\n+   * Connect two queries by a Set operator.\n+   *\n+   * Supported Set operators are:\n+   * - UNION [DISTINCT]\n+   * - UNION ALL\n+   * - EXCEPT [DISTINCT]\n+   * - INTERSECT [DISTINCT]\n+   */\n+  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) {\n+    val left = plan(ctx.left)\n+    val right = plan(ctx.right)\n+    val all = Option(ctx.setQuantifier()).exists(_.ALL != null)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.UNION if all =>\n+        Union(left, right)\n+      case SqlBaseParser.UNION =>\n+        Distinct(Union(left, right))\n+      case SqlBaseParser.INTERSECT if all =>\n+        notSupported(\"INTERSECT ALL is not supported.\", ctx)\n+      case SqlBaseParser.INTERSECT =>\n+        Intersect(left, right)\n+      case SqlBaseParser.EXCEPT if all =>\n+        notSupported(\"EXCEPT ALL is not supported.\", ctx)\n+      case SqlBaseParser.EXCEPT =>\n+        Except(left, right)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[WithWindowDefinition]] operator to a logical plan.\n+   */\n+  private def withWindows(\n+      ctx: WindowsContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Collect all window specifications defined in the WINDOW clause.\n+    val baseWindowMap = ctx.namedWindow.asScala.map {\n+      wCtx =>\n+        (wCtx.identifier.getText, typedVisit[WindowSpec](wCtx.windowSpec))\n+    }.toMap\n+\n+    // Handle cases like\n+    // window w1 as (partition by p_mfgr order by p_name\n+    //               range between 2 preceding and 2 following),\n+    //        w2 as w1\n+    val windowMap = baseWindowMap.mapValues {\n+      case WindowSpecReference(name) => baseWindowMap(name).asInstanceOf[WindowSpecDefinition]\n+      case spec: WindowSpecDefinition => spec\n+    }\n+    WithWindowDefinition(windowMap, query)\n+  }\n+\n+  /**\n+   * Add an [[Aggregate]] to a logical plan.\n+   */\n+  private def withAggregation(\n+      ctx: AggregationContext,\n+      selectExpressions: Seq[NamedExpression],\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    val groupByExpressions = expressionList(groupingExpressions)\n+\n+    if (GROUPING != null) {\n+      // GROUP BY .... GROUPING SETS (...)\n+      // TODO use new expression set here?\n+      val expressionMap = groupByExpressions.zipWithIndex.toMap\n+      val numExpressions = expressionMap.size\n+      val mask = (1 << numExpressions) - 1\n+      val masks = ctx.groupingSet.asScala.map {\n+        _.expression.asScala.foldLeft(mask) {\n+          case (bitmap, eCtx) =>\n+            // Find the index of the expression.\n+            val e = typedVisit[Expression](eCtx)\n+            val index = expressionMap.find(_._1.semanticEquals(e)).map(_._2).getOrElse(\n+              throw new AnalysisException(\n+                s\"${e.treeString} doesn't show up in the GROUP BY list\"))\n+            // 0 means that the column at the given index is a grouping column, 1 means it is not,\n+            // so we unset the bit in bitmap.\n+            bitmap & ~(1 << (numExpressions - 1 - index))\n+        }\n+      }\n+      GroupingSets(masks, groupByExpressions, query, selectExpressions)\n+    } else {\n+      // GROUP BY .... (WITH CUBE | WITH ROLLUP)?\n+      val mappedGroupByExpressions = if (CUBE != null) {\n+        Seq(Cube(groupByExpressions))\n+      } else if (ROLLUP != null) {\n+        Seq(Rollup(groupByExpressions))\n+      } else {\n+        groupByExpressions\n+      }\n+      Aggregate(mappedGroupByExpressions, selectExpressions, query)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[Generate]] (Lateral View) to a logical plan.\n+   */\n+  private def withGenerate(\n+      query: LogicalPlan,\n+      ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) {\n+    val expressions = expressionList(ctx.expression)\n+\n+    // Create the generator.\n+    // TODO Add support for other generators.\n+    val generator = ctx.qualifiedName.getText.toLowerCase match {\n+      case \"explode\" if expressions.size == 1 =>\n+        Explode(expressions.head)\n+      case \"json_tuple\" =>\n+        JsonTuple(expressions)\n+      case other =>\n+        notSupported(s\"Generator function '$other' is not supported\", ctx)\n+    }\n+\n+    Generate(\n+      generator,\n+      join = true,\n+      outer = ctx.OUTER != null,\n+      Some(ctx.tblName.getText.toLowerCase),\n+      ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.apply),\n+      query)\n+  }\n+\n+  /**\n+   * Create a join between two logical plans.\n+   */\n+  override def visitJoinRelation(ctx: JoinRelationContext): LogicalPlan = withOrigin(ctx) {\n+    val baseJoinType = ctx.joinType match {\n+      case jt if jt.FULL != null => FullOuter\n+      case jt if jt.SEMI != null => LeftSemi\n+      case jt if jt.LEFT != null => LeftOuter\n+      case jt if jt.RIGHT != null => RightOuter\n+      case _ => Inner\n+    }\n+    val joinType = if (ctx.NATURAL != null) {\n+      NaturalJoin(baseJoinType)\n+    } else {\n+      baseJoinType\n+    }\n+\n+    val left = plan(ctx.left)\n+    val right = if (ctx.right != null) {\n+      plan(ctx.right)\n+    } else {\n+      plan(ctx.rightRelation)\n+    }\n+    assert(left != null, \"Left side should not be null\", ctx)\n+    assert(right != null, \"Right side should not be null\", ctx)\n+    Join(left, right, joinType, Option(ctx.booleanExpression).map(expression))\n+  }\n+\n+  /**\n+   * Create a sampled relation. This returns a [[Sample]] operator when sampling is requested.\n+   *\n+   * This currently supports the following sampling methods:\n+   * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.\n+   * - TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages\n+   * are defined as a number between 0 and 100.\n+   * - TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a 'x' divided by 'y' fraction.\n+   */\n+  override def visitSampledRelation(ctx: SampledRelationContext): LogicalPlan = withOrigin(ctx) {\n+    val relation = plan(ctx.relationPrimary)\n+\n+    // Create a sampled plan if we need one.\n+    def sample(fraction: Double): Sample = {\n+      Sample(0.0, fraction, withReplacement = false, (math.random * 1000).toInt, relation)(true)\n+    }\n+\n+    // Sample the relation if we have to.\n+    relation.optional(ctx.sampleType) {\n+      ctx.sampleType.getType match {\n+        case SqlBaseParser.ROWS =>\n+          Limit(expression(ctx.expression), relation)\n+\n+        case SqlBaseParser.PERCENTLIT =>\n+          val fraction = ctx.percentage.getText.toDouble\n+          // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling\n+          // function takes X PERCENT as the input and the range of X is [0, 100], we need to\n+          // adjust the fraction.\n+          val eps = RandomSampler.roundingEpsilon\n+          require(fraction >= 0.0 - eps && fraction <= 100.0 + eps,\n+            s\"Sampling fraction ($fraction) must be on interval [0, 100]\")\n+          sample(fraction / 100.0d)\n+\n+        case SqlBaseParser.BUCKET if ctx.ON != null =>\n+          notSupported(\"TABLESAMPLE(BUCKET x OUT OF y ON id) is not supported\", ctx)\n+\n+        case SqlBaseParser.BUCKET =>\n+          sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a sub-query.\n+   */\n+  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryNoWith)\n+  }\n+\n+  /**\n+   * Create an un-aliased table reference. This is typically used for top-level table references,\n+   * for example:\n+   * {{{\n+   *   INSERT INTO db.tbl2\n+   *   TABLE db.tbl1\n+   * }}}\n+   */\n+  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(visitTableIdentifier(ctx.tableIdentifier), None)\n+  }\n+\n+  /**\n+   * Create an aliased table reference. This is typically used in FROM clauses.\n+   */\n+  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(\n+      visitTableIdentifier(ctx.tableIdentifier),\n+      Option(ctx.identifier).map(_.getText))\n+  }\n+\n+  /**\n+   * Create an inline table (a virtual table in Hive parlance).\n+   */\n+  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) {\n+    // Get the backing expressions.\n+    val expressions = ctx.expression.asScala.map(expression)\n+\n+    // Validate and evaluate the rows.\n+    val (structType, structConstructor) = expressions.head.dataType match {\n+      case st: StructType =>\n+        (st, (e: Expression) => e)\n+      case dt =>\n+        val st = CreateStruct(Seq(expressions.head)).dataType\n+        (st, (e: Expression) => CreateStruct(Seq(e)))\n+    }\n+    val rows = expressions.map {\n+      case expression =>\n+        assert(expression.foldable, \"All expressions in an inline table must be constants.\", ctx)\n+        val safe = Cast(structConstructor(expression), structType)\n+        safe.eval().asInstanceOf[InternalRow]\n+    }\n+\n+    // Construct attributes.\n+    val baseAttributes = structType.toAttributes\n+    val attributes = if (ctx.columnAliases != null) {\n+      val aliases = visitColumnAliases(ctx.columnAliases)\n+      assert(aliases.size == baseAttributes.size,\n+        \"Number of aliases must match the number of fields in an inline table.\", ctx)\n+      baseAttributes.zip(aliases).map(p => p._1.withName(p._2))\n+    } else {\n+      baseAttributes\n+    }\n+\n+    LocalRelation(attributes, rows)\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a join relation. This is practically the same as\n+   * visitAliasedQuery and visitNamedExpression, ANTLR4 however requires us to use 3 different\n+   * hooks.\n+   */\n+  override def visitAliasedRelation(ctx: AliasedRelationContext): LogicalPlan = withOrigin(ctx) {\n+    aliasPlan(ctx.identifier, plan(ctx.relation()))\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a sub-query. This is practically the same as\n+   * visitAliasedRelation and visitNamedExpression, ANTLR4 however requires us to use 3 different\n+   * hooks.\n+   */\n+  override def visitAliasedQuery(ctx: AliasedQueryContext): LogicalPlan = withOrigin(ctx) {\n+    aliasPlan(ctx.identifier, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a LogicalPlan. The alias is allowed to be optional.\n+   */\n+  private def aliasPlan(alias: IdentifierContext, plan: LogicalPlan): LogicalPlan = {\n+    plan.optional(alias) {\n+      SubqueryAlias(alias.getText, plan)\n+    }\n+  }\n+\n+  /**\n+   * Create a Sequence of Strings for a parenthesis enclosed alias list.\n+   */\n+  override def visitColumnAliases(ctx: ColumnAliasesContext): Seq[String] = withOrigin(ctx) {\n+    visitColumnAliasList(ctx.columnAliasList)\n+  }\n+\n+  /**\n+   * Create a Sequence of Strings for an alias list.\n+   */\n+  override def visitColumnAliasList(ctx: ColumnAliasListContext): Seq[String] = withOrigin(ctx) {\n+    ctx.identifier.asScala.map(_.getText)\n+  }\n+\n+  /**\n+   * Create a [[TableIdentifier]] from a 'tableName' or 'databaseName'.'tableName' pattern.\n+   */\n+  override def visitTableIdentifier(\n+      ctx: TableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    TableIdentifier(ctx.table.getText, Option(ctx.db).map(_.getText))\n+  }\n+\n+  /* ********************************************************************************************\n+   * Expression parsing\n+   * ******************************************************************************************** */\n+  private def expression(tree: ParserRuleContext): Expression = typedVisit(tree)\n+\n+  private def expressionList(trees: java.util.List[ExpressionContext]): Seq[Expression] = {\n+    trees.asScala.map(expression)\n+  }\n+\n+  private def invertIfNotDefined(expression: Expression, not: TerminalNode): Expression = {\n+    if (not != null) {\n+      Not(expression)\n+    } else {\n+      expression\n+    }\n+  }\n+\n+  override def visitStar(ctx: StarContext): Expression = withOrigin(ctx) {\n+    UnresolvedStar(Option(ctx.qualifiedName()).map(_.identifier.asScala.map(_.getText)))\n+  }\n+\n+  override def visitNamedExpression(ctx: NamedExpressionContext): Expression = withOrigin(ctx) {\n+    val e = expression(ctx.expression)\n+    if (ctx.identifier != null) {\n+      Alias(e, ctx.identifier.getText)()\n+    } else if (ctx.columnAliases != null) {\n+      MultiAlias(e, visitColumnAliases(ctx.columnAliases))\n+    } else {\n+      e\n+    }\n+  }\n+\n+  override def visitLogicalBinary(ctx: LogicalBinaryContext): Expression = withOrigin(ctx) {\n+    val left = expression(ctx.left)\n+    val right = expression(ctx.right)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.AND =>\n+        And(left, right)\n+      case SqlBaseParser.OR =>\n+        Or(left, right)\n+    }"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "That one's now merged\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-03-08T19:54:16Z",
    "diffHunk": "@@ -0,0 +1,1128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ParseUtils\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.trees.CurrentOrigin\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import AstBuilder._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          notSupported(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(unquote(pattern.getText)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }.toMap\n+      With(query, ctes)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryOrganization).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryOrganization).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).toSeq.flatMap {\n+      _.partitionVal.asScala.map {\n+        pVal => (pVal.identifier.getText, Option(pVal.constant).map(c => unquote(c.getText)))\n+      }\n+    }.toMap\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan.\n+   */\n+  private def withQueryOrganization(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      notSupported(\"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // LIMIT\n+    val withLimit = withOrder.optional(limit) {\n+      Limit(typedVisit(limit), withOrder)\n+    }\n+\n+    // WINDOWS\n+    withLimit.optionalMap(windows)(withWindows)\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    val withFilter = relation.optional(where) {\n+      Filter(expression(where), relation)\n+    }\n+\n+    // Expressions.\n+    val expressions = namedExpression.asScala.map(visit).map {\n+      case e: Expression => UnresolvedAlias(e)\n+    }\n+\n+    // Create either a transform or a regular query.\n+    kind.getType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Create the attributes.\n+        val attributes = if (colTypeList != null) {\n+          // Typed return columns.\n+          visitColTypeList(colTypeList).toAttributes\n+        } else if (columnAliasList != null) {\n+          // Untyped return columns.\n+          visitColumnAliasList(columnAliasList).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          unquote(script.getText),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, outRowFormat, outRecordReader))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(withFilter)(withGenerate)\n+\n+        // Add aggregation with having or a project.\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, expressions, withLateralView).optionalMap(having) {\n+            case (h, p) => Filter(expression(h), p)\n+          }\n+        } else {\n+          Project(expressions, withLateralView)\n+        }\n+\n+        // Distinct\n+        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n+          Distinct(withProject)\n+        } else {\n+          withProject\n+        }\n+\n+        // Window\n+        withDistinct.optionalMap(windows)(withWindows)\n+    }\n+  }\n+\n+  /**\n+   * Create a (Hive based) [[ScriptInputOutputSchema]].\n+   */\n+  protected def withScriptIOSchema(\n+      inRowFormat: RowFormatContext,\n+      outRowFormat: RowFormatContext,\n+      outRecordReader: Token): ScriptInputOutputSchema = null\n+\n+  /**\n+   * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma\n+   * separated) relations here, these get converted into a single plan by condition-less inner join.\n+   */\n+  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n+    ctx.relation.asScala.map(plan).reduceLeft(Join(_, _, Inner, None))\n+  }\n+\n+  /**\n+   * Connect two queries by a Set operator.\n+   *\n+   * Supported Set operators are:\n+   * - UNION [DISTINCT]\n+   * - UNION ALL\n+   * - EXCEPT [DISTINCT]\n+   * - INTERSECT [DISTINCT]\n+   */\n+  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) {\n+    val left = plan(ctx.left)\n+    val right = plan(ctx.right)\n+    val all = Option(ctx.setQuantifier()).exists(_.ALL != null)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.UNION if all =>\n+        Union(left, right)\n+      case SqlBaseParser.UNION =>\n+        Distinct(Union(left, right))\n+      case SqlBaseParser.INTERSECT if all =>\n+        notSupported(\"INTERSECT ALL is not supported.\", ctx)\n+      case SqlBaseParser.INTERSECT =>\n+        Intersect(left, right)\n+      case SqlBaseParser.EXCEPT if all =>\n+        notSupported(\"EXCEPT ALL is not supported.\", ctx)\n+      case SqlBaseParser.EXCEPT =>\n+        Except(left, right)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[WithWindowDefinition]] operator to a logical plan.\n+   */\n+  private def withWindows(\n+      ctx: WindowsContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Collect all window specifications defined in the WINDOW clause.\n+    val baseWindowMap = ctx.namedWindow.asScala.map {\n+      wCtx =>\n+        (wCtx.identifier.getText, typedVisit[WindowSpec](wCtx.windowSpec))\n+    }.toMap\n+\n+    // Handle cases like\n+    // window w1 as (partition by p_mfgr order by p_name\n+    //               range between 2 preceding and 2 following),\n+    //        w2 as w1\n+    val windowMap = baseWindowMap.mapValues {\n+      case WindowSpecReference(name) => baseWindowMap(name).asInstanceOf[WindowSpecDefinition]\n+      case spec: WindowSpecDefinition => spec\n+    }\n+    WithWindowDefinition(windowMap, query)\n+  }\n+\n+  /**\n+   * Add an [[Aggregate]] to a logical plan.\n+   */\n+  private def withAggregation(\n+      ctx: AggregationContext,\n+      selectExpressions: Seq[NamedExpression],\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    val groupByExpressions = expressionList(groupingExpressions)\n+\n+    if (GROUPING != null) {\n+      // GROUP BY .... GROUPING SETS (...)\n+      // TODO use new expression set here?\n+      val expressionMap = groupByExpressions.zipWithIndex.toMap\n+      val numExpressions = expressionMap.size\n+      val mask = (1 << numExpressions) - 1\n+      val masks = ctx.groupingSet.asScala.map {\n+        _.expression.asScala.foldLeft(mask) {\n+          case (bitmap, eCtx) =>\n+            // Find the index of the expression.\n+            val e = typedVisit[Expression](eCtx)\n+            val index = expressionMap.find(_._1.semanticEquals(e)).map(_._2).getOrElse(\n+              throw new AnalysisException(\n+                s\"${e.treeString} doesn't show up in the GROUP BY list\"))\n+            // 0 means that the column at the given index is a grouping column, 1 means it is not,\n+            // so we unset the bit in bitmap.\n+            bitmap & ~(1 << (numExpressions - 1 - index))\n+        }\n+      }\n+      GroupingSets(masks, groupByExpressions, query, selectExpressions)\n+    } else {\n+      // GROUP BY .... (WITH CUBE | WITH ROLLUP)?\n+      val mappedGroupByExpressions = if (CUBE != null) {\n+        Seq(Cube(groupByExpressions))\n+      } else if (ROLLUP != null) {\n+        Seq(Rollup(groupByExpressions))\n+      } else {\n+        groupByExpressions\n+      }\n+      Aggregate(mappedGroupByExpressions, selectExpressions, query)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[Generate]] (Lateral View) to a logical plan.\n+   */\n+  private def withGenerate(\n+      query: LogicalPlan,\n+      ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) {\n+    val expressions = expressionList(ctx.expression)\n+\n+    // Create the generator.\n+    // TODO Add support for other generators.\n+    val generator = ctx.qualifiedName.getText.toLowerCase match {\n+      case \"explode\" if expressions.size == 1 =>\n+        Explode(expressions.head)\n+      case \"json_tuple\" =>\n+        JsonTuple(expressions)\n+      case other =>\n+        notSupported(s\"Generator function '$other' is not supported\", ctx)\n+    }\n+\n+    Generate(\n+      generator,\n+      join = true,\n+      outer = ctx.OUTER != null,\n+      Some(ctx.tblName.getText.toLowerCase),\n+      ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.apply),\n+      query)\n+  }\n+\n+  /**\n+   * Create a join between two logical plans.\n+   */\n+  override def visitJoinRelation(ctx: JoinRelationContext): LogicalPlan = withOrigin(ctx) {\n+    val baseJoinType = ctx.joinType match {\n+      case jt if jt.FULL != null => FullOuter\n+      case jt if jt.SEMI != null => LeftSemi\n+      case jt if jt.LEFT != null => LeftOuter\n+      case jt if jt.RIGHT != null => RightOuter\n+      case _ => Inner\n+    }\n+    val joinType = if (ctx.NATURAL != null) {\n+      NaturalJoin(baseJoinType)\n+    } else {\n+      baseJoinType\n+    }\n+\n+    val left = plan(ctx.left)\n+    val right = if (ctx.right != null) {\n+      plan(ctx.right)\n+    } else {\n+      plan(ctx.rightRelation)\n+    }\n+    assert(left != null, \"Left side should not be null\", ctx)\n+    assert(right != null, \"Right side should not be null\", ctx)\n+    Join(left, right, joinType, Option(ctx.booleanExpression).map(expression))\n+  }\n+\n+  /**\n+   * Create a sampled relation. This returns a [[Sample]] operator when sampling is requested.\n+   *\n+   * This currently supports the following sampling methods:\n+   * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.\n+   * - TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages\n+   * are defined as a number between 0 and 100.\n+   * - TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a 'x' divided by 'y' fraction.\n+   */\n+  override def visitSampledRelation(ctx: SampledRelationContext): LogicalPlan = withOrigin(ctx) {\n+    val relation = plan(ctx.relationPrimary)\n+\n+    // Create a sampled plan if we need one.\n+    def sample(fraction: Double): Sample = {\n+      Sample(0.0, fraction, withReplacement = false, (math.random * 1000).toInt, relation)(true)\n+    }\n+\n+    // Sample the relation if we have to.\n+    relation.optional(ctx.sampleType) {\n+      ctx.sampleType.getType match {\n+        case SqlBaseParser.ROWS =>\n+          Limit(expression(ctx.expression), relation)\n+\n+        case SqlBaseParser.PERCENTLIT =>\n+          val fraction = ctx.percentage.getText.toDouble\n+          // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling\n+          // function takes X PERCENT as the input and the range of X is [0, 100], we need to\n+          // adjust the fraction.\n+          val eps = RandomSampler.roundingEpsilon\n+          require(fraction >= 0.0 - eps && fraction <= 100.0 + eps,\n+            s\"Sampling fraction ($fraction) must be on interval [0, 100]\")\n+          sample(fraction / 100.0d)\n+\n+        case SqlBaseParser.BUCKET if ctx.ON != null =>\n+          notSupported(\"TABLESAMPLE(BUCKET x OUT OF y ON id) is not supported\", ctx)\n+\n+        case SqlBaseParser.BUCKET =>\n+          sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a sub-query.\n+   */\n+  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryNoWith)\n+  }\n+\n+  /**\n+   * Create an un-aliased table reference. This is typically used for top-level table references,\n+   * for example:\n+   * {{{\n+   *   INSERT INTO db.tbl2\n+   *   TABLE db.tbl1\n+   * }}}\n+   */\n+  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(visitTableIdentifier(ctx.tableIdentifier), None)\n+  }\n+\n+  /**\n+   * Create an aliased table reference. This is typically used in FROM clauses.\n+   */\n+  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(\n+      visitTableIdentifier(ctx.tableIdentifier),\n+      Option(ctx.identifier).map(_.getText))\n+  }\n+\n+  /**\n+   * Create an inline table (a virtual table in Hive parlance).\n+   */\n+  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) {\n+    // Get the backing expressions.\n+    val expressions = ctx.expression.asScala.map(expression)\n+\n+    // Validate and evaluate the rows.\n+    val (structType, structConstructor) = expressions.head.dataType match {\n+      case st: StructType =>\n+        (st, (e: Expression) => e)\n+      case dt =>\n+        val st = CreateStruct(Seq(expressions.head)).dataType\n+        (st, (e: Expression) => CreateStruct(Seq(e)))\n+    }\n+    val rows = expressions.map {\n+      case expression =>\n+        assert(expression.foldable, \"All expressions in an inline table must be constants.\", ctx)\n+        val safe = Cast(structConstructor(expression), structType)\n+        safe.eval().asInstanceOf[InternalRow]\n+    }\n+\n+    // Construct attributes.\n+    val baseAttributes = structType.toAttributes\n+    val attributes = if (ctx.columnAliases != null) {\n+      val aliases = visitColumnAliases(ctx.columnAliases)\n+      assert(aliases.size == baseAttributes.size,\n+        \"Number of aliases must match the number of fields in an inline table.\", ctx)\n+      baseAttributes.zip(aliases).map(p => p._1.withName(p._2))\n+    } else {\n+      baseAttributes\n+    }\n+\n+    LocalRelation(attributes, rows)\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a join relation. This is practically the same as\n+   * visitAliasedQuery and visitNamedExpression, ANTLR4 however requires us to use 3 different\n+   * hooks.\n+   */\n+  override def visitAliasedRelation(ctx: AliasedRelationContext): LogicalPlan = withOrigin(ctx) {\n+    aliasPlan(ctx.identifier, plan(ctx.relation()))\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a sub-query. This is practically the same as\n+   * visitAliasedRelation and visitNamedExpression, ANTLR4 however requires us to use 3 different\n+   * hooks.\n+   */\n+  override def visitAliasedQuery(ctx: AliasedQueryContext): LogicalPlan = withOrigin(ctx) {\n+    aliasPlan(ctx.identifier, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a LogicalPlan. The alias is allowed to be optional.\n+   */\n+  private def aliasPlan(alias: IdentifierContext, plan: LogicalPlan): LogicalPlan = {\n+    plan.optional(alias) {\n+      SubqueryAlias(alias.getText, plan)\n+    }\n+  }\n+\n+  /**\n+   * Create a Sequence of Strings for a parenthesis enclosed alias list.\n+   */\n+  override def visitColumnAliases(ctx: ColumnAliasesContext): Seq[String] = withOrigin(ctx) {\n+    visitColumnAliasList(ctx.columnAliasList)\n+  }\n+\n+  /**\n+   * Create a Sequence of Strings for an alias list.\n+   */\n+  override def visitColumnAliasList(ctx: ColumnAliasListContext): Seq[String] = withOrigin(ctx) {\n+    ctx.identifier.asScala.map(_.getText)\n+  }\n+\n+  /**\n+   * Create a [[TableIdentifier]] from a 'tableName' or 'databaseName'.'tableName' pattern.\n+   */\n+  override def visitTableIdentifier(\n+      ctx: TableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    TableIdentifier(ctx.table.getText, Option(ctx.db).map(_.getText))\n+  }\n+\n+  /* ********************************************************************************************\n+   * Expression parsing\n+   * ******************************************************************************************** */\n+  private def expression(tree: ParserRuleContext): Expression = typedVisit(tree)\n+\n+  private def expressionList(trees: java.util.List[ExpressionContext]): Seq[Expression] = {\n+    trees.asScala.map(expression)\n+  }\n+\n+  private def invertIfNotDefined(expression: Expression, not: TerminalNode): Expression = {\n+    if (not != null) {\n+      Not(expression)\n+    } else {\n+      expression\n+    }\n+  }\n+\n+  override def visitStar(ctx: StarContext): Expression = withOrigin(ctx) {\n+    UnresolvedStar(Option(ctx.qualifiedName()).map(_.identifier.asScala.map(_.getText)))\n+  }\n+\n+  override def visitNamedExpression(ctx: NamedExpressionContext): Expression = withOrigin(ctx) {\n+    val e = expression(ctx.expression)\n+    if (ctx.identifier != null) {\n+      Alias(e, ctx.identifier.getText)()\n+    } else if (ctx.columnAliases != null) {\n+      MultiAlias(e, visitColumnAliases(ctx.columnAliases))\n+    } else {\n+      e\n+    }\n+  }\n+\n+  override def visitLogicalBinary(ctx: LogicalBinaryContext): Expression = withOrigin(ctx) {\n+    val left = expression(ctx.left)\n+    val right = expression(ctx.right)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.AND =>\n+        And(left, right)\n+      case SqlBaseParser.OR =>\n+        Or(left, right)\n+    }"
  }],
  "prId": 11557
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "I want to replace the current DataType parsing methods with these two methods. This is for a follow-up PR.\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-03-07T12:21:42Z",
    "diffHunk": "@@ -0,0 +1,1128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ParseUtils\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.trees.CurrentOrigin\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import AstBuilder._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          notSupported(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(unquote(pattern.getText)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }.toMap\n+      With(query, ctes)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryOrganization).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryOrganization).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).toSeq.flatMap {\n+      _.partitionVal.asScala.map {\n+        pVal => (pVal.identifier.getText, Option(pVal.constant).map(c => unquote(c.getText)))\n+      }\n+    }.toMap\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan.\n+   */\n+  private def withQueryOrganization(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      notSupported(\"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // LIMIT\n+    val withLimit = withOrder.optional(limit) {\n+      Limit(typedVisit(limit), withOrder)\n+    }\n+\n+    // WINDOWS\n+    withLimit.optionalMap(windows)(withWindows)\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    val withFilter = relation.optional(where) {\n+      Filter(expression(where), relation)\n+    }\n+\n+    // Expressions.\n+    val expressions = namedExpression.asScala.map(visit).map {\n+      case e: Expression => UnresolvedAlias(e)\n+    }\n+\n+    // Create either a transform or a regular query.\n+    kind.getType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Create the attributes.\n+        val attributes = if (colTypeList != null) {\n+          // Typed return columns.\n+          visitColTypeList(colTypeList).toAttributes\n+        } else if (columnAliasList != null) {\n+          // Untyped return columns.\n+          visitColumnAliasList(columnAliasList).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          unquote(script.getText),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, outRowFormat, outRecordReader))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(withFilter)(withGenerate)\n+\n+        // Add aggregation with having or a project.\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, expressions, withLateralView).optionalMap(having) {\n+            case (h, p) => Filter(expression(h), p)\n+          }\n+        } else {\n+          Project(expressions, withLateralView)\n+        }\n+\n+        // Distinct\n+        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n+          Distinct(withProject)\n+        } else {\n+          withProject\n+        }\n+\n+        // Window\n+        withDistinct.optionalMap(windows)(withWindows)\n+    }\n+  }\n+\n+  /**\n+   * Create a (Hive based) [[ScriptInputOutputSchema]].\n+   */\n+  protected def withScriptIOSchema(\n+      inRowFormat: RowFormatContext,\n+      outRowFormat: RowFormatContext,\n+      outRecordReader: Token): ScriptInputOutputSchema = null\n+\n+  /**\n+   * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma\n+   * separated) relations here, these get converted into a single plan by condition-less inner join.\n+   */\n+  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n+    ctx.relation.asScala.map(plan).reduceLeft(Join(_, _, Inner, None))\n+  }\n+\n+  /**\n+   * Connect two queries by a Set operator.\n+   *\n+   * Supported Set operators are:\n+   * - UNION [DISTINCT]\n+   * - UNION ALL\n+   * - EXCEPT [DISTINCT]\n+   * - INTERSECT [DISTINCT]\n+   */\n+  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) {\n+    val left = plan(ctx.left)\n+    val right = plan(ctx.right)\n+    val all = Option(ctx.setQuantifier()).exists(_.ALL != null)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.UNION if all =>\n+        Union(left, right)\n+      case SqlBaseParser.UNION =>\n+        Distinct(Union(left, right))\n+      case SqlBaseParser.INTERSECT if all =>\n+        notSupported(\"INTERSECT ALL is not supported.\", ctx)\n+      case SqlBaseParser.INTERSECT =>\n+        Intersect(left, right)\n+      case SqlBaseParser.EXCEPT if all =>\n+        notSupported(\"EXCEPT ALL is not supported.\", ctx)\n+      case SqlBaseParser.EXCEPT =>\n+        Except(left, right)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[WithWindowDefinition]] operator to a logical plan.\n+   */\n+  private def withWindows(\n+      ctx: WindowsContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Collect all window specifications defined in the WINDOW clause.\n+    val baseWindowMap = ctx.namedWindow.asScala.map {\n+      wCtx =>\n+        (wCtx.identifier.getText, typedVisit[WindowSpec](wCtx.windowSpec))\n+    }.toMap\n+\n+    // Handle cases like\n+    // window w1 as (partition by p_mfgr order by p_name\n+    //               range between 2 preceding and 2 following),\n+    //        w2 as w1\n+    val windowMap = baseWindowMap.mapValues {\n+      case WindowSpecReference(name) => baseWindowMap(name).asInstanceOf[WindowSpecDefinition]\n+      case spec: WindowSpecDefinition => spec\n+    }\n+    WithWindowDefinition(windowMap, query)\n+  }\n+\n+  /**\n+   * Add an [[Aggregate]] to a logical plan.\n+   */\n+  private def withAggregation(\n+      ctx: AggregationContext,\n+      selectExpressions: Seq[NamedExpression],\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    val groupByExpressions = expressionList(groupingExpressions)\n+\n+    if (GROUPING != null) {\n+      // GROUP BY .... GROUPING SETS (...)\n+      // TODO use new expression set here?\n+      val expressionMap = groupByExpressions.zipWithIndex.toMap\n+      val numExpressions = expressionMap.size\n+      val mask = (1 << numExpressions) - 1\n+      val masks = ctx.groupingSet.asScala.map {\n+        _.expression.asScala.foldLeft(mask) {\n+          case (bitmap, eCtx) =>\n+            // Find the index of the expression.\n+            val e = typedVisit[Expression](eCtx)\n+            val index = expressionMap.find(_._1.semanticEquals(e)).map(_._2).getOrElse(\n+              throw new AnalysisException(\n+                s\"${e.treeString} doesn't show up in the GROUP BY list\"))\n+            // 0 means that the column at the given index is a grouping column, 1 means it is not,\n+            // so we unset the bit in bitmap.\n+            bitmap & ~(1 << (numExpressions - 1 - index))\n+        }\n+      }\n+      GroupingSets(masks, groupByExpressions, query, selectExpressions)\n+    } else {\n+      // GROUP BY .... (WITH CUBE | WITH ROLLUP)?\n+      val mappedGroupByExpressions = if (CUBE != null) {\n+        Seq(Cube(groupByExpressions))\n+      } else if (ROLLUP != null) {\n+        Seq(Rollup(groupByExpressions))\n+      } else {\n+        groupByExpressions\n+      }\n+      Aggregate(mappedGroupByExpressions, selectExpressions, query)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[Generate]] (Lateral View) to a logical plan.\n+   */\n+  private def withGenerate(\n+      query: LogicalPlan,\n+      ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) {\n+    val expressions = expressionList(ctx.expression)\n+\n+    // Create the generator.\n+    // TODO Add support for other generators.\n+    val generator = ctx.qualifiedName.getText.toLowerCase match {\n+      case \"explode\" if expressions.size == 1 =>\n+        Explode(expressions.head)\n+      case \"json_tuple\" =>\n+        JsonTuple(expressions)\n+      case other =>\n+        notSupported(s\"Generator function '$other' is not supported\", ctx)\n+    }\n+\n+    Generate(\n+      generator,\n+      join = true,\n+      outer = ctx.OUTER != null,\n+      Some(ctx.tblName.getText.toLowerCase),\n+      ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.apply),\n+      query)\n+  }\n+\n+  /**\n+   * Create a join between two logical plans.\n+   */\n+  override def visitJoinRelation(ctx: JoinRelationContext): LogicalPlan = withOrigin(ctx) {\n+    val baseJoinType = ctx.joinType match {\n+      case jt if jt.FULL != null => FullOuter\n+      case jt if jt.SEMI != null => LeftSemi\n+      case jt if jt.LEFT != null => LeftOuter\n+      case jt if jt.RIGHT != null => RightOuter\n+      case _ => Inner\n+    }\n+    val joinType = if (ctx.NATURAL != null) {\n+      NaturalJoin(baseJoinType)\n+    } else {\n+      baseJoinType\n+    }\n+\n+    val left = plan(ctx.left)\n+    val right = if (ctx.right != null) {\n+      plan(ctx.right)\n+    } else {\n+      plan(ctx.rightRelation)\n+    }\n+    assert(left != null, \"Left side should not be null\", ctx)\n+    assert(right != null, \"Right side should not be null\", ctx)\n+    Join(left, right, joinType, Option(ctx.booleanExpression).map(expression))\n+  }\n+\n+  /**\n+   * Create a sampled relation. This returns a [[Sample]] operator when sampling is requested.\n+   *\n+   * This currently supports the following sampling methods:\n+   * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.\n+   * - TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages\n+   * are defined as a number between 0 and 100.\n+   * - TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a 'x' divided by 'y' fraction.\n+   */\n+  override def visitSampledRelation(ctx: SampledRelationContext): LogicalPlan = withOrigin(ctx) {\n+    val relation = plan(ctx.relationPrimary)\n+\n+    // Create a sampled plan if we need one.\n+    def sample(fraction: Double): Sample = {\n+      Sample(0.0, fraction, withReplacement = false, (math.random * 1000).toInt, relation)(true)\n+    }\n+\n+    // Sample the relation if we have to.\n+    relation.optional(ctx.sampleType) {\n+      ctx.sampleType.getType match {\n+        case SqlBaseParser.ROWS =>\n+          Limit(expression(ctx.expression), relation)\n+\n+        case SqlBaseParser.PERCENTLIT =>\n+          val fraction = ctx.percentage.getText.toDouble\n+          // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling\n+          // function takes X PERCENT as the input and the range of X is [0, 100], we need to\n+          // adjust the fraction.\n+          val eps = RandomSampler.roundingEpsilon\n+          require(fraction >= 0.0 - eps && fraction <= 100.0 + eps,\n+            s\"Sampling fraction ($fraction) must be on interval [0, 100]\")\n+          sample(fraction / 100.0d)\n+\n+        case SqlBaseParser.BUCKET if ctx.ON != null =>\n+          notSupported(\"TABLESAMPLE(BUCKET x OUT OF y ON id) is not supported\", ctx)\n+\n+        case SqlBaseParser.BUCKET =>\n+          sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a sub-query.\n+   */\n+  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryNoWith)\n+  }\n+\n+  /**\n+   * Create an un-aliased table reference. This is typically used for top-level table references,\n+   * for example:\n+   * {{{\n+   *   INSERT INTO db.tbl2\n+   *   TABLE db.tbl1\n+   * }}}\n+   */\n+  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(visitTableIdentifier(ctx.tableIdentifier), None)\n+  }\n+\n+  /**\n+   * Create an aliased table reference. This is typically used in FROM clauses.\n+   */\n+  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(\n+      visitTableIdentifier(ctx.tableIdentifier),\n+      Option(ctx.identifier).map(_.getText))\n+  }\n+\n+  /**\n+   * Create an inline table (a virtual table in Hive parlance).\n+   */\n+  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) {\n+    // Get the backing expressions.\n+    val expressions = ctx.expression.asScala.map(expression)\n+\n+    // Validate and evaluate the rows.\n+    val (structType, structConstructor) = expressions.head.dataType match {\n+      case st: StructType =>\n+        (st, (e: Expression) => e)\n+      case dt =>\n+        val st = CreateStruct(Seq(expressions.head)).dataType\n+        (st, (e: Expression) => CreateStruct(Seq(e)))\n+    }\n+    val rows = expressions.map {\n+      case expression =>\n+        assert(expression.foldable, \"All expressions in an inline table must be constants.\", ctx)\n+        val safe = Cast(structConstructor(expression), structType)\n+        safe.eval().asInstanceOf[InternalRow]\n+    }\n+\n+    // Construct attributes.\n+    val baseAttributes = structType.toAttributes\n+    val attributes = if (ctx.columnAliases != null) {\n+      val aliases = visitColumnAliases(ctx.columnAliases)\n+      assert(aliases.size == baseAttributes.size,\n+        \"Number of aliases must match the number of fields in an inline table.\", ctx)\n+      baseAttributes.zip(aliases).map(p => p._1.withName(p._2))\n+    } else {\n+      baseAttributes\n+    }\n+\n+    LocalRelation(attributes, rows)\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a join relation. This is practically the same as\n+   * visitAliasedQuery and visitNamedExpression, ANTLR4 however requires us to use 3 different\n+   * hooks.\n+   */\n+  override def visitAliasedRelation(ctx: AliasedRelationContext): LogicalPlan = withOrigin(ctx) {\n+    aliasPlan(ctx.identifier, plan(ctx.relation()))\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a sub-query. This is practically the same as\n+   * visitAliasedRelation and visitNamedExpression, ANTLR4 however requires us to use 3 different\n+   * hooks.\n+   */\n+  override def visitAliasedQuery(ctx: AliasedQueryContext): LogicalPlan = withOrigin(ctx) {\n+    aliasPlan(ctx.identifier, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create an alias (SubqueryAlias) for a LogicalPlan. The alias is allowed to be optional.\n+   */\n+  private def aliasPlan(alias: IdentifierContext, plan: LogicalPlan): LogicalPlan = {\n+    plan.optional(alias) {\n+      SubqueryAlias(alias.getText, plan)\n+    }\n+  }\n+\n+  /**\n+   * Create a Sequence of Strings for a parenthesis enclosed alias list.\n+   */\n+  override def visitColumnAliases(ctx: ColumnAliasesContext): Seq[String] = withOrigin(ctx) {\n+    visitColumnAliasList(ctx.columnAliasList)\n+  }\n+\n+  /**\n+   * Create a Sequence of Strings for an alias list.\n+   */\n+  override def visitColumnAliasList(ctx: ColumnAliasListContext): Seq[String] = withOrigin(ctx) {\n+    ctx.identifier.asScala.map(_.getText)\n+  }\n+\n+  /**\n+   * Create a [[TableIdentifier]] from a 'tableName' or 'databaseName'.'tableName' pattern.\n+   */\n+  override def visitTableIdentifier(\n+      ctx: TableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    TableIdentifier(ctx.table.getText, Option(ctx.db).map(_.getText))\n+  }\n+\n+  /* ********************************************************************************************\n+   * Expression parsing\n+   * ******************************************************************************************** */\n+  private def expression(tree: ParserRuleContext): Expression = typedVisit(tree)\n+\n+  private def expressionList(trees: java.util.List[ExpressionContext]): Seq[Expression] = {\n+    trees.asScala.map(expression)\n+  }\n+\n+  private def invertIfNotDefined(expression: Expression, not: TerminalNode): Expression = {\n+    if (not != null) {\n+      Not(expression)\n+    } else {\n+      expression\n+    }\n+  }\n+\n+  override def visitStar(ctx: StarContext): Expression = withOrigin(ctx) {\n+    UnresolvedStar(Option(ctx.qualifiedName()).map(_.identifier.asScala.map(_.getText)))\n+  }\n+\n+  override def visitNamedExpression(ctx: NamedExpressionContext): Expression = withOrigin(ctx) {\n+    val e = expression(ctx.expression)\n+    if (ctx.identifier != null) {\n+      Alias(e, ctx.identifier.getText)()\n+    } else if (ctx.columnAliases != null) {\n+      MultiAlias(e, visitColumnAliases(ctx.columnAliases))\n+    } else {\n+      e\n+    }\n+  }\n+\n+  override def visitLogicalBinary(ctx: LogicalBinaryContext): Expression = withOrigin(ctx) {\n+    val left = expression(ctx.left)\n+    val right = expression(ctx.right)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.AND =>\n+        And(left, right)\n+      case SqlBaseParser.OR =>\n+        Or(left, right)\n+    }\n+  }\n+\n+  override def visitLogicalNot(ctx: LogicalNotContext): Expression = withOrigin(ctx) {\n+    Not(expression(ctx.booleanExpression()))\n+  }\n+\n+  override def visitExists(ctx: ExistsContext): Expression = {\n+    notSupported(\"Exists is not supported.\", ctx)\n+  }\n+\n+  override def visitComparison(ctx: ComparisonContext): Expression = withOrigin(ctx) {\n+    val left = expression(ctx.value)\n+    val right = expression(ctx.right)\n+    val operator = ctx.comparisonOperator().getChild(0).asInstanceOf[TerminalNode]\n+    operator.getSymbol.getType match {\n+      case SqlBaseParser.EQ =>\n+        EqualTo(left, right)\n+      case SqlBaseParser.NSEQ =>\n+        EqualNullSafe(left, right)\n+      case SqlBaseParser.NEQ =>\n+        Not(EqualTo(left, right))\n+      case SqlBaseParser.LT =>\n+        LessThan(left, right)\n+      case SqlBaseParser.LTE =>\n+        LessThanOrEqual(left, right)\n+      case SqlBaseParser.GT =>\n+        GreaterThan(left, right)\n+      case SqlBaseParser.GTE =>\n+        GreaterThanOrEqual(left, right)\n+    }\n+  }\n+\n+  override def visitBetween(ctx: BetweenContext): Expression = withOrigin(ctx) {\n+    val value = expression(ctx.value)\n+    val between = And(\n+      GreaterThanOrEqual(value, expression(ctx.lower)),\n+      LessThanOrEqual(value, expression(ctx.upper)))\n+    invertIfNotDefined(between, ctx.NOT)\n+  }\n+\n+  override def visitInList(ctx: InListContext): Expression = withOrigin(ctx) {\n+    val in = In(expression(ctx.value), ctx.expression().asScala.map(expression))\n+    invertIfNotDefined(in, ctx.NOT)\n+  }\n+\n+  override def visitInSubquery(ctx: InSubqueryContext): Expression = {\n+    notSupported(\"IN with a Sub-query is currently not supported.\", ctx)\n+  }\n+\n+  override def visitLike(ctx: LikeContext): Expression = {\n+    val left = expression(ctx.value)\n+    val right = expression(ctx.pattern)\n+    val like = ctx.like.getType match {\n+      case SqlBaseParser.LIKE =>\n+        Like(left, right)\n+      case SqlBaseParser.RLIKE =>\n+        RLike(left, right)\n+    }\n+    invertIfNotDefined(like, ctx.NOT)\n+  }\n+\n+  override def visitNullPredicate(ctx: NullPredicateContext): Expression = withOrigin(ctx) {\n+    val value = expression(ctx.value)\n+    if (ctx.NOT != null) {\n+      IsNotNull(value)\n+    } else {\n+      IsNull(value)\n+    }\n+  }\n+\n+  override def visitArithmeticBinary(ctx: ArithmeticBinaryContext): Expression = withOrigin(ctx) {\n+    val left = expression(ctx.left)\n+    val right = expression(ctx.right)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.ASTERISK =>\n+        Multiply(left, right)\n+      case SqlBaseParser.SLASH =>\n+        Divide(left, right)\n+      case SqlBaseParser.PERCENT =>\n+        Remainder(left, right)\n+      case SqlBaseParser.DIV =>\n+        Cast(Divide(left, right), LongType)\n+      case SqlBaseParser.PLUS =>\n+        Add(left, right)\n+      case SqlBaseParser.MINUS =>\n+        Subtract(left, right)\n+      case SqlBaseParser.AMPERSAND =>\n+        BitwiseAnd(left, right)\n+      case SqlBaseParser.HAT =>\n+        BitwiseXor(left, right)\n+      case SqlBaseParser.PIPE =>\n+        BitwiseXor(left, right)\n+    }\n+  }\n+\n+  override def visitArithmeticUnary(ctx: ArithmeticUnaryContext): Expression = withOrigin(ctx) {\n+    val value = expression(ctx.valueExpression)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.PLUS =>\n+        value\n+      case SqlBaseParser.MINUS =>\n+        UnaryMinus(value)\n+      case SqlBaseParser.TILDE =>\n+        BitwiseNot(value)\n+    }\n+  }\n+\n+  override def visitCast(ctx: CastContext): Expression = withOrigin(ctx) {\n+    Cast(expression(ctx.expression), typedVisit(ctx.dataType))\n+  }\n+\n+  override def visitPrimitiveDatatype(ctx: PrimitiveDatatypeContext): DataType = withOrigin(ctx) {"
  }],
  "prId": 11557
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "why is this called QueryOrganization?\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-03-22T21:19:55Z",
    "diffHunk": "@@ -0,0 +1,1450 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ParseUtils\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.trees.CurrentOrigin\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import AstBuilder._\n+  import ParseUtils._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {\n+    visit(ctx.dataType).asInstanceOf[DataType]\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          throw new ParseException(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(unescapeSQLString(pattern.getText)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }\n+\n+      // Check for duplicate names.\n+      ctes.groupBy(_._1).filter(_._2.size > 1).foreach {\n+        case (name, _) =>\n+          throw new ParseException(\n+            s\"Name '$name' is used for multiple common table expressions\", ctx)\n+      }\n+\n+      With(query, ctes.toMap)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryOrganization).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryOrganization).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).toSeq.flatMap {\n+      _.partitionVal.asScala.map {\n+        pVal =>\n+          val name = pVal.identifier.getText\n+          val value = Option(pVal.constant).map(c => expression(c).toString)\n+          name -> value\n+      }\n+    }.toMap\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan.\n+   */\n+  private def withQueryOrganization("
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "It contains all the clauses that organize (order/distribute) the result of the query. I couldn't really think of something better.\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-03-22T22:03:47Z",
    "diffHunk": "@@ -0,0 +1,1450 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ParseUtils\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.trees.CurrentOrigin\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import AstBuilder._\n+  import ParseUtils._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {\n+    visit(ctx.dataType).asInstanceOf[DataType]\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          throw new ParseException(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(unescapeSQLString(pattern.getText)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }\n+\n+      // Check for duplicate names.\n+      ctes.groupBy(_._1).filter(_._2.size > 1).foreach {\n+        case (name, _) =>\n+          throw new ParseException(\n+            s\"Name '$name' is used for multiple common table expressions\", ctx)\n+      }\n+\n+      With(query, ctes.toMap)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryOrganization).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryOrganization).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).toSeq.flatMap {\n+      _.partitionVal.asScala.map {\n+        pVal =>\n+          val name = pVal.identifier.getText\n+          val value = Option(pVal.constant).map(c => expression(c).toString)\n+          name -> value\n+      }\n+    }.toMap\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan.\n+   */\n+  private def withQueryOrganization("
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "`withQueryClauses`?\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-03-23T15:56:31Z",
    "diffHunk": "@@ -0,0 +1,1450 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ParseUtils\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.trees.CurrentOrigin\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import AstBuilder._\n+  import ParseUtils._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {\n+    visit(ctx.dataType).asInstanceOf[DataType]\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          throw new ParseException(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(unescapeSQLString(pattern.getText)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }\n+\n+      // Check for duplicate names.\n+      ctes.groupBy(_._1).filter(_._2.size > 1).foreach {\n+        case (name, _) =>\n+          throw new ParseException(\n+            s\"Name '$name' is used for multiple common table expressions\", ctx)\n+      }\n+\n+      With(query, ctes.toMap)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryOrganization).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryOrganization).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).toSeq.flatMap {\n+      _.partitionVal.asScala.map {\n+        pVal =>\n+          val name = pVal.identifier.getText\n+          val value = Option(pVal.constant).map(c => expression(c).toString)\n+          name -> value\n+      }\n+    }.toMap\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan.\n+   */\n+  private def withQueryOrganization("
  }],
  "prId": 11557
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how do we make sure all expression are of same type?\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-08-05T07:23:35Z",
    "diffHunk": "@@ -0,0 +1,1452 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import ParserUtils._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {\n+    visit(ctx.dataType).asInstanceOf[DataType]\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Make sure we do not try to create a plan for a native command.\n+   */\n+  override def visitExecuteNativeCommand(ctx: ExecuteNativeCommandContext): LogicalPlan = null\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          throw new ParseException(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(string(pattern)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }\n+\n+      // Check for duplicate names.\n+      ctes.groupBy(_._1).filter(_._2.size > 1).foreach {\n+        case (name, _) =>\n+          throw new ParseException(\n+            s\"Name '$name' is used for multiple common table expressions\", ctx)\n+      }\n+\n+      With(query, ctes.toMap)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryResultClauses).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryResultClauses).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).map(visitPartitionSpec).getOrElse(Map.empty)\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Create a partition specification map.\n+   */\n+  override def visitPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, Option[String]] = withOrigin(ctx) {\n+    ctx.partitionVal.asScala.map { pVal =>\n+      val name = pVal.identifier.getText.toLowerCase\n+      val value = Option(pVal.constant).map(visitStringConstant)\n+      name -> value\n+    }.toMap\n+  }\n+\n+  /**\n+   * Create a partition specification map without optional values.\n+   */\n+  protected def visitNonOptionalPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, String] = withOrigin(ctx) {\n+    visitPartitionSpec(ctx).mapValues(_.orNull).map(identity)\n+  }\n+\n+  /**\n+   * Convert a constant of any type into a string. This is typically used in DDL commands, and its\n+   * main purpose is to prevent slight differences due to back to back conversions i.e.:\n+   * String -> Literal -> String.\n+   */\n+  protected def visitStringConstant(ctx: ConstantContext): String = withOrigin(ctx) {\n+    ctx match {\n+      case s: StringLiteralContext => createString(s)\n+      case o => o.getText\n+    }\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan. These\n+   * clauses determine the shape (ordering/partitioning/rows) of the query result.\n+   */\n+  private def withQueryResultClauses(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      throw new ParseException(\n+        \"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // WINDOWS\n+    val withWindow = withOrder.optionalMap(windows)(withWindows)\n+\n+    // LIMIT\n+    withWindow.optional(limit) {\n+      Limit(typedVisit(limit), withWindow)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan = {\n+      Filter(expression(ctx), plan)\n+    }\n+\n+    // Expressions.\n+    val expressions = Option(namedExpressionSeq).toSeq\n+      .flatMap(_.namedExpression.asScala)\n+      .map(typedVisit[Expression])\n+\n+    // Create either a transform or a regular query.\n+    val specType = Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)\n+    specType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Add where.\n+        val withFilter = relation.optionalMap(where)(filter)\n+\n+        // Create the attributes.\n+        val (attributes, schemaLess) = if (colTypeList != null) {\n+          // Typed return columns.\n+          (createStructType(colTypeList).toAttributes, false)\n+        } else if (identifierSeq != null) {\n+          // Untyped return columns.\n+          val attrs = visitIdentifierSeq(identifierSeq).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+          (attrs, false)\n+        } else {\n+          (Seq(AttributeReference(\"key\", StringType)(),\n+            AttributeReference(\"value\", StringType)()), true)\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          string(script),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(relation)(withGenerate)\n+\n+        // Add where.\n+        val withFilter = withLateralView.optionalMap(where)(filter)\n+\n+        // Add aggregation or a project.\n+        val namedExpressions = expressions.map {\n+          case e: NamedExpression => e\n+          case e: Expression => UnresolvedAlias(e)\n+        }\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, namedExpressions, withFilter)\n+        } else if (namedExpressions.nonEmpty) {\n+          Project(namedExpressions, withFilter)\n+        } else {\n+          withFilter\n+        }\n+\n+        // Having\n+        val withHaving = withProject.optional(having) {\n+          // Note that we added a cast to boolean. If the expression itself is already boolean,\n+          // the optimizer will get rid of the unnecessary cast.\n+          Filter(Cast(expression(having), BooleanType), withProject)\n+        }\n+\n+        // Distinct\n+        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n+          Distinct(withHaving)\n+        } else {\n+          withHaving\n+        }\n+\n+        // Window\n+        withDistinct.optionalMap(windows)(withWindows)\n+    }\n+  }\n+\n+  /**\n+   * Create a (Hive based) [[ScriptInputOutputSchema]].\n+   */\n+  protected def withScriptIOSchema(\n+      inRowFormat: RowFormatContext,\n+      recordWriter: Token,\n+      outRowFormat: RowFormatContext,\n+      recordReader: Token,\n+      schemaLess: Boolean): ScriptInputOutputSchema = null\n+\n+  /**\n+   * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma\n+   * separated) relations here, these get converted into a single plan by condition-less inner join.\n+   */\n+  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n+    val from = ctx.relation.asScala.map(plan).reduceLeft(Join(_, _, Inner, None))\n+    ctx.lateralView.asScala.foldLeft(from)(withGenerate)\n+  }\n+\n+  /**\n+   * Connect two queries by a Set operator.\n+   *\n+   * Supported Set operators are:\n+   * - UNION [DISTINCT]\n+   * - UNION ALL\n+   * - EXCEPT [DISTINCT]\n+   * - INTERSECT [DISTINCT]\n+   */\n+  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) {\n+    val left = plan(ctx.left)\n+    val right = plan(ctx.right)\n+    val all = Option(ctx.setQuantifier()).exists(_.ALL != null)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.UNION if all =>\n+        Union(left, right)\n+      case SqlBaseParser.UNION =>\n+        Distinct(Union(left, right))\n+      case SqlBaseParser.INTERSECT if all =>\n+        throw new ParseException(\"INTERSECT ALL is not supported.\", ctx)\n+      case SqlBaseParser.INTERSECT =>\n+        Intersect(left, right)\n+      case SqlBaseParser.EXCEPT if all =>\n+        throw new ParseException(\"EXCEPT ALL is not supported.\", ctx)\n+      case SqlBaseParser.EXCEPT =>\n+        Except(left, right)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[WithWindowDefinition]] operator to a logical plan.\n+   */\n+  private def withWindows(\n+      ctx: WindowsContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Collect all window specifications defined in the WINDOW clause.\n+    val baseWindowMap = ctx.namedWindow.asScala.map {\n+      wCtx =>\n+        (wCtx.identifier.getText, typedVisit[WindowSpec](wCtx.windowSpec))\n+    }.toMap\n+\n+    // Handle cases like\n+    // window w1 as (partition by p_mfgr order by p_name\n+    //               range between 2 preceding and 2 following),\n+    //        w2 as w1\n+    val windowMapView = baseWindowMap.mapValues {\n+      case WindowSpecReference(name) =>\n+        baseWindowMap.get(name) match {\n+          case Some(spec: WindowSpecDefinition) =>\n+            spec\n+          case Some(ref) =>\n+            throw new ParseException(s\"Window reference '$name' is not a window specification\", ctx)\n+          case None =>\n+            throw new ParseException(s\"Cannot resolve window reference '$name'\", ctx)\n+        }\n+      case spec: WindowSpecDefinition => spec\n+    }\n+\n+    // Note that mapValues creates a view instead of materialized map. We force materialization by\n+    // mapping over identity.\n+    WithWindowDefinition(windowMapView.map(identity), query)\n+  }\n+\n+  /**\n+   * Add an [[Aggregate]] to a logical plan.\n+   */\n+  private def withAggregation(\n+      ctx: AggregationContext,\n+      selectExpressions: Seq[NamedExpression],\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    val groupByExpressions = expressionList(groupingExpressions)\n+\n+    if (GROUPING != null) {\n+      // GROUP BY .... GROUPING SETS (...)\n+      val expressionMap = groupByExpressions.zipWithIndex.toMap\n+      val numExpressions = expressionMap.size\n+      val mask = (1 << numExpressions) - 1\n+      val masks = ctx.groupingSet.asScala.map {\n+        _.expression.asScala.foldLeft(mask) {\n+          case (bitmap, eCtx) =>\n+            // Find the index of the expression.\n+            val e = typedVisit[Expression](eCtx)\n+            val index = expressionMap.find(_._1.semanticEquals(e)).map(_._2).getOrElse(\n+              throw new ParseException(\n+                s\"$e doesn't show up in the GROUP BY list\", ctx))\n+            // 0 means that the column at the given index is a grouping column, 1 means it is not,\n+            // so we unset the bit in bitmap.\n+            bitmap & ~(1 << (numExpressions - 1 - index))\n+        }\n+      }\n+      GroupingSets(masks, groupByExpressions, query, selectExpressions)\n+    } else {\n+      // GROUP BY .... (WITH CUBE | WITH ROLLUP)?\n+      val mappedGroupByExpressions = if (CUBE != null) {\n+        Seq(Cube(groupByExpressions))\n+      } else if (ROLLUP != null) {\n+        Seq(Rollup(groupByExpressions))\n+      } else {\n+        groupByExpressions\n+      }\n+      Aggregate(mappedGroupByExpressions, selectExpressions, query)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[Generate]] (Lateral View) to a logical plan.\n+   */\n+  private def withGenerate(\n+      query: LogicalPlan,\n+      ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) {\n+    val expressions = expressionList(ctx.expression)\n+\n+    // Create the generator.\n+    val generator = ctx.qualifiedName.getText.toLowerCase match {\n+      case \"explode\" if expressions.size == 1 =>\n+        Explode(expressions.head)\n+      case \"json_tuple\" =>\n+        JsonTuple(expressions)\n+      case other =>\n+        withGenerator(other, expressions, ctx)\n+    }\n+\n+    Generate(\n+      generator,\n+      join = true,\n+      outer = ctx.OUTER != null,\n+      Some(ctx.tblName.getText.toLowerCase),\n+      ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.apply),\n+      query)\n+  }\n+\n+  /**\n+   * Create a [[Generator]]. Override this method in order to support custom Generators.\n+   */\n+  protected def withGenerator(\n+      name: String,\n+      expressions: Seq[Expression],\n+      ctx: LateralViewContext): Generator = {\n+    throw new ParseException(s\"Generator function '$name' is not supported\", ctx)\n+  }\n+\n+  /**\n+   * Create a joins between two or more logical plans.\n+   */\n+  override def visitJoinRelation(ctx: JoinRelationContext): LogicalPlan = withOrigin(ctx) {\n+    /** Build a join between two plans. */\n+    def join(ctx: JoinRelationContext, left: LogicalPlan, right: LogicalPlan): Join = {\n+      val baseJoinType = ctx.joinType match {\n+        case null => Inner\n+        case jt if jt.FULL != null => FullOuter\n+        case jt if jt.SEMI != null => LeftSemi\n+        case jt if jt.LEFT != null => LeftOuter\n+        case jt if jt.RIGHT != null => RightOuter\n+        case _ => Inner\n+      }\n+\n+      // Resolve the join type and join condition\n+      val (joinType, condition) = Option(ctx.joinCriteria) match {\n+        case Some(c) if c.USING != null =>\n+          val columns = c.identifier.asScala.map { column =>\n+            UnresolvedAttribute.quoted(column.getText)\n+          }\n+          (UsingJoin(baseJoinType, columns), None)\n+        case Some(c) if c.booleanExpression != null =>\n+          (baseJoinType, Option(expression(c.booleanExpression)))\n+        case None if ctx.NATURAL != null =>\n+          (NaturalJoin(baseJoinType), None)\n+        case None =>\n+          (baseJoinType, None)\n+      }\n+      Join(left, right, joinType, condition)\n+    }\n+\n+    // Handle all consecutive join clauses. ANTLR produces a right nested tree in which the the\n+    // first join clause is at the top. However fields of previously referenced tables can be used\n+    // in following join clauses. The tree needs to be reversed in order to make this work.\n+    var result = plan(ctx.left)\n+    var current = ctx\n+    while (current != null) {\n+      current.right match {\n+        case right: JoinRelationContext =>\n+          result = join(current, result, plan(right.left))\n+          current = right\n+        case right =>\n+          result = join(current, result, plan(right))\n+          current = null\n+      }\n+    }\n+    result\n+  }\n+\n+  /**\n+   * Add a [[Sample]] to a logical plan.\n+   *\n+   * This currently supports the following sampling methods:\n+   * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.\n+   * - TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages\n+   * are defined as a number between 0 and 100.\n+   * - TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a 'x' divided by 'y' fraction.\n+   */\n+  private def withSample(ctx: SampleContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Create a sampled plan if we need one.\n+    def sample(fraction: Double): Sample = {\n+      // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling\n+      // function takes X PERCENT as the input and the range of X is [0, 100], we need to\n+      // adjust the fraction.\n+      val eps = RandomSampler.roundingEpsilon\n+      assert(fraction >= 0.0 - eps && fraction <= 1.0 + eps,\n+        s\"Sampling fraction ($fraction) must be on interval [0, 1]\",\n+        ctx)\n+      Sample(0.0, fraction, withReplacement = false, (math.random * 1000).toInt, query)(true)\n+    }\n+\n+    ctx.sampleType.getType match {\n+      case SqlBaseParser.ROWS =>\n+        Limit(expression(ctx.expression), query)\n+\n+      case SqlBaseParser.PERCENTLIT =>\n+        val fraction = ctx.percentage.getText.toDouble\n+        sample(fraction / 100.0d)\n+\n+      case SqlBaseParser.BUCKET if ctx.ON != null =>\n+        throw new ParseException(\"TABLESAMPLE(BUCKET x OUT OF y ON id) is not supported\", ctx)\n+\n+      case SqlBaseParser.BUCKET =>\n+        sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a sub-query.\n+   */\n+  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryNoWith)\n+  }\n+\n+  /**\n+   * Create an un-aliased table reference. This is typically used for top-level table references,\n+   * for example:\n+   * {{{\n+   *   INSERT INTO db.tbl2\n+   *   TABLE db.tbl1\n+   * }}}\n+   */\n+  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(visitTableIdentifier(ctx.tableIdentifier), None)\n+  }\n+\n+  /**\n+   * Create an aliased table reference. This is typically used in FROM clauses.\n+   */\n+  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) {\n+    val table = UnresolvedRelation(\n+      visitTableIdentifier(ctx.tableIdentifier),\n+      Option(ctx.identifier).map(_.getText))\n+    table.optionalMap(ctx.sample)(withSample)\n+  }\n+\n+  /**\n+   * Create an inline table (a virtual table in Hive parlance).\n+   */\n+  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) {\n+    // Get the backing expressions.\n+    val expressions = ctx.expression.asScala.map { eCtx =>\n+      val e = expression(eCtx)\n+      assert(e.foldable, \"All expressions in an inline table must be constants.\", eCtx)\n+      e\n+    }\n+\n+    // Validate and evaluate the rows.\n+    val (structType, structConstructor) = expressions.head.dataType match {",
    "line": 687
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "We infer the type of the first row, and then cast all other rows to the first row's type. See: https://github.com/apache/spark/pull/11557/files/6f1c535162397f01acf0405bdc80b8c4c141fc64#diff-05222a1d022860b15a2fc1ec3445b368R696\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-08-05T07:44:28Z",
    "diffHunk": "@@ -0,0 +1,1452 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import ParserUtils._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {\n+    visit(ctx.dataType).asInstanceOf[DataType]\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Make sure we do not try to create a plan for a native command.\n+   */\n+  override def visitExecuteNativeCommand(ctx: ExecuteNativeCommandContext): LogicalPlan = null\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          throw new ParseException(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(string(pattern)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }\n+\n+      // Check for duplicate names.\n+      ctes.groupBy(_._1).filter(_._2.size > 1).foreach {\n+        case (name, _) =>\n+          throw new ParseException(\n+            s\"Name '$name' is used for multiple common table expressions\", ctx)\n+      }\n+\n+      With(query, ctes.toMap)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryResultClauses).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryResultClauses).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).map(visitPartitionSpec).getOrElse(Map.empty)\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Create a partition specification map.\n+   */\n+  override def visitPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, Option[String]] = withOrigin(ctx) {\n+    ctx.partitionVal.asScala.map { pVal =>\n+      val name = pVal.identifier.getText.toLowerCase\n+      val value = Option(pVal.constant).map(visitStringConstant)\n+      name -> value\n+    }.toMap\n+  }\n+\n+  /**\n+   * Create a partition specification map without optional values.\n+   */\n+  protected def visitNonOptionalPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, String] = withOrigin(ctx) {\n+    visitPartitionSpec(ctx).mapValues(_.orNull).map(identity)\n+  }\n+\n+  /**\n+   * Convert a constant of any type into a string. This is typically used in DDL commands, and its\n+   * main purpose is to prevent slight differences due to back to back conversions i.e.:\n+   * String -> Literal -> String.\n+   */\n+  protected def visitStringConstant(ctx: ConstantContext): String = withOrigin(ctx) {\n+    ctx match {\n+      case s: StringLiteralContext => createString(s)\n+      case o => o.getText\n+    }\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan. These\n+   * clauses determine the shape (ordering/partitioning/rows) of the query result.\n+   */\n+  private def withQueryResultClauses(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      throw new ParseException(\n+        \"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // WINDOWS\n+    val withWindow = withOrder.optionalMap(windows)(withWindows)\n+\n+    // LIMIT\n+    withWindow.optional(limit) {\n+      Limit(typedVisit(limit), withWindow)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan = {\n+      Filter(expression(ctx), plan)\n+    }\n+\n+    // Expressions.\n+    val expressions = Option(namedExpressionSeq).toSeq\n+      .flatMap(_.namedExpression.asScala)\n+      .map(typedVisit[Expression])\n+\n+    // Create either a transform or a regular query.\n+    val specType = Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)\n+    specType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Add where.\n+        val withFilter = relation.optionalMap(where)(filter)\n+\n+        // Create the attributes.\n+        val (attributes, schemaLess) = if (colTypeList != null) {\n+          // Typed return columns.\n+          (createStructType(colTypeList).toAttributes, false)\n+        } else if (identifierSeq != null) {\n+          // Untyped return columns.\n+          val attrs = visitIdentifierSeq(identifierSeq).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+          (attrs, false)\n+        } else {\n+          (Seq(AttributeReference(\"key\", StringType)(),\n+            AttributeReference(\"value\", StringType)()), true)\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          string(script),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(relation)(withGenerate)\n+\n+        // Add where.\n+        val withFilter = withLateralView.optionalMap(where)(filter)\n+\n+        // Add aggregation or a project.\n+        val namedExpressions = expressions.map {\n+          case e: NamedExpression => e\n+          case e: Expression => UnresolvedAlias(e)\n+        }\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, namedExpressions, withFilter)\n+        } else if (namedExpressions.nonEmpty) {\n+          Project(namedExpressions, withFilter)\n+        } else {\n+          withFilter\n+        }\n+\n+        // Having\n+        val withHaving = withProject.optional(having) {\n+          // Note that we added a cast to boolean. If the expression itself is already boolean,\n+          // the optimizer will get rid of the unnecessary cast.\n+          Filter(Cast(expression(having), BooleanType), withProject)\n+        }\n+\n+        // Distinct\n+        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n+          Distinct(withHaving)\n+        } else {\n+          withHaving\n+        }\n+\n+        // Window\n+        withDistinct.optionalMap(windows)(withWindows)\n+    }\n+  }\n+\n+  /**\n+   * Create a (Hive based) [[ScriptInputOutputSchema]].\n+   */\n+  protected def withScriptIOSchema(\n+      inRowFormat: RowFormatContext,\n+      recordWriter: Token,\n+      outRowFormat: RowFormatContext,\n+      recordReader: Token,\n+      schemaLess: Boolean): ScriptInputOutputSchema = null\n+\n+  /**\n+   * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma\n+   * separated) relations here, these get converted into a single plan by condition-less inner join.\n+   */\n+  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n+    val from = ctx.relation.asScala.map(plan).reduceLeft(Join(_, _, Inner, None))\n+    ctx.lateralView.asScala.foldLeft(from)(withGenerate)\n+  }\n+\n+  /**\n+   * Connect two queries by a Set operator.\n+   *\n+   * Supported Set operators are:\n+   * - UNION [DISTINCT]\n+   * - UNION ALL\n+   * - EXCEPT [DISTINCT]\n+   * - INTERSECT [DISTINCT]\n+   */\n+  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) {\n+    val left = plan(ctx.left)\n+    val right = plan(ctx.right)\n+    val all = Option(ctx.setQuantifier()).exists(_.ALL != null)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.UNION if all =>\n+        Union(left, right)\n+      case SqlBaseParser.UNION =>\n+        Distinct(Union(left, right))\n+      case SqlBaseParser.INTERSECT if all =>\n+        throw new ParseException(\"INTERSECT ALL is not supported.\", ctx)\n+      case SqlBaseParser.INTERSECT =>\n+        Intersect(left, right)\n+      case SqlBaseParser.EXCEPT if all =>\n+        throw new ParseException(\"EXCEPT ALL is not supported.\", ctx)\n+      case SqlBaseParser.EXCEPT =>\n+        Except(left, right)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[WithWindowDefinition]] operator to a logical plan.\n+   */\n+  private def withWindows(\n+      ctx: WindowsContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Collect all window specifications defined in the WINDOW clause.\n+    val baseWindowMap = ctx.namedWindow.asScala.map {\n+      wCtx =>\n+        (wCtx.identifier.getText, typedVisit[WindowSpec](wCtx.windowSpec))\n+    }.toMap\n+\n+    // Handle cases like\n+    // window w1 as (partition by p_mfgr order by p_name\n+    //               range between 2 preceding and 2 following),\n+    //        w2 as w1\n+    val windowMapView = baseWindowMap.mapValues {\n+      case WindowSpecReference(name) =>\n+        baseWindowMap.get(name) match {\n+          case Some(spec: WindowSpecDefinition) =>\n+            spec\n+          case Some(ref) =>\n+            throw new ParseException(s\"Window reference '$name' is not a window specification\", ctx)\n+          case None =>\n+            throw new ParseException(s\"Cannot resolve window reference '$name'\", ctx)\n+        }\n+      case spec: WindowSpecDefinition => spec\n+    }\n+\n+    // Note that mapValues creates a view instead of materialized map. We force materialization by\n+    // mapping over identity.\n+    WithWindowDefinition(windowMapView.map(identity), query)\n+  }\n+\n+  /**\n+   * Add an [[Aggregate]] to a logical plan.\n+   */\n+  private def withAggregation(\n+      ctx: AggregationContext,\n+      selectExpressions: Seq[NamedExpression],\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    val groupByExpressions = expressionList(groupingExpressions)\n+\n+    if (GROUPING != null) {\n+      // GROUP BY .... GROUPING SETS (...)\n+      val expressionMap = groupByExpressions.zipWithIndex.toMap\n+      val numExpressions = expressionMap.size\n+      val mask = (1 << numExpressions) - 1\n+      val masks = ctx.groupingSet.asScala.map {\n+        _.expression.asScala.foldLeft(mask) {\n+          case (bitmap, eCtx) =>\n+            // Find the index of the expression.\n+            val e = typedVisit[Expression](eCtx)\n+            val index = expressionMap.find(_._1.semanticEquals(e)).map(_._2).getOrElse(\n+              throw new ParseException(\n+                s\"$e doesn't show up in the GROUP BY list\", ctx))\n+            // 0 means that the column at the given index is a grouping column, 1 means it is not,\n+            // so we unset the bit in bitmap.\n+            bitmap & ~(1 << (numExpressions - 1 - index))\n+        }\n+      }\n+      GroupingSets(masks, groupByExpressions, query, selectExpressions)\n+    } else {\n+      // GROUP BY .... (WITH CUBE | WITH ROLLUP)?\n+      val mappedGroupByExpressions = if (CUBE != null) {\n+        Seq(Cube(groupByExpressions))\n+      } else if (ROLLUP != null) {\n+        Seq(Rollup(groupByExpressions))\n+      } else {\n+        groupByExpressions\n+      }\n+      Aggregate(mappedGroupByExpressions, selectExpressions, query)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[Generate]] (Lateral View) to a logical plan.\n+   */\n+  private def withGenerate(\n+      query: LogicalPlan,\n+      ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) {\n+    val expressions = expressionList(ctx.expression)\n+\n+    // Create the generator.\n+    val generator = ctx.qualifiedName.getText.toLowerCase match {\n+      case \"explode\" if expressions.size == 1 =>\n+        Explode(expressions.head)\n+      case \"json_tuple\" =>\n+        JsonTuple(expressions)\n+      case other =>\n+        withGenerator(other, expressions, ctx)\n+    }\n+\n+    Generate(\n+      generator,\n+      join = true,\n+      outer = ctx.OUTER != null,\n+      Some(ctx.tblName.getText.toLowerCase),\n+      ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.apply),\n+      query)\n+  }\n+\n+  /**\n+   * Create a [[Generator]]. Override this method in order to support custom Generators.\n+   */\n+  protected def withGenerator(\n+      name: String,\n+      expressions: Seq[Expression],\n+      ctx: LateralViewContext): Generator = {\n+    throw new ParseException(s\"Generator function '$name' is not supported\", ctx)\n+  }\n+\n+  /**\n+   * Create a joins between two or more logical plans.\n+   */\n+  override def visitJoinRelation(ctx: JoinRelationContext): LogicalPlan = withOrigin(ctx) {\n+    /** Build a join between two plans. */\n+    def join(ctx: JoinRelationContext, left: LogicalPlan, right: LogicalPlan): Join = {\n+      val baseJoinType = ctx.joinType match {\n+        case null => Inner\n+        case jt if jt.FULL != null => FullOuter\n+        case jt if jt.SEMI != null => LeftSemi\n+        case jt if jt.LEFT != null => LeftOuter\n+        case jt if jt.RIGHT != null => RightOuter\n+        case _ => Inner\n+      }\n+\n+      // Resolve the join type and join condition\n+      val (joinType, condition) = Option(ctx.joinCriteria) match {\n+        case Some(c) if c.USING != null =>\n+          val columns = c.identifier.asScala.map { column =>\n+            UnresolvedAttribute.quoted(column.getText)\n+          }\n+          (UsingJoin(baseJoinType, columns), None)\n+        case Some(c) if c.booleanExpression != null =>\n+          (baseJoinType, Option(expression(c.booleanExpression)))\n+        case None if ctx.NATURAL != null =>\n+          (NaturalJoin(baseJoinType), None)\n+        case None =>\n+          (baseJoinType, None)\n+      }\n+      Join(left, right, joinType, condition)\n+    }\n+\n+    // Handle all consecutive join clauses. ANTLR produces a right nested tree in which the the\n+    // first join clause is at the top. However fields of previously referenced tables can be used\n+    // in following join clauses. The tree needs to be reversed in order to make this work.\n+    var result = plan(ctx.left)\n+    var current = ctx\n+    while (current != null) {\n+      current.right match {\n+        case right: JoinRelationContext =>\n+          result = join(current, result, plan(right.left))\n+          current = right\n+        case right =>\n+          result = join(current, result, plan(right))\n+          current = null\n+      }\n+    }\n+    result\n+  }\n+\n+  /**\n+   * Add a [[Sample]] to a logical plan.\n+   *\n+   * This currently supports the following sampling methods:\n+   * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.\n+   * - TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages\n+   * are defined as a number between 0 and 100.\n+   * - TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a 'x' divided by 'y' fraction.\n+   */\n+  private def withSample(ctx: SampleContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Create a sampled plan if we need one.\n+    def sample(fraction: Double): Sample = {\n+      // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling\n+      // function takes X PERCENT as the input and the range of X is [0, 100], we need to\n+      // adjust the fraction.\n+      val eps = RandomSampler.roundingEpsilon\n+      assert(fraction >= 0.0 - eps && fraction <= 1.0 + eps,\n+        s\"Sampling fraction ($fraction) must be on interval [0, 1]\",\n+        ctx)\n+      Sample(0.0, fraction, withReplacement = false, (math.random * 1000).toInt, query)(true)\n+    }\n+\n+    ctx.sampleType.getType match {\n+      case SqlBaseParser.ROWS =>\n+        Limit(expression(ctx.expression), query)\n+\n+      case SqlBaseParser.PERCENTLIT =>\n+        val fraction = ctx.percentage.getText.toDouble\n+        sample(fraction / 100.0d)\n+\n+      case SqlBaseParser.BUCKET if ctx.ON != null =>\n+        throw new ParseException(\"TABLESAMPLE(BUCKET x OUT OF y ON id) is not supported\", ctx)\n+\n+      case SqlBaseParser.BUCKET =>\n+        sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a sub-query.\n+   */\n+  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryNoWith)\n+  }\n+\n+  /**\n+   * Create an un-aliased table reference. This is typically used for top-level table references,\n+   * for example:\n+   * {{{\n+   *   INSERT INTO db.tbl2\n+   *   TABLE db.tbl1\n+   * }}}\n+   */\n+  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(visitTableIdentifier(ctx.tableIdentifier), None)\n+  }\n+\n+  /**\n+   * Create an aliased table reference. This is typically used in FROM clauses.\n+   */\n+  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) {\n+    val table = UnresolvedRelation(\n+      visitTableIdentifier(ctx.tableIdentifier),\n+      Option(ctx.identifier).map(_.getText))\n+    table.optionalMap(ctx.sample)(withSample)\n+  }\n+\n+  /**\n+   * Create an inline table (a virtual table in Hive parlance).\n+   */\n+  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) {\n+    // Get the backing expressions.\n+    val expressions = ctx.expression.asScala.map { eCtx =>\n+      val e = expression(eCtx)\n+      assert(e.foldable, \"All expressions in an inline table must be constants.\", eCtx)\n+      e\n+    }\n+\n+    // Validate and evaluate the rows.\n+    val (structType, structConstructor) = expressions.head.dataType match {",
    "line": 687
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "what if it's not castable? and what if the first one is int and the second one is long? Logically we should cast the first one to long, how does other database handle it?\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-08-05T07:49:05Z",
    "diffHunk": "@@ -0,0 +1,1452 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import ParserUtils._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {\n+    visit(ctx.dataType).asInstanceOf[DataType]\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Make sure we do not try to create a plan for a native command.\n+   */\n+  override def visitExecuteNativeCommand(ctx: ExecuteNativeCommandContext): LogicalPlan = null\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          throw new ParseException(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(string(pattern)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }\n+\n+      // Check for duplicate names.\n+      ctes.groupBy(_._1).filter(_._2.size > 1).foreach {\n+        case (name, _) =>\n+          throw new ParseException(\n+            s\"Name '$name' is used for multiple common table expressions\", ctx)\n+      }\n+\n+      With(query, ctes.toMap)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryResultClauses).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryResultClauses).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).map(visitPartitionSpec).getOrElse(Map.empty)\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Create a partition specification map.\n+   */\n+  override def visitPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, Option[String]] = withOrigin(ctx) {\n+    ctx.partitionVal.asScala.map { pVal =>\n+      val name = pVal.identifier.getText.toLowerCase\n+      val value = Option(pVal.constant).map(visitStringConstant)\n+      name -> value\n+    }.toMap\n+  }\n+\n+  /**\n+   * Create a partition specification map without optional values.\n+   */\n+  protected def visitNonOptionalPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, String] = withOrigin(ctx) {\n+    visitPartitionSpec(ctx).mapValues(_.orNull).map(identity)\n+  }\n+\n+  /**\n+   * Convert a constant of any type into a string. This is typically used in DDL commands, and its\n+   * main purpose is to prevent slight differences due to back to back conversions i.e.:\n+   * String -> Literal -> String.\n+   */\n+  protected def visitStringConstant(ctx: ConstantContext): String = withOrigin(ctx) {\n+    ctx match {\n+      case s: StringLiteralContext => createString(s)\n+      case o => o.getText\n+    }\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan. These\n+   * clauses determine the shape (ordering/partitioning/rows) of the query result.\n+   */\n+  private def withQueryResultClauses(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      throw new ParseException(\n+        \"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // WINDOWS\n+    val withWindow = withOrder.optionalMap(windows)(withWindows)\n+\n+    // LIMIT\n+    withWindow.optional(limit) {\n+      Limit(typedVisit(limit), withWindow)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan = {\n+      Filter(expression(ctx), plan)\n+    }\n+\n+    // Expressions.\n+    val expressions = Option(namedExpressionSeq).toSeq\n+      .flatMap(_.namedExpression.asScala)\n+      .map(typedVisit[Expression])\n+\n+    // Create either a transform or a regular query.\n+    val specType = Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)\n+    specType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Add where.\n+        val withFilter = relation.optionalMap(where)(filter)\n+\n+        // Create the attributes.\n+        val (attributes, schemaLess) = if (colTypeList != null) {\n+          // Typed return columns.\n+          (createStructType(colTypeList).toAttributes, false)\n+        } else if (identifierSeq != null) {\n+          // Untyped return columns.\n+          val attrs = visitIdentifierSeq(identifierSeq).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+          (attrs, false)\n+        } else {\n+          (Seq(AttributeReference(\"key\", StringType)(),\n+            AttributeReference(\"value\", StringType)()), true)\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          string(script),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(relation)(withGenerate)\n+\n+        // Add where.\n+        val withFilter = withLateralView.optionalMap(where)(filter)\n+\n+        // Add aggregation or a project.\n+        val namedExpressions = expressions.map {\n+          case e: NamedExpression => e\n+          case e: Expression => UnresolvedAlias(e)\n+        }\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, namedExpressions, withFilter)\n+        } else if (namedExpressions.nonEmpty) {\n+          Project(namedExpressions, withFilter)\n+        } else {\n+          withFilter\n+        }\n+\n+        // Having\n+        val withHaving = withProject.optional(having) {\n+          // Note that we added a cast to boolean. If the expression itself is already boolean,\n+          // the optimizer will get rid of the unnecessary cast.\n+          Filter(Cast(expression(having), BooleanType), withProject)\n+        }\n+\n+        // Distinct\n+        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n+          Distinct(withHaving)\n+        } else {\n+          withHaving\n+        }\n+\n+        // Window\n+        withDistinct.optionalMap(windows)(withWindows)\n+    }\n+  }\n+\n+  /**\n+   * Create a (Hive based) [[ScriptInputOutputSchema]].\n+   */\n+  protected def withScriptIOSchema(\n+      inRowFormat: RowFormatContext,\n+      recordWriter: Token,\n+      outRowFormat: RowFormatContext,\n+      recordReader: Token,\n+      schemaLess: Boolean): ScriptInputOutputSchema = null\n+\n+  /**\n+   * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma\n+   * separated) relations here, these get converted into a single plan by condition-less inner join.\n+   */\n+  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n+    val from = ctx.relation.asScala.map(plan).reduceLeft(Join(_, _, Inner, None))\n+    ctx.lateralView.asScala.foldLeft(from)(withGenerate)\n+  }\n+\n+  /**\n+   * Connect two queries by a Set operator.\n+   *\n+   * Supported Set operators are:\n+   * - UNION [DISTINCT]\n+   * - UNION ALL\n+   * - EXCEPT [DISTINCT]\n+   * - INTERSECT [DISTINCT]\n+   */\n+  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) {\n+    val left = plan(ctx.left)\n+    val right = plan(ctx.right)\n+    val all = Option(ctx.setQuantifier()).exists(_.ALL != null)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.UNION if all =>\n+        Union(left, right)\n+      case SqlBaseParser.UNION =>\n+        Distinct(Union(left, right))\n+      case SqlBaseParser.INTERSECT if all =>\n+        throw new ParseException(\"INTERSECT ALL is not supported.\", ctx)\n+      case SqlBaseParser.INTERSECT =>\n+        Intersect(left, right)\n+      case SqlBaseParser.EXCEPT if all =>\n+        throw new ParseException(\"EXCEPT ALL is not supported.\", ctx)\n+      case SqlBaseParser.EXCEPT =>\n+        Except(left, right)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[WithWindowDefinition]] operator to a logical plan.\n+   */\n+  private def withWindows(\n+      ctx: WindowsContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Collect all window specifications defined in the WINDOW clause.\n+    val baseWindowMap = ctx.namedWindow.asScala.map {\n+      wCtx =>\n+        (wCtx.identifier.getText, typedVisit[WindowSpec](wCtx.windowSpec))\n+    }.toMap\n+\n+    // Handle cases like\n+    // window w1 as (partition by p_mfgr order by p_name\n+    //               range between 2 preceding and 2 following),\n+    //        w2 as w1\n+    val windowMapView = baseWindowMap.mapValues {\n+      case WindowSpecReference(name) =>\n+        baseWindowMap.get(name) match {\n+          case Some(spec: WindowSpecDefinition) =>\n+            spec\n+          case Some(ref) =>\n+            throw new ParseException(s\"Window reference '$name' is not a window specification\", ctx)\n+          case None =>\n+            throw new ParseException(s\"Cannot resolve window reference '$name'\", ctx)\n+        }\n+      case spec: WindowSpecDefinition => spec\n+    }\n+\n+    // Note that mapValues creates a view instead of materialized map. We force materialization by\n+    // mapping over identity.\n+    WithWindowDefinition(windowMapView.map(identity), query)\n+  }\n+\n+  /**\n+   * Add an [[Aggregate]] to a logical plan.\n+   */\n+  private def withAggregation(\n+      ctx: AggregationContext,\n+      selectExpressions: Seq[NamedExpression],\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    val groupByExpressions = expressionList(groupingExpressions)\n+\n+    if (GROUPING != null) {\n+      // GROUP BY .... GROUPING SETS (...)\n+      val expressionMap = groupByExpressions.zipWithIndex.toMap\n+      val numExpressions = expressionMap.size\n+      val mask = (1 << numExpressions) - 1\n+      val masks = ctx.groupingSet.asScala.map {\n+        _.expression.asScala.foldLeft(mask) {\n+          case (bitmap, eCtx) =>\n+            // Find the index of the expression.\n+            val e = typedVisit[Expression](eCtx)\n+            val index = expressionMap.find(_._1.semanticEquals(e)).map(_._2).getOrElse(\n+              throw new ParseException(\n+                s\"$e doesn't show up in the GROUP BY list\", ctx))\n+            // 0 means that the column at the given index is a grouping column, 1 means it is not,\n+            // so we unset the bit in bitmap.\n+            bitmap & ~(1 << (numExpressions - 1 - index))\n+        }\n+      }\n+      GroupingSets(masks, groupByExpressions, query, selectExpressions)\n+    } else {\n+      // GROUP BY .... (WITH CUBE | WITH ROLLUP)?\n+      val mappedGroupByExpressions = if (CUBE != null) {\n+        Seq(Cube(groupByExpressions))\n+      } else if (ROLLUP != null) {\n+        Seq(Rollup(groupByExpressions))\n+      } else {\n+        groupByExpressions\n+      }\n+      Aggregate(mappedGroupByExpressions, selectExpressions, query)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[Generate]] (Lateral View) to a logical plan.\n+   */\n+  private def withGenerate(\n+      query: LogicalPlan,\n+      ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) {\n+    val expressions = expressionList(ctx.expression)\n+\n+    // Create the generator.\n+    val generator = ctx.qualifiedName.getText.toLowerCase match {\n+      case \"explode\" if expressions.size == 1 =>\n+        Explode(expressions.head)\n+      case \"json_tuple\" =>\n+        JsonTuple(expressions)\n+      case other =>\n+        withGenerator(other, expressions, ctx)\n+    }\n+\n+    Generate(\n+      generator,\n+      join = true,\n+      outer = ctx.OUTER != null,\n+      Some(ctx.tblName.getText.toLowerCase),\n+      ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.apply),\n+      query)\n+  }\n+\n+  /**\n+   * Create a [[Generator]]. Override this method in order to support custom Generators.\n+   */\n+  protected def withGenerator(\n+      name: String,\n+      expressions: Seq[Expression],\n+      ctx: LateralViewContext): Generator = {\n+    throw new ParseException(s\"Generator function '$name' is not supported\", ctx)\n+  }\n+\n+  /**\n+   * Create a joins between two or more logical plans.\n+   */\n+  override def visitJoinRelation(ctx: JoinRelationContext): LogicalPlan = withOrigin(ctx) {\n+    /** Build a join between two plans. */\n+    def join(ctx: JoinRelationContext, left: LogicalPlan, right: LogicalPlan): Join = {\n+      val baseJoinType = ctx.joinType match {\n+        case null => Inner\n+        case jt if jt.FULL != null => FullOuter\n+        case jt if jt.SEMI != null => LeftSemi\n+        case jt if jt.LEFT != null => LeftOuter\n+        case jt if jt.RIGHT != null => RightOuter\n+        case _ => Inner\n+      }\n+\n+      // Resolve the join type and join condition\n+      val (joinType, condition) = Option(ctx.joinCriteria) match {\n+        case Some(c) if c.USING != null =>\n+          val columns = c.identifier.asScala.map { column =>\n+            UnresolvedAttribute.quoted(column.getText)\n+          }\n+          (UsingJoin(baseJoinType, columns), None)\n+        case Some(c) if c.booleanExpression != null =>\n+          (baseJoinType, Option(expression(c.booleanExpression)))\n+        case None if ctx.NATURAL != null =>\n+          (NaturalJoin(baseJoinType), None)\n+        case None =>\n+          (baseJoinType, None)\n+      }\n+      Join(left, right, joinType, condition)\n+    }\n+\n+    // Handle all consecutive join clauses. ANTLR produces a right nested tree in which the the\n+    // first join clause is at the top. However fields of previously referenced tables can be used\n+    // in following join clauses. The tree needs to be reversed in order to make this work.\n+    var result = plan(ctx.left)\n+    var current = ctx\n+    while (current != null) {\n+      current.right match {\n+        case right: JoinRelationContext =>\n+          result = join(current, result, plan(right.left))\n+          current = right\n+        case right =>\n+          result = join(current, result, plan(right))\n+          current = null\n+      }\n+    }\n+    result\n+  }\n+\n+  /**\n+   * Add a [[Sample]] to a logical plan.\n+   *\n+   * This currently supports the following sampling methods:\n+   * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.\n+   * - TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages\n+   * are defined as a number between 0 and 100.\n+   * - TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a 'x' divided by 'y' fraction.\n+   */\n+  private def withSample(ctx: SampleContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Create a sampled plan if we need one.\n+    def sample(fraction: Double): Sample = {\n+      // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling\n+      // function takes X PERCENT as the input and the range of X is [0, 100], we need to\n+      // adjust the fraction.\n+      val eps = RandomSampler.roundingEpsilon\n+      assert(fraction >= 0.0 - eps && fraction <= 1.0 + eps,\n+        s\"Sampling fraction ($fraction) must be on interval [0, 1]\",\n+        ctx)\n+      Sample(0.0, fraction, withReplacement = false, (math.random * 1000).toInt, query)(true)\n+    }\n+\n+    ctx.sampleType.getType match {\n+      case SqlBaseParser.ROWS =>\n+        Limit(expression(ctx.expression), query)\n+\n+      case SqlBaseParser.PERCENTLIT =>\n+        val fraction = ctx.percentage.getText.toDouble\n+        sample(fraction / 100.0d)\n+\n+      case SqlBaseParser.BUCKET if ctx.ON != null =>\n+        throw new ParseException(\"TABLESAMPLE(BUCKET x OUT OF y ON id) is not supported\", ctx)\n+\n+      case SqlBaseParser.BUCKET =>\n+        sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a sub-query.\n+   */\n+  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryNoWith)\n+  }\n+\n+  /**\n+   * Create an un-aliased table reference. This is typically used for top-level table references,\n+   * for example:\n+   * {{{\n+   *   INSERT INTO db.tbl2\n+   *   TABLE db.tbl1\n+   * }}}\n+   */\n+  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(visitTableIdentifier(ctx.tableIdentifier), None)\n+  }\n+\n+  /**\n+   * Create an aliased table reference. This is typically used in FROM clauses.\n+   */\n+  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) {\n+    val table = UnresolvedRelation(\n+      visitTableIdentifier(ctx.tableIdentifier),\n+      Option(ctx.identifier).map(_.getText))\n+    table.optionalMap(ctx.sample)(withSample)\n+  }\n+\n+  /**\n+   * Create an inline table (a virtual table in Hive parlance).\n+   */\n+  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) {\n+    // Get the backing expressions.\n+    val expressions = ctx.expression.asScala.map { eCtx =>\n+      val e = expression(eCtx)\n+      assert(e.foldable, \"All expressions in an inline table must be constants.\", eCtx)\n+      e\n+    }\n+\n+    // Validate and evaluate the rows.\n+    val (structType, structConstructor) = expressions.head.dataType match {",
    "line": 687
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "As discussed offline: The current run is to use the schema of the first row and to force other rows to comply to that schema. This is semantically very different from the same thing written as a number of UNION ALL statements, e.g.: `SELECT 1 AS id , 'a' AS name UNION ALL SELECT 2, 'b' UNION ALL SELECT 2, 'c'`\n\nFixing this has not the highest priority. The fix should probably be using `UNION ALL`s instead of a `LocalRelation` (the Optimizer should be able to rewrite this into a LocalRelation eventually).\n",
    "commit": "6f1c535162397f01acf0405bdc80b8c4c141fc64",
    "createdAt": "2016-08-05T11:17:36Z",
    "diffHunk": "@@ -0,0 +1,1452 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.parser.ng\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.antlr.v4.runtime.{ParserRuleContext, Token}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.parser.ng.SqlBaseParser._\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.random.RandomSampler\n+\n+/**\n+ * The AstBuilder converts an ANTLR4 ParseTree into a catalyst Expression, LogicalPlan or\n+ * TableIdentifier.\n+ */\n+class AstBuilder extends SqlBaseBaseVisitor[AnyRef] with Logging {\n+  import ParserUtils._\n+\n+  protected def typedVisit[T](ctx: ParseTree): T = {\n+    ctx.accept(this).asInstanceOf[T]\n+  }\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitSingleExpression(ctx: SingleExpressionContext): Expression = withOrigin(ctx) {\n+    visitNamedExpression(ctx.namedExpression)\n+  }\n+\n+  override def visitSingleTableIdentifier(\n+      ctx: SingleTableIdentifierContext): TableIdentifier = withOrigin(ctx) {\n+    visitTableIdentifier(ctx.tableIdentifier)\n+  }\n+\n+  override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {\n+    visit(ctx.dataType).asInstanceOf[DataType]\n+  }\n+\n+  /* ********************************************************************************************\n+   * Plan parsing\n+   * ******************************************************************************************** */\n+  protected def plan(tree: ParserRuleContext): LogicalPlan = typedVisit(tree)\n+\n+  /**\n+   * Make sure we do not try to create a plan for a native command.\n+   */\n+  override def visitExecuteNativeCommand(ctx: ExecuteNativeCommandContext): LogicalPlan = null\n+\n+  /**\n+   * Create a plan for a SHOW FUNCTIONS command.\n+   */\n+  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    if (qualifiedName != null) {\n+      val names = qualifiedName().identifier().asScala.map(_.getText).toList\n+      names match {\n+        case db :: name :: Nil =>\n+          ShowFunctions(Some(db), Some(name))\n+        case name :: Nil =>\n+          ShowFunctions(None, Some(name))\n+        case _ =>\n+          throw new ParseException(\"SHOW FUNCTIONS unsupported name\", ctx)\n+      }\n+    } else if (pattern != null) {\n+      ShowFunctions(None, Some(string(pattern)))\n+    } else {\n+      ShowFunctions(None, None)\n+    }\n+  }\n+\n+  /**\n+   * Create a plan for a DESCRIBE FUNCTION command.\n+   */\n+  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) {\n+    val functionName = ctx.qualifiedName().identifier().asScala.map(_.getText).mkString(\".\")\n+    DescribeFunction(functionName, ctx.EXTENDED != null)\n+  }\n+\n+  /**\n+   * Create a top-level plan with Common Table Expressions.\n+   */\n+  override def visitQuery(ctx: QueryContext): LogicalPlan = withOrigin(ctx) {\n+    val query = plan(ctx.queryNoWith)\n+\n+    // Apply CTEs\n+    query.optional(ctx.ctes) {\n+      val ctes = ctx.ctes.namedQuery.asScala.map {\n+        case nCtx =>\n+          val namedQuery = visitNamedQuery(nCtx)\n+          (namedQuery.alias, namedQuery)\n+      }\n+\n+      // Check for duplicate names.\n+      ctes.groupBy(_._1).filter(_._2.size > 1).foreach {\n+        case (name, _) =>\n+          throw new ParseException(\n+            s\"Name '$name' is used for multiple common table expressions\", ctx)\n+      }\n+\n+      With(query, ctes.toMap)\n+    }\n+  }\n+\n+  /**\n+   * Create a named logical plan.\n+   *\n+   * This is only used for Common Table Expressions.\n+   */\n+  override def visitNamedQuery(ctx: NamedQueryContext): SubqueryAlias = withOrigin(ctx) {\n+    SubqueryAlias(ctx.name.getText, plan(ctx.queryNoWith))\n+  }\n+\n+  /**\n+   * Create a logical plan which allows for multiple inserts using one 'from' statement. These\n+   * queries have the following SQL form:\n+   * {{{\n+   *   [WITH cte...]?\n+   *   FROM src\n+   *   [INSERT INTO tbl1 SELECT *]+\n+   * }}}\n+   * For example:\n+   * {{{\n+   *   FROM db.tbl1 A\n+   *   INSERT INTO dbo.tbl1 SELECT * WHERE A.value = 10 LIMIT 5\n+   *   INSERT INTO dbo.tbl2 SELECT * WHERE A.value = 12\n+   * }}}\n+   * This (Hive) feature cannot be combined with set-operators.\n+   */\n+  override def visitMultiInsertQuery(ctx: MultiInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    val from = visitFromClause(ctx.fromClause)\n+\n+    // Build the insert clauses.\n+    val inserts = ctx.multiInsertQueryBody.asScala.map {\n+      body =>\n+        assert(body.querySpecification.fromClause == null,\n+          \"Multi-Insert queries cannot have a FROM clause in their individual SELECT statements\",\n+          body)\n+\n+        withQuerySpecification(body.querySpecification, from).\n+          // Add organization statements.\n+          optionalMap(body.queryOrganization)(withQueryResultClauses).\n+          // Add insert.\n+          optionalMap(body.insertInto())(withInsertInto)\n+    }\n+\n+    // If there are multiple INSERTS just UNION them together into one query.\n+    inserts match {\n+      case Seq(query) => query\n+      case queries => Union(queries)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a regular (single-insert) query.\n+   */\n+  override def visitSingleInsertQuery(\n+      ctx: SingleInsertQueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryTerm).\n+      // Add organization statements.\n+      optionalMap(ctx.queryOrganization)(withQueryResultClauses).\n+      // Add insert.\n+      optionalMap(ctx.insertInto())(withInsertInto)\n+  }\n+\n+  /**\n+   * Add an INSERT INTO [TABLE]/INSERT OVERWRITE TABLE operation to the logical plan.\n+   */\n+  private def withInsertInto(\n+      ctx: InsertIntoContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    val tableIdent = visitTableIdentifier(ctx.tableIdentifier)\n+    val partitionKeys = Option(ctx.partitionSpec).map(visitPartitionSpec).getOrElse(Map.empty)\n+\n+    InsertIntoTable(\n+      UnresolvedRelation(tableIdent, None),\n+      partitionKeys,\n+      query,\n+      ctx.OVERWRITE != null,\n+      ctx.EXISTS != null)\n+  }\n+\n+  /**\n+   * Create a partition specification map.\n+   */\n+  override def visitPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, Option[String]] = withOrigin(ctx) {\n+    ctx.partitionVal.asScala.map { pVal =>\n+      val name = pVal.identifier.getText.toLowerCase\n+      val value = Option(pVal.constant).map(visitStringConstant)\n+      name -> value\n+    }.toMap\n+  }\n+\n+  /**\n+   * Create a partition specification map without optional values.\n+   */\n+  protected def visitNonOptionalPartitionSpec(\n+      ctx: PartitionSpecContext): Map[String, String] = withOrigin(ctx) {\n+    visitPartitionSpec(ctx).mapValues(_.orNull).map(identity)\n+  }\n+\n+  /**\n+   * Convert a constant of any type into a string. This is typically used in DDL commands, and its\n+   * main purpose is to prevent slight differences due to back to back conversions i.e.:\n+   * String -> Literal -> String.\n+   */\n+  protected def visitStringConstant(ctx: ConstantContext): String = withOrigin(ctx) {\n+    ctx match {\n+      case s: StringLiteralContext => createString(s)\n+      case o => o.getText\n+    }\n+  }\n+\n+  /**\n+   * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan. These\n+   * clauses determine the shape (ordering/partitioning/rows) of the query result.\n+   */\n+  private def withQueryResultClauses(\n+      ctx: QueryOrganizationContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause.\n+    val withOrder = if (\n+      !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // ORDER BY ...\n+      Sort(order.asScala.map(visitSortItem), global = true, query)\n+    } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ...\n+      Sort(sort.asScala.map(visitSortItem), global = false, query)\n+    } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // DISTRIBUTE BY ...\n+      RepartitionByExpression(expressionList(distributeBy), query)\n+    } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // SORT BY ... DISTRIBUTE BY ...\n+      Sort(\n+        sort.asScala.map(visitSortItem),\n+        global = false,\n+        RepartitionByExpression(expressionList(distributeBy), query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) {\n+      // CLUSTER BY ...\n+      val expressions = expressionList(clusterBy)\n+      Sort(\n+        expressions.map(SortOrder(_, Ascending)),\n+        global = false,\n+        RepartitionByExpression(expressions, query))\n+    } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) {\n+      // [EMPTY]\n+      query\n+    } else {\n+      throw new ParseException(\n+        \"Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported\", ctx)\n+    }\n+\n+    // WINDOWS\n+    val withWindow = withOrder.optionalMap(windows)(withWindows)\n+\n+    // LIMIT\n+    withWindow.optional(limit) {\n+      Limit(typedVisit(limit), withWindow)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan using a query specification.\n+   */\n+  override def visitQuerySpecification(\n+      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n+    val from = OneRowRelation.optional(ctx.fromClause) {\n+      visitFromClause(ctx.fromClause)\n+    }\n+    withQuerySpecification(ctx, from)\n+  }\n+\n+  /**\n+   * Add a query specification to a logical plan. The query specification is the core of the logical\n+   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n+   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n+   *\n+   * Note that query hints are ignored (both by the parser and the builder).\n+   */\n+  private def withQuerySpecification(\n+      ctx: QuerySpecificationContext,\n+      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+\n+    // WHERE\n+    def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan = {\n+      Filter(expression(ctx), plan)\n+    }\n+\n+    // Expressions.\n+    val expressions = Option(namedExpressionSeq).toSeq\n+      .flatMap(_.namedExpression.asScala)\n+      .map(typedVisit[Expression])\n+\n+    // Create either a transform or a regular query.\n+    val specType = Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)\n+    specType match {\n+      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n+        // Transform\n+\n+        // Add where.\n+        val withFilter = relation.optionalMap(where)(filter)\n+\n+        // Create the attributes.\n+        val (attributes, schemaLess) = if (colTypeList != null) {\n+          // Typed return columns.\n+          (createStructType(colTypeList).toAttributes, false)\n+        } else if (identifierSeq != null) {\n+          // Untyped return columns.\n+          val attrs = visitIdentifierSeq(identifierSeq).map { name =>\n+            AttributeReference(name, StringType, nullable = true)()\n+          }\n+          (attrs, false)\n+        } else {\n+          (Seq(AttributeReference(\"key\", StringType)(),\n+            AttributeReference(\"value\", StringType)()), true)\n+        }\n+\n+        // Create the transform.\n+        ScriptTransformation(\n+          expressions,\n+          string(script),\n+          attributes,\n+          withFilter,\n+          withScriptIOSchema(inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))\n+\n+      case SqlBaseParser.SELECT =>\n+        // Regular select\n+\n+        // Add lateral views.\n+        val withLateralView = ctx.lateralView.asScala.foldLeft(relation)(withGenerate)\n+\n+        // Add where.\n+        val withFilter = withLateralView.optionalMap(where)(filter)\n+\n+        // Add aggregation or a project.\n+        val namedExpressions = expressions.map {\n+          case e: NamedExpression => e\n+          case e: Expression => UnresolvedAlias(e)\n+        }\n+        val withProject = if (aggregation != null) {\n+          withAggregation(aggregation, namedExpressions, withFilter)\n+        } else if (namedExpressions.nonEmpty) {\n+          Project(namedExpressions, withFilter)\n+        } else {\n+          withFilter\n+        }\n+\n+        // Having\n+        val withHaving = withProject.optional(having) {\n+          // Note that we added a cast to boolean. If the expression itself is already boolean,\n+          // the optimizer will get rid of the unnecessary cast.\n+          Filter(Cast(expression(having), BooleanType), withProject)\n+        }\n+\n+        // Distinct\n+        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n+          Distinct(withHaving)\n+        } else {\n+          withHaving\n+        }\n+\n+        // Window\n+        withDistinct.optionalMap(windows)(withWindows)\n+    }\n+  }\n+\n+  /**\n+   * Create a (Hive based) [[ScriptInputOutputSchema]].\n+   */\n+  protected def withScriptIOSchema(\n+      inRowFormat: RowFormatContext,\n+      recordWriter: Token,\n+      outRowFormat: RowFormatContext,\n+      recordReader: Token,\n+      schemaLess: Boolean): ScriptInputOutputSchema = null\n+\n+  /**\n+   * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma\n+   * separated) relations here, these get converted into a single plan by condition-less inner join.\n+   */\n+  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n+    val from = ctx.relation.asScala.map(plan).reduceLeft(Join(_, _, Inner, None))\n+    ctx.lateralView.asScala.foldLeft(from)(withGenerate)\n+  }\n+\n+  /**\n+   * Connect two queries by a Set operator.\n+   *\n+   * Supported Set operators are:\n+   * - UNION [DISTINCT]\n+   * - UNION ALL\n+   * - EXCEPT [DISTINCT]\n+   * - INTERSECT [DISTINCT]\n+   */\n+  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) {\n+    val left = plan(ctx.left)\n+    val right = plan(ctx.right)\n+    val all = Option(ctx.setQuantifier()).exists(_.ALL != null)\n+    ctx.operator.getType match {\n+      case SqlBaseParser.UNION if all =>\n+        Union(left, right)\n+      case SqlBaseParser.UNION =>\n+        Distinct(Union(left, right))\n+      case SqlBaseParser.INTERSECT if all =>\n+        throw new ParseException(\"INTERSECT ALL is not supported.\", ctx)\n+      case SqlBaseParser.INTERSECT =>\n+        Intersect(left, right)\n+      case SqlBaseParser.EXCEPT if all =>\n+        throw new ParseException(\"EXCEPT ALL is not supported.\", ctx)\n+      case SqlBaseParser.EXCEPT =>\n+        Except(left, right)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[WithWindowDefinition]] operator to a logical plan.\n+   */\n+  private def withWindows(\n+      ctx: WindowsContext,\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Collect all window specifications defined in the WINDOW clause.\n+    val baseWindowMap = ctx.namedWindow.asScala.map {\n+      wCtx =>\n+        (wCtx.identifier.getText, typedVisit[WindowSpec](wCtx.windowSpec))\n+    }.toMap\n+\n+    // Handle cases like\n+    // window w1 as (partition by p_mfgr order by p_name\n+    //               range between 2 preceding and 2 following),\n+    //        w2 as w1\n+    val windowMapView = baseWindowMap.mapValues {\n+      case WindowSpecReference(name) =>\n+        baseWindowMap.get(name) match {\n+          case Some(spec: WindowSpecDefinition) =>\n+            spec\n+          case Some(ref) =>\n+            throw new ParseException(s\"Window reference '$name' is not a window specification\", ctx)\n+          case None =>\n+            throw new ParseException(s\"Cannot resolve window reference '$name'\", ctx)\n+        }\n+      case spec: WindowSpecDefinition => spec\n+    }\n+\n+    // Note that mapValues creates a view instead of materialized map. We force materialization by\n+    // mapping over identity.\n+    WithWindowDefinition(windowMapView.map(identity), query)\n+  }\n+\n+  /**\n+   * Add an [[Aggregate]] to a logical plan.\n+   */\n+  private def withAggregation(\n+      ctx: AggregationContext,\n+      selectExpressions: Seq[NamedExpression],\n+      query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    import ctx._\n+    val groupByExpressions = expressionList(groupingExpressions)\n+\n+    if (GROUPING != null) {\n+      // GROUP BY .... GROUPING SETS (...)\n+      val expressionMap = groupByExpressions.zipWithIndex.toMap\n+      val numExpressions = expressionMap.size\n+      val mask = (1 << numExpressions) - 1\n+      val masks = ctx.groupingSet.asScala.map {\n+        _.expression.asScala.foldLeft(mask) {\n+          case (bitmap, eCtx) =>\n+            // Find the index of the expression.\n+            val e = typedVisit[Expression](eCtx)\n+            val index = expressionMap.find(_._1.semanticEquals(e)).map(_._2).getOrElse(\n+              throw new ParseException(\n+                s\"$e doesn't show up in the GROUP BY list\", ctx))\n+            // 0 means that the column at the given index is a grouping column, 1 means it is not,\n+            // so we unset the bit in bitmap.\n+            bitmap & ~(1 << (numExpressions - 1 - index))\n+        }\n+      }\n+      GroupingSets(masks, groupByExpressions, query, selectExpressions)\n+    } else {\n+      // GROUP BY .... (WITH CUBE | WITH ROLLUP)?\n+      val mappedGroupByExpressions = if (CUBE != null) {\n+        Seq(Cube(groupByExpressions))\n+      } else if (ROLLUP != null) {\n+        Seq(Rollup(groupByExpressions))\n+      } else {\n+        groupByExpressions\n+      }\n+      Aggregate(mappedGroupByExpressions, selectExpressions, query)\n+    }\n+  }\n+\n+  /**\n+   * Add a [[Generate]] (Lateral View) to a logical plan.\n+   */\n+  private def withGenerate(\n+      query: LogicalPlan,\n+      ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) {\n+    val expressions = expressionList(ctx.expression)\n+\n+    // Create the generator.\n+    val generator = ctx.qualifiedName.getText.toLowerCase match {\n+      case \"explode\" if expressions.size == 1 =>\n+        Explode(expressions.head)\n+      case \"json_tuple\" =>\n+        JsonTuple(expressions)\n+      case other =>\n+        withGenerator(other, expressions, ctx)\n+    }\n+\n+    Generate(\n+      generator,\n+      join = true,\n+      outer = ctx.OUTER != null,\n+      Some(ctx.tblName.getText.toLowerCase),\n+      ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.apply),\n+      query)\n+  }\n+\n+  /**\n+   * Create a [[Generator]]. Override this method in order to support custom Generators.\n+   */\n+  protected def withGenerator(\n+      name: String,\n+      expressions: Seq[Expression],\n+      ctx: LateralViewContext): Generator = {\n+    throw new ParseException(s\"Generator function '$name' is not supported\", ctx)\n+  }\n+\n+  /**\n+   * Create a joins between two or more logical plans.\n+   */\n+  override def visitJoinRelation(ctx: JoinRelationContext): LogicalPlan = withOrigin(ctx) {\n+    /** Build a join between two plans. */\n+    def join(ctx: JoinRelationContext, left: LogicalPlan, right: LogicalPlan): Join = {\n+      val baseJoinType = ctx.joinType match {\n+        case null => Inner\n+        case jt if jt.FULL != null => FullOuter\n+        case jt if jt.SEMI != null => LeftSemi\n+        case jt if jt.LEFT != null => LeftOuter\n+        case jt if jt.RIGHT != null => RightOuter\n+        case _ => Inner\n+      }\n+\n+      // Resolve the join type and join condition\n+      val (joinType, condition) = Option(ctx.joinCriteria) match {\n+        case Some(c) if c.USING != null =>\n+          val columns = c.identifier.asScala.map { column =>\n+            UnresolvedAttribute.quoted(column.getText)\n+          }\n+          (UsingJoin(baseJoinType, columns), None)\n+        case Some(c) if c.booleanExpression != null =>\n+          (baseJoinType, Option(expression(c.booleanExpression)))\n+        case None if ctx.NATURAL != null =>\n+          (NaturalJoin(baseJoinType), None)\n+        case None =>\n+          (baseJoinType, None)\n+      }\n+      Join(left, right, joinType, condition)\n+    }\n+\n+    // Handle all consecutive join clauses. ANTLR produces a right nested tree in which the the\n+    // first join clause is at the top. However fields of previously referenced tables can be used\n+    // in following join clauses. The tree needs to be reversed in order to make this work.\n+    var result = plan(ctx.left)\n+    var current = ctx\n+    while (current != null) {\n+      current.right match {\n+        case right: JoinRelationContext =>\n+          result = join(current, result, plan(right.left))\n+          current = right\n+        case right =>\n+          result = join(current, result, plan(right))\n+          current = null\n+      }\n+    }\n+    result\n+  }\n+\n+  /**\n+   * Add a [[Sample]] to a logical plan.\n+   *\n+   * This currently supports the following sampling methods:\n+   * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.\n+   * - TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages\n+   * are defined as a number between 0 and 100.\n+   * - TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a 'x' divided by 'y' fraction.\n+   */\n+  private def withSample(ctx: SampleContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n+    // Create a sampled plan if we need one.\n+    def sample(fraction: Double): Sample = {\n+      // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling\n+      // function takes X PERCENT as the input and the range of X is [0, 100], we need to\n+      // adjust the fraction.\n+      val eps = RandomSampler.roundingEpsilon\n+      assert(fraction >= 0.0 - eps && fraction <= 1.0 + eps,\n+        s\"Sampling fraction ($fraction) must be on interval [0, 1]\",\n+        ctx)\n+      Sample(0.0, fraction, withReplacement = false, (math.random * 1000).toInt, query)(true)\n+    }\n+\n+    ctx.sampleType.getType match {\n+      case SqlBaseParser.ROWS =>\n+        Limit(expression(ctx.expression), query)\n+\n+      case SqlBaseParser.PERCENTLIT =>\n+        val fraction = ctx.percentage.getText.toDouble\n+        sample(fraction / 100.0d)\n+\n+      case SqlBaseParser.BUCKET if ctx.ON != null =>\n+        throw new ParseException(\"TABLESAMPLE(BUCKET x OUT OF y ON id) is not supported\", ctx)\n+\n+      case SqlBaseParser.BUCKET =>\n+        sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble)\n+    }\n+  }\n+\n+  /**\n+   * Create a logical plan for a sub-query.\n+   */\n+  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) {\n+    plan(ctx.queryNoWith)\n+  }\n+\n+  /**\n+   * Create an un-aliased table reference. This is typically used for top-level table references,\n+   * for example:\n+   * {{{\n+   *   INSERT INTO db.tbl2\n+   *   TABLE db.tbl1\n+   * }}}\n+   */\n+  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) {\n+    UnresolvedRelation(visitTableIdentifier(ctx.tableIdentifier), None)\n+  }\n+\n+  /**\n+   * Create an aliased table reference. This is typically used in FROM clauses.\n+   */\n+  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) {\n+    val table = UnresolvedRelation(\n+      visitTableIdentifier(ctx.tableIdentifier),\n+      Option(ctx.identifier).map(_.getText))\n+    table.optionalMap(ctx.sample)(withSample)\n+  }\n+\n+  /**\n+   * Create an inline table (a virtual table in Hive parlance).\n+   */\n+  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) {\n+    // Get the backing expressions.\n+    val expressions = ctx.expression.asScala.map { eCtx =>\n+      val e = expression(eCtx)\n+      assert(e.foldable, \"All expressions in an inline table must be constants.\", eCtx)\n+      e\n+    }\n+\n+    // Validate and evaluate the rows.\n+    val (structType, structConstructor) = expressions.head.dataType match {",
    "line": 687
  }],
  "prId": 11557
}]