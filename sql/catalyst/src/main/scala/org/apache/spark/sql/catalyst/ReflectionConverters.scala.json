[{
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "While extracted from ScalaReflection, I think these mainly do not use reflection, and so we could possibly call them CatalystTypeConverters or something.\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:35:44Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "Maybe `createToCatalystConverter`\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:35:58Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "and `createToScalaConverter`\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:36:13Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {\n+          converters(idx) = createCatalystConverter(iter.next().dataType)\n+          idx += 1\n+        }\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case r: Row => convertRowWithConverters(r, structType, converters)\n+\n+            case p: Product =>\n+              val ar = new Array[Any](structType.size)\n+              val iter = p.productIterator\n+              var idx = 0\n+              while (idx < structType.size) {\n+                ar(idx) = converters(idx)(iter.next())\n+                idx += 1\n+              }\n+              new GenericRowWithSchema(ar, structType)\n+\n+            case null => null\n+          }\n+        }\n+\n+      case _ =>\n+        (item: Any) => extractOption(item) match {\n+          case d: BigDecimal => Decimal(d)\n+          case d: java.math.BigDecimal => Decimal(d)\n+          case d: java.sql.Date => DateUtils.fromJavaDate(d)\n+          case other => other\n+        }\n+    }\n+  }\n+\n+  /** Converts Catalyst types used internally in rows to standard Scala types */\n+  def convertToScala(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (d, udt: UserDefinedType[_]) => udt.deserialize(d)\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToScala(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToScala(k, mapType.keyType) -> convertToScala(v, mapType.valueType)\n+    }\n+    case (r: Row, s: StructType) => convertRowToScala(r, s)\n+    case (d: Decimal, _: DecimalType) => d.toJavaBigDecimal\n+    case (i: Int, DateType) => DateUtils.toJavaDate(i)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Catalyst types to Scala type.\n+   */\n+  private[sql] def createScalaConverter(dataType: DataType): Any => Any = dataType match {"
  }, {
    "author": {
      "login": "aarondav"
    },
    "body": "Out of curiosity: is this function `@tailrec`-able?\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:55:43Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {\n+          converters(idx) = createCatalystConverter(iter.next().dataType)\n+          idx += 1\n+        }\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case r: Row => convertRowWithConverters(r, structType, converters)\n+\n+            case p: Product =>\n+              val ar = new Array[Any](structType.size)\n+              val iter = p.productIterator\n+              var idx = 0\n+              while (idx < structType.size) {\n+                ar(idx) = converters(idx)(iter.next())\n+                idx += 1\n+              }\n+              new GenericRowWithSchema(ar, structType)\n+\n+            case null => null\n+          }\n+        }\n+\n+      case _ =>\n+        (item: Any) => extractOption(item) match {\n+          case d: BigDecimal => Decimal(d)\n+          case d: java.math.BigDecimal => Decimal(d)\n+          case d: java.sql.Date => DateUtils.fromJavaDate(d)\n+          case other => other\n+        }\n+    }\n+  }\n+\n+  /** Converts Catalyst types used internally in rows to standard Scala types */\n+  def convertToScala(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (d, udt: UserDefinedType[_]) => udt.deserialize(d)\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToScala(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToScala(k, mapType.keyType) -> convertToScala(v, mapType.valueType)\n+    }\n+    case (r: Row, s: StructType) => convertRowToScala(r, s)\n+    case (d: Decimal, _: DecimalType) => d.toJavaBigDecimal\n+    case (i: Int, DateType) => DateUtils.toJavaDate(i)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Catalyst types to Scala type.\n+   */\n+  private[sql] def createScalaConverter(dataType: DataType): Any => Any = dataType match {"
  }, {
    "author": {
      "login": "vlyubin"
    },
    "body": "No...\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T04:46:59Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {\n+          converters(idx) = createCatalystConverter(iter.next().dataType)\n+          idx += 1\n+        }\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case r: Row => convertRowWithConverters(r, structType, converters)\n+\n+            case p: Product =>\n+              val ar = new Array[Any](structType.size)\n+              val iter = p.productIterator\n+              var idx = 0\n+              while (idx < structType.size) {\n+                ar(idx) = converters(idx)(iter.next())\n+                idx += 1\n+              }\n+              new GenericRowWithSchema(ar, structType)\n+\n+            case null => null\n+          }\n+        }\n+\n+      case _ =>\n+        (item: Any) => extractOption(item) match {\n+          case d: BigDecimal => Decimal(d)\n+          case d: java.math.BigDecimal => Decimal(d)\n+          case d: java.sql.Date => DateUtils.fromJavaDate(d)\n+          case other => other\n+        }\n+    }\n+  }\n+\n+  /** Converts Catalyst types used internally in rows to standard Scala types */\n+  def convertToScala(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (d, udt: UserDefinedType[_]) => udt.deserialize(d)\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToScala(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToScala(k, mapType.keyType) -> convertToScala(v, mapType.valueType)\n+    }\n+    case (r: Row, s: StructType) => convertRowToScala(r, s)\n+    case (d: Decimal, _: DecimalType) => d.toJavaBigDecimal\n+    case (i: Int, DateType) => DateUtils.toJavaDate(i)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Catalyst types to Scala type.\n+   */\n+  private[sql] def createScalaConverter(dataType: DataType): Any => Any = dataType match {"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "wait maybe just\n`case opt: Option[_] => opt.orNull`\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:36:57Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {"
  }, {
    "author": {
      "login": "vlyubin"
    },
    "body": "Ok\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T04:47:07Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "I think @davies's original point (correct me if wrong) was that we should also not call udt.serialize if item == null. We could do this by doing\n\n``` scala\nextractOption(item) match {\n  case null => null\n  case other => udt.serialize(other)\n}\n```\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:40:31Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "@marmbrus Should we handle java Lists?\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:45:00Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "It would almost certainly be faster to construct a scala.collection.mutable.HashMap\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:45:35Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))"
  }, {
    "author": {
      "login": "vlyubin"
    },
    "body": "Whoops. Originally I thought the maps have to be mutable, so I would call `toMap` in the end, and that would probably be faster on the list than HashMap, but since they do have to be mutable, obviously that's bad. I was still returning an immutable map before ...\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T04:17:57Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "It is not necessary to use the while-loop optimization out here, since this is not per-item. I think @davies made a suggestion like\n\n``` scala\nstructType.fields.zipWithIndex.foreach { case (element, idx) =>\n  converters(idx) = createCatalystConvertert(element.dataType)\n}\n```\n\nor, exploiting the fact further, something like\n`structType.fields.map(_.dataType).map(createCatalystConverter).toArray`\nwhich would probably be only negligibly slower in a per-partition function.\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:50:29Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {"
  }, {
    "author": {
      "login": "aarondav"
    },
    "body": "I see you used `s.fields.map(f => createCatalystConverter(f.dataType))` below, I think that's fine to copy up here.\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:57:31Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "nit: if one case statement in a single `match` used multiple lines, they all should, so put this guy and the null on their own\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:52:39Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {\n+          converters(idx) = createCatalystConverter(iter.next().dataType)\n+          idx += 1\n+        }\n+        (item: Any) => {\n+          extractOption(item) match {"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "(note that this method horribly violates the rule I claimed about putting all of the statements on their own line if any one needs it, and I think one can see the readability suffers somewhat for it)\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:54:28Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "these others were decided to be necessary?\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:56:26Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {\n+          converters(idx) = createCatalystConverter(iter.next().dataType)\n+          idx += 1\n+        }\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case r: Row => convertRowWithConverters(r, structType, converters)\n+\n+            case p: Product =>\n+              val ar = new Array[Any](structType.size)\n+              val iter = p.productIterator\n+              var idx = 0\n+              while (idx < structType.size) {\n+                ar(idx) = converters(idx)(iter.next())\n+                idx += 1\n+              }\n+              new GenericRowWithSchema(ar, structType)\n+\n+            case null => null\n+          }\n+        }\n+\n+      case _ =>\n+        (item: Any) => extractOption(item) match {\n+          case d: BigDecimal => Decimal(d)\n+          case d: java.math.BigDecimal => Decimal(d)\n+          case d: java.sql.Date => DateUtils.fromJavaDate(d)\n+          case other => other\n+        }\n+    }\n+  }\n+\n+  /** Converts Catalyst types used internally in rows to standard Scala types */\n+  def convertToScala(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (d, udt: UserDefinedType[_]) => udt.deserialize(d)\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToScala(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToScala(k, mapType.keyType) -> convertToScala(v, mapType.valueType)\n+    }\n+    case (r: Row, s: StructType) => convertRowToScala(r, s)\n+    case (d: Decimal, _: DecimalType) => d.toJavaBigDecimal\n+    case (i: Int, DateType) => DateUtils.toJavaDate(i)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Catalyst types to Scala type.\n+   */\n+  private[sql] def createScalaConverter(dataType: DataType): Any => Any = dataType match {\n+    // Check UDT first since UDTs can override other types\n+    case udt: UserDefinedType[_] =>\n+      (item: Any) => if (item == null) null else udt.deserialize(item)\n+\n+    case arrayType: ArrayType =>\n+      val elementConverter = createScalaConverter(arrayType.elementType)\n+      (item: Any) => if (item == null) null else item.asInstanceOf[Seq[_]].map(elementConverter)\n+\n+    case mapType: MapType =>\n+      val keyConverter = createScalaConverter(mapType.keyType)\n+      val valueConverter = createScalaConverter(mapType.valueType)\n+      (item: Any) => if (item == null) {\n+        null\n+      } else {\n+        item.asInstanceOf[Map[_, _]].map { case (k, v) =>\n+          keyConverter(k) -> valueConverter(v)\n+        }\n+      }\n+\n+    case s: StructType =>\n+      val converters = createScalaConvertersForStruct(s)\n+      (item: Any) => item match {\n+        case r: Row => convertRowWithConverters(r, s, converters)\n+        case other => other"
  }, {
    "author": {
      "login": "vlyubin"
    },
    "body": "The one for row wasn't, but for DateType, for example, I think it makes sense to leave other clause.\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T05:22:43Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {\n+          converters(idx) = createCatalystConverter(iter.next().dataType)\n+          idx += 1\n+        }\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case r: Row => convertRowWithConverters(r, structType, converters)\n+\n+            case p: Product =>\n+              val ar = new Array[Any](structType.size)\n+              val iter = p.productIterator\n+              var idx = 0\n+              while (idx < structType.size) {\n+                ar(idx) = converters(idx)(iter.next())\n+                idx += 1\n+              }\n+              new GenericRowWithSchema(ar, structType)\n+\n+            case null => null\n+          }\n+        }\n+\n+      case _ =>\n+        (item: Any) => extractOption(item) match {\n+          case d: BigDecimal => Decimal(d)\n+          case d: java.math.BigDecimal => Decimal(d)\n+          case d: java.sql.Date => DateUtils.fromJavaDate(d)\n+          case other => other\n+        }\n+    }\n+  }\n+\n+  /** Converts Catalyst types used internally in rows to standard Scala types */\n+  def convertToScala(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (d, udt: UserDefinedType[_]) => udt.deserialize(d)\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToScala(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToScala(k, mapType.keyType) -> convertToScala(v, mapType.valueType)\n+    }\n+    case (r: Row, s: StructType) => convertRowToScala(r, s)\n+    case (d: Decimal, _: DecimalType) => d.toJavaBigDecimal\n+    case (i: Int, DateType) => DateUtils.toJavaDate(i)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Catalyst types to Scala type.\n+   */\n+  private[sql] def createScalaConverter(dataType: DataType): Any => Any = dataType match {\n+    // Check UDT first since UDTs can override other types\n+    case udt: UserDefinedType[_] =>\n+      (item: Any) => if (item == null) null else udt.deserialize(item)\n+\n+    case arrayType: ArrayType =>\n+      val elementConverter = createScalaConverter(arrayType.elementType)\n+      (item: Any) => if (item == null) null else item.asInstanceOf[Seq[_]].map(elementConverter)\n+\n+    case mapType: MapType =>\n+      val keyConverter = createScalaConverter(mapType.keyType)\n+      val valueConverter = createScalaConverter(mapType.valueType)\n+      (item: Any) => if (item == null) {\n+        null\n+      } else {\n+        item.asInstanceOf[Map[_, _]].map { case (k, v) =>\n+          keyConverter(k) -> valueConverter(v)\n+        }\n+      }\n+\n+    case s: StructType =>\n+      val converters = createScalaConvertersForStruct(s)\n+      (item: Any) => item match {\n+        case r: Row => convertRowWithConverters(r, s, converters)\n+        case other => other"
  }],
  "prId": 5279
}, {
  "comments": [{
    "author": {
      "login": "aarondav"
    },
    "body": "This implementation is so clear that I would actually just inline it: `s.fields.map(f => createScalaConverter(f.dataType))`\n",
    "commit": "e75a38799943a03289c403e4f6821b5fe4f4a9b4",
    "createdAt": "2015-04-08T02:57:54Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst\n+\n+import java.util.{Map => JavaMap}\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Functions to convert Scala types to Catalyst types and vice versa.\n+ */\n+object ReflectionConverters {\n+  // The Predef.Map is scala.collection.immutable.Map.\n+  // Since the map values can be mutable, we explicitly import scala.collection.Map at here.\n+  import scala.collection.Map\n+\n+  /**\n+   * Converts Scala objects to catalyst rows / types.\n+   * Note: This is always called after schemaFor has been called.\n+   *       This ordering is important for UDT registration.\n+   */\n+  def convertToCatalyst(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (obj, udt: UserDefinedType[_]) => udt.serialize(obj)\n+    case (o: Option[_], _) => o.map(convertToCatalyst(_, dataType)).orNull\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToCatalyst(_, arrayType.elementType))\n+    case (s: Array[_], arrayType: ArrayType) =>\n+      s.toSeq.map(convertToCatalyst(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToCatalyst(k, mapType.keyType) -> convertToCatalyst(v, mapType.valueType)\n+    }\n+    case (jmap: JavaMap[_, _], mapType: MapType) =>\n+      val iter = jmap.entrySet.iterator\n+      var listOfEntries: List[(Any, Any)] = List()\n+      while (iter.hasNext) {\n+        val entry = iter.next()\n+        listOfEntries :+= (convertToCatalyst(entry.getKey, mapType.keyType),\n+          convertToCatalyst(entry.getValue, mapType.valueType))\n+      }\n+      listOfEntries.toMap\n+    case (p: Product, structType: StructType) =>\n+      val ar = new Array[Any](structType.size)\n+      val iter = p.productIterator\n+      var idx = 0\n+      while (idx < structType.size) {\n+        ar(idx) = convertToCatalyst(iter.next(), structType.fields(idx).dataType)\n+        idx += 1\n+      }\n+      new GenericRowWithSchema(ar, structType)\n+    case (d: BigDecimal, _) => Decimal(d)\n+    case (d: java.math.BigDecimal, _) => Decimal(d)\n+    case (d: java.sql.Date, _) => DateUtils.fromJavaDate(d)\n+    case (r: Row, structType: StructType) =>\n+      val converters = structType.fields.map {\n+        f => (item: Any) => convertToCatalyst(item, f.dataType)\n+      }\n+      convertRowWithConverters(r, structType, converters)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Scala objects to the specified catalyst type.\n+   */\n+  private[sql] def createCatalystConverter(dataType: DataType): Any => Any = {\n+    def extractOption(item: Any): Any = item match {\n+      case s: Some[_] => s.get\n+      case None => null\n+      case other => other\n+    }\n+\n+    dataType match {\n+      // Check UDT first since UDTs can override other types\n+      case udt: UserDefinedType[_] =>\n+        (item) => {\n+          if (item == None) null else udt.serialize(extractOption(item))\n+        }\n+\n+      case arrayType: ArrayType =>\n+        val elementConverter = createCatalystConverter(arrayType.elementType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case a: Array[_] => a.toSeq.map(elementConverter)\n+            case s: Seq[_] => s.map(elementConverter)\n+            case null => null\n+          }\n+        }\n+\n+      case mapType: MapType =>\n+        val keyConverter = createCatalystConverter(mapType.keyType)\n+        val valueConverter = createCatalystConverter(mapType.valueType)\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case m: Map[_, _] =>\n+              m.map { case (k, v) =>\n+                keyConverter(k) -> valueConverter(v)\n+              }\n+\n+            case jmap: JavaMap[_, _] =>\n+              val iter = jmap.entrySet.iterator\n+              var listOfEntries: List[(Any, Any)] = List()\n+              while (iter.hasNext) {\n+                val entry = iter.next()\n+                listOfEntries :+= (keyConverter(entry.getKey), valueConverter(entry.getValue))\n+              }\n+              listOfEntries.toMap\n+\n+            case null => null\n+          }\n+        }\n+\n+      case structType: StructType =>\n+        val converters = new Array[Any => Any](structType.length)\n+        val iter = structType.fields.iterator\n+        var idx = 0\n+        while (iter.hasNext) {\n+          converters(idx) = createCatalystConverter(iter.next().dataType)\n+          idx += 1\n+        }\n+        (item: Any) => {\n+          extractOption(item) match {\n+            case r: Row => convertRowWithConverters(r, structType, converters)\n+\n+            case p: Product =>\n+              val ar = new Array[Any](structType.size)\n+              val iter = p.productIterator\n+              var idx = 0\n+              while (idx < structType.size) {\n+                ar(idx) = converters(idx)(iter.next())\n+                idx += 1\n+              }\n+              new GenericRowWithSchema(ar, structType)\n+\n+            case null => null\n+          }\n+        }\n+\n+      case _ =>\n+        (item: Any) => extractOption(item) match {\n+          case d: BigDecimal => Decimal(d)\n+          case d: java.math.BigDecimal => Decimal(d)\n+          case d: java.sql.Date => DateUtils.fromJavaDate(d)\n+          case other => other\n+        }\n+    }\n+  }\n+\n+  /** Converts Catalyst types used internally in rows to standard Scala types */\n+  def convertToScala(a: Any, dataType: DataType): Any = (a, dataType) match {\n+    // Check UDT first since UDTs can override other types\n+    case (d, udt: UserDefinedType[_]) => udt.deserialize(d)\n+    case (s: Seq[_], arrayType: ArrayType) => s.map(convertToScala(_, arrayType.elementType))\n+    case (m: Map[_, _], mapType: MapType) => m.map { case (k, v) =>\n+      convertToScala(k, mapType.keyType) -> convertToScala(v, mapType.valueType)\n+    }\n+    case (r: Row, s: StructType) => convertRowToScala(r, s)\n+    case (d: Decimal, _: DecimalType) => d.toJavaBigDecimal\n+    case (i: Int, DateType) => DateUtils.toJavaDate(i)\n+    case (other, _) => other\n+  }\n+\n+  /**\n+   * Creates a converter function that will convert Catalyst types to Scala type.\n+   */\n+  private[sql] def createScalaConverter(dataType: DataType): Any => Any = dataType match {\n+    // Check UDT first since UDTs can override other types\n+    case udt: UserDefinedType[_] =>\n+      (item: Any) => if (item == null) null else udt.deserialize(item)\n+\n+    case arrayType: ArrayType =>\n+      val elementConverter = createScalaConverter(arrayType.elementType)\n+      (item: Any) => if (item == null) null else item.asInstanceOf[Seq[_]].map(elementConverter)\n+\n+    case mapType: MapType =>\n+      val keyConverter = createScalaConverter(mapType.keyType)\n+      val valueConverter = createScalaConverter(mapType.valueType)\n+      (item: Any) => if (item == null) {\n+        null\n+      } else {\n+        item.asInstanceOf[Map[_, _]].map { case (k, v) =>\n+          keyConverter(k) -> valueConverter(v)\n+        }\n+      }\n+\n+    case s: StructType =>\n+      val converters = createScalaConvertersForStruct(s)"
  }],
  "prId": 5279
}]