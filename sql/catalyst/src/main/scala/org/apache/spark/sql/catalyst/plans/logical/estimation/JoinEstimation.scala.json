[{
  "comments": [{
    "author": {
      "login": "wzhfy"
    },
    "body": "Here, Hive uses an exponential decay to compute the denominator when number of join keys > number of join tables, i.e. ndv1 * ndv2^(1/2) * ndv3^(1/4)... I just use a more conservative strategy by max(ndv1, ndv2, ...). I'm not sure which one is better. Do you know any theoretical or empirical support for hive's strategy? @rxin @srinathshankar ",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-09T07:18:28Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator."
  }, {
    "author": {
      "login": "ioana-delaney"
    },
    "body": "They probably estimate for the number of distinct values in a vector of columns using uniform distribution.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-15T00:51:03Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator."
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "If we are not ready for expressions, I think we should not handle `Cast` either, as it may be tricky to handle overflow correctly(e.g. cast long to int)",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-09T14:25:22Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *\n+   * @return Return the updated statistics after join. Return `None` if the join type is not\n+   *         supported, or we don't have enough statistics for estimation.\n+   */\n+  // scalastyle:on\n+  def estimate(join: Join): Option[Statistics] = join match {\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)\n+        if supportsJoinType(joinType) && hasRowCountStat(left, right) =>\n+\n+      // 1. Compute the denominator\n+      var ndvDenom: BigInt = -1\n+      val keyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val leftStats = left.statistics\n+      val rightStats = right.statistics\n+      val intersectedStats = new mutable.HashMap[String, ColumnStat]()\n+      var i = 0\n+      while(i < keyPairs.length && ndvDenom != 0) {\n+        val (leftKey, rightKey) = keyPairs(i)\n+        // Do estimation if we have enough statistics\n+        if (hasColumnStat((left, leftKey), (right, rightKey))) {\n+          val leftKeyStats = leftStats.colStats(leftKey.name)\n+          val rightKeyStats = rightStats.colStats(rightKey.name)\n+\n+          // Check if the two sides are disjoint\n+          val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+          val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+          if (Range.isIntersected(lRange, rRange)) {\n+            // Get the largest ndv among pairs of join keys\n+            val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+            if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+\n+            // Update intersected column stats\n+            val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+            val (newMin1, newMax1, newMin2, newMax2) =\n+              Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+            intersectedStats.put(leftKey.name, intersectedColumnStat(leftKeyStats, minNdv,\n+              newMin1, newMax1))\n+            intersectedStats.put(rightKey.name, intersectedColumnStat(rightKeyStats, minNdv,\n+              newMin2, newMax2))\n+          } else {\n+            // Set ndvDenom to zero to indicate that this join should have no output\n+            ndvDenom = 0\n+          }\n+        }\n+        i += 1\n+      }\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val outputRows: BigInt = if (ndvDenom == 0) {\n+        // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+        0\n+      } else if (ndvDenom < 0) {\n+        // There isn't join keys or column stats for any of the join key pairs, we estimate like\n+        // cartesian product.\n+        leftRows * rightRows\n+      } else {\n+        ceil(BigDecimal(leftRows * rightRows) / BigDecimal(ndvDenom))\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val originalStats = leftStats.colStats ++ rightStats.colStats\n+      val outputWithStat = join.output.filter(attr => originalStats.contains(attr.name))\n+\n+      val outputColStats = new mutable.HashMap[String, ColumnStat]()\n+      if (ndvDenom == 0) {\n+        // empty output\n+        outputWithStat.foreach(a => outputColStats.put(a.name, emptyColumnStat(a.dataType)))\n+      } else if (ndvDenom < 0) {\n+        // cartesian product, column stats will not change\n+        outputWithStat.foreach(a => outputColStats.put(a.name, originalStats(a.name)))\n+      } else {\n+        val leftRatio = BigDecimal(outputRows) / BigDecimal(leftRows)\n+        val rightRatio = BigDecimal(outputRows) / BigDecimal(rightRows)\n+        outputWithStat.foreach { a =>\n+          // check if this attribute is a join key\n+          if (intersectedStats.contains(a.name)) {\n+            outputColStats.put(a.name, intersectedStats(a.name))\n+          } else {\n+            val oldColStat = originalStats(a.name)\n+            val oldNdv = oldColStat.distinctCount\n+            // We only change (scale down) the number of distinct values if the number of rows\n+            // decreases after join, because join won't produce new values even if the number of\n+            // rows increases.\n+            val newNdv = if (left.outputSet.contains(a) && leftRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * leftRatio)\n+            } else if (right.outputSet.contains(a) && rightRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * rightRatio)\n+            } else {\n+              oldNdv\n+            }\n+            outputColStats.put(a.name, oldColStat.copy(distinctCount = newNdv, nullCount = 0))\n+          }\n+        }\n+      }\n+\n+      Some(Statistics(\n+        sizeInBytes = outputRows * getRowSize(join.output, outputColStats.toMap),\n+        rowCount = Some(outputRows),\n+        colStats = outputColStats.toMap,\n+        isBroadcastable = false))\n+\n+    case _ => None\n+  }\n+\n+  def emptyColumnStat(dataType: DataType): ColumnStat = {\n+    ColumnStat(distinctCount = 0, min = None, max = None, nullCount = 0,\n+      avgLen = dataType.defaultSize, maxLen = dataType.defaultSize)\n+  }\n+\n+  def intersectedColumnStat(\n+      origin: ColumnStat,\n+      newDistinctCount: BigInt,\n+      newMin: Option[Any],\n+      newMax: Option[Any]): ColumnStat = {\n+    origin.copy(distinctCount = newDistinctCount, min = newMin, max = newMax, nullCount = 0)\n+  }\n+\n+  def extractJoinKeys(\n+      leftKeys: Seq[Expression],\n+      rightKeys: Seq[Expression]): Seq[(AttributeReference, AttributeReference)] = {\n+    leftKeys.zip(rightKeys).flatMap {\n+      case (ExtractAttr(left), ExtractAttr(right)) => Some((left, right))\n+      // Currently we don't deal with equal joins like key1 = key2 + 5."
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Yes, but I'm a little worried about losing estimation opportunities because of this rare case (in my understanding this kind of downgrading cast is not common)?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-10T02:26:01Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *\n+   * @return Return the updated statistics after join. Return `None` if the join type is not\n+   *         supported, or we don't have enough statistics for estimation.\n+   */\n+  // scalastyle:on\n+  def estimate(join: Join): Option[Statistics] = join match {\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)\n+        if supportsJoinType(joinType) && hasRowCountStat(left, right) =>\n+\n+      // 1. Compute the denominator\n+      var ndvDenom: BigInt = -1\n+      val keyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val leftStats = left.statistics\n+      val rightStats = right.statistics\n+      val intersectedStats = new mutable.HashMap[String, ColumnStat]()\n+      var i = 0\n+      while(i < keyPairs.length && ndvDenom != 0) {\n+        val (leftKey, rightKey) = keyPairs(i)\n+        // Do estimation if we have enough statistics\n+        if (hasColumnStat((left, leftKey), (right, rightKey))) {\n+          val leftKeyStats = leftStats.colStats(leftKey.name)\n+          val rightKeyStats = rightStats.colStats(rightKey.name)\n+\n+          // Check if the two sides are disjoint\n+          val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+          val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+          if (Range.isIntersected(lRange, rRange)) {\n+            // Get the largest ndv among pairs of join keys\n+            val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+            if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+\n+            // Update intersected column stats\n+            val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+            val (newMin1, newMax1, newMin2, newMax2) =\n+              Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+            intersectedStats.put(leftKey.name, intersectedColumnStat(leftKeyStats, minNdv,\n+              newMin1, newMax1))\n+            intersectedStats.put(rightKey.name, intersectedColumnStat(rightKeyStats, minNdv,\n+              newMin2, newMax2))\n+          } else {\n+            // Set ndvDenom to zero to indicate that this join should have no output\n+            ndvDenom = 0\n+          }\n+        }\n+        i += 1\n+      }\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val outputRows: BigInt = if (ndvDenom == 0) {\n+        // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+        0\n+      } else if (ndvDenom < 0) {\n+        // There isn't join keys or column stats for any of the join key pairs, we estimate like\n+        // cartesian product.\n+        leftRows * rightRows\n+      } else {\n+        ceil(BigDecimal(leftRows * rightRows) / BigDecimal(ndvDenom))\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val originalStats = leftStats.colStats ++ rightStats.colStats\n+      val outputWithStat = join.output.filter(attr => originalStats.contains(attr.name))\n+\n+      val outputColStats = new mutable.HashMap[String, ColumnStat]()\n+      if (ndvDenom == 0) {\n+        // empty output\n+        outputWithStat.foreach(a => outputColStats.put(a.name, emptyColumnStat(a.dataType)))\n+      } else if (ndvDenom < 0) {\n+        // cartesian product, column stats will not change\n+        outputWithStat.foreach(a => outputColStats.put(a.name, originalStats(a.name)))\n+      } else {\n+        val leftRatio = BigDecimal(outputRows) / BigDecimal(leftRows)\n+        val rightRatio = BigDecimal(outputRows) / BigDecimal(rightRows)\n+        outputWithStat.foreach { a =>\n+          // check if this attribute is a join key\n+          if (intersectedStats.contains(a.name)) {\n+            outputColStats.put(a.name, intersectedStats(a.name))\n+          } else {\n+            val oldColStat = originalStats(a.name)\n+            val oldNdv = oldColStat.distinctCount\n+            // We only change (scale down) the number of distinct values if the number of rows\n+            // decreases after join, because join won't produce new values even if the number of\n+            // rows increases.\n+            val newNdv = if (left.outputSet.contains(a) && leftRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * leftRatio)\n+            } else if (right.outputSet.contains(a) && rightRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * rightRatio)\n+            } else {\n+              oldNdv\n+            }\n+            outputColStats.put(a.name, oldColStat.copy(distinctCount = newNdv, nullCount = 0))\n+          }\n+        }\n+      }\n+\n+      Some(Statistics(\n+        sizeInBytes = outputRows * getRowSize(join.output, outputColStats.toMap),\n+        rowCount = Some(outputRows),\n+        colStats = outputColStats.toMap,\n+        isBroadcastable = false))\n+\n+    case _ => None\n+  }\n+\n+  def emptyColumnStat(dataType: DataType): ColumnStat = {\n+    ColumnStat(distinctCount = 0, min = None, max = None, nullCount = 0,\n+      avgLen = dataType.defaultSize, maxLen = dataType.defaultSize)\n+  }\n+\n+  def intersectedColumnStat(\n+      origin: ColumnStat,\n+      newDistinctCount: BigInt,\n+      newMin: Option[Any],\n+      newMax: Option[Any]): ColumnStat = {\n+    origin.copy(distinctCount = newDistinctCount, min = newMin, max = newMax, nullCount = 0)\n+  }\n+\n+  def extractJoinKeys(\n+      leftKeys: Seq[Expression],\n+      rightKeys: Seq[Expression]): Seq[(AttributeReference, AttributeReference)] = {\n+    leftKeys.zip(rightKeys).flatMap {\n+      case (ExtractAttr(left), ExtractAttr(right)) => Some((left, right))\n+      // Currently we don't deal with equal joins like key1 = key2 + 5."
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Can we define a new parent class for such downgrading cast? Will this change be big?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-10T02:28:15Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *\n+   * @return Return the updated statistics after join. Return `None` if the join type is not\n+   *         supported, or we don't have enough statistics for estimation.\n+   */\n+  // scalastyle:on\n+  def estimate(join: Join): Option[Statistics] = join match {\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)\n+        if supportsJoinType(joinType) && hasRowCountStat(left, right) =>\n+\n+      // 1. Compute the denominator\n+      var ndvDenom: BigInt = -1\n+      val keyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val leftStats = left.statistics\n+      val rightStats = right.statistics\n+      val intersectedStats = new mutable.HashMap[String, ColumnStat]()\n+      var i = 0\n+      while(i < keyPairs.length && ndvDenom != 0) {\n+        val (leftKey, rightKey) = keyPairs(i)\n+        // Do estimation if we have enough statistics\n+        if (hasColumnStat((left, leftKey), (right, rightKey))) {\n+          val leftKeyStats = leftStats.colStats(leftKey.name)\n+          val rightKeyStats = rightStats.colStats(rightKey.name)\n+\n+          // Check if the two sides are disjoint\n+          val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+          val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+          if (Range.isIntersected(lRange, rRange)) {\n+            // Get the largest ndv among pairs of join keys\n+            val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+            if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+\n+            // Update intersected column stats\n+            val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+            val (newMin1, newMax1, newMin2, newMax2) =\n+              Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+            intersectedStats.put(leftKey.name, intersectedColumnStat(leftKeyStats, minNdv,\n+              newMin1, newMax1))\n+            intersectedStats.put(rightKey.name, intersectedColumnStat(rightKeyStats, minNdv,\n+              newMin2, newMax2))\n+          } else {\n+            // Set ndvDenom to zero to indicate that this join should have no output\n+            ndvDenom = 0\n+          }\n+        }\n+        i += 1\n+      }\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val outputRows: BigInt = if (ndvDenom == 0) {\n+        // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+        0\n+      } else if (ndvDenom < 0) {\n+        // There isn't join keys or column stats for any of the join key pairs, we estimate like\n+        // cartesian product.\n+        leftRows * rightRows\n+      } else {\n+        ceil(BigDecimal(leftRows * rightRows) / BigDecimal(ndvDenom))\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val originalStats = leftStats.colStats ++ rightStats.colStats\n+      val outputWithStat = join.output.filter(attr => originalStats.contains(attr.name))\n+\n+      val outputColStats = new mutable.HashMap[String, ColumnStat]()\n+      if (ndvDenom == 0) {\n+        // empty output\n+        outputWithStat.foreach(a => outputColStats.put(a.name, emptyColumnStat(a.dataType)))\n+      } else if (ndvDenom < 0) {\n+        // cartesian product, column stats will not change\n+        outputWithStat.foreach(a => outputColStats.put(a.name, originalStats(a.name)))\n+      } else {\n+        val leftRatio = BigDecimal(outputRows) / BigDecimal(leftRows)\n+        val rightRatio = BigDecimal(outputRows) / BigDecimal(rightRows)\n+        outputWithStat.foreach { a =>\n+          // check if this attribute is a join key\n+          if (intersectedStats.contains(a.name)) {\n+            outputColStats.put(a.name, intersectedStats(a.name))\n+          } else {\n+            val oldColStat = originalStats(a.name)\n+            val oldNdv = oldColStat.distinctCount\n+            // We only change (scale down) the number of distinct values if the number of rows\n+            // decreases after join, because join won't produce new values even if the number of\n+            // rows increases.\n+            val newNdv = if (left.outputSet.contains(a) && leftRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * leftRatio)\n+            } else if (right.outputSet.contains(a) && rightRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * rightRatio)\n+            } else {\n+              oldNdv\n+            }\n+            outputColStats.put(a.name, oldColStat.copy(distinctCount = newNdv, nullCount = 0))\n+          }\n+        }\n+      }\n+\n+      Some(Statistics(\n+        sizeInBytes = outputRows * getRowSize(join.output, outputColStats.toMap),\n+        rowCount = Some(outputRows),\n+        colStats = outputColStats.toMap,\n+        isBroadcastable = false))\n+\n+    case _ => None\n+  }\n+\n+  def emptyColumnStat(dataType: DataType): ColumnStat = {\n+    ColumnStat(distinctCount = 0, min = None, max = None, nullCount = 0,\n+      avgLen = dataType.defaultSize, maxLen = dataType.defaultSize)\n+  }\n+\n+  def intersectedColumnStat(\n+      origin: ColumnStat,\n+      newDistinctCount: BigInt,\n+      newMin: Option[Any],\n+      newMax: Option[Any]): ColumnStat = {\n+    origin.copy(distinctCount = newDistinctCount, min = newMin, max = newMax, nullCount = 0)\n+  }\n+\n+  def extractJoinKeys(\n+      leftKeys: Seq[Expression],\n+      rightKeys: Seq[Expression]): Seq[(AttributeReference, AttributeReference)] = {\n+    leftKeys.zip(rightKeys).flatMap {\n+      case (ExtractAttr(left), ExtractAttr(right)) => Some((left, right))\n+      // Currently we don't deal with equal joins like key1 = key2 + 5."
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I don't think we should add a new expression just for the current implementation limitations. BTW, only handle `Cast` may also lose a lot of estimation opportunities, we should support expression before we release this feature.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-10T05:49:50Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *\n+   * @return Return the updated statistics after join. Return `None` if the join type is not\n+   *         supported, or we don't have enough statistics for estimation.\n+   */\n+  // scalastyle:on\n+  def estimate(join: Join): Option[Statistics] = join match {\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)\n+        if supportsJoinType(joinType) && hasRowCountStat(left, right) =>\n+\n+      // 1. Compute the denominator\n+      var ndvDenom: BigInt = -1\n+      val keyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val leftStats = left.statistics\n+      val rightStats = right.statistics\n+      val intersectedStats = new mutable.HashMap[String, ColumnStat]()\n+      var i = 0\n+      while(i < keyPairs.length && ndvDenom != 0) {\n+        val (leftKey, rightKey) = keyPairs(i)\n+        // Do estimation if we have enough statistics\n+        if (hasColumnStat((left, leftKey), (right, rightKey))) {\n+          val leftKeyStats = leftStats.colStats(leftKey.name)\n+          val rightKeyStats = rightStats.colStats(rightKey.name)\n+\n+          // Check if the two sides are disjoint\n+          val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+          val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+          if (Range.isIntersected(lRange, rRange)) {\n+            // Get the largest ndv among pairs of join keys\n+            val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+            if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+\n+            // Update intersected column stats\n+            val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+            val (newMin1, newMax1, newMin2, newMax2) =\n+              Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+            intersectedStats.put(leftKey.name, intersectedColumnStat(leftKeyStats, minNdv,\n+              newMin1, newMax1))\n+            intersectedStats.put(rightKey.name, intersectedColumnStat(rightKeyStats, minNdv,\n+              newMin2, newMax2))\n+          } else {\n+            // Set ndvDenom to zero to indicate that this join should have no output\n+            ndvDenom = 0\n+          }\n+        }\n+        i += 1\n+      }\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val outputRows: BigInt = if (ndvDenom == 0) {\n+        // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+        0\n+      } else if (ndvDenom < 0) {\n+        // There isn't join keys or column stats for any of the join key pairs, we estimate like\n+        // cartesian product.\n+        leftRows * rightRows\n+      } else {\n+        ceil(BigDecimal(leftRows * rightRows) / BigDecimal(ndvDenom))\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val originalStats = leftStats.colStats ++ rightStats.colStats\n+      val outputWithStat = join.output.filter(attr => originalStats.contains(attr.name))\n+\n+      val outputColStats = new mutable.HashMap[String, ColumnStat]()\n+      if (ndvDenom == 0) {\n+        // empty output\n+        outputWithStat.foreach(a => outputColStats.put(a.name, emptyColumnStat(a.dataType)))\n+      } else if (ndvDenom < 0) {\n+        // cartesian product, column stats will not change\n+        outputWithStat.foreach(a => outputColStats.put(a.name, originalStats(a.name)))\n+      } else {\n+        val leftRatio = BigDecimal(outputRows) / BigDecimal(leftRows)\n+        val rightRatio = BigDecimal(outputRows) / BigDecimal(rightRows)\n+        outputWithStat.foreach { a =>\n+          // check if this attribute is a join key\n+          if (intersectedStats.contains(a.name)) {\n+            outputColStats.put(a.name, intersectedStats(a.name))\n+          } else {\n+            val oldColStat = originalStats(a.name)\n+            val oldNdv = oldColStat.distinctCount\n+            // We only change (scale down) the number of distinct values if the number of rows\n+            // decreases after join, because join won't produce new values even if the number of\n+            // rows increases.\n+            val newNdv = if (left.outputSet.contains(a) && leftRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * leftRatio)\n+            } else if (right.outputSet.contains(a) && rightRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * rightRatio)\n+            } else {\n+              oldNdv\n+            }\n+            outputColStats.put(a.name, oldColStat.copy(distinctCount = newNdv, nullCount = 0))\n+          }\n+        }\n+      }\n+\n+      Some(Statistics(\n+        sizeInBytes = outputRows * getRowSize(join.output, outputColStats.toMap),\n+        rowCount = Some(outputRows),\n+        colStats = outputColStats.toMap,\n+        isBroadcastable = false))\n+\n+    case _ => None\n+  }\n+\n+  def emptyColumnStat(dataType: DataType): ColumnStat = {\n+    ColumnStat(distinctCount = 0, min = None, max = None, nullCount = 0,\n+      avgLen = dataType.defaultSize, maxLen = dataType.defaultSize)\n+  }\n+\n+  def intersectedColumnStat(\n+      origin: ColumnStat,\n+      newDistinctCount: BigInt,\n+      newMin: Option[Any],\n+      newMax: Option[Any]): ColumnStat = {\n+    origin.copy(distinctCount = newDistinctCount, min = newMin, max = newMax, nullCount = 0)\n+  }\n+\n+  def extractJoinKeys(\n+      leftKeys: Seq[Expression],\n+      rightKeys: Seq[Expression]): Seq[(AttributeReference, AttributeReference)] = {\n+    leftKeys.zip(rightKeys).flatMap {\n+      case (ExtractAttr(left), ExtractAttr(right)) => Some((left, right))\n+      // Currently we don't deal with equal joins like key1 = key2 + 5."
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "ioana-delaney"
    },
    "body": "Can we also include a short description of how column stats are computed after the join?",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-15T00:48:28Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Yes, of course",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-15T01:10:36Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "ioana-delaney"
    },
    "body": "I have a design comment. Given a join, this function computes predicate selectivity, plan cardinality, and column statistics. I wonder if it would make sense to encapsulate predicate selectivity computation in its own function i.e. selectivity is a property of a predicate and cardinality (i.e. number of rows) is a property of the data stream. Also, there might be different ways to compute selectivity of a predicate (e.g. uniform vs non uniform distribution) and therefore it might make sense to separate the computation of the two properties. Then, maybe in the future, selectivity hint might be used to overwrite the default CBO selectivity computation.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-15T01:20:53Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *\n+   * @return Return the updated statistics after join. Return `None` if the join type is not\n+   *         supported, or we don't have enough statistics for estimation.\n+   */\n+  // scalastyle:on\n+  def estimate(join: Join): Option[Statistics] = join match {\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)\n+        if supportsJoinType(joinType) && hasRowCountStat(left, right) =>\n+\n+      // 1. Compute the denominator\n+      var ndvDenom: BigInt = -1\n+      val keyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val leftStats = left.statistics\n+      val rightStats = right.statistics\n+      val intersectedStats = new mutable.HashMap[String, ColumnStat]()\n+      var i = 0\n+      while(i < keyPairs.length && ndvDenom != 0) {\n+        val (leftKey, rightKey) = keyPairs(i)\n+        // Do estimation if we have enough statistics\n+        if (hasColumnStat((left, leftKey), (right, rightKey))) {\n+          val leftKeyStats = leftStats.colStats(leftKey.name)\n+          val rightKeyStats = rightStats.colStats(rightKey.name)\n+\n+          // Check if the two sides are disjoint\n+          val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+          val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+          if (Range.isIntersected(lRange, rRange)) {\n+            // Get the largest ndv among pairs of join keys\n+            val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+            if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+\n+            // Update intersected column stats\n+            val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+            val (newMin1, newMax1, newMin2, newMax2) =\n+              Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+            intersectedStats.put(leftKey.name, intersectedColumnStat(leftKeyStats, minNdv,\n+              newMin1, newMax1))\n+            intersectedStats.put(rightKey.name, intersectedColumnStat(rightKeyStats, minNdv,\n+              newMin2, newMax2))\n+          } else {\n+            // Set ndvDenom to zero to indicate that this join should have no output\n+            ndvDenom = 0\n+          }\n+        }\n+        i += 1\n+      }\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val outputRows: BigInt = if (ndvDenom == 0) {\n+        // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+        0\n+      } else if (ndvDenom < 0) {\n+        // There isn't join keys or column stats for any of the join key pairs, we estimate like\n+        // cartesian product.\n+        leftRows * rightRows\n+      } else {\n+        ceil(BigDecimal(leftRows * rightRows) / BigDecimal(ndvDenom))\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val originalStats = leftStats.colStats ++ rightStats.colStats\n+      val outputWithStat = join.output.filter(attr => originalStats.contains(attr.name))\n+\n+      val outputColStats = new mutable.HashMap[String, ColumnStat]()\n+      if (ndvDenom == 0) {\n+        // empty output\n+        outputWithStat.foreach(a => outputColStats.put(a.name, emptyColumnStat(a.dataType)))\n+      } else if (ndvDenom < 0) {\n+        // cartesian product, column stats will not change\n+        outputWithStat.foreach(a => outputColStats.put(a.name, originalStats(a.name)))\n+      } else {\n+        val leftRatio = BigDecimal(outputRows) / BigDecimal(leftRows)\n+        val rightRatio = BigDecimal(outputRows) / BigDecimal(rightRows)\n+        outputWithStat.foreach { a =>\n+          // check if this attribute is a join key\n+          if (intersectedStats.contains(a.name)) {\n+            outputColStats.put(a.name, intersectedStats(a.name))\n+          } else {\n+            val oldColStat = originalStats(a.name)\n+            val oldNdv = oldColStat.distinctCount\n+            // We only change (scale down) the number of distinct values if the number of rows\n+            // decreases after join, because join won't produce new values even if the number of\n+            // rows increases.\n+            val newNdv = if (left.outputSet.contains(a) && leftRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * leftRatio)\n+            } else if (right.outputSet.contains(a) && rightRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * rightRatio)\n+            } else {\n+              oldNdv\n+            }\n+            outputColStats.put(a.name, oldColStat.copy(distinctCount = newNdv, nullCount = 0))\n+          }\n+        }\n+      }\n+\n+      Some(Statistics(\n+        sizeInBytes = outputRows * getRowSize(join.output, outputColStats.toMap),\n+        rowCount = Some(outputRows),\n+        colStats = outputColStats.toMap,\n+        isBroadcastable = false))\n+\n+    case _ => None\n+  }"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Thanks for the advice. We will consider to have such function when we check in the code for Filter estimation, then we can look at this more comprehensively. I think the pr will be submitted soon.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-15T12:38:27Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation {\n+  import EstimationUtils._\n+\n+  // scalastyle:off\n+  /**\n+   * Estimate output size and number of rows after a join operator, and propogate updated column\n+   * statsitics.\n+   * The number of rows of A inner join B on A.k1 = B.k1 is estimated by this basic formula:\n+   * T(A IJ B) = T(A) * T(B) / max(V(A.k1), V(B.k1)), where V is the number of distinct values of\n+   * that column. The underlying assumption for this formula is: each value of the smaller domain\n+   * is included in the larger domain.\n+   * Generally, inner join with multiple join keys can also be estimated based on the above\n+   * formula:\n+   * T(A IJ B) = T(A) * T(B) / (max(V(A.k1), V(B.k1)) * max(V(A.k2), V(B.k2)) * ... * max(V(A.kn), V(B.kn)))\n+   * However, the denominator can become very large and excessively reduce the result, so we use a\n+   * conservative strategy to take only the largest max(V(A.ki), V(B.ki)) as the denominator.\n+   *\n+   * @return Return the updated statistics after join. Return `None` if the join type is not\n+   *         supported, or we don't have enough statistics for estimation.\n+   */\n+  // scalastyle:on\n+  def estimate(join: Join): Option[Statistics] = join match {\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)\n+        if supportsJoinType(joinType) && hasRowCountStat(left, right) =>\n+\n+      // 1. Compute the denominator\n+      var ndvDenom: BigInt = -1\n+      val keyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val leftStats = left.statistics\n+      val rightStats = right.statistics\n+      val intersectedStats = new mutable.HashMap[String, ColumnStat]()\n+      var i = 0\n+      while(i < keyPairs.length && ndvDenom != 0) {\n+        val (leftKey, rightKey) = keyPairs(i)\n+        // Do estimation if we have enough statistics\n+        if (hasColumnStat((left, leftKey), (right, rightKey))) {\n+          val leftKeyStats = leftStats.colStats(leftKey.name)\n+          val rightKeyStats = rightStats.colStats(rightKey.name)\n+\n+          // Check if the two sides are disjoint\n+          val lRange = Range(leftKeyStats.min, leftKeyStats.max, leftKey.dataType)\n+          val rRange = Range(rightKeyStats.min, rightKeyStats.max, rightKey.dataType)\n+          if (Range.isIntersected(lRange, rRange)) {\n+            // Get the largest ndv among pairs of join keys\n+            val maxNdv = leftKeyStats.distinctCount.max(rightKeyStats.distinctCount)\n+            if (maxNdv > ndvDenom) ndvDenom = maxNdv\n+\n+            // Update intersected column stats\n+            val minNdv = leftKeyStats.distinctCount.min(rightKeyStats.distinctCount)\n+            val (newMin1, newMax1, newMin2, newMax2) =\n+              Range.intersect(lRange, rRange, leftKey.dataType, rightKey.dataType)\n+            intersectedStats.put(leftKey.name, intersectedColumnStat(leftKeyStats, minNdv,\n+              newMin1, newMax1))\n+            intersectedStats.put(rightKey.name, intersectedColumnStat(rightKeyStats, minNdv,\n+              newMin2, newMax2))\n+          } else {\n+            // Set ndvDenom to zero to indicate that this join should have no output\n+            ndvDenom = 0\n+          }\n+        }\n+        i += 1\n+      }\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val outputRows: BigInt = if (ndvDenom == 0) {\n+        // One of the join key pairs is disjoint, thus the two sides of join is disjoint.\n+        0\n+      } else if (ndvDenom < 0) {\n+        // There isn't join keys or column stats for any of the join key pairs, we estimate like\n+        // cartesian product.\n+        leftRows * rightRows\n+      } else {\n+        ceil(BigDecimal(leftRows * rightRows) / BigDecimal(ndvDenom))\n+      }\n+\n+      // 3. Update statistics based on the output of join\n+      val originalStats = leftStats.colStats ++ rightStats.colStats\n+      val outputWithStat = join.output.filter(attr => originalStats.contains(attr.name))\n+\n+      val outputColStats = new mutable.HashMap[String, ColumnStat]()\n+      if (ndvDenom == 0) {\n+        // empty output\n+        outputWithStat.foreach(a => outputColStats.put(a.name, emptyColumnStat(a.dataType)))\n+      } else if (ndvDenom < 0) {\n+        // cartesian product, column stats will not change\n+        outputWithStat.foreach(a => outputColStats.put(a.name, originalStats(a.name)))\n+      } else {\n+        val leftRatio = BigDecimal(outputRows) / BigDecimal(leftRows)\n+        val rightRatio = BigDecimal(outputRows) / BigDecimal(rightRows)\n+        outputWithStat.foreach { a =>\n+          // check if this attribute is a join key\n+          if (intersectedStats.contains(a.name)) {\n+            outputColStats.put(a.name, intersectedStats(a.name))\n+          } else {\n+            val oldColStat = originalStats(a.name)\n+            val oldNdv = oldColStat.distinctCount\n+            // We only change (scale down) the number of distinct values if the number of rows\n+            // decreases after join, because join won't produce new values even if the number of\n+            // rows increases.\n+            val newNdv = if (left.outputSet.contains(a) && leftRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * leftRatio)\n+            } else if (right.outputSet.contains(a) && rightRatio < 1) {\n+              ceil(BigDecimal(oldNdv) * rightRatio)\n+            } else {\n+              oldNdv\n+            }\n+            outputColStats.put(a.name, oldColStat.copy(distinctCount = newNdv, nullCount = 0))\n+          }\n+        }\n+      }\n+\n+      Some(Statistics(\n+        sizeInBytes = outputRows * getRowSize(join.output, outputColStats.toMap),\n+        rowCount = Some(outputRows),\n+        colStats = outputColStats.toMap,\n+        isBroadcastable = false))\n+\n+    case _ => None\n+  }"
  }],
  "prId": 16228
}, {
  "comments": [{
    "author": {
      "login": "wzhfy"
    },
    "body": "@Tagar I take your advice about full outer join, but with a little change by lower bounding it using `innerRows`.",
    "commit": "8182123f09328ddffabcc1d180c0309f550489f8",
    "createdAt": "2016-12-23T08:31:37Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, Expression}\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Join, Statistics}\n+import org.apache.spark.sql.catalyst.plans.logical.estimation.EstimationUtils._\n+import org.apache.spark.sql.types.DataType\n+\n+\n+object JoinEstimation extends Logging {\n+  /**\n+   * Estimate statistics after join. Return `None` if the join type is not supported, or we don't\n+   * have enough statistics for estimation.\n+   */\n+  def estimate(join: Join): Option[Statistics] = {\n+    join.joinType match {\n+      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>\n+        InnerOuterEstimation(join).doEstimate()\n+      case LeftSemi | LeftAnti =>\n+        LeftSemiAntiEstimation(join).doEstimate()\n+      case _ =>\n+        logDebug(s\"Unsupported join type: ${join.joinType}\")\n+        None\n+    }\n+  }\n+}\n+\n+case class InnerOuterEstimation(join: Join) extends Logging {\n+\n+  /**\n+   * Estimate output size and number of rows after a join operator, and update output column stats.\n+   */\n+  def doEstimate(): Option[Statistics] = join match {\n+    case _ if !allPlanhaveRowCountStat(join.left, join.right) =>\n+      None\n+\n+    case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>\n+      // 1. Compute join selectivity\n+      val leftStats = left.statistics\n+      val rightStats = right.statistics\n+      val joinKeyPairs = extractJoinKeys(leftKeys, rightKeys)\n+      val selectivity = joinSelectivity(joinKeyPairs, leftStats, rightStats)\n+\n+      // 2. Estimate the number of output rows\n+      val leftRows = leftStats.rowCount.get\n+      val rightRows = rightStats.rowCount.get\n+      val innerRows = ceil(BigDecimal(leftRows * rightRows) * selectivity)\n+\n+      // Make sure outputRows won't be too small based on join type.\n+      val outputRows = joinType match {\n+        case LeftOuter =>\n+          // All rows from left side should be in the result.\n+          leftRows.max(innerRows)\n+        case RightOuter =>\n+          // All rows from right side should be in the result.\n+          rightRows.max(innerRows)\n+        case FullOuter =>\n+          // Simulate full outer join as obtaining the number of elements in the union of two\n+          // finite sets: A \\cup B = A + B - A \\cap B => A FOJ B = A + B - A IJ B.\n+          // But the \"inner join\" part can be much larger than A \\cap B, making the simulated\n+          // result much smaller. To prevent this, we choose the larger one between the simulated\n+          // part and the inner part.\n+          (leftRows + rightRows - innerRows).max(innerRows)"
  }],
  "prId": 16228
}]