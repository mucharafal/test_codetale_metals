[{
  "comments": [{
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@marmbrus Primitive types mentioned twice ? Is it ok ?\n",
    "commit": "4b51ad781da69c73b12e92673ac0f4cf57bea370",
    "createdAt": "2015-12-01T06:45:21Z",
    "diffHunk": "@@ -26,13 +29,51 @@ import org.apache.spark.sql.catalyst.expressions.{DecodeUsingSerializer, BoundRe\n import org.apache.spark.sql.types._\n \n /**\n+ * :: Experimental ::\n  * Used to convert a JVM object of type `T` to and from the internal Spark SQL representation.\n  *\n- * Encoders are not intended to be thread-safe and thus they are allow to avoid internal locking\n- * and reuse internal buffers to improve performance.\n+ * == Scala ==\n+ * Encoders are generally created automatically though implicits from a `SQLContext`.\n+ *\n+ * {{{\n+ *   import sqlContext.implicits._\n+ *\n+ *   val ds = Seq(1, 2, 3).toDS() // implicitly provided (sqlContext.implicits.newIntEncoder)\n+ * }}}\n+ *\n+ * == Java ==\n+ * Encoders are specified by calling static methods on [[Encoders]].\n+ *\n+ * {{{\n+ *   List<String> data = Arrays.asList(\"abc\", \"abc\", \"xyz\");\n+ *   Dataset<String> ds = context.createDataset(data, Encoders.STRING());\n+ * }}}\n+ *\n+ * Encoders can be composed into tuples:\n+ *\n+ * {{{\n+ *   Encoder<Tuple2<Integer, String>> encoder2 = Encoders.tuple(Encoders.INT(), Encoders.STRING());\n+ *   List<Tuple2<Integer, String>> data2 = Arrays.asList(new scala.Tuple2(1, \"a\");\n+ *   Dataset<Tuple2<Integer, String>> ds2 = context.createDataset(data2, encoder2);\n+ * }}}\n+ *\n+ * Or constructed from Java Beans:\n+ *\n+ * {{{\n+ *   Encoders.bean(MyClass.class);\n+ * }}}\n+ *\n+ * == Implementation ==\n+ *  - Encoders are not intended to be thread-safe and thus they are allowed to avoid internal\n+ *  locking and reuse internal buffers to improve performance.\n  *\n  * @since 1.6.0\n  */\n+@Experimental\n+@implicitNotFound(\"Unable to find encoder for type stored in a Dataset.  Primitive types \" +\n+  \"(Int, String, etc) and Products (case classes) and primitive types are supported by \" +\n+  \"importing sqlContext.implicits._  Support for serializing other types will be added in future \" +"
  }],
  "prId": 10060
}, {
  "comments": [{
    "author": {
      "login": "BenFradet"
    },
    "body": "I might be mistaken but I think you meant to write \"through\" and not \"though\".\n",
    "commit": "4b51ad781da69c73b12e92673ac0f4cf57bea370",
    "createdAt": "2015-12-01T14:27:18Z",
    "diffHunk": "@@ -26,13 +29,51 @@ import org.apache.spark.sql.catalyst.expressions.{DecodeUsingSerializer, BoundRe\n import org.apache.spark.sql.types._\n \n /**\n+ * :: Experimental ::\n  * Used to convert a JVM object of type `T` to and from the internal Spark SQL representation.\n  *\n- * Encoders are not intended to be thread-safe and thus they are allow to avoid internal locking\n- * and reuse internal buffers to improve performance.\n+ * == Scala ==\n+ * Encoders are generally created automatically though implicits from a `SQLContext`."
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "It would also be great to expand this slightly and explain what can be inferred automatically right now.\n",
    "commit": "4b51ad781da69c73b12e92673ac0f4cf57bea370",
    "createdAt": "2015-12-02T00:49:54Z",
    "diffHunk": "@@ -26,13 +29,51 @@ import org.apache.spark.sql.catalyst.expressions.{DecodeUsingSerializer, BoundRe\n import org.apache.spark.sql.types._\n \n /**\n+ * :: Experimental ::\n  * Used to convert a JVM object of type `T` to and from the internal Spark SQL representation.\n  *\n- * Encoders are not intended to be thread-safe and thus they are allow to avoid internal locking\n- * and reuse internal buffers to improve performance.\n+ * == Scala ==\n+ * Encoders are generally created automatically though implicits from a `SQLContext`."
  }],
  "prId": 10060
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "import order\n",
    "commit": "4b51ad781da69c73b12e92673ac0f4cf57bea370",
    "createdAt": "2015-12-02T00:48:51Z",
    "diffHunk": "@@ -19,6 +19,9 @@ package org.apache.spark.sql\n \n import java.lang.reflect.Modifier\n \n+import org.apache.spark.annotation.Experimental"
  }],
  "prId": 10060
}, {
  "comments": [{
    "author": {
      "login": "BenFradet"
    },
    "body": "I'm not sure I understand this sentence: \"allowed to avoid\" is troubling me.\n",
    "commit": "4b51ad781da69c73b12e92673ac0f4cf57bea370",
    "createdAt": "2015-12-08T20:26:21Z",
    "diffHunk": "@@ -19,20 +19,60 @@ package org.apache.spark.sql\n \n import java.lang.reflect.Modifier\n \n+import scala.annotation.implicitNotFound\n import scala.reflect.{ClassTag, classTag}\n \n+import org.apache.spark.annotation.Experimental\n import org.apache.spark.sql.catalyst.encoders.{ExpressionEncoder, encoderFor}\n import org.apache.spark.sql.catalyst.expressions.{DecodeUsingSerializer, BoundReference, EncodeUsingSerializer}\n import org.apache.spark.sql.types._\n \n /**\n+ * :: Experimental ::\n  * Used to convert a JVM object of type `T` to and from the internal Spark SQL representation.\n  *\n- * Encoders are not intended to be thread-safe and thus they are allow to avoid internal locking\n- * and reuse internal buffers to improve performance.\n+ * == Scala ==\n+ * Encoders are generally created automatically through implicits from a `SQLContext`.\n+ *\n+ * {{{\n+ *   import sqlContext.implicits._\n+ *\n+ *   val ds = Seq(1, 2, 3).toDS() // implicitly provided (sqlContext.implicits.newIntEncoder)\n+ * }}}\n+ *\n+ * == Java ==\n+ * Encoders are specified by calling static methods on [[Encoders]].\n+ *\n+ * {{{\n+ *   List<String> data = Arrays.asList(\"abc\", \"abc\", \"xyz\");\n+ *   Dataset<String> ds = context.createDataset(data, Encoders.STRING());\n+ * }}}\n+ *\n+ * Encoders can be composed into tuples:\n+ *\n+ * {{{\n+ *   Encoder<Tuple2<Integer, String>> encoder2 = Encoders.tuple(Encoders.INT(), Encoders.STRING());\n+ *   List<Tuple2<Integer, String>> data2 = Arrays.asList(new scala.Tuple2(1, \"a\");\n+ *   Dataset<Tuple2<Integer, String>> ds2 = context.createDataset(data2, encoder2);\n+ * }}}\n+ *\n+ * Or constructed from Java Beans:\n+ *\n+ * {{{\n+ *   Encoders.bean(MyClass.class);\n+ * }}}\n+ *\n+ * == Implementation ==\n+ *  - Encoders are not intended to be thread-safe and thus they are allowed to avoid internal"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "updated, is that better?\n",
    "commit": "4b51ad781da69c73b12e92673ac0f4cf57bea370",
    "createdAt": "2015-12-08T22:00:42Z",
    "diffHunk": "@@ -19,20 +19,60 @@ package org.apache.spark.sql\n \n import java.lang.reflect.Modifier\n \n+import scala.annotation.implicitNotFound\n import scala.reflect.{ClassTag, classTag}\n \n+import org.apache.spark.annotation.Experimental\n import org.apache.spark.sql.catalyst.encoders.{ExpressionEncoder, encoderFor}\n import org.apache.spark.sql.catalyst.expressions.{DecodeUsingSerializer, BoundReference, EncodeUsingSerializer}\n import org.apache.spark.sql.types._\n \n /**\n+ * :: Experimental ::\n  * Used to convert a JVM object of type `T` to and from the internal Spark SQL representation.\n  *\n- * Encoders are not intended to be thread-safe and thus they are allow to avoid internal locking\n- * and reuse internal buffers to improve performance.\n+ * == Scala ==\n+ * Encoders are generally created automatically through implicits from a `SQLContext`.\n+ *\n+ * {{{\n+ *   import sqlContext.implicits._\n+ *\n+ *   val ds = Seq(1, 2, 3).toDS() // implicitly provided (sqlContext.implicits.newIntEncoder)\n+ * }}}\n+ *\n+ * == Java ==\n+ * Encoders are specified by calling static methods on [[Encoders]].\n+ *\n+ * {{{\n+ *   List<String> data = Arrays.asList(\"abc\", \"abc\", \"xyz\");\n+ *   Dataset<String> ds = context.createDataset(data, Encoders.STRING());\n+ * }}}\n+ *\n+ * Encoders can be composed into tuples:\n+ *\n+ * {{{\n+ *   Encoder<Tuple2<Integer, String>> encoder2 = Encoders.tuple(Encoders.INT(), Encoders.STRING());\n+ *   List<Tuple2<Integer, String>> data2 = Arrays.asList(new scala.Tuple2(1, \"a\");\n+ *   Dataset<Tuple2<Integer, String>> ds2 = context.createDataset(data2, encoder2);\n+ * }}}\n+ *\n+ * Or constructed from Java Beans:\n+ *\n+ * {{{\n+ *   Encoders.bean(MyClass.class);\n+ * }}}\n+ *\n+ * == Implementation ==\n+ *  - Encoders are not intended to be thread-safe and thus they are allowed to avoid internal"
  }, {
    "author": {
      "login": "BenFradet"
    },
    "body": "It's way clearer, yup.\n",
    "commit": "4b51ad781da69c73b12e92673ac0f4cf57bea370",
    "createdAt": "2015-12-09T07:02:44Z",
    "diffHunk": "@@ -19,20 +19,60 @@ package org.apache.spark.sql\n \n import java.lang.reflect.Modifier\n \n+import scala.annotation.implicitNotFound\n import scala.reflect.{ClassTag, classTag}\n \n+import org.apache.spark.annotation.Experimental\n import org.apache.spark.sql.catalyst.encoders.{ExpressionEncoder, encoderFor}\n import org.apache.spark.sql.catalyst.expressions.{DecodeUsingSerializer, BoundReference, EncodeUsingSerializer}\n import org.apache.spark.sql.types._\n \n /**\n+ * :: Experimental ::\n  * Used to convert a JVM object of type `T` to and from the internal Spark SQL representation.\n  *\n- * Encoders are not intended to be thread-safe and thus they are allow to avoid internal locking\n- * and reuse internal buffers to improve performance.\n+ * == Scala ==\n+ * Encoders are generally created automatically through implicits from a `SQLContext`.\n+ *\n+ * {{{\n+ *   import sqlContext.implicits._\n+ *\n+ *   val ds = Seq(1, 2, 3).toDS() // implicitly provided (sqlContext.implicits.newIntEncoder)\n+ * }}}\n+ *\n+ * == Java ==\n+ * Encoders are specified by calling static methods on [[Encoders]].\n+ *\n+ * {{{\n+ *   List<String> data = Arrays.asList(\"abc\", \"abc\", \"xyz\");\n+ *   Dataset<String> ds = context.createDataset(data, Encoders.STRING());\n+ * }}}\n+ *\n+ * Encoders can be composed into tuples:\n+ *\n+ * {{{\n+ *   Encoder<Tuple2<Integer, String>> encoder2 = Encoders.tuple(Encoders.INT(), Encoders.STRING());\n+ *   List<Tuple2<Integer, String>> data2 = Arrays.asList(new scala.Tuple2(1, \"a\");\n+ *   Dataset<Tuple2<Integer, String>> ds2 = context.createDataset(data2, encoder2);\n+ * }}}\n+ *\n+ * Or constructed from Java Beans:\n+ *\n+ * {{{\n+ *   Encoders.bean(MyClass.class);\n+ * }}}\n+ *\n+ * == Implementation ==\n+ *  - Encoders are not intended to be thread-safe and thus they are allowed to avoid internal"
  }],
  "prId": 10060
}]