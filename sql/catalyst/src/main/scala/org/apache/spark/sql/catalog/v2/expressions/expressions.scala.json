[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "What's the semantics of this? an arbitrary function?",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T18:29:04Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(",
    "line": 131
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Some unknown transform.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T20:43:30Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(",
    "line": 131
  }],
  "prId": 24117
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should not abuse `lazy val`. `def` is better here.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T18:32:00Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Please be more clear. Why is using a `lazy val` an abuse?",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T20:43:55Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`lazy val` has overhead, if we check the generated java bytecode, `lazy val` needs to add extra member variables to the class, and needs to take care of multi-thread, etc.\r\n\r\nA simple method returning a string constant does not worth the overhead of `lazy val`.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T20:48:55Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I see what you mean about some overhead, but I don't think this is an abuse. This may look like one because it is a lazy val referencing the value of another lazy val, but this must be a lazy val because it is a lazy the parent class.\r\n\r\nMost of the `describe` implementations are lazy to avoid repeated string manipulation when we can just keep track of the computed value.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T21:27:36Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`lazy val` has much more overhead than string manipulation, AFAIK. We should avoid using `lazy val` in the parent class as well.\r\n\r\nIf we call `describe` a million times per second, then maybe `lazy val` can bring some benefits like less GC. But I don't think this is the case here.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T21:42:07Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "BTW, we shouldn't consider performance at this stage, and using `def` is more natural here.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T21:42:52Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I agree that we don't really need to care about performance here, but I think `lazy val` is more natural.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T21:48:46Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "If you check the `toString` and `sql` methods in various expressions, logical plans, phyiscal plans, most of them use `def` instead of `lazy val`.\r\n\r\nMaybe we can't reach an agreement about which one is more natural, but `def` is definitely more consistent with the Spark code base.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T21:56:19Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Is this important to change, or is this a preference?",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T22:57:11Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think it's more than a preference. PRs should keep the code style consistent with the code base.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T23:28:07Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe"
  }],
  "prId": 24117
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "what if the name part contains ` as well?",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T20:41:55Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe\n+}\n+\n+private[sql] final case class YearTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"year\"\n+}\n+\n+private[sql] final case class MonthTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"month\"\n+}\n+\n+private[sql] final case class DateTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"date\"\n+}\n+\n+private[sql] final case class DateHourTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"date_hour\"\n+}\n+\n+private[sql] final case class LiteralValue[T](value: T, dataType: DataType) extends Literal[T] {\n+  override def describe: String = {\n+    if (dataType.isInstanceOf[StringType]) {\n+      s\"'$value'\"\n+    } else {\n+      s\"$value\"\n+    }\n+  }\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class FieldReference(fieldNames: Seq[String]) extends NamedReference {\n+  override def describe: String = fieldNames.map(quote).mkString(\".\")\n+  override def toString: String = describe\n+\n+  private def quote(part: String): String = {\n+    if (part.contains(\".\")) {\n+      s\"`$part`\""
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "nvm, this is same as `UnresolvedAttribute.name`",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T20:42:38Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe\n+}\n+\n+private[sql] final case class YearTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"year\"\n+}\n+\n+private[sql] final case class MonthTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"month\"\n+}\n+\n+private[sql] final case class DateTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"date\"\n+}\n+\n+private[sql] final case class DateHourTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"date_hour\"\n+}\n+\n+private[sql] final case class LiteralValue[T](value: T, dataType: DataType) extends Literal[T] {\n+  override def describe: String = {\n+    if (dataType.isInstanceOf[StringType]) {\n+      s\"'$value'\"\n+    } else {\n+      s\"$value\"\n+    }\n+  }\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class FieldReference(fieldNames: Seq[String]) extends NamedReference {\n+  override def describe: String = fieldNames.map(quote).mkString(\".\")\n+  override def toString: String = describe\n+\n+  private def quote(part: String): String = {\n+    if (part.contains(\".\")) {\n+      s\"`$part`\""
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Actually, I think you're right. This is intended to be a representation that we could send through the parser to return the exact same value, so this should double to escape it. That is done in `IdentifierWithDatabase` for other identifiers.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-03-29T21:31:06Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference.describe)\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name).toArray)\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override lazy val references: Array[NamedReference] = Array(ref)\n+\n+  override lazy val arguments: Array[Expression] = Array(ref)\n+\n+  override lazy val describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override lazy val name: String = \"bucket\"\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override lazy val describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override lazy val arguments: Array[Expression] = args.toArray\n+\n+  override lazy val references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override lazy val describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"identity\"\n+  override lazy val describe: String = ref.describe\n+}\n+\n+private[sql] final case class YearTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"year\"\n+}\n+\n+private[sql] final case class MonthTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"month\"\n+}\n+\n+private[sql] final case class DateTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"date\"\n+}\n+\n+private[sql] final case class DateHourTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override lazy val name: String = \"date_hour\"\n+}\n+\n+private[sql] final case class LiteralValue[T](value: T, dataType: DataType) extends Literal[T] {\n+  override def describe: String = {\n+    if (dataType.isInstanceOf[StringType]) {\n+      s\"'$value'\"\n+    } else {\n+      s\"$value\"\n+    }\n+  }\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class FieldReference(fieldNames: Seq[String]) extends NamedReference {\n+  override def describe: String = fieldNames.map(quote).mkString(\".\")\n+  override def toString: String = describe\n+\n+  private def quote(part: String): String = {\n+    if (part.contains(\".\")) {\n+      s\"`$part`\""
  }],
  "prId": 24117
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "in general i think we should not put internal classes in the same package as public interfaces, especially now we are using Java to define interfaces.\r\n\r\npackage level visibility gets elided in bytecode, so from library developers' perspective they will just see LogicalExpressions together with all the other public interfaces.\r\n",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-02T00:34:36Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions",
    "line": 18
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I'll fix this when I hear back from you on where to move the public parts of this API.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-02T16:06:49Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions",
    "line": 18
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@rxin, where do you want these classes to go?",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-03T17:45:35Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions",
    "line": 18
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Nevermind, I see a response on a different comment above. It still isn't clear where these should move to and it sounds like the suggestion to reorganize is broader than just the expressions.\r\n\r\nIf we need to move `expressions`, then let's decide where to move it to. If we need to reorganize `catalog.v2`, then we should do that in a separate commit. I would be fine also getting this in and fixing packages in a follow-up.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-03T18:05:08Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions",
    "line": 18
  }],
  "prId": 24117
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "Between this and the vararg version I will keep only one. Too error prone to overload things this way.\r\n",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-02T00:35:19Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference).map { ref =>\n+        val parts = ref.fieldNames\n+        if (parts.size > 1) {\n+          throw new AnalysisException(s\"Cannot partition by nested column: $ref\")\n+        } else {\n+          parts(0)\n+        }\n+      }\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name))\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Fixed.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-02T16:18:00Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference).map { ref =>\n+        val parts = ref.fieldNames\n+        if (parts.size > 1) {\n+          throw new AnalysisException(s\"Cannot partition by nested column: $ref\")\n+        } else {\n+          parts(0)\n+        }\n+      }\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name))\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)"
  }],
  "prId": 24117
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why it's private? Do we expect the data source implementation to catch it?",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-04T00:58:00Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference).map { ref =>\n+        val parts = ref.fieldNames\n+        if (parts.size > 1) {\n+          throw new AnalysisException(s\"Cannot partition by nested column: $ref\")\n+        } else {\n+          parts(0)\n+        }\n+      }\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name))\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override def references: Array[NamedReference] = Array(ref)\n+\n+  override def arguments: Array[Expression] = Array(ref)\n+\n+  override def describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override val name: String = \"bucket\"\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override def describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override def arguments: Array[Expression] = args.toArray\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override val name: String = \"identity\"\n+  override def describe: String = ref.describe\n+}\n+\n+private[sql] final case class YearTransform("
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "The Spark implementations private because catalogs should not rely on specific implementation classes. The API is an interface and catalogs must rely on that.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-04T16:54:12Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference).map { ref =>\n+        val parts = ref.fieldNames\n+        if (parts.size > 1) {\n+          throw new AnalysisException(s\"Cannot partition by nested column: $ref\")\n+        } else {\n+          parts(0)\n+        }\n+      }\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name))\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override def references: Array[NamedReference] = Array(ref)\n+\n+  override def arguments: Array[Expression] = Array(ref)\n+\n+  override def describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override val name: String = \"bucket\"\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override def describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override def arguments: Array[Expression] = args.toArray\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override val name: String = \"identity\"\n+  override def describe: String = ref.describe\n+}\n+\n+private[sql] final case class YearTransform("
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "say I'm a table catalog developer, if I want to support `PARTITION BY year(ts)`, I need to call `Transform.name` and `Transform.arguments` to check if it's a year transform?",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-04T17:03:40Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference).map { ref =>\n+        val parts = ref.fieldNames\n+        if (parts.size > 1) {\n+          throw new AnalysisException(s\"Cannot partition by nested column: $ref\")\n+        } else {\n+          parts(0)\n+        }\n+      }\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name))\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override def references: Array[NamedReference] = Array(ref)\n+\n+  override def arguments: Array[Expression] = Array(ref)\n+\n+  override def describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override val name: String = \"bucket\"\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override def describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override def arguments: Array[Expression] = args.toArray\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override val name: String = \"identity\"\n+  override def describe: String = ref.describe\n+}\n+\n+private[sql] final case class YearTransform("
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "`Transform.name` will be `year`. `Transform.arguments` will contain the column, as will `Transform.references`.",
    "commit": "a4a87ac7582688e883e42980d95126dd05166c64",
    "createdAt": "2019-04-04T17:13:25Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2.expressions\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst\n+import org.apache.spark.sql.catalyst.catalog.BucketSpec\n+import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{DataType, IntegerType, StringType}\n+\n+/**\n+ * Helper methods for working with the logical expressions API.\n+ *\n+ * Factory methods can be used when referencing the logical expression nodes is ambiguous because\n+ * logical and internal expressions are used.\n+ */\n+private[sql] object LogicalExpressions {\n+  // a generic parser that is only used for parsing multi-part field names.\n+  // because this is only used for field names, the SQL conf passed in does not matter.\n+  private lazy val parser = new CatalystSqlParser(SQLConf.get)\n+\n+  def fromPartitionColumns(columns: String*): Array[IdentityTransform] =\n+    columns.map(identity).toArray\n+\n+  def fromBucketSpec(spec: BucketSpec): BucketTransform = {\n+    if (spec.sortColumnNames.nonEmpty) {\n+      throw new AnalysisException(\n+        s\"Cannot convert bucketing with sort columns to a transform: $spec\")\n+    }\n+\n+    bucket(spec.numBuckets, spec.bucketColumnNames: _*)\n+  }\n+\n+  implicit class TransformHelper(transforms: Seq[Transform]) {\n+    def asPartitionColumns: Seq[String] = {\n+      val (idTransforms, nonIdTransforms) = transforms.partition(_.isInstanceOf[IdentityTransform])\n+\n+      if (nonIdTransforms.nonEmpty) {\n+        throw new AnalysisException(\"Transforms cannot be converted to partition columns: \" +\n+            nonIdTransforms.map(_.describe).mkString(\", \"))\n+      }\n+\n+      idTransforms.map(_.asInstanceOf[IdentityTransform]).map(_.reference).map { ref =>\n+        val parts = ref.fieldNames\n+        if (parts.size > 1) {\n+          throw new AnalysisException(s\"Cannot partition by nested column: $ref\")\n+        } else {\n+          parts(0)\n+        }\n+      }\n+    }\n+  }\n+\n+  def literal[T](value: T): LiteralValue[T] = {\n+    val internalLit = catalyst.expressions.Literal(value)\n+    literal(value, internalLit.dataType)\n+  }\n+\n+  def literal[T](value: T, dataType: DataType): LiteralValue[T] = LiteralValue(value, dataType)\n+\n+  def reference(name: String): NamedReference =\n+    FieldReference(parser.parseMultipartIdentifier(name))\n+\n+  def apply(name: String, arguments: Array[Expression]): Transform = ApplyTransform(name, arguments)\n+\n+  def apply(name: String, arguments: Expression*): Transform = ApplyTransform(name, arguments)\n+\n+  def bucket(numBuckets: Int, columns: Array[String]): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def bucket(numBuckets: Int, columns: String*): BucketTransform =\n+    BucketTransform(literal(numBuckets, IntegerType), columns.map(reference))\n+\n+  def identity(column: String): IdentityTransform = IdentityTransform(reference(column))\n+\n+  def year(column: String): YearTransform = YearTransform(reference(column))\n+\n+  def month(column: String): MonthTransform = MonthTransform(reference(column))\n+\n+  def date(column: String): DateTransform = DateTransform(reference(column))\n+\n+  def dateHour(column: String): DateHourTransform = DateHourTransform(reference(column))\n+}\n+\n+/**\n+ * Base class for simple transforms of a single column.\n+ */\n+private[sql] abstract class SingleColumnTransform(ref: NamedReference) extends Transform {\n+\n+  def reference: NamedReference = ref\n+\n+  override def references: Array[NamedReference] = Array(ref)\n+\n+  override def arguments: Array[Expression] = Array(ref)\n+\n+  override def describe: String = name + \"(\" + reference.describe + \")\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class BucketTransform(\n+    numBuckets: Literal[Int],\n+    columns: Seq[NamedReference]) extends Transform {\n+\n+  override val name: String = \"bucket\"\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def arguments: Array[Expression] = numBuckets +: columns.toArray\n+\n+  override def describe: String = s\"bucket(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class ApplyTransform(\n+    name: String,\n+    args: Seq[Expression]) extends Transform {\n+\n+  override def arguments: Array[Expression] = args.toArray\n+\n+  override def references: Array[NamedReference] = {\n+    arguments\n+        .filter(_.isInstanceOf[NamedReference])\n+        .map(_.asInstanceOf[NamedReference])\n+  }\n+\n+  override def describe: String = s\"$name(${arguments.map(_.describe).mkString(\", \")})\"\n+\n+  override def toString: String = describe\n+}\n+\n+private[sql] final case class IdentityTransform(\n+    ref: NamedReference) extends SingleColumnTransform(ref) {\n+  override val name: String = \"identity\"\n+  override def describe: String = ref.describe\n+}\n+\n+private[sql] final case class YearTransform("
  }],
  "prId": 24117
}]