[{
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "The current implementation takes a two step approach, first it evaluates the expressions and puts them in an intermediate row and it then converts this row to an `UnsafeRow`. We could also just create a converter from `InternalRow` to `UnsafeRow` and punt the projection work of to a `InterpretedMutableProjection`.",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-06T14:42:05Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.expressions.{Expression, GenericInternalRow, Nondeterministic, SpecializedGetters, UnsafeArrayData, UnsafeMapData, UnsafeProjection, UnsafeProjectionCreator, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {"
  }],
  "prId": 20750
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "it's a little tricky to depend on the default size, can we match `ByteType` directly here?",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-08T01:39:36Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.analysis.CleanupAliases\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Expression, GenericInternalRow, Nondeterministic, SpecializedGetters, StatefulNondeterministic, UnsafeArrayData, UnsafeMapData, UnsafeProjection, UnsafeProjectionCreator, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful non deterministic expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: StatefulNondeterministic => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,\n+      fields: Array[StructField]): InternalRow => Unit = {\n+    val numFields = fields.length\n+\n+    // Create field writers.\n+    val fieldWriters = fields.map { field =>\n+      generateFieldWriter(bufferHolder, rowWriter, field.dataType, field.nullable)\n+    }\n+    // Create basic writer.\n+    row => {\n+      var i = 0\n+      while (i < numFields) {\n+        fieldWriters(i).apply(row, i)\n+        i += 1\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Generate a writer function for a struct field, array element, map key or map value. The\n+   * generated function writes the element at an index in a [[SpecializedGetters]] object (row\n+   * or array) to the given buffer using the given [[UnsafeWriter]].\n+   */\n+  private def generateFieldWriter(\n+      bufferHolder: BufferHolder,\n+      writer: UnsafeWriter,\n+      dt: DataType,\n+      nullable: Boolean): (SpecializedGetters, Int) => Unit = {\n+\n+    // Create the the basic writer.\n+    val unsafeWriter: (SpecializedGetters, Int) => Unit = dt match {\n+      case BooleanType =>\n+        (v, i) => writer.write(i, v.getBoolean(i))\n+\n+      case ByteType =>\n+        (v, i) => writer.write(i, v.getByte(i))\n+\n+      case ShortType =>\n+        (v, i) => writer.write(i, v.getShort(i))\n+\n+      case IntegerType | DateType =>\n+        (v, i) => writer.write(i, v.getInt(i))\n+\n+      case LongType | TimestampType =>\n+        (v, i) => writer.write(i, v.getLong(i))\n+\n+      case FloatType =>\n+        (v, i) => writer.write(i, v.getFloat(i))\n+\n+      case DoubleType =>\n+        (v, i) => writer.write(i, v.getDouble(i))\n+\n+      case DecimalType.Fixed(precision, scale) =>\n+        (v, i) => writer.write(i, v.getDecimal(i, precision, scale), precision, scale)\n+\n+      case CalendarIntervalType =>\n+        (v, i) => writer.write(i, v.getInterval(i))\n+\n+      case BinaryType =>\n+        (v, i) => writer.write(i, v.getBinary(i))\n+\n+      case StringType =>\n+        (v, i) => writer.write(i, v.getUTF8String(i))\n+\n+      case StructType(fields) =>\n+        val numFields = fields.length\n+        val rowWriter = new UnsafeRowWriter(bufferHolder, numFields)\n+        val structWriter = generateStructWriter(bufferHolder, rowWriter, fields)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getStruct(i, fields.length) match {\n+            case row: UnsafeRow =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                row.getBaseObject,\n+                row.getBaseOffset,\n+                row.getSizeInBytes)\n+            case row =>\n+              // Nested struct. We don't know where this will start because a row can be\n+              // variable length, so we need to update the offsets and zero out the bit mask.\n+              rowWriter.reset()\n+              structWriter.apply(row)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case ArrayType(elementType, containsNull) =>\n+        val arrayWriter = new UnsafeArrayWriter\n+        val elementSize = getElementSize(elementType)\n+        val elementWriter = generateFieldWriter(\n+          bufferHolder,\n+          arrayWriter,\n+          elementType,\n+          containsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          writeArray(bufferHolder, arrayWriter, elementWriter, v.getArray(i), elementSize)\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case MapType(keyType, valueType, valueContainsNull) =>\n+        val keyArrayWriter = new UnsafeArrayWriter\n+        val keySize = getElementSize(keyType)\n+        val keyWriter = generateFieldWriter(\n+          bufferHolder,\n+          keyArrayWriter,\n+          keyType,\n+          nullable = false)\n+        val valueArrayWriter = new UnsafeArrayWriter\n+        val valueSize = getElementSize(valueType)\n+        val valueWriter = generateFieldWriter(\n+          bufferHolder,\n+          valueArrayWriter,\n+          valueType,\n+          valueContainsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getMap(i) match {\n+            case map: UnsafeMapData =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                map.getBaseObject,\n+                map.getBaseOffset,\n+                map.getSizeInBytes)\n+            case map =>\n+              // preserve 8 bytes to write the key array numBytes later.\n+              bufferHolder.grow(8)\n+              bufferHolder.cursor += 8\n+\n+              // Write the keys and write the numBytes of key array into the first 8 bytes.\n+              writeArray(bufferHolder, keyArrayWriter, keyWriter, map.keyArray(), keySize)\n+              Platform.putLong(bufferHolder.buffer, tmpCursor, bufferHolder.cursor - tmpCursor - 8)\n+\n+              // Write the values.\n+              writeArray(bufferHolder, valueArrayWriter, valueWriter, map.valueArray(), valueSize)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case udt: UserDefinedType[_] =>\n+        generateFieldWriter(bufferHolder, writer, udt.sqlType, nullable)\n+\n+      case NullType =>\n+        (_, _) => {}\n+\n+      case _ =>\n+        throw new SparkException(s\"Unsupported data type $dt\")\n+    }\n+\n+    // Wrap the writer with a null safe version if the field is nullable.\n+    (dt, nullable) match {\n+      case (_: UserDefinedType[_], _) =>\n+        // The null wrapper depends on the sql type and not on the UDT.\n+        unsafeWriter\n+      case (DecimalType.Fixed(precision, scale), true) if precision > Decimal.MAX_LONG_DIGITS =>\n+        (v, i) => {\n+          // We can't call setNullAt() for DecimalType with precision larger than 18.\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.write(i, null.asInstanceOf[Decimal], precision, scale)\n+          }\n+        }\n+      case (_, true) if dt.defaultSize == 1 =>"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Sure",
    "commit": "3197d7f55a01c505f9a8500d39660b9bcdda3ae3",
    "createdAt": "2018-03-08T09:53:07Z",
    "diffHunk": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.catalyst.analysis.CleanupAliases\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Expression, GenericInternalRow, Nondeterministic, SpecializedGetters, StatefulNondeterministic, UnsafeArrayData, UnsafeMapData, UnsafeProjection, UnsafeProjectionCreator, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeArrayWriter, UnsafeRowWriter, UnsafeWriter}\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types.{UserDefinedType, _}\n+import org.apache.spark.unsafe.Platform\n+\n+/**\n+ * An interpreted unsafe projection. This class reuses the [[UnsafeRow]] it produces, a consumer\n+ * should copy the row if it is being buffered. This class is not thread safe.\n+ *\n+ * @param expressions that produces the resulting fields. These expressions must be bound\n+ *                    to a schema.\n+ */\n+class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {\n+  import InterpretedUnsafeProjection._\n+\n+  /** Number of (top level) fields in the resulting row. */\n+  private[this] val numFields = expressions.length\n+\n+  /** Array that expression results. */\n+  private[this] val values = new Array[Any](numFields)\n+\n+  /** The row representing the expression results. */\n+  private[this] val intermediate = new GenericInternalRow(values)\n+\n+  /** The row returned by the projection. */\n+  private[this] val result = new UnsafeRow(numFields)\n+\n+  /** The buffer which holds the resulting row's backing data. */\n+  private[this] val holder = new BufferHolder(result, numFields * 32)\n+\n+  /** The writer that writes the intermediate result to the result row. */\n+  private[this] val writer: InternalRow => Unit = {\n+    val rowWriter = new UnsafeRowWriter(holder, numFields)\n+    val baseWriter = generateStructWriter(\n+      holder,\n+      rowWriter,\n+      expressions.map(e => StructField(\"\", e.dataType, e.nullable)))\n+    if (!expressions.exists(_.nullable)) {\n+      // No nullable fields. The top-level null bit mask will always be zeroed out.\n+      baseWriter\n+    } else {\n+      // Zero out the null bit mask before we write the row.\n+      row => {\n+        rowWriter.zeroOutNullBytes()\n+        baseWriter(row)\n+      }\n+    }\n+  }\n+\n+  override def initialize(partitionIndex: Int): Unit = {\n+    expressions.foreach(_.foreach {\n+      case n: Nondeterministic => n.initialize(partitionIndex)\n+      case _ =>\n+    })\n+  }\n+\n+  override def apply(row: InternalRow): UnsafeRow = {\n+    // Put the expression results in the intermediate row.\n+    var i = 0\n+    while (i < numFields) {\n+      values(i) = expressions(i).eval(row)\n+      i += 1\n+    }\n+\n+    // Write the intermediate row to an unsafe row.\n+    holder.reset()\n+    writer(intermediate)\n+    result.setTotalSize(holder.totalSize())\n+    result\n+  }\n+}\n+\n+/**\n+ * Helper functions for creating an [[InterpretedUnsafeProjection]].\n+ */\n+object InterpretedUnsafeProjection extends UnsafeProjectionCreator {\n+\n+  /**\n+   * Returns an [[UnsafeProjection]] for given sequence of bound Expressions.\n+   */\n+  override protected def createProjection(exprs: Seq[Expression]): UnsafeProjection = {\n+    // We need to make sure that we do not reuse stateful non deterministic expressions.\n+    val cleanedExpressions = exprs.map(_.transform {\n+      case s: StatefulNondeterministic => s.freshCopy()\n+    })\n+    new InterpretedUnsafeProjection(cleanedExpressions.toArray)\n+  }\n+\n+  /**\n+   * Generate a struct writer function. The generated function writes an [[InternalRow]] to the\n+   * given buffer using the given [[UnsafeRowWriter]].\n+   */\n+  private def generateStructWriter(\n+      bufferHolder: BufferHolder,\n+      rowWriter: UnsafeRowWriter,\n+      fields: Array[StructField]): InternalRow => Unit = {\n+    val numFields = fields.length\n+\n+    // Create field writers.\n+    val fieldWriters = fields.map { field =>\n+      generateFieldWriter(bufferHolder, rowWriter, field.dataType, field.nullable)\n+    }\n+    // Create basic writer.\n+    row => {\n+      var i = 0\n+      while (i < numFields) {\n+        fieldWriters(i).apply(row, i)\n+        i += 1\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Generate a writer function for a struct field, array element, map key or map value. The\n+   * generated function writes the element at an index in a [[SpecializedGetters]] object (row\n+   * or array) to the given buffer using the given [[UnsafeWriter]].\n+   */\n+  private def generateFieldWriter(\n+      bufferHolder: BufferHolder,\n+      writer: UnsafeWriter,\n+      dt: DataType,\n+      nullable: Boolean): (SpecializedGetters, Int) => Unit = {\n+\n+    // Create the the basic writer.\n+    val unsafeWriter: (SpecializedGetters, Int) => Unit = dt match {\n+      case BooleanType =>\n+        (v, i) => writer.write(i, v.getBoolean(i))\n+\n+      case ByteType =>\n+        (v, i) => writer.write(i, v.getByte(i))\n+\n+      case ShortType =>\n+        (v, i) => writer.write(i, v.getShort(i))\n+\n+      case IntegerType | DateType =>\n+        (v, i) => writer.write(i, v.getInt(i))\n+\n+      case LongType | TimestampType =>\n+        (v, i) => writer.write(i, v.getLong(i))\n+\n+      case FloatType =>\n+        (v, i) => writer.write(i, v.getFloat(i))\n+\n+      case DoubleType =>\n+        (v, i) => writer.write(i, v.getDouble(i))\n+\n+      case DecimalType.Fixed(precision, scale) =>\n+        (v, i) => writer.write(i, v.getDecimal(i, precision, scale), precision, scale)\n+\n+      case CalendarIntervalType =>\n+        (v, i) => writer.write(i, v.getInterval(i))\n+\n+      case BinaryType =>\n+        (v, i) => writer.write(i, v.getBinary(i))\n+\n+      case StringType =>\n+        (v, i) => writer.write(i, v.getUTF8String(i))\n+\n+      case StructType(fields) =>\n+        val numFields = fields.length\n+        val rowWriter = new UnsafeRowWriter(bufferHolder, numFields)\n+        val structWriter = generateStructWriter(bufferHolder, rowWriter, fields)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getStruct(i, fields.length) match {\n+            case row: UnsafeRow =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                row.getBaseObject,\n+                row.getBaseOffset,\n+                row.getSizeInBytes)\n+            case row =>\n+              // Nested struct. We don't know where this will start because a row can be\n+              // variable length, so we need to update the offsets and zero out the bit mask.\n+              rowWriter.reset()\n+              structWriter.apply(row)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case ArrayType(elementType, containsNull) =>\n+        val arrayWriter = new UnsafeArrayWriter\n+        val elementSize = getElementSize(elementType)\n+        val elementWriter = generateFieldWriter(\n+          bufferHolder,\n+          arrayWriter,\n+          elementType,\n+          containsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          writeArray(bufferHolder, arrayWriter, elementWriter, v.getArray(i), elementSize)\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case MapType(keyType, valueType, valueContainsNull) =>\n+        val keyArrayWriter = new UnsafeArrayWriter\n+        val keySize = getElementSize(keyType)\n+        val keyWriter = generateFieldWriter(\n+          bufferHolder,\n+          keyArrayWriter,\n+          keyType,\n+          nullable = false)\n+        val valueArrayWriter = new UnsafeArrayWriter\n+        val valueSize = getElementSize(valueType)\n+        val valueWriter = generateFieldWriter(\n+          bufferHolder,\n+          valueArrayWriter,\n+          valueType,\n+          valueContainsNull)\n+        (v, i) => {\n+          val tmpCursor = bufferHolder.cursor\n+          v.getMap(i) match {\n+            case map: UnsafeMapData =>\n+              writeUnsafeData(\n+                bufferHolder,\n+                map.getBaseObject,\n+                map.getBaseOffset,\n+                map.getSizeInBytes)\n+            case map =>\n+              // preserve 8 bytes to write the key array numBytes later.\n+              bufferHolder.grow(8)\n+              bufferHolder.cursor += 8\n+\n+              // Write the keys and write the numBytes of key array into the first 8 bytes.\n+              writeArray(bufferHolder, keyArrayWriter, keyWriter, map.keyArray(), keySize)\n+              Platform.putLong(bufferHolder.buffer, tmpCursor, bufferHolder.cursor - tmpCursor - 8)\n+\n+              // Write the values.\n+              writeArray(bufferHolder, valueArrayWriter, valueWriter, map.valueArray(), valueSize)\n+          }\n+          writer.setOffsetAndSize(i, tmpCursor, bufferHolder.cursor - tmpCursor)\n+        }\n+\n+      case udt: UserDefinedType[_] =>\n+        generateFieldWriter(bufferHolder, writer, udt.sqlType, nullable)\n+\n+      case NullType =>\n+        (_, _) => {}\n+\n+      case _ =>\n+        throw new SparkException(s\"Unsupported data type $dt\")\n+    }\n+\n+    // Wrap the writer with a null safe version if the field is nullable.\n+    (dt, nullable) match {\n+      case (_: UserDefinedType[_], _) =>\n+        // The null wrapper depends on the sql type and not on the UDT.\n+        unsafeWriter\n+      case (DecimalType.Fixed(precision, scale), true) if precision > Decimal.MAX_LONG_DIGITS =>\n+        (v, i) => {\n+          // We can't call setNullAt() for DecimalType with precision larger than 18.\n+          if (!v.isNullAt(i)) {\n+            unsafeWriter(v, i)\n+          } else {\n+            writer.write(i, null.asInstanceOf[Decimal], precision, scale)\n+          }\n+        }\n+      case (_, true) if dt.defaultSize == 1 =>"
  }],
  "prId": 20750
}]