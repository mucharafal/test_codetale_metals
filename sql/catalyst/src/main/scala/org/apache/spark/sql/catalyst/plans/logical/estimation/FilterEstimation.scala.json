[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this is not thread safe. maybe turn FilterEstimation into a class.\r\n",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:40:50Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r"
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "Agreed.  fixed.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-09T02:12:32Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r"
  }],
  "prId": 16395
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can you add some documentation here on the high level algorithm?\r\n",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:41:21Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "basically i spent 2 mins reading this code and i have no idea how it works.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:50:31Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r"
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "fixed.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-09T02:12:48Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r"
  }],
  "prId": 16395
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "document what \"update\" means.\r\n",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:41:48Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r"
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "fixed.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-09T02:13:10Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r"
  }],
  "prId": 16395
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "please use `//` to document inline comments\r\n\r\n`/** */` are reserved for classdocs/function docs.\r\n",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:46:31Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+\r\n+    /**\r\n+     * decide if the value is in [min, max] of the column.\r\n+     * We currently don't store min/max for binary/string type.\r\n+     * Hence, we assume it is in boundary for binary/string type.\r\n+     */\r\n+    val inBoundary: Boolean = attrRef.dataType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+        val lit = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+        (lit >= statsRange.min) && (lit <= statsRange.max)\r\n+\r\n+      case _ => true  /** for String/Binary type */\r\n+    }\r\n+\r\n+    val percent: Double =\r\n+      if (inBoundary) {\r\n+\r\n+        if (update) {\r\n+          /**\r\n+           * We update ColumnStat structure after apply this equality predicate.\r\n+           * Set distinctCount to 1.  Set nullCount to 0.\r\n+           */\r\n+          val newStats = attrRef.dataType match {\r\n+            case _: NumericType | DateType | TimestampType =>\r\n+              val newValue = Some(literal.value)\r\n+              aColStat.copy(distinctCount = 1, min = newValue,\r\n+                max = newValue, nullCount = 0)\r\n+            case _ => aColStat.copy(distinctCount = 1, nullCount = 0)\r\n+          }\r\n+          mutableColStats += (attrRef.exprId -> newStats)\r\n+        }\r\n+\r\n+        1.0 / ndv.toDouble\r\n+      } else {\r\n+        0.0\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  def evaluateInSet(\r\n+      attrRef: AttributeReference,\r\n+      hSet: Set[Any],\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+    val aType = attrRef.dataType\r\n+\r\n+    // use [min, max] to filter the original hSet\r\n+    val validQuerySet = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, aType).asInstanceOf[NumericRange]\r\n+        hSet.map(e => numericLiteralToBigDecimal(e, aType, true)).\r\n+          filter(e => e >= statsRange.min && e <= statsRange.max)\r\n+\r\n+      /** We assume the whole set since there is no min/max information for String/Binary type */\r\n+      case StringType | BinaryType => hSet\r\n+    }\r\n+    if (validQuerySet.isEmpty) {\r\n+      return 0.0\r\n+    }\r\n+\r\n+    val newNdv = validQuerySet.size\r\n+    val(newMax, newMin) = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val tmpSet: Set[Double] = validQuerySet.map(e => e.toString.toDouble)\r\n+        (Some(tmpSet.max), Some(tmpSet.min))\r\n+      case _ =>\r\n+        (None, None)\r\n+    }\r\n+\r\n+    if (update) {\r\n+      val newStats = attrRef.dataType match {\r\n+        case _: NumericType | DateType | TimestampType =>\r\n+          aColStat.copy(distinctCount = newNdv, min = newMin,\r\n+            max = newMax, nullCount = 0)\r\n+        case StringType | BinaryType =>\r\n+          aColStat.copy(distinctCount = newNdv, nullCount = 0)\r\n+      }\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    /**\r\n+     * return the filter selectivity.  Without advanced statistics such as histograms,\r\n+     * we have to assume uniform distribution.\r\n+     */\r\n+    math.min(1.0, validQuerySet.size / ndv.toDouble)\r\n+  }\r\n+\r\n+  def evaluateBinaryForNumeric(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    var percent = 1.0\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+    val statsRange =\r\n+      Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+\r\n+    val literalValueBD = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+\r\n+    /** determine the overlapping degree between predicate range and column's range */\r\n+    val (noOverlap: Boolean, completeOverlap: Boolean) = op match {\r\n+      case LessThan(l, r) =>\r\n+        (literalValueBD <= statsRange.min, literalValueBD > statsRange.max)\r\n+      case LessThanOrEqual(l, r) =>\r\n+        (literalValueBD < statsRange.min, literalValueBD >= statsRange.max)\r\n+      case GreaterThan(l, r) =>\r\n+        (literalValueBD >= statsRange.max, literalValueBD < statsRange.min)\r\n+      case GreaterThanOrEqual(l, r) =>\r\n+        (literalValueBD > statsRange.max, literalValueBD <= statsRange.min)\r\n+    }\r\n+\r\n+    if (noOverlap) {\r\n+      percent = 0.0\r\n+    } else if (completeOverlap) {\r\n+      percent = 1.0\r\n+    } else {\r\n+      /** this is partial overlap case */\r\n+      var newMax = aColStat.max\r\n+      var newMin = aColStat.min\r\n+      var newNdv = ndv\r\n+      val literalToDouble = literalValueBD.toDouble\r\n+      val maxToDouble = BigDecimal(statsRange.max).toDouble\r\n+      val minToDouble = BigDecimal(statsRange.min).toDouble\r\n+\r\n+      /**\r"
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "fixed.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-09T02:13:35Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+\r\n+    /**\r\n+     * decide if the value is in [min, max] of the column.\r\n+     * We currently don't store min/max for binary/string type.\r\n+     * Hence, we assume it is in boundary for binary/string type.\r\n+     */\r\n+    val inBoundary: Boolean = attrRef.dataType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+        val lit = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+        (lit >= statsRange.min) && (lit <= statsRange.max)\r\n+\r\n+      case _ => true  /** for String/Binary type */\r\n+    }\r\n+\r\n+    val percent: Double =\r\n+      if (inBoundary) {\r\n+\r\n+        if (update) {\r\n+          /**\r\n+           * We update ColumnStat structure after apply this equality predicate.\r\n+           * Set distinctCount to 1.  Set nullCount to 0.\r\n+           */\r\n+          val newStats = attrRef.dataType match {\r\n+            case _: NumericType | DateType | TimestampType =>\r\n+              val newValue = Some(literal.value)\r\n+              aColStat.copy(distinctCount = 1, min = newValue,\r\n+                max = newValue, nullCount = 0)\r\n+            case _ => aColStat.copy(distinctCount = 1, nullCount = 0)\r\n+          }\r\n+          mutableColStats += (attrRef.exprId -> newStats)\r\n+        }\r\n+\r\n+        1.0 / ndv.toDouble\r\n+      } else {\r\n+        0.0\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  def evaluateInSet(\r\n+      attrRef: AttributeReference,\r\n+      hSet: Set[Any],\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+    val aType = attrRef.dataType\r\n+\r\n+    // use [min, max] to filter the original hSet\r\n+    val validQuerySet = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, aType).asInstanceOf[NumericRange]\r\n+        hSet.map(e => numericLiteralToBigDecimal(e, aType, true)).\r\n+          filter(e => e >= statsRange.min && e <= statsRange.max)\r\n+\r\n+      /** We assume the whole set since there is no min/max information for String/Binary type */\r\n+      case StringType | BinaryType => hSet\r\n+    }\r\n+    if (validQuerySet.isEmpty) {\r\n+      return 0.0\r\n+    }\r\n+\r\n+    val newNdv = validQuerySet.size\r\n+    val(newMax, newMin) = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val tmpSet: Set[Double] = validQuerySet.map(e => e.toString.toDouble)\r\n+        (Some(tmpSet.max), Some(tmpSet.min))\r\n+      case _ =>\r\n+        (None, None)\r\n+    }\r\n+\r\n+    if (update) {\r\n+      val newStats = attrRef.dataType match {\r\n+        case _: NumericType | DateType | TimestampType =>\r\n+          aColStat.copy(distinctCount = newNdv, min = newMin,\r\n+            max = newMax, nullCount = 0)\r\n+        case StringType | BinaryType =>\r\n+          aColStat.copy(distinctCount = newNdv, nullCount = 0)\r\n+      }\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    /**\r\n+     * return the filter selectivity.  Without advanced statistics such as histograms,\r\n+     * we have to assume uniform distribution.\r\n+     */\r\n+    math.min(1.0, validQuerySet.size / ndv.toDouble)\r\n+  }\r\n+\r\n+  def evaluateBinaryForNumeric(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    var percent = 1.0\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+    val statsRange =\r\n+      Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+\r\n+    val literalValueBD = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+\r\n+    /** determine the overlapping degree between predicate range and column's range */\r\n+    val (noOverlap: Boolean, completeOverlap: Boolean) = op match {\r\n+      case LessThan(l, r) =>\r\n+        (literalValueBD <= statsRange.min, literalValueBD > statsRange.max)\r\n+      case LessThanOrEqual(l, r) =>\r\n+        (literalValueBD < statsRange.min, literalValueBD >= statsRange.max)\r\n+      case GreaterThan(l, r) =>\r\n+        (literalValueBD >= statsRange.max, literalValueBD < statsRange.min)\r\n+      case GreaterThanOrEqual(l, r) =>\r\n+        (literalValueBD > statsRange.max, literalValueBD <= statsRange.min)\r\n+    }\r\n+\r\n+    if (noOverlap) {\r\n+      percent = 0.0\r\n+    } else if (completeOverlap) {\r\n+      percent = 1.0\r\n+    } else {\r\n+      /** this is partial overlap case */\r\n+      var newMax = aColStat.max\r\n+      var newMin = aColStat.min\r\n+      var newNdv = ndv\r\n+      val literalToDouble = literalValueBD.toDouble\r\n+      val maxToDouble = BigDecimal(statsRange.max).toDouble\r\n+      val minToDouble = BigDecimal(statsRange.min).toDouble\r\n+\r\n+      /**\r"
  }],
  "prId": 16395
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "what's the return value? selectivity?",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:47:13Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r"
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "Yes, the return value is a double value showing the percentage of rows meeting a given condition.  Also I will add comments for this method in JavaDoc style.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-09T02:16:33Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r"
  }],
  "prId": 16395
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "what's the return value? selectivity?",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:47:18Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+\r\n+    /**\r\n+     * decide if the value is in [min, max] of the column.\r\n+     * We currently don't store min/max for binary/string type.\r\n+     * Hence, we assume it is in boundary for binary/string type.\r\n+     */\r\n+    val inBoundary: Boolean = attrRef.dataType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+        val lit = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+        (lit >= statsRange.min) && (lit <= statsRange.max)\r\n+\r\n+      case _ => true  /** for String/Binary type */\r\n+    }\r\n+\r\n+    val percent: Double =\r\n+      if (inBoundary) {\r\n+\r\n+        if (update) {\r\n+          /**\r\n+           * We update ColumnStat structure after apply this equality predicate.\r\n+           * Set distinctCount to 1.  Set nullCount to 0.\r\n+           */\r\n+          val newStats = attrRef.dataType match {\r\n+            case _: NumericType | DateType | TimestampType =>\r\n+              val newValue = Some(literal.value)\r\n+              aColStat.copy(distinctCount = 1, min = newValue,\r\n+                max = newValue, nullCount = 0)\r\n+            case _ => aColStat.copy(distinctCount = 1, nullCount = 0)\r\n+          }\r\n+          mutableColStats += (attrRef.exprId -> newStats)\r\n+        }\r\n+\r\n+        1.0 / ndv.toDouble\r\n+      } else {\r\n+        0.0\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  def evaluateInSet(\r"
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "Yes, the return value is a double value showing the percentage of rows meeting a given condition. Also I will add comments for this method in JavaDoc style.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-09T02:17:28Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+\r\n+    /**\r\n+     * decide if the value is in [min, max] of the column.\r\n+     * We currently don't store min/max for binary/string type.\r\n+     * Hence, we assume it is in boundary for binary/string type.\r\n+     */\r\n+    val inBoundary: Boolean = attrRef.dataType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+        val lit = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+        (lit >= statsRange.min) && (lit <= statsRange.max)\r\n+\r\n+      case _ => true  /** for String/Binary type */\r\n+    }\r\n+\r\n+    val percent: Double =\r\n+      if (inBoundary) {\r\n+\r\n+        if (update) {\r\n+          /**\r\n+           * We update ColumnStat structure after apply this equality predicate.\r\n+           * Set distinctCount to 1.  Set nullCount to 0.\r\n+           */\r\n+          val newStats = attrRef.dataType match {\r\n+            case _: NumericType | DateType | TimestampType =>\r\n+              val newValue = Some(literal.value)\r\n+              aColStat.copy(distinctCount = 1, min = newValue,\r\n+                max = newValue, nullCount = 0)\r\n+            case _ => aColStat.copy(distinctCount = 1, nullCount = 0)\r\n+          }\r\n+          mutableColStats += (attrRef.exprId -> newStats)\r\n+        }\r\n+\r\n+        1.0 / ndv.toDouble\r\n+      } else {\r\n+        0.0\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  def evaluateInSet(\r"
  }],
  "prId": 16395
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "what's the return value? selectivity?",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-06T06:47:23Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+\r\n+    /**\r\n+     * decide if the value is in [min, max] of the column.\r\n+     * We currently don't store min/max for binary/string type.\r\n+     * Hence, we assume it is in boundary for binary/string type.\r\n+     */\r\n+    val inBoundary: Boolean = attrRef.dataType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+        val lit = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+        (lit >= statsRange.min) && (lit <= statsRange.max)\r\n+\r\n+      case _ => true  /** for String/Binary type */\r\n+    }\r\n+\r\n+    val percent: Double =\r\n+      if (inBoundary) {\r\n+\r\n+        if (update) {\r\n+          /**\r\n+           * We update ColumnStat structure after apply this equality predicate.\r\n+           * Set distinctCount to 1.  Set nullCount to 0.\r\n+           */\r\n+          val newStats = attrRef.dataType match {\r\n+            case _: NumericType | DateType | TimestampType =>\r\n+              val newValue = Some(literal.value)\r\n+              aColStat.copy(distinctCount = 1, min = newValue,\r\n+                max = newValue, nullCount = 0)\r\n+            case _ => aColStat.copy(distinctCount = 1, nullCount = 0)\r\n+          }\r\n+          mutableColStats += (attrRef.exprId -> newStats)\r\n+        }\r\n+\r\n+        1.0 / ndv.toDouble\r\n+      } else {\r\n+        0.0\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  def evaluateInSet(\r\n+      attrRef: AttributeReference,\r\n+      hSet: Set[Any],\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+    val aType = attrRef.dataType\r\n+\r\n+    // use [min, max] to filter the original hSet\r\n+    val validQuerySet = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, aType).asInstanceOf[NumericRange]\r\n+        hSet.map(e => numericLiteralToBigDecimal(e, aType, true)).\r\n+          filter(e => e >= statsRange.min && e <= statsRange.max)\r\n+\r\n+      /** We assume the whole set since there is no min/max information for String/Binary type */\r\n+      case StringType | BinaryType => hSet\r\n+    }\r\n+    if (validQuerySet.isEmpty) {\r\n+      return 0.0\r\n+    }\r\n+\r\n+    val newNdv = validQuerySet.size\r\n+    val(newMax, newMin) = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val tmpSet: Set[Double] = validQuerySet.map(e => e.toString.toDouble)\r\n+        (Some(tmpSet.max), Some(tmpSet.min))\r\n+      case _ =>\r\n+        (None, None)\r\n+    }\r\n+\r\n+    if (update) {\r\n+      val newStats = attrRef.dataType match {\r\n+        case _: NumericType | DateType | TimestampType =>\r\n+          aColStat.copy(distinctCount = newNdv, min = newMin,\r\n+            max = newMax, nullCount = 0)\r\n+        case StringType | BinaryType =>\r\n+          aColStat.copy(distinctCount = newNdv, nullCount = 0)\r\n+      }\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    /**\r\n+     * return the filter selectivity.  Without advanced statistics such as histograms,\r\n+     * we have to assume uniform distribution.\r\n+     */\r\n+    math.min(1.0, validQuerySet.size / ndv.toDouble)\r\n+  }\r\n+\r\n+  def evaluateBinaryForNumeric(\r"
  }, {
    "author": {
      "login": "ron8hu"
    },
    "body": "Yes, the return value is a double value showing the percentage of rows meeting a given condition. Also I will add comments for this method in JavaDoc style.",
    "commit": "a48a4fd7dbc1e926cbe0836017dda86ca7486002",
    "createdAt": "2017-01-09T02:17:48Z",
    "diffHunk": "@@ -0,0 +1,479 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.catalyst.plans.logical.estimation\r\n+\r\n+import scala.collection.immutable.{HashSet, Map}\r\n+import scala.collection.mutable\r\n+\r\n+import org.apache.spark.internal.Logging\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.sql.catalyst.plans.logical._\r\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.unsafe.types.UTF8String\r\n+\r\n+\r\n+object FilterEstimation extends Logging {\r\n+\r\n+  /**\r\n+   * We use a mutable colStats because we need to update the corresponding ColumnStat\r\n+   * for a column after we apply a predicate condition.\r\n+   */\r\n+  private var mutableColStats: mutable.Map[ExprId, ColumnStat] = mutable.Map.empty\r\n+\r\n+  def estimate(plan: Filter): Option[Statistics] = {\r\n+    val stats: Statistics = plan.child.statistics\r\n+    if (stats.rowCount.isEmpty) return None\r\n+\r\n+    /** save a mutable copy of colStats so that we can later change it recursively */\r\n+    val statsExprIdMap: Map[ExprId, ColumnStat] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._2))\r\n+    mutableColStats = mutable.Map.empty ++= statsExprIdMap\r\n+\r\n+    /** save a copy of ExprId-to-Attribute map for later conversion use */\r\n+    val expridToAttrMap: Map[ExprId, Attribute] =\r\n+      stats.attributeStats.map(kv => (kv._1.exprId, kv._1))\r\n+\r\n+    /** estimate selectivity for this filter */\r\n+    val percent: Double = calculateConditions(plan, plan.condition)\r\n+\r\n+    /** copy mutableColStats contents to an immutable AttributeMap */\r\n+    val mutableAttributeStats: mutable.Map[Attribute, ColumnStat] =\r\n+      mutableColStats.map(kv => (expridToAttrMap(kv._1) -> kv._2))\r\n+    val newColStats = AttributeMap(mutableAttributeStats.toSeq)\r\n+\r\n+    val filteredRowCountValue: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(stats.rowCount.get) * percent)\r\n+    val avgRowSize = BigDecimal(EstimationUtils.getRowSize(plan.output, newColStats))\r\n+    val filteredSizeInBytes: BigInt =\r\n+      EstimationUtils.ceil(BigDecimal(filteredRowCountValue) * avgRowSize)\r\n+\r\n+    Some(stats.copy(sizeInBytes = filteredSizeInBytes, rowCount = Some(filteredRowCountValue),\r\n+      attributeStats = newColStats))\r\n+  }\r\n+\r\n+  def calculateConditions(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      update: Boolean = true)\r\n+    : Double = {\r\n+    /**\r\n+     * For conditions linked by And, we need to update stats after a condition estimation\r\n+     * so that the stats will be more accurate for subsequent estimation.\r\n+     * For conditions linked by OR, we do not update stats after a condition estimation.\r\n+     */\r\n+    condition match {\r\n+      case And(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update)\r\n+        val p2 = calculateConditions(plan, cond2, update)\r\n+        p1 * p2\r\n+\r\n+      case Or(cond1, cond2) =>\r\n+        val p1 = calculateConditions(plan, cond1, update = false)\r\n+        val p2 = calculateConditions(plan, cond2, update = false)\r\n+        math.min(1.0, p1 + p2 - (p1 * p2))\r\n+\r\n+      case Not(cond) => calculateSingleCondition(plan, cond, isNot = true, update = false)\r\n+      case _ => calculateSingleCondition(plan, condition, isNot = false, update)\r\n+    }\r\n+  }\r\n+\r\n+  def calculateSingleCondition(\r\n+      plan: Filter,\r\n+      condition: Expression,\r\n+      isNot: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    var notSupported: Boolean = false\r\n+    val percent: Double = condition match {\r\n+      /**\r\n+       * Currently we only support binary predicates where one side is a column,\r\n+       * and the other is a literal.\r\n+       * Note that: all binary predicate computing methods assume the literal is at the right side,\r\n+       * so we will change the predicate order if not.\r\n+       */\r\n+      case op@LessThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThan(ar, l), ar, l, update)\r\n+\r\n+      case op@LessThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@LessThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(GreaterThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThan(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThan(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThan(ar, l), ar, l, update)\r\n+\r\n+      case op@GreaterThanOrEqual(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@GreaterThanOrEqual(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(LessThanOrEqual(ar, l), ar, l, update)\r\n+\r\n+      /** EqualTo does not care about the order */\r\n+      case op@EqualTo(ExtractAttr(ar), l: Literal) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+      case op@EqualTo(l: Literal, ExtractAttr(ar)) =>\r\n+        evaluateBinary(op, ar, l, update)\r\n+\r\n+      case In(ExtractAttr(ar), expList) if !expList.exists(!_.isInstanceOf[Literal]) =>\r\n+        /**\r\n+         * Expression [In (value, seq[Literal])] will be replaced with optimized version\r\n+         * [InSet (value, HashSet[Literal])] in Optimizer, but only for list.size > 10.\r\n+         * Here we convert In into InSet anyway, because they share the same processing logic.\r\n+         */\r\n+        val hSet = expList.map(e => e.eval())\r\n+        evaluateInSet(ar, HashSet() ++ hSet, update)\r\n+\r\n+      case InSet(ExtractAttr(ar), set) =>\r\n+        evaluateInSet(ar, set, update)\r\n+\r\n+      /**\r\n+       * It's difficult to estimate IsNull after outer joins.  Hence,\r\n+       * we support IsNull and IsNotNull only when the child is a leaf node (table).\r\n+       */\r\n+      case IsNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, true, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case IsNotNull(ExtractAttr(ar)) =>\r\n+        if (plan.child.isInstanceOf[LeafNode ]) {\r\n+          evaluateIsNull(plan, ar, false, update)\r\n+        }\r\n+        else 1.0\r\n+\r\n+      case _ =>\r\n+        /**\r\n+         * TODO: it's difficult to support string operators without advanced statistics.\r\n+         * Hence, these string operators Like(_, _) | Contains(_, _) | StartsWith(_, _)\r\n+         * | EndsWith(_, _) are not supported yet\r\n+         */\r\n+        logDebug(\"[CBO] Unsupported filter condition: \" + condition)\r\n+        notSupported = true\r\n+        1.0\r\n+    }\r\n+    if (notSupported) {\r\n+      1.0\r\n+    } else if (isNot) {\r\n+      1.0 - percent\r\n+    } else {\r\n+      percent\r\n+    }\r\n+  }\r\n+\r\n+  def evaluateIsNull(\r\n+      plan: Filter,\r\n+      attrRef: AttributeReference,\r\n+      isNull: Boolean,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val rowCountValue = plan.child.statistics.rowCount.get\r\n+    val nullPercent: BigDecimal =\r\n+      if (rowCountValue == 0) 0.0\r\n+      else BigDecimal(aColStat.nullCount)/BigDecimal(rowCountValue)\r\n+\r\n+    if (update) {\r\n+      val newStats =\r\n+        if (isNull) aColStat.copy(distinctCount = 0, min = None, max = None)\r\n+        else aColStat.copy(nullCount = 0)\r\n+\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    val percent =\r\n+      if (isNull) nullPercent.toDouble\r\n+      else {\r\n+        /** ISNOTNULL(column) */\r\n+        1.0 - nullPercent.toDouble\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  /** This method evaluates binary comparison operators such as =, <, <=, >, >= */\r\n+  def evaluateBinary(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    /** Make sure that the Date/Timestamp literal is a valid one */\r\n+    attrRef.dataType match {\r\n+      case DateType =>\r\n+        val dateLiteral = DateTimeUtils.stringToDate(literal.value.asInstanceOf[UTF8String])\r\n+        if (dateLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Date literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case TimestampType =>\r\n+        val tsLiteral = DateTimeUtils.stringToTimestamp(literal.value.asInstanceOf[UTF8String])\r\n+        if (tsLiteral.isEmpty) {\r\n+          logDebug(\"[CBO] Timestamp literal is wrong, No statistics for \" + attrRef)\r\n+          return 1.0\r\n+        }\r\n+      case _ =>\r\n+    }\r\n+\r\n+    op match {\r\n+      case EqualTo(l, r) => evaluateEqualTo(op, attrRef, literal, update)\r\n+      case _ =>\r\n+        attrRef.dataType match {\r\n+          case _: NumericType | DateType | TimestampType =>\r\n+            evaluateBinaryForNumeric(op, attrRef, literal, update)\r\n+          case StringType | BinaryType =>\r\n+            /**\r\n+             * TODO: It is difficult to support other binary comparisons for String/Binary\r\n+             * type without min/max and advanced statistics like histogram.\r\n+             */\r\n+            logDebug(\"[CBO] No statistics for String/Binary type \" + attrRef)\r\n+            return 1.0\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * This method converts a numeric or Literal value of numeric type to a BigDecimal value.\r\n+   * If isNumeric is true, then it is a numeric value.  Otherwise, it is a Literal value.\r\n+   */\r\n+  def numericLiteralToBigDecimal(\r\n+       literal: Any,\r\n+       dataType: DataType,\r\n+       isNumeric: Boolean = false)\r\n+    : BigDecimal = {\r\n+    dataType match {\r\n+      case _: IntegralType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Long])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Long])\r\n+      case _: FractionalType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[Double])\r\n+        else BigDecimal(literal.asInstanceOf[Literal].value.asInstanceOf[Double])\r\n+      case DateType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val dateLiteral = DateTimeUtils.stringToDate(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(dateLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+      case TimestampType =>\r\n+        if (isNumeric) BigDecimal(literal.asInstanceOf[BigInt])\r\n+        else {\r\n+          val tsLiteral = DateTimeUtils.stringToTimestamp(\r\n+            literal.asInstanceOf[Literal].value.asInstanceOf[UTF8String])\r\n+          BigDecimal(tsLiteral.asInstanceOf[BigInt])\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /** This method evaluates the equality predicate for all data types. */\r\n+  def evaluateEqualTo(\r\n+      op: BinaryComparison,\r\n+      attrRef: AttributeReference,\r\n+      literal: Literal,\r\n+      update: Boolean)\r\n+    : Double = {\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+\r\n+    /**\r\n+     * decide if the value is in [min, max] of the column.\r\n+     * We currently don't store min/max for binary/string type.\r\n+     * Hence, we assume it is in boundary for binary/string type.\r\n+     */\r\n+    val inBoundary: Boolean = attrRef.dataType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, attrRef.dataType).asInstanceOf[NumericRange]\r\n+        val lit = numericLiteralToBigDecimal(literal, attrRef.dataType)\r\n+        (lit >= statsRange.min) && (lit <= statsRange.max)\r\n+\r\n+      case _ => true  /** for String/Binary type */\r\n+    }\r\n+\r\n+    val percent: Double =\r\n+      if (inBoundary) {\r\n+\r\n+        if (update) {\r\n+          /**\r\n+           * We update ColumnStat structure after apply this equality predicate.\r\n+           * Set distinctCount to 1.  Set nullCount to 0.\r\n+           */\r\n+          val newStats = attrRef.dataType match {\r\n+            case _: NumericType | DateType | TimestampType =>\r\n+              val newValue = Some(literal.value)\r\n+              aColStat.copy(distinctCount = 1, min = newValue,\r\n+                max = newValue, nullCount = 0)\r\n+            case _ => aColStat.copy(distinctCount = 1, nullCount = 0)\r\n+          }\r\n+          mutableColStats += (attrRef.exprId -> newStats)\r\n+        }\r\n+\r\n+        1.0 / ndv.toDouble\r\n+      } else {\r\n+        0.0\r\n+      }\r\n+\r\n+    percent\r\n+  }\r\n+\r\n+  def evaluateInSet(\r\n+      attrRef: AttributeReference,\r\n+      hSet: Set[Any],\r\n+      update: Boolean)\r\n+    : Double = {\r\n+    if (!mutableColStats.contains(attrRef.exprId)) {\r\n+      logDebug(\"[CBO] No statistics for \" + attrRef)\r\n+      return 1.0\r\n+    }\r\n+\r\n+    val aColStat = mutableColStats(attrRef.exprId)\r\n+    val ndv = aColStat.distinctCount\r\n+    val aType = attrRef.dataType\r\n+\r\n+    // use [min, max] to filter the original hSet\r\n+    val validQuerySet = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val statsRange =\r\n+          Range(aColStat.min, aColStat.max, aType).asInstanceOf[NumericRange]\r\n+        hSet.map(e => numericLiteralToBigDecimal(e, aType, true)).\r\n+          filter(e => e >= statsRange.min && e <= statsRange.max)\r\n+\r\n+      /** We assume the whole set since there is no min/max information for String/Binary type */\r\n+      case StringType | BinaryType => hSet\r\n+    }\r\n+    if (validQuerySet.isEmpty) {\r\n+      return 0.0\r\n+    }\r\n+\r\n+    val newNdv = validQuerySet.size\r\n+    val(newMax, newMin) = aType match {\r\n+      case _: NumericType | DateType | TimestampType =>\r\n+        val tmpSet: Set[Double] = validQuerySet.map(e => e.toString.toDouble)\r\n+        (Some(tmpSet.max), Some(tmpSet.min))\r\n+      case _ =>\r\n+        (None, None)\r\n+    }\r\n+\r\n+    if (update) {\r\n+      val newStats = attrRef.dataType match {\r\n+        case _: NumericType | DateType | TimestampType =>\r\n+          aColStat.copy(distinctCount = newNdv, min = newMin,\r\n+            max = newMax, nullCount = 0)\r\n+        case StringType | BinaryType =>\r\n+          aColStat.copy(distinctCount = newNdv, nullCount = 0)\r\n+      }\r\n+      mutableColStats += (attrRef.exprId -> newStats)\r\n+    }\r\n+\r\n+    /**\r\n+     * return the filter selectivity.  Without advanced statistics such as histograms,\r\n+     * we have to assume uniform distribution.\r\n+     */\r\n+    math.min(1.0, validQuerySet.size / ndv.toDouble)\r\n+  }\r\n+\r\n+  def evaluateBinaryForNumeric(\r"
  }],
  "prId": 16395
}]