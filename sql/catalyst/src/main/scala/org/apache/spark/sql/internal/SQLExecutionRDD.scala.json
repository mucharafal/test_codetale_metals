[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: use `SQLExecution.EXECUTION_ID_KEY` instead of hardcode",
    "commit": "11848f59c6230d935127e0d939dcc89610b45c74",
    "createdAt": "2019-09-02T06:32:41Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.internal\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD(\n+    var sqlRDD: RDD[InternalRow], @transient conf: SQLConf) extends RDD[InternalRow](sqlRDD) {\n+  private val sqlConfigs = conf.getAllConfs\n+\n+  override val partitioner = firstParent[InternalRow].partitioner\n+\n+  override def getPartitions: Array[Partition] = firstParent[InternalRow].partitions\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    // If we are in the context of a tracked SQL operation, `SQLExecution.EXECUTION_ID_KEY` is set\n+    // and we have nothing to do here. Otherwise, we use the `SQLConf` captured at the creation of\n+    // this RDD.\n+    if (context.getLocalProperty(\"spark.sql.execution.id\") == null) {"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "I cannot, because `SQLExecution ` is in core and here we are in catalyst, so we are missing the dependency..",
    "commit": "11848f59c6230d935127e0d939dcc89610b45c74",
    "createdAt": "2019-09-02T08:14:38Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.internal\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD(\n+    var sqlRDD: RDD[InternalRow], @transient conf: SQLConf) extends RDD[InternalRow](sqlRDD) {\n+  private val sqlConfigs = conf.getAllConfs\n+\n+  override val partitioner = firstParent[InternalRow].partitioner\n+\n+  override def getPartitions: Array[Partition] = firstParent[InternalRow].partitions\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    // If we are in the context of a tracked SQL operation, `SQLExecution.EXECUTION_ID_KEY` is set\n+    // and we have nothing to do here. Otherwise, we use the `SQLConf` captured at the creation of\n+    // this RDD.\n+    if (context.getLocalProperty(\"spark.sql.execution.id\") == null) {"
  }],
  "prId": 25643
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is this really needed? `sqlRDD` is an constructor parameter which won't be kept as a member variable.",
    "commit": "11848f59c6230d935127e0d939dcc89610b45c74",
    "createdAt": "2019-09-02T06:34:30Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.internal\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD(\n+    var sqlRDD: RDD[InternalRow], @transient conf: SQLConf) extends RDD[InternalRow](sqlRDD) {\n+  private val sqlConfigs = conf.getAllConfs\n+\n+  override val partitioner = firstParent[InternalRow].partitioner\n+\n+  override def getPartitions: Array[Partition] = firstParent[InternalRow].partitions\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    // If we are in the context of a tracked SQL operation, `SQLExecution.EXECUTION_ID_KEY` is set\n+    // and we have nothing to do here. Otherwise, we use the `SQLConf` captured at the creation of\n+    // this RDD.\n+    if (context.getLocalProperty(\"spark.sql.execution.id\") == null) {\n+      val originalLocalProps = sqlConfigs.collect {\n+        case (key, value) if key.startsWith(\"spark\") =>\n+          val originalValue = context.getLocalProperty(key)\n+          context.getLocalProperties.setProperty(key, value)\n+          (key, originalValue)\n+      }\n+\n+      try {\n+        firstParent[InternalRow].iterator(split, context)\n+      } finally {\n+        for ((key, value) <- originalLocalProps) {\n+          if (value == null) {\n+            context.getLocalProperties.remove(key)\n+          } else {\n+            context.getLocalProperties.setProperty(key, value)\n+          }\n+        }\n+      }\n+    } else {\n+      firstParent[InternalRow].iterator(split, context)\n+    }\n+  }\n+\n+  override def clearDependencies() {"
  }],
  "prId": 25643
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Why is this in catalyst and not in core?",
    "commit": "11848f59c6230d935127e0d939dcc89610b45c74",
    "createdAt": "2019-09-02T10:10:17Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.internal\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD("
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "thanks, I'm moving ti",
    "commit": "11848f59c6230d935127e0d939dcc89610b45c74",
    "createdAt": "2019-09-02T12:01:42Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.internal\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD("
  }],
  "prId": 25643
}]