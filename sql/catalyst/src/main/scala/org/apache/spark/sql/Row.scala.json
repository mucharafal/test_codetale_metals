[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@hvanhovell, how about reusing `JacksonGenerator` in our JSON datasource?",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T08:10:30Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "There's `pretty` option for `prettyJson` too.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T08:11:14Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, right, schema can be unknown .. ",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T10:06:37Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Well you still need the schema. The main reason for not using Jackson generator is that we need to convert back to an internal row and this is super slow.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T11:51:08Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Hm, this API looks already pretty slow though, and I suspect this API should not be called in a critical path .. ?\r\nIf it's supposed to be used in a critical path, we might rather have to provide a API to make a convert function given schema (so that we avoid type dispatch for every row).\r\n\r\nOne rather minor concern is that the JSON representation for a row seems different comparing to JSON datasource. e.g.)  https://github.com/apache/spark/pull/26013/files#r331463832 and https://github.com/apache/spark/pull/26013/files#diff-78ce4e47d137bbb0d4350ad732b48d5bR576-R578\r\n\r\nand here a bit duplicates the codes ..\r\n",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T13:22:20Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "So two things to consider here.\r\n\r\nI want to use this in StreamingQueryProgress right? All the JSON serialization there is based on the json4s AST and not strings (which is what JacksonGenerator produces). \r\n\r\nThere is a difference between it being slow, and what you are suggesting. The latter being crazy inefficient. Let's break that down:\r\n- Row to InternalRow conversion. You will need to create a converter per row because there is currently no way we can safely cache a converter. You can either use `ScalaReflection` or `RowEncoder` here, the latter is particularly bad because it uses code generation (which takes in the order of mills and which is weakly cached on the driver).\r\n- Setting up the JacksonGenerator, again this is uncached and we need to set up the same thing for each tuple. \r\n- Generating the string.\r\n\r\nDo you see my point here? Or shall I write a benchmark?\r\n",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T18:56:12Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "There's one API case we dropped performance improvement in `Row` as an example (see https://github.com/apache/spark/pull/23271).\r\n\r\n```scala\r\n  @deprecated(\"This method is deprecated and will be removed in future versions.\", \"3.0.0\")\r\n  def merge(rows: Row*): Row = {\r\n    // TODO: Improve the performance of this if used in performance critical part.\r\n    new GenericRow(rows.flatMap(_.toSeq).toArray)\r\n  }\r\n```\r\n\r\nDo you mind if I ask to add `@Unstable` or `@Private` for these new APIs instead just for future improvement in case, with `@since` in the Scaladoc?\r\n\r\n`Row` itself is marked as `@Stable` so it might better explicitly note that this can be changed in the future. With this LGTM.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-10T00:15:56Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "I will mark them as @Unstable. @Private is debatable, because it is not really meant as an internal only API.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-14T15:30:17Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)",
    "line": 37
  }],
  "prId": 26013
}, {
  "comments": [{
    "author": {
      "login": "MaxGekk"
    },
    "body": "Base64? Why do you need this special case?",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T11:53:07Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))",
    "line": 81
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "How else are you going to represent a binary in JSON?",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T11:55:36Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))",
    "line": 81
  }, {
    "author": {
      "login": "MaxGekk"
    },
    "body": "JSON has the array type, right? it could be `[1, 2, 3]`.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T17:11:49Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))",
    "line": 81
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "We need a way to distinguish binary type and byte array. They are 2 different types in Spark and we should display them differently in JSON.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T09:06:37Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))",
    "line": 81
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "In the current `JacksonGenerator` a `BinaryType` is emitted as a base64 encoded string. Arrays of bytes are treated differently and are emitted as a JSON array.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T09:17:50Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))",
    "line": 81
  }],
  "prId": 26013
}, {
  "comments": [{
    "author": {
      "login": "MaxGekk"
    },
    "body": "```suggestion\r\n    lazy val zoneId = DateTimeUtils.getZoneId(SQLConf.get.sessionLocalTimeZone)\r\n```",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T17:03:52Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "+1",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T09:08:45Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)"
  }],
  "prId": 26013
}, {
  "comments": [{
    "author": {
      "login": "MaxGekk"
    },
    "body": "Why do you use explictit `.apply`?",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-04T17:04:44Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Why not? In some cases it is clearer. However the only reason I am responding to this is because IMO we shouldn't dwell too much on style nits, or wait with them until a final review round.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T09:20:15Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)"
  }, {
    "author": {
      "login": "MaxGekk"
    },
    "body": "> Why not? In some cases it is clearer.\r\n\r\nThanks for the answer. I was just wondered why you desugared this code, but didn't desugar `match ... case`, for instance. ;-)",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T17:45:57Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)"
  }],
  "prId": 26013
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is it really worth to have a special format for string-type-key map?",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T09:10:59Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))\n+      case (d: LocalDate, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.localDateToDays(d)))\n+      case (d: Date, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.fromJavaDate(d)))\n+      case (i: Instant, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.instantToMicros(i)))\n+      case (t: Timestamp, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.fromJavaTimestamp(t)))\n+      case (a: Array[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(a.iterator, elementType)\n+      case (s: Seq[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(s.iterator, elementType)\n+      case (m: Map[String @unchecked, _], MapType(StringType, valueType, _)) =>",
    "line": 94
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "The reason would that is emits more readable JSON. This is similar to the way StreamingQueryProgress is rendering maps. I can revert if you feel strongly about this.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T09:23:59Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))\n+      case (d: LocalDate, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.localDateToDays(d)))\n+      case (d: Date, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.fromJavaDate(d)))\n+      case (i: Instant, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.instantToMicros(i)))\n+      case (t: Timestamp, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.fromJavaTimestamp(t)))\n+      case (a: Array[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(a.iterator, elementType)\n+      case (s: Seq[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(s.iterator, elementType)\n+      case (m: Map[String @unchecked, _], MapType(StringType, valueType, _)) =>",
    "line": 94
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Do we need to convert the JSON string back to a Row? If we do then I think it's better to keep the ser/de simply. If not I'm fine with the code here.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T11:05:41Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))\n+      case (d: LocalDate, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.localDateToDays(d)))\n+      case (d: Date, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.fromJavaDate(d)))\n+      case (i: Instant, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.instantToMicros(i)))\n+      case (t: Timestamp, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.fromJavaTimestamp(t)))\n+      case (a: Array[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(a.iterator, elementType)\n+      case (s: Seq[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(s.iterator, elementType)\n+      case (m: Map[String @unchecked, _], MapType(StringType, valueType, _)) =>",
    "line": 94
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "In its current form it is not really meant to be converted back.",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-07T18:58:38Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))\n+      case (d: LocalDate, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.localDateToDays(d)))\n+      case (d: Date, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.fromJavaDate(d)))\n+      case (i: Instant, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.instantToMicros(i)))\n+      case (t: Timestamp, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.fromJavaTimestamp(t)))\n+      case (a: Array[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(a.iterator, elementType)\n+      case (s: Seq[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(s.iterator, elementType)\n+      case (m: Map[String @unchecked, _], MapType(StringType, valueType, _)) =>",
    "line": 94
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Can other primitive types like Int be good for this format too?",
    "commit": "43c2d249614359e80f61c275b16e0a498abcb842",
    "createdAt": "2019-10-10T20:06:50Z",
    "diffHunk": "@@ -501,4 +513,88 @@ trait Row extends Serializable {\n   private def getAnyValAs[T <: AnyVal](i: Int): T =\n     if (isNullAt(i)) throw new NullPointerException(s\"Value at index $i is null\")\n     else getAs[T](i)\n+\n+  /** The compact JSON representation of this row. */\n+  def json: String = compact(jsonValue)\n+\n+  /** The pretty (i.e. indented) JSON representation of this row. */\n+  def prettyJson: String = pretty(render(jsonValue))\n+\n+  /**\n+   * JSON representation of the row.\n+   *\n+   * Note that this only supports the data types that are also supported by\n+   * [[org.apache.spark.sql.catalyst.encoders.RowEncoder]].\n+   *\n+   * @return the JSON representation of the row.\n+   */\n+  private[sql] def jsonValue: JValue = {\n+    require(schema != null, \"JSON serialization requires a non-null schema.\")\n+\n+    lazy val timeZone = TimeZone.getTimeZone(SQLConf.get.sessionLocalTimeZone)\n+    lazy val dateFormatter = DateFormatter.apply(timeZone.toZoneId)\n+    lazy val timestampFormatter = TimestampFormatter.apply(timeZone.toZoneId)\n+\n+    // Convert an iterator of values to a json array\n+    def iteratorToJsonArray(iterator: Iterator[_], elementType: DataType): JArray = {\n+      JArray(iterator.map(toJson(_, elementType)).toList)\n+    }\n+\n+    // Convert a value to json.\n+    def toJson(value: Any, dataType: DataType): JValue = (value, dataType) match {\n+      case (null, _) => JNull\n+      case (b: Boolean, _) => JBool(b)\n+      case (b: Byte, _) => JLong(b)\n+      case (s: Short, _) => JLong(s)\n+      case (i: Int, _) => JLong(i)\n+      case (l: Long, _) => JLong(l)\n+      case (f: Float, _) => JDouble(f)\n+      case (d: Double, _) => JDouble(d)\n+      case (d: BigDecimal, _) => JDecimal(d)\n+      case (d: java.math.BigDecimal, _) => JDecimal(d)\n+      case (d: Decimal, _) => JDecimal(d.toBigDecimal)\n+      case (s: String, _) => JString(s)\n+      case (b: Array[Byte], BinaryType) =>\n+        JString(Base64.getEncoder.encodeToString(b))\n+      case (d: LocalDate, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.localDateToDays(d)))\n+      case (d: Date, _) =>\n+        JString(dateFormatter.format(DateTimeUtils.fromJavaDate(d)))\n+      case (i: Instant, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.instantToMicros(i)))\n+      case (t: Timestamp, _) =>\n+        JString(timestampFormatter.format(DateTimeUtils.fromJavaTimestamp(t)))\n+      case (a: Array[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(a.iterator, elementType)\n+      case (s: Seq[_], ArrayType(elementType, _)) =>\n+        iteratorToJsonArray(s.iterator, elementType)\n+      case (m: Map[String @unchecked, _], MapType(StringType, valueType, _)) =>",
    "line": 94
  }],
  "prId": 26013
}]