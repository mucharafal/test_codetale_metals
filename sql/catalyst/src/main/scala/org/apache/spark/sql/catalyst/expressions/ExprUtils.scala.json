[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about\r\n```\r\nif (expr.isFoldable && expr.dataType == StringType) {\r\n  DataType.fromDDL(expr.eval().asInstanceOf[UTF8String].toString)\r\n}\r\n```",
    "commit": "3aa79d4e438a84ea7566f38afd3f2a18fd7cfbed",
    "createdAt": "2018-10-29T03:01:07Z",
    "diffHunk": "@@ -19,14 +19,39 @@ package org.apache.spark.sql.catalyst.expressions\n \n import org.apache.spark.sql.AnalysisException\n import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n-import org.apache.spark.sql.types.{MapType, StringType, StructType}\n+import org.apache.spark.sql.types.{DataType, MapType, StringType, StructType}\n+import org.apache.spark.unsafe.types.UTF8String\n \n object ExprUtils {\n \n-  def evalSchemaExpr(exp: Expression): StructType = exp match {\n-    case Literal(s, StringType) => StructType.fromDDL(s.toString)\n+  def evalSchemaExpr(exp: Expression): StructType = {\n+    // Use `DataType.fromDDL` since the type string can be struct<...>.\n+    val dataType = exp match {\n+      case Literal(s, StringType) =>\n+        DataType.fromDDL(s.toString)\n+      case e @ SchemaOfCsv(_: Literal, _) =>\n+        val ddlSchema = e.eval(EmptyRow).asInstanceOf[UTF8String]\n+        DataType.fromDDL(ddlSchema.toString)\n+      case e => throw new AnalysisException(\n+        \"Schema should be specified in DDL format as a string literal or output of \" +\n+          s\"the schema_of_csv function instead of ${e.sql}\")\n+    }\n+\n+    if (!dataType.isInstanceOf[StructType]) {\n+      throw new AnalysisException(\n+        s\"Schema should be struct type but got ${dataType.sql}.\")\n+    }\n+    dataType.asInstanceOf[StructType]\n+  }\n+\n+  def evalTypeExpr(exp: Expression): DataType = exp match {\n+    case Literal(s, StringType) => DataType.fromDDL(s.toString)",
    "line": 33
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "we also need to update https://github.com/apache/spark/pull/22666/files#diff-5321c01e95bffc4413c5f3457696213eR157\r\n\r\n in case the constant folding rule is disabled.",
    "commit": "3aa79d4e438a84ea7566f38afd3f2a18fd7cfbed",
    "createdAt": "2018-10-29T03:05:49Z",
    "diffHunk": "@@ -19,14 +19,39 @@ package org.apache.spark.sql.catalyst.expressions\n \n import org.apache.spark.sql.AnalysisException\n import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n-import org.apache.spark.sql.types.{MapType, StringType, StructType}\n+import org.apache.spark.sql.types.{DataType, MapType, StringType, StructType}\n+import org.apache.spark.unsafe.types.UTF8String\n \n object ExprUtils {\n \n-  def evalSchemaExpr(exp: Expression): StructType = exp match {\n-    case Literal(s, StringType) => StructType.fromDDL(s.toString)\n+  def evalSchemaExpr(exp: Expression): StructType = {\n+    // Use `DataType.fromDDL` since the type string can be struct<...>.\n+    val dataType = exp match {\n+      case Literal(s, StringType) =>\n+        DataType.fromDDL(s.toString)\n+      case e @ SchemaOfCsv(_: Literal, _) =>\n+        val ddlSchema = e.eval(EmptyRow).asInstanceOf[UTF8String]\n+        DataType.fromDDL(ddlSchema.toString)\n+      case e => throw new AnalysisException(\n+        \"Schema should be specified in DDL format as a string literal or output of \" +\n+          s\"the schema_of_csv function instead of ${e.sql}\")\n+    }\n+\n+    if (!dataType.isInstanceOf[StructType]) {\n+      throw new AnalysisException(\n+        s\"Schema should be struct type but got ${dataType.sql}.\")\n+    }\n+    dataType.asInstanceOf[StructType]\n+  }\n+\n+  def evalTypeExpr(exp: Expression): DataType = exp match {\n+    case Literal(s, StringType) => DataType.fromDDL(s.toString)",
    "line": 33
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yup, that's what I initially thought that we should allow constant-foldable expressions as well but just decided to follow the initial intent - literal only support. I wasn't also sure about when we would need constant folding to construct a JSON example because I suspected that's usually copied and pasted from, for instance, a file.",
    "commit": "3aa79d4e438a84ea7566f38afd3f2a18fd7cfbed",
    "createdAt": "2018-10-29T03:10:35Z",
    "diffHunk": "@@ -19,14 +19,39 @@ package org.apache.spark.sql.catalyst.expressions\n \n import org.apache.spark.sql.AnalysisException\n import org.apache.spark.sql.catalyst.util.ArrayBasedMapData\n-import org.apache.spark.sql.types.{MapType, StringType, StructType}\n+import org.apache.spark.sql.types.{DataType, MapType, StringType, StructType}\n+import org.apache.spark.unsafe.types.UTF8String\n \n object ExprUtils {\n \n-  def evalSchemaExpr(exp: Expression): StructType = exp match {\n-    case Literal(s, StringType) => StructType.fromDDL(s.toString)\n+  def evalSchemaExpr(exp: Expression): StructType = {\n+    // Use `DataType.fromDDL` since the type string can be struct<...>.\n+    val dataType = exp match {\n+      case Literal(s, StringType) =>\n+        DataType.fromDDL(s.toString)\n+      case e @ SchemaOfCsv(_: Literal, _) =>\n+        val ddlSchema = e.eval(EmptyRow).asInstanceOf[UTF8String]\n+        DataType.fromDDL(ddlSchema.toString)\n+      case e => throw new AnalysisException(\n+        \"Schema should be specified in DDL format as a string literal or output of \" +\n+          s\"the schema_of_csv function instead of ${e.sql}\")\n+    }\n+\n+    if (!dataType.isInstanceOf[StructType]) {\n+      throw new AnalysisException(\n+        s\"Schema should be struct type but got ${dataType.sql}.\")\n+    }\n+    dataType.asInstanceOf[StructType]\n+  }\n+\n+  def evalTypeExpr(exp: Expression): DataType = exp match {\n+    case Literal(s, StringType) => DataType.fromDDL(s.toString)",
    "line": 33
  }],
  "prId": 22666
}]