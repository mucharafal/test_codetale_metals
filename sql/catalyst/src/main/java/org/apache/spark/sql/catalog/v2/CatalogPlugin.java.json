[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "it's weird to ask the catalog plugin to report name and `initialize` requires a name.",
    "commit": "7c64a2678de28224e1dda8c662ca750c5438fc9b",
    "createdAt": "2019-02-28T03:26:43Z",
    "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import org.apache.spark.sql.internal.SQLConf;\n+\n+/**\n+ * A marker interface to provide a catalog implementation for Spark.\n+ * <p>\n+ * Implementations can provide catalog functions by implementing additional interfaces for tables,\n+ * views, and functions.\n+ * <p>\n+ * Catalog implementations must implement this marker interface to be loaded by\n+ * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n+ * required public no-arg constructor. After creating an instance, it will be configured by calling\n+ * {@link #initialize(String, CaseInsensitiveStringMap)}.\n+ * <p>\n+ * Catalog implementations are registered to a name by adding a configuration option to Spark:\n+ * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n+ * in the Spark configuration that share the catalog name prefix,\n+ * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n+ * string map of options in initialization with the prefix removed. An additional property,\n+ * {@code name}, is also added to the options and will contain the catalog's name; in this case,\n+ * \"catalog-name\".\n+ */\n+public interface CatalogPlugin {\n+  /**\n+   * Called to initialize configuration.\n+   * <p>\n+   * This method is called once, just after the provider is instantiated.\n+   *\n+   * @param name the name used to identify and load this catalog\n+   * @param options a case-insensitive string map of configuration\n+   */\n+  void initialize(String name, CaseInsensitiveStringMap options);",
    "line": 52
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall this be `void initialize(CaseInsensitiveStringMap options);`?",
    "commit": "7c64a2678de28224e1dda8c662ca750c5438fc9b",
    "createdAt": "2019-02-28T03:27:02Z",
    "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import org.apache.spark.sql.internal.SQLConf;\n+\n+/**\n+ * A marker interface to provide a catalog implementation for Spark.\n+ * <p>\n+ * Implementations can provide catalog functions by implementing additional interfaces for tables,\n+ * views, and functions.\n+ * <p>\n+ * Catalog implementations must implement this marker interface to be loaded by\n+ * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n+ * required public no-arg constructor. After creating an instance, it will be configured by calling\n+ * {@link #initialize(String, CaseInsensitiveStringMap)}.\n+ * <p>\n+ * Catalog implementations are registered to a name by adding a configuration option to Spark:\n+ * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n+ * in the Spark configuration that share the catalog name prefix,\n+ * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n+ * string map of options in initialization with the prefix removed. An additional property,\n+ * {@code name}, is also added to the options and will contain the catalog's name; in this case,\n+ * \"catalog-name\".\n+ */\n+public interface CatalogPlugin {\n+  /**\n+   * Called to initialize configuration.\n+   * <p>\n+   * This method is called once, just after the provider is instantiated.\n+   *\n+   * @param name the name used to identify and load this catalog\n+   * @param options a case-insensitive string map of configuration\n+   */\n+  void initialize(String name, CaseInsensitiveStringMap options);",
    "line": 52
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "The catalog is passed the name that was used to identify it. For example, say I have a REST-based catalog endpoint that I'm configuring in two cases, like this:\r\n\r\n```\r\nspark.sql.catalog.prod = com.example.MyCatalogPlugin\r\nspark.sql.catalog.prod.connuri = http://prod.catalog.example.com:80/\r\nspark.sql.catalog.test = com.example.MyCatalogPlugin\r\nspark.sql.catalog.test.connuri = http://test.catalog.example.com:80/\r\n```\r\n\r\n`MyCatalogPlugin` is instantiated and configured twice and both times is passed the name it is configured with, `prod` and `test`.\r\n\r\nAdding a getter for `name` just makes it easy to identify the catalog without Spark keeping track of name -> catalog instance everywhere.",
    "commit": "7c64a2678de28224e1dda8c662ca750c5438fc9b",
    "createdAt": "2019-02-28T18:49:55Z",
    "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import org.apache.spark.sql.internal.SQLConf;\n+\n+/**\n+ * A marker interface to provide a catalog implementation for Spark.\n+ * <p>\n+ * Implementations can provide catalog functions by implementing additional interfaces for tables,\n+ * views, and functions.\n+ * <p>\n+ * Catalog implementations must implement this marker interface to be loaded by\n+ * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n+ * required public no-arg constructor. After creating an instance, it will be configured by calling\n+ * {@link #initialize(String, CaseInsensitiveStringMap)}.\n+ * <p>\n+ * Catalog implementations are registered to a name by adding a configuration option to Spark:\n+ * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n+ * in the Spark configuration that share the catalog name prefix,\n+ * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n+ * string map of options in initialization with the prefix removed. An additional property,\n+ * {@code name}, is also added to the options and will contain the catalog's name; in this case,\n+ * \"catalog-name\".\n+ */\n+public interface CatalogPlugin {\n+  /**\n+   * Called to initialize configuration.\n+   * <p>\n+   * This method is called once, just after the provider is instantiated.\n+   *\n+   * @param name the name used to identify and load this catalog\n+   * @param options a case-insensitive string map of configuration\n+   */\n+  void initialize(String name, CaseInsensitiveStringMap options);",
    "line": 52
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "In this case, how would the `MyCatalogPlugin` report its name? `prod` or `test`?",
    "commit": "7c64a2678de28224e1dda8c662ca750c5438fc9b",
    "createdAt": "2019-03-01T05:15:09Z",
    "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import org.apache.spark.sql.internal.SQLConf;\n+\n+/**\n+ * A marker interface to provide a catalog implementation for Spark.\n+ * <p>\n+ * Implementations can provide catalog functions by implementing additional interfaces for tables,\n+ * views, and functions.\n+ * <p>\n+ * Catalog implementations must implement this marker interface to be loaded by\n+ * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n+ * required public no-arg constructor. After creating an instance, it will be configured by calling\n+ * {@link #initialize(String, CaseInsensitiveStringMap)}.\n+ * <p>\n+ * Catalog implementations are registered to a name by adding a configuration option to Spark:\n+ * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n+ * in the Spark configuration that share the catalog name prefix,\n+ * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n+ * string map of options in initialization with the prefix removed. An additional property,\n+ * {@code name}, is also added to the options and will contain the catalog's name; in this case,\n+ * \"catalog-name\".\n+ */\n+public interface CatalogPlugin {\n+  /**\n+   * Called to initialize configuration.\n+   * <p>\n+   * This method is called once, just after the provider is instantiated.\n+   *\n+   * @param name the name used to identify and load this catalog\n+   * @param options a case-insensitive string map of configuration\n+   */\n+  void initialize(String name, CaseInsensitiveStringMap options);",
    "line": 52
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "^ depends on which catalog instance. one would say `prod` the other would say `test`",
    "commit": "7c64a2678de28224e1dda8c662ca750c5438fc9b",
    "createdAt": "2019-03-01T07:02:58Z",
    "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import org.apache.spark.sql.internal.SQLConf;\n+\n+/**\n+ * A marker interface to provide a catalog implementation for Spark.\n+ * <p>\n+ * Implementations can provide catalog functions by implementing additional interfaces for tables,\n+ * views, and functions.\n+ * <p>\n+ * Catalog implementations must implement this marker interface to be loaded by\n+ * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n+ * required public no-arg constructor. After creating an instance, it will be configured by calling\n+ * {@link #initialize(String, CaseInsensitiveStringMap)}.\n+ * <p>\n+ * Catalog implementations are registered to a name by adding a configuration option to Spark:\n+ * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n+ * in the Spark configuration that share the catalog name prefix,\n+ * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n+ * string map of options in initialization with the prefix removed. An additional property,\n+ * {@code name}, is also added to the options and will contain the catalog's name; in this case,\n+ * \"catalog-name\".\n+ */\n+public interface CatalogPlugin {\n+  /**\n+   * Called to initialize configuration.\n+   * <p>\n+   * This method is called once, just after the provider is instantiated.\n+   *\n+   * @param name the name used to identify and load this catalog\n+   * @param options a case-insensitive string map of configuration\n+   */\n+  void initialize(String name, CaseInsensitiveStringMap options);",
    "line": 52
  }],
  "prId": 23915
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We need to document that, this method can only be called after `initialize` is called.",
    "commit": "7c64a2678de28224e1dda8c662ca750c5438fc9b",
    "createdAt": "2019-03-06T15:17:49Z",
    "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A marker interface to provide a catalog implementation for Spark.\n+ * <p>\n+ * Implementations can provide catalog functions by implementing additional interfaces for tables,\n+ * views, and functions.\n+ * <p>\n+ * Catalog implementations must implement this marker interface to be loaded by\n+ * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n+ * required public no-arg constructor. After creating an instance, it will be configured by calling\n+ * {@link #initialize(String, CaseInsensitiveStringMap)}.\n+ * <p>\n+ * Catalog implementations are registered to a name by adding a configuration option to Spark:\n+ * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n+ * in the Spark configuration that share the catalog name prefix,\n+ * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n+ * string map of options in initialization with the prefix removed. An additional property,\n+ * {@code name}, is also added to the options and will contain the catalog's name; in this case,\n+ * \"catalog-name\".\n+ */\n+public interface CatalogPlugin {\n+  /**\n+   * Called to initialize configuration.\n+   * <p>\n+   * This method is called once, just after the provider is instantiated.\n+   *\n+   * @param name the name used to identify and load this catalog\n+   * @param options a case-insensitive string map of configuration\n+   */\n+  void initialize(String name, CaseInsensitiveStringMap options);\n+\n+  /**\n+   * Called to get this catalog's name.\n+   */\n+  String name();",
    "line": 60
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Done.",
    "commit": "7c64a2678de28224e1dda8c662ca750c5438fc9b",
    "createdAt": "2019-03-06T16:49:33Z",
    "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A marker interface to provide a catalog implementation for Spark.\n+ * <p>\n+ * Implementations can provide catalog functions by implementing additional interfaces for tables,\n+ * views, and functions.\n+ * <p>\n+ * Catalog implementations must implement this marker interface to be loaded by\n+ * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n+ * required public no-arg constructor. After creating an instance, it will be configured by calling\n+ * {@link #initialize(String, CaseInsensitiveStringMap)}.\n+ * <p>\n+ * Catalog implementations are registered to a name by adding a configuration option to Spark:\n+ * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n+ * in the Spark configuration that share the catalog name prefix,\n+ * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n+ * string map of options in initialization with the prefix removed. An additional property,\n+ * {@code name}, is also added to the options and will contain the catalog's name; in this case,\n+ * \"catalog-name\".\n+ */\n+public interface CatalogPlugin {\n+  /**\n+   * Called to initialize configuration.\n+   * <p>\n+   * This method is called once, just after the provider is instantiated.\n+   *\n+   * @param name the name used to identify and load this catalog\n+   * @param options a case-insensitive string map of configuration\n+   */\n+  void initialize(String name, CaseInsensitiveStringMap options);\n+\n+  /**\n+   * Called to get this catalog's name.\n+   */\n+  String name();",
    "line": 60
  }],
  "prId": 23915
}]