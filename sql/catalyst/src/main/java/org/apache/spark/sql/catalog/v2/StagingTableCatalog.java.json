[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Looks like this doc is unfinished.",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-06-17T18:55:11Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's",
    "line": 48
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "It's not, the comment continues to the links in the following line.",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-06-17T23:27:56Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's",
    "line": 48
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Oops, I misread it. Thanks!",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-06-17T23:37:27Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's",
    "line": 48
  }],
  "prId": 24798
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Can we move this API out to a new PR and implement atomic CTAS? I think REPLACE TABLE is not a blocker to this API and we don't have to do them together. This can also help us move forward faster, since designing a new SQL syntax (REPLACE TABLE) usually needs more time to get consensus.",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-07-11T11:29:59Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's\n+ * {@link BatchWrite#commit(WriterCommitMessage[])} is called,\n+ * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can\n+ * complete both the data write and the metadata swap operation atomically.\n+ */\n+public interface StagingTableCatalog extends TableCatalog {",
    "line": 53
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I would think that `REPLACE TABLE` and `REPLACE TABLE AS SELECT` is about as simple as one can get for this kind of feature. Can we stick with that for now? I'd really like to see `REPLACE` get in soon, it's a blocker for some of my workflows.",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-07-11T17:41:22Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's\n+ * {@link BatchWrite#commit(WriterCommitMessage[])} is called,\n+ * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can\n+ * complete both the data write and the metadata swap operation atomically.\n+ */\n+public interface StagingTableCatalog extends TableCatalog {",
    "line": 53
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Also thinking about this a bit more, perhaps the reason why `REPLACE TABLE` isn't supported in other database systems as standard is because all those systems support transactional operations. So users typically open a transaction, run the table creation, and then commit the operation.\r\n\r\nSince we don't have support for specifically opening and closing transactions, we have to support this use case via an atomic `REPLACE` keyword.",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-07-11T20:09:51Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's\n+ * {@link BatchWrite#commit(WriterCommitMessage[])} is called,\n+ * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can\n+ * complete both the data write and the metadata swap operation atomically.\n+ */\n+public interface StagingTableCatalog extends TableCatalog {",
    "line": 53
  }],
  "prId": 24798
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "spurious `;`?",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-07-11T21:30:56Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's\n+ * {@link BatchWrite#commit(WriterCommitMessage[])} is called,\n+ * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can\n+ * complete both the data write and the metadata swap operation atomically.\n+ */\n+public interface StagingTableCatalog extends TableCatalog {\n+\n+  /**\n+   * Stage the creation of a table, preparing it to be committed into the metastore.\n+   * <p>\n+   * When the table is committed, the contents of any writes performed by the Spark planner are\n+   * committed along with the metadata about the table passed into this method's arguments. If the\n+   * table exists when this method is called, the method should throw an exception accordingly. If\n+   * another process concurrently creates the table before this table's staged changes are\n+   * committed, an exception should be thrown by {@link StagedTable#commitStagedChanges()}.\n+   *\n+   * @param ident a table identifier\n+   * @param schema the schema of the new table, as a struct type\n+   * @param partitions transforms to use for partitioning data in the table\n+   * @param properties a string map of table properties\n+   * @return metadata for the new table\n+   * @throws TableAlreadyExistsException If a table or view already exists for the identifier\n+   * @throws UnsupportedOperationException If a requested partition transform is not supported\n+   * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)\n+   */\n+  StagedTable stageCreate(\n+      Identifier ident,\n+      StructType schema,\n+      Transform[] partitions,\n+      Map<String, String> properties) throws TableAlreadyExistsException, NoSuchNamespaceException;\n+  ;"
  }],
  "prId": 24798
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I'm a little confused here, replace table does require the table exists, right?",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-07-17T12:40:54Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's\n+ * {@link BatchWrite#commit(WriterCommitMessage[])} is called,\n+ * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can\n+ * complete both the data write and the metadata swap operation atomically.\n+ */\n+public interface StagingTableCatalog extends TableCatalog {\n+\n+  /**\n+   * Stage the creation of a table, preparing it to be committed into the metastore.\n+   * <p>\n+   * When the table is committed, the contents of any writes performed by the Spark planner are\n+   * committed along with the metadata about the table passed into this method's arguments. If the\n+   * table exists when this method is called, the method should throw an exception accordingly. If\n+   * another process concurrently creates the table before this table's staged changes are\n+   * committed, an exception should be thrown by {@link StagedTable#commitStagedChanges()}.\n+   *\n+   * @param ident a table identifier\n+   * @param schema the schema of the new table, as a struct type\n+   * @param partitions transforms to use for partitioning data in the table\n+   * @param properties a string map of table properties\n+   * @return metadata for the new table\n+   * @throws TableAlreadyExistsException If a table or view already exists for the identifier\n+   * @throws UnsupportedOperationException If a requested partition transform is not supported\n+   * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)\n+   */\n+  StagedTable stageCreate(\n+      Identifier ident,\n+      StructType schema,\n+      Transform[] partitions,\n+      Map<String, String> properties) throws TableAlreadyExistsException, NoSuchNamespaceException;\n+\n+  /**\n+   * Stage the replacement of a table, preparing it to be committed into the metastore when the\n+   * returned table's {@link StagedTable#commitStagedChanges()} is called.\n+   * <p>\n+   * When the table is committed, the contents of any writes performed by the Spark planner are\n+   * committed along with the metadata about the table passed into this method's arguments. If the\n+   * table exists, the metadata and the contents of this table replace the metadata and contents of\n+   * the existing table. If the table does not exist, it should be created in the metastore. If a\n+   * concurrent process commits changes to the table's data or metadata in the metastore while the\n+   * write is being performed but before the staged changes are committed, the catalog can decide\n+   * whether to move forward with the table replacement anyways or abort the commit operation.\n+   *\n+   * @param ident a table identifier\n+   * @param schema the schema of the new table, as a struct type\n+   * @param partitions transforms to use for partitioning data in the table\n+   * @param properties a string map of table properties\n+   * @return metadata for the new table\n+   * @throws TableAlreadyExistsException If a table or view already exists for the identifier"
  }],
  "prId": 24798
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think the implementation should throw `TableNotFoundException` if the table to replace doesn't exist.",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-07-19T06:14:30Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2;\n+\n+import java.util.Map;\n+\n+import org.apache.spark.sql.catalog.v2.expressions.Transform;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.sources.v2.StagedTable;\n+import org.apache.spark.sql.sources.v2.SupportsWrite;\n+import org.apache.spark.sql.sources.v2.writer.BatchWrite;\n+import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n+ * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n+ * SELECT or REPLACE TABLE AS SELECT operations.\n+ * <p>\n+ * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n+ * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n+ * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n+ * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n+ * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n+ * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the\n+ * write operation fails, the catalog will have already dropped the table, and the planner cannot\n+ * roll back the dropping of the table.\n+ * <p>\n+ * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n+ * creation and the replacement of a table. After the table's\n+ * {@link BatchWrite#commit(WriterCommitMessage[])} is called,\n+ * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can\n+ * complete both the data write and the metadata swap operation atomically.\n+ */\n+public interface StagingTableCatalog extends TableCatalog {\n+\n+  /**\n+   * Stage the creation of a table, preparing it to be committed into the metastore.\n+   * <p>\n+   * When the table is committed, the contents of any writes performed by the Spark planner are\n+   * committed along with the metadata about the table passed into this method's arguments. If the\n+   * table exists when this method is called, the method should throw an exception accordingly. If\n+   * another process concurrently creates the table before this table's staged changes are\n+   * committed, an exception should be thrown by {@link StagedTable#commitStagedChanges()}.\n+   *\n+   * @param ident a table identifier\n+   * @param schema the schema of the new table, as a struct type\n+   * @param partitions transforms to use for partitioning data in the table\n+   * @param properties a string map of table properties\n+   * @return metadata for the new table\n+   * @throws TableAlreadyExistsException If a table or view already exists for the identifier\n+   * @throws UnsupportedOperationException If a requested partition transform is not supported\n+   * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)\n+   */\n+  StagedTable stageCreate(\n+      Identifier ident,\n+      StructType schema,\n+      Transform[] partitions,\n+      Map<String, String> properties) throws TableAlreadyExistsException, NoSuchNamespaceException;\n+\n+  /**\n+   * Stage the replacement of a table, preparing it to be committed into the metastore when the\n+   * returned table's {@link StagedTable#commitStagedChanges()} is called.\n+   * <p>\n+   * When the table is committed, the contents of any writes performed by the Spark planner are\n+   * committed along with the metadata about the table passed into this method's arguments. If the\n+   * table exists, the metadata and the contents of this table replace the metadata and contents of\n+   * the existing table. If a concurrent process commits changes to the table's data or metadata\n+   * while the write is being performed but before the staged changes are committed, the catalog\n+   * can decide whether to move forward with the table replacement anyways or abort the commit\n+   * operation.\n+   * <p>\n+   * If the table does not exist, committing the staged changes should fail. This differs from the\n+   * semantics of {@link #stageCreateOrReplace(Identifier, StructType, Transform[], Map)}, which\n+   * should create the table in the data source if the table does not exist at the time of\n+   * committing the operation.\n+   *\n+   * @param ident a table identifier\n+   * @param schema the schema of the new table, as a struct type\n+   * @param partitions transforms to use for partitioning data in the table\n+   * @param properties a string map of table properties\n+   * @return metadata for the new table\n+   * @throws UnsupportedOperationException If a requested partition transform is not supported\n+   * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)",
    "line": 103
  }],
  "prId": 24798
}]