[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Hi @jose-torres , can you double-check if my explanation is correct?",
    "commit": "a7d0c55677d519843086f54ef9502e5482ea2765",
    "createdAt": "2019-08-06T12:27:34Z",
    "diffHunk": "@@ -21,13 +21,27 @@\n import org.apache.spark.sql.sources.v2.reader.InputPartition;\n import org.apache.spark.sql.sources.v2.reader.PartitionReader;\n import org.apache.spark.sql.sources.v2.reader.PartitionReaderFactory;\n-import org.apache.spark.sql.sources.v2.reader.Scan;\n \n /**\n- * A {@link SparkDataStream} for streaming queries with micro-batch mode.\n+ * An interface that defines how to scan the data from data source for micro-batch streaming\n+ * processing.\n+ *\n+ * The scanning procedure is:",
    "line": 11
  }],
  "prId": 25180
}]