[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "when this can return `None`?",
    "commit": "1792bb6dee912408ce3943e2f942e6cde34714ed",
    "createdAt": "2019-06-22T11:03:23Z",
    "diffHunk": "@@ -115,228 +161,108 @@ private class OrcFilterConverter(val dataTypeMap: Map[String, DataType]) {\n     case _ => value\n   }\n \n-  import org.apache.spark.sql.sources._\n-  import OrcFilters._\n-\n   /**\n-   * Builds a SearchArgument for a Filter by first trimming the non-convertible nodes, and then\n-   * only building the remaining convertible nodes.\n-   *\n-   * Doing the conversion in this way avoids the computational complexity problems introduced by\n-   * checking whether a node is convertible while building it. The approach implemented here has\n-   * complexity that's linear in the size of the Filter tree - O(number of Filter nodes) - we run\n-   * a single pass over the tree to trim it, and then another pass on the trimmed tree to convert\n-   * the remaining nodes.\n+   * Build a SearchArgument and return the builder so far.\n    *\n-   * The alternative approach of checking-while-building can (and did) result\n-   * in exponential complexity in the height of the tree, causing perf problems with Filters with\n-   * as few as ~35 nodes if they were skewed.\n+   * @param dataTypeMap a map from the attribute name to its data type.\n+   * @param expression the input filter predicates.\n+   * @param builder the input SearchArgument.Builder.\n+   * @return the builder so far.\n    */\n-  private[sql] def buildSearchArgument(\n+  private def buildSearchArgument(\n+      dataTypeMap: Map[String, DataType],\n       expression: Filter,\n       builder: Builder): Option[Builder] = {"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "When the input `expression ` is totally non-convertible, E.g a leaf predicate \"StringContains\"",
    "commit": "1792bb6dee912408ce3943e2f942e6cde34714ed",
    "createdAt": "2019-06-22T14:18:58Z",
    "diffHunk": "@@ -115,228 +161,108 @@ private class OrcFilterConverter(val dataTypeMap: Map[String, DataType]) {\n     case _ => value\n   }\n \n-  import org.apache.spark.sql.sources._\n-  import OrcFilters._\n-\n   /**\n-   * Builds a SearchArgument for a Filter by first trimming the non-convertible nodes, and then\n-   * only building the remaining convertible nodes.\n-   *\n-   * Doing the conversion in this way avoids the computational complexity problems introduced by\n-   * checking whether a node is convertible while building it. The approach implemented here has\n-   * complexity that's linear in the size of the Filter tree - O(number of Filter nodes) - we run\n-   * a single pass over the tree to trim it, and then another pass on the trimmed tree to convert\n-   * the remaining nodes.\n+   * Build a SearchArgument and return the builder so far.\n    *\n-   * The alternative approach of checking-while-building can (and did) result\n-   * in exponential complexity in the height of the tree, causing perf problems with Filters with\n-   * as few as ~35 nodes if they were skewed.\n+   * @param dataTypeMap a map from the attribute name to its data type.\n+   * @param expression the input filter predicates.\n+   * @param builder the input SearchArgument.Builder.\n+   * @return the builder so far.\n    */\n-  private[sql] def buildSearchArgument(\n+  private def buildSearchArgument(\n+      dataTypeMap: Map[String, DataType],\n       expression: Filter,\n       builder: Builder): Option[Builder] = {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "isn't it guaranteed that we only call `buildSearchArgument` with convertible filters?",
    "commit": "1792bb6dee912408ce3943e2f942e6cde34714ed",
    "createdAt": "2019-06-23T02:00:07Z",
    "diffHunk": "@@ -115,228 +161,108 @@ private class OrcFilterConverter(val dataTypeMap: Map[String, DataType]) {\n     case _ => value\n   }\n \n-  import org.apache.spark.sql.sources._\n-  import OrcFilters._\n-\n   /**\n-   * Builds a SearchArgument for a Filter by first trimming the non-convertible nodes, and then\n-   * only building the remaining convertible nodes.\n-   *\n-   * Doing the conversion in this way avoids the computational complexity problems introduced by\n-   * checking whether a node is convertible while building it. The approach implemented here has\n-   * complexity that's linear in the size of the Filter tree - O(number of Filter nodes) - we run\n-   * a single pass over the tree to trim it, and then another pass on the trimmed tree to convert\n-   * the remaining nodes.\n+   * Build a SearchArgument and return the builder so far.\n    *\n-   * The alternative approach of checking-while-building can (and did) result\n-   * in exponential complexity in the height of the tree, causing perf problems with Filters with\n-   * as few as ~35 nodes if they were skewed.\n+   * @param dataTypeMap a map from the attribute name to its data type.\n+   * @param expression the input filter predicates.\n+   * @param builder the input SearchArgument.Builder.\n+   * @return the builder so far.\n    */\n-  private[sql] def buildSearchArgument(\n+  private def buildSearchArgument(\n+      dataTypeMap: Map[String, DataType],\n       expression: Filter,\n       builder: Builder): Option[Builder] = {"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "You're right. I have updated the code.",
    "commit": "1792bb6dee912408ce3943e2f942e6cde34714ed",
    "createdAt": "2019-06-23T06:30:01Z",
    "diffHunk": "@@ -115,228 +161,108 @@ private class OrcFilterConverter(val dataTypeMap: Map[String, DataType]) {\n     case _ => value\n   }\n \n-  import org.apache.spark.sql.sources._\n-  import OrcFilters._\n-\n   /**\n-   * Builds a SearchArgument for a Filter by first trimming the non-convertible nodes, and then\n-   * only building the remaining convertible nodes.\n-   *\n-   * Doing the conversion in this way avoids the computational complexity problems introduced by\n-   * checking whether a node is convertible while building it. The approach implemented here has\n-   * complexity that's linear in the size of the Filter tree - O(number of Filter nodes) - we run\n-   * a single pass over the tree to trim it, and then another pass on the trimmed tree to convert\n-   * the remaining nodes.\n+   * Build a SearchArgument and return the builder so far.\n    *\n-   * The alternative approach of checking-while-building can (and did) result\n-   * in exponential complexity in the height of the tree, causing perf problems with Filters with\n-   * as few as ~35 nodes if they were skewed.\n+   * @param dataTypeMap a map from the attribute name to its data type.\n+   * @param expression the input filter predicates.\n+   * @param builder the input SearchArgument.Builder.\n+   * @return the builder so far.\n    */\n-  private[sql] def buildSearchArgument(\n+  private def buildSearchArgument(\n+      dataTypeMap: Map[String, DataType],\n       expression: Filter,\n       builder: Builder): Option[Builder] = {"
  }],
  "prId": 24910
}]