[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `Vectorized ORC Row Batch.`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:41:06Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:44:09Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: For this kind of simple one line comment, use `// xxx`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:41:46Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:41:39Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `... from ORC row batch`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:42:15Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:44:21Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "The column IDs of the physical ORC file schema which are required by this reader. `-1` means this required column doesn't exist in the ORC file.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:42:57Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Updated.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:44:15Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This provides the same information as the variable name, we don't need this comment.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:43:11Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep. It's removed.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:44:44Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`The result columnar batch for ...`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:43:33Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Done.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:45:51Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`... of the result columnar batch`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:43:57Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Done.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:46:17Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This provides the same information as the variable name, we don't need this comment.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:44:07Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep. It's removed.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:46:38Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows."
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "4 space identation.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:44:52Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:48:46Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: always write\r\n```\r\nfor (...) {\r\n  ...\r\n}\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:45:16Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thanks!",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:49:51Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`== -1` is more precise",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:49:09Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Correct.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:51:19Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how would this happen?",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T02:50:10Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "It happens when calling on the empty file or usually at the end of the file.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:52:12Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "For empty file or end of the file, we would hit the previous condition `rowsRead == totalRows`, isn't it?",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T06:28:27Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "~Initially, both are 0.~ I was confused with other. Let me check again.\r\n  ",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T07:09:17Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "initially we set up `totoalRowCount` in `initiliaze`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:02:19Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@cloud-fan . This happens when PPD.\r\nThe total number of row shows all rows here. When PPD is applied, the next batch can be empty on the fly. ORC doesn't fill the unqualified rows into row batches.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:22:08Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "in java we can use for loop\r\n```\r\nfor (int i = 0; i < requiresIds.length; i++)\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T03:28:45Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Oops. I'll rewrite the others, too.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:53:45Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "add an assert that `requiredSchema.length == requesredColIds.length`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T03:29:28Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:50:57Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We don't need this, it's already handled in `setRequiredSchema`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T03:30:46Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Correct. It's removed.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T06:00:34Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about we follow parquet and call it `initBatch`?",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T03:31:39Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema("
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Good idea. It's removed.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:48:34Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema("
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this can be `requiredFields: Array[StructField]`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T03:34:08Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "use for loop in java. please update all other places.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T03:35:51Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep. It's done.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T06:01:00Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "don't do this, pass a `memoryMode` parameter in `initBatch`, like parquet",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T03:37:11Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);\n+              index += 1;\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendByte((byte)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendShort((short)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendInt((int)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendLong(fromTimestampColumnVector(data, index));\n+              index += 1;\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendFloat((float)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendByteArray(data.vector[index], data.start[index], data.length[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                data.vector[index]);\n+              index += 1;\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        } else {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendBoolean(vector[index] == 1);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendByte((byte)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendShort((short)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendInt((int)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof LongType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendLong(vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendLong(fromTimestampColumnVector(vector, index));\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendFloat((float)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof DoubleType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendDouble(vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendByteArray(\n+                  vector.vector[index], vector.start[index], vector.length[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            HiveDecimalWritable[] vector = ((DecimalColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                appendDecimalWritable(\n+                  toColumn,\n+                  decimalType.precision(),\n+                  decimalType.scale(),\n+                  vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        }\n+      }\n+      i += 1;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Default memory mode for ColumnarBatch.\n+   */\n+  public static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.ON_HEAP;"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Then, we need the following, too.\r\n```scala\r\n        val vectorizedReader = new VectorizedParquetRecordReader(\r\n          convertTz.orNull, enableOffHeapColumnVector && taskContext.isDefined)\r\n```\r\n\r\n```scala\r\n  /**\r\n   * The memory mode of the columnarBatch\r\n   */\r\n  private final MemoryMode MEMORY_MODE;\r\n\r\n  public VectorizedParquetRecordReader(TimeZone convertTz, boolean useOffHeap) {\r\n    this.convertTz = convertTz;\r\n    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\r\n  }\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T06:04:59Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);\n+              index += 1;\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendByte((byte)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendShort((short)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendInt((int)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendLong(fromTimestampColumnVector(data, index));\n+              index += 1;\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendFloat((float)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendByteArray(data.vector[index], data.start[index], data.length[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                data.vector[index]);\n+              index += 1;\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        } else {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendBoolean(vector[index] == 1);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendByte((byte)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendShort((short)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendInt((int)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof LongType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendLong(vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendLong(fromTimestampColumnVector(vector, index));\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendFloat((float)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof DoubleType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendDouble(vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendByteArray(\n+                  vector.vector[index], vector.start[index], vector.length[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            HiveDecimalWritable[] vector = ((DecimalColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                appendDecimalWritable(\n+                  toColumn,\n+                  decimalType.precision(),\n+                  decimalType.scale(),\n+                  vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        }\n+      }\n+      i += 1;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Default memory mode for ColumnarBatch.\n+   */\n+  public static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.ON_HEAP;"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "I added like the above and didn't add `memoryMode` parameter at `initBatch`.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T06:10:16Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);\n+              index += 1;\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendByte((byte)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendShort((short)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendInt((int)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendLong(fromTimestampColumnVector(data, index));\n+              index += 1;\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendFloat((float)data[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendByteArray(data.vector[index], data.start[index], data.length[index]);\n+              index += 1;\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                data.vector[index]);\n+              index += 1;\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        } else {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendBoolean(vector[index] == 1);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendByte((byte)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendShort((short)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendInt((int)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof LongType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendLong(vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendLong(fromTimestampColumnVector(vector, index));\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendFloat((float)vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof DoubleType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendDouble(vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                toColumn.appendByteArray(\n+                  vector.vector[index], vector.start[index], vector.length[index]);\n+              }\n+              index += 1;\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            HiveDecimalWritable[] vector = ((DecimalColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.appendNull();\n+              } else {\n+                appendDecimalWritable(\n+                  toColumn,\n+                  decimalType.precision(),\n+                  decimalType.scale(),\n+                  vector[index]);\n+              }\n+              index += 1;\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        }\n+      }\n+      i += 1;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Default memory mode for ColumnarBatch.\n+   */\n+  public static final MemoryMode DEFAULT_MEMORY_MODE = MemoryMode.ON_HEAP;"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "henrify"
    },
    "body": "The nextBatch() method is now so long/complex that JVM may have difficulties optimizing it fully. I'm not sure if this will have real-world performance impact, but splitting it to per-type methods would not hurt readability either.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T04:52:30Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "yea, at least we should have 3 methods for repeated data, non-nullable data and nullable data.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:14:00Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "It's updated like the following now.\r\n```scala\r\nif (fromColumn.isRepeating) {\r\n  putRepeatingValues(batchSize, field, fromColumn, toColumn);\r\n} else if (fromColumn.noNulls) {\r\n  putNonNullValues(batchSize, field, fromColumn, toColumn);\r\n} else {\r\n  putValues(batchSize, field, fromColumn, toColumn);\r\n}\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T09:13:09Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "henrify"
    },
    "body": "Actually, you know the number of rows in advance. Wouldn't it possible to call reserve() once, and then use the putX() API instead of appendX() API inside the loops? That should be significantly faster.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T05:59:14Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "good catch! Since we allocated a big enough batch in `initBatch`, I think here we should call the `putXXX` API.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T06:31:10Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);"
  }, {
    "author": {
      "login": "henrify"
    },
    "body": "I checked this bit more, and it seems that the code path after putX() is bimorphic and 100% biased. If you split each for loop to it's own method and use putX(),  then the entire contents of the for loops will be basically reduced to a single call of Unsafe.putX().",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T06:56:17Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thank you, @henrify . I'll update like that.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T07:05:43Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);"
  }, {
    "author": {
      "login": "henrify"
    },
    "body": "Thinking this further, the ColumnVector API supports direct memory copy. For long, double, binary and string you might be able to scrap the for loops entirely, and copy ORC's memory directly to Spark's memory, with just 1 call per batch, which would be way faster than looping. Check e.g. putDoubles(int rowId, int count, double[] src, int srcIndex).",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T07:23:09Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "For long and double, we don't have loop already.\r\nFor `binary` and `string`, is it possible?",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T07:48:41Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);"
  }, {
    "author": {
      "login": "henrify"
    },
    "body": "Ah that's right. I read the code for the nullable variant, which uses (and needs to use) loop.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:08:28Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `setRequiredSchema` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  /**\n+   * ORC File Reader.\n+   */\n+  private Reader reader;\n+\n+  /**\n+   * Vectorized Row Batch.\n+   */\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * Requested Column IDs.\n+   */\n+  private int[] requestedColIds;\n+\n+  /**\n+   * Record reader from row batch.\n+   */\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  /**\n+   * Required Schema.\n+   */\n+  private StructType requiredSchema;\n+\n+  /**\n+   * ColumnarBatch for vectorized execution by whole-stage codegen.\n+   */\n+  private ColumnarBatch columnarBatch;\n+\n+  /**\n+   * Writable column vectors of ColumnarBatch.\n+   */\n+  private WritableColumnVector[] columnVectors;\n+\n+  /**\n+   * The number of rows read and considered to be returned.\n+   */\n+  private long rowsReturned = 0L;\n+\n+  /**\n+   * Total number of rows.\n+   */\n+  private long totalRowCount = 0L;\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `setRequiredSchema` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+            OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Set required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void setRequiredSchema(\n+    TypeDescription orcSchema,\n+    int[] requestedColIds,\n+    StructType requiredSchema,\n+    StructType partitionSchema,\n+    InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields())\n+      resultSchema = resultSchema.add(f);\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (DEFAULT_MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] < 0) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    int i = 0;\n+    while (i < requiredSchema.length()) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] < 0) {\n+        toColumn.appendNulls(batchSize);\n+      } else {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.appendNulls(batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.appendBooleans(batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.appendBytes(batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.appendShorts(batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.appendInts(batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.appendLongs(batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.appendLongs(batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.appendFloats(batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.appendDoubles(batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              int index = 0;\n+              while (index < batchSize) {\n+                toColumn.appendByteArray(data.vector[0], data.start[0], data.length[0]);\n+                index += 1;\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              appendDecimalWritable(\n+                toColumn,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            int index = 0;\n+            while (index < batchSize) {\n+              toColumn.appendBoolean(data[index] == 1);"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this could be `StructField[] requiredFields`",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T07:58:35Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Oh, sorry. I missed this. Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:01:48Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit:\r\n```\r\nthis.requiredFields = requiredSchema.fields();\r\nthis.requestedColIds = requestedColIds;\r\nassert(requiredSchema.length() == requestedColIds.length);\r\n\r\nStructType resultSchema = new StructType(this.requiredFields);\r\n...\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:01:08Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:23:26Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "do we need this? In `initBatch` we allocated a big enough batch, and `batchSize` here should always be smaller than the batch capacity.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:03:54Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think we only need it for string/binary type.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:11:03Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yes. It's removed.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T11:07:13Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "2 space indentation",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:08:51Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              putDecimalWritable(\n+                toColumn,\n+                index,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                data.vector[index]);\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        } else {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putBoolean(index, vector[index] == 1);\n+              }\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByte(index, (byte)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putShort(index, (short)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putInt(index, (int)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof LongType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, fromTimestampColumnVector(vector, index));\n+              }\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putFloat(index, (float)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof DoubleType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putDouble(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByteArray(\n+                  index, vector.vector[index], vector.start[index], vector.length[index]);\n+              }\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            HiveDecimalWritable[] vector = ((DecimalColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                putDecimalWritable(\n+                  toColumn,\n+                  index,\n+                  decimalType.precision(),\n+                  decimalType.scale(),\n+                  vector[index]);\n+              }\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * The default size of batch. We use this value for both ORC and Spark consistently\n+   * because they have different default values like the following.\n+   *\n+   * - ORC's VectorizedRowBatch.DEFAULT_SIZE = 1024\n+   * - Spark's ColumnarBatch.DEFAULT_BATCH_SIZE = 4 * 1024\n+   */\n+  public static final int DEFAULT_SIZE = 4 * 1024;\n+\n+  /**\n+   * Returns the number of micros since epoch from an element of TimestampColumnVector.\n+   */\n+  private static long fromTimestampColumnVector(TimestampColumnVector vector, int index) {\n+    return vector.time[index] * 1000L + vector.nanos[index] / 1000L;\n+  }\n+\n+  /**\n+   * Put a `HiveDecimalWritable` to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritable(\n+      WritableColumnVector toColumn,\n+      int index,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+      Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());\n+    value.changePrecision(precision, scale);\n+\n+    if (precision <= Decimal.MAX_INT_DIGITS()) {\n+      toColumn.putInt(index, (int) value.toUnscaledLong());\n+    } else if (precision <= Decimal.MAX_LONG_DIGITS()) {\n+      toColumn.putLong(index, value.toUnscaledLong());\n+    } else {\n+      byte[] bytes = value.toJavaBigDecimal().unscaledValue().toByteArray();\n+      toColumn.putByteArray(index, bytes, 0, bytes.length);\n+    }\n+  }\n+\n+  /**\n+   * Put `HiveDecimalWritable`s to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritables(\n+      WritableColumnVector toColumn,\n+      int size,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+            Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yep.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:31:39Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              putDecimalWritable(\n+                toColumn,\n+                index,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                data.vector[index]);\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        } else {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putBoolean(index, vector[index] == 1);\n+              }\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByte(index, (byte)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putShort(index, (short)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putInt(index, (int)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof LongType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, fromTimestampColumnVector(vector, index));\n+              }\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putFloat(index, (float)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof DoubleType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putDouble(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByteArray(\n+                  index, vector.vector[index], vector.start[index], vector.length[index]);\n+              }\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            HiveDecimalWritable[] vector = ((DecimalColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                putDecimalWritable(\n+                  toColumn,\n+                  index,\n+                  decimalType.precision(),\n+                  decimalType.scale(),\n+                  vector[index]);\n+              }\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * The default size of batch. We use this value for both ORC and Spark consistently\n+   * because they have different default values like the following.\n+   *\n+   * - ORC's VectorizedRowBatch.DEFAULT_SIZE = 1024\n+   * - Spark's ColumnarBatch.DEFAULT_BATCH_SIZE = 4 * 1024\n+   */\n+  public static final int DEFAULT_SIZE = 4 * 1024;\n+\n+  /**\n+   * Returns the number of micros since epoch from an element of TimestampColumnVector.\n+   */\n+  private static long fromTimestampColumnVector(TimestampColumnVector vector, int index) {\n+    return vector.time[index] * 1000L + vector.nanos[index] / 1000L;\n+  }\n+\n+  /**\n+   * Put a `HiveDecimalWritable` to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritable(\n+      WritableColumnVector toColumn,\n+      int index,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+      Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());\n+    value.changePrecision(precision, scale);\n+\n+    if (precision <= Decimal.MAX_INT_DIGITS()) {\n+      toColumn.putInt(index, (int) value.toUnscaledLong());\n+    } else if (precision <= Decimal.MAX_LONG_DIGITS()) {\n+      toColumn.putLong(index, value.toUnscaledLong());\n+    } else {\n+      byte[] bytes = value.toJavaBigDecimal().unscaledValue().toByteArray();\n+      toColumn.putByteArray(index, bytes, 0, bytes.length);\n+    }\n+  }\n+\n+  /**\n+   * Put `HiveDecimalWritable`s to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritables(\n+      WritableColumnVector toColumn,\n+      int size,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+            Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "There is a faster way:\r\n```\r\ntoColumn.arrayDate().putBytes(data.vector[0])\r\nfor (int index = 0; index < batchSize; index++) {\r\n  toColumn.putArray(index, data.start[0], data.length[0])\r\n}\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:17:01Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thanks. I updated like the following.\r\n```\r\ntoColumn.putByteArray(0, data.vector[0]);\r\nfor (int index = 0; index < batchSize; index++) {\r\n  toColumn.putArray(index, data.start[0], data.length[0]);\r\n}\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:56:06Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We missed string/binary here.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:17:42Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+            }\n+          } else if (type instanceof DecimalType) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Did I miss something in [Line 290](https://github.com/apache/spark/pull/19943/files/3a0702ae0b31f762c9f3da06d267a02ec8d1a23b#diff-d0cab20acc0e0c3b27ec27b479e0f912R290)?",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:31:23Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+            }\n+          } else if (type instanceof DecimalType) {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "oh I missed it...",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:36:54Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+            }\n+          } else if (type instanceof DecimalType) {"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can apply https://github.com/apache/spark/pull/19943/files/3a0702ae0b31f762c9f3da06d267a02ec8d1a23b#r160088927 to here too",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:38:02Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              putDecimalWritable(\n+                toColumn,\n+                index,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                data.vector[index]);\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        } else {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putBoolean(index, vector[index] == 1);\n+              }\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByte(index, (byte)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putShort(index, (short)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putInt(index, (int)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof LongType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, fromTimestampColumnVector(vector, index));\n+              }\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putFloat(index, (float)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof DoubleType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putDouble(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByteArray(\n+                  index, vector.vector[index], vector.start[index], vector.length[index]);\n+              }\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            HiveDecimalWritable[] vector = ((DecimalColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                putDecimalWritable(\n+                  toColumn,\n+                  index,\n+                  decimalType.precision(),\n+                  decimalType.scale(),\n+                  vector[index]);\n+              }\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * The default size of batch. We use this value for both ORC and Spark consistently\n+   * because they have different default values like the following.\n+   *\n+   * - ORC's VectorizedRowBatch.DEFAULT_SIZE = 1024\n+   * - Spark's ColumnarBatch.DEFAULT_BATCH_SIZE = 4 * 1024\n+   */\n+  public static final int DEFAULT_SIZE = 4 * 1024;\n+\n+  /**\n+   * Returns the number of micros since epoch from an element of TimestampColumnVector.\n+   */\n+  private static long fromTimestampColumnVector(TimestampColumnVector vector, int index) {\n+    return vector.time[index] * 1000L + vector.nanos[index] / 1000L;\n+  }\n+\n+  /**\n+   * Put a `HiveDecimalWritable` to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritable(\n+      WritableColumnVector toColumn,\n+      int index,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+      Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());\n+    value.changePrecision(precision, scale);\n+\n+    if (precision <= Decimal.MAX_INT_DIGITS()) {\n+      toColumn.putInt(index, (int) value.toUnscaledLong());\n+    } else if (precision <= Decimal.MAX_LONG_DIGITS()) {\n+      toColumn.putLong(index, value.toUnscaledLong());\n+    } else {\n+      byte[] bytes = value.toJavaBigDecimal().unscaledValue().toByteArray();\n+      toColumn.putByteArray(index, bytes, 0, bytes.length);\n+    }\n+  }\n+\n+  /**\n+   * Put `HiveDecimalWritable`s to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritables(\n+      WritableColumnVector toColumn,\n+      int size,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+            Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());\n+    value.changePrecision(precision, scale);\n+\n+    if (precision <= Decimal.MAX_INT_DIGITS()) {\n+      toColumn.putInts(0, size, (int) value.toUnscaledLong());\n+    } else if (precision <= Decimal.MAX_LONG_DIGITS()) {\n+      toColumn.putLongs(0, size, value.toUnscaledLong());\n+    } else {\n+      byte[] bytes = value.toJavaBigDecimal().unscaledValue().toByteArray();\n+      for (int index = 0; index < size; index++) {\n+        toColumn.putByteArray(index, bytes, 0, bytes.length);"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Sure!",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T09:00:33Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              putDecimalWritable(\n+                toColumn,\n+                index,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                data.vector[index]);\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        } else {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putBoolean(index, vector[index] == 1);\n+              }\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByte(index, (byte)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putShort(index, (short)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putInt(index, (int)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof LongType) {\n+            long[] vector = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putLong(index, fromTimestampColumnVector(vector, index));\n+              }\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putFloat(index, (float)vector[index]);\n+              }\n+            }\n+          } else if (type instanceof DoubleType) {\n+            double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putDouble(index, vector[index]);\n+              }\n+            }\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                toColumn.putByteArray(\n+                  index, vector.vector[index], vector.start[index], vector.length[index]);\n+              }\n+            }\n+          } else if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)type;\n+            HiveDecimalWritable[] vector = ((DecimalColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              if (fromColumn.isNull[index]) {\n+                toColumn.putNull(index);\n+              } else {\n+                putDecimalWritable(\n+                  toColumn,\n+                  index,\n+                  decimalType.precision(),\n+                  decimalType.scale(),\n+                  vector[index]);\n+              }\n+            }\n+          } else {\n+            throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+          }\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * The default size of batch. We use this value for both ORC and Spark consistently\n+   * because they have different default values like the following.\n+   *\n+   * - ORC's VectorizedRowBatch.DEFAULT_SIZE = 1024\n+   * - Spark's ColumnarBatch.DEFAULT_BATCH_SIZE = 4 * 1024\n+   */\n+  public static final int DEFAULT_SIZE = 4 * 1024;\n+\n+  /**\n+   * Returns the number of micros since epoch from an element of TimestampColumnVector.\n+   */\n+  private static long fromTimestampColumnVector(TimestampColumnVector vector, int index) {\n+    return vector.time[index] * 1000L + vector.nanos[index] / 1000L;\n+  }\n+\n+  /**\n+   * Put a `HiveDecimalWritable` to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritable(\n+      WritableColumnVector toColumn,\n+      int index,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+      Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());\n+    value.changePrecision(precision, scale);\n+\n+    if (precision <= Decimal.MAX_INT_DIGITS()) {\n+      toColumn.putInt(index, (int) value.toUnscaledLong());\n+    } else if (precision <= Decimal.MAX_LONG_DIGITS()) {\n+      toColumn.putLong(index, value.toUnscaledLong());\n+    } else {\n+      byte[] bytes = value.toJavaBigDecimal().unscaledValue().toByteArray();\n+      toColumn.putByteArray(index, bytes, 0, bytes.length);\n+    }\n+  }\n+\n+  /**\n+   * Put `HiveDecimalWritable`s to a `WritableColumnVector`.\n+   */\n+  private static void putDecimalWritables(\n+      WritableColumnVector toColumn,\n+      int size,\n+      int precision,\n+      int scale,\n+      HiveDecimalWritable decimalWritable) {\n+    HiveDecimal decimal = decimalWritable.getHiveDecimal();\n+    Decimal value =\n+            Decimal.apply(decimal.bigDecimalValue(), decimal.precision(), decimal.scale());\n+    value.changePrecision(precision, scale);\n+\n+    if (precision <= Decimal.MAX_INT_DIGITS()) {\n+      toColumn.putInts(0, size, (int) value.toUnscaledLong());\n+    } else if (precision <= Decimal.MAX_LONG_DIGITS()) {\n+      toColumn.putLongs(0, size, value.toUnscaledLong());\n+    } else {\n+      byte[] bytes = value.toJavaBigDecimal().unscaledValue().toByteArray();\n+      for (int index = 0; index < size; index++) {\n+        toColumn.putByteArray(index, bytes, 0, bytes.length);"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "not related, but why ORC design the API like this? If the data is `byte[][]`, why we would ever need `start` and `length`?",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T08:46:38Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);"
  }, {
    "author": {
      "login": "henrify"
    },
    "body": "@cloud-fan Heh i just wrote about the same confusion at the exact same time. I checked some of our old code (we have custom ORC writer) and the start and length vectors are actually used on the other side: the writer can use single contiguous block and mark boundaries with those, exactly the same way i thought we could use in the read side.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T09:01:37Z",
    "diffHunk": "@@ -0,0 +1,482 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructType requiredSchema;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructType requiredSchema,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    StructType resultSchema = new StructType(requiredSchema.fields());\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+    this.requiredSchema = requiredSchema;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredSchema.length() == requestedColIds.length);\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredSchema.fields().length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredSchema.length(); i++) {\n+      StructField field = requiredSchema.fields()[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          if (fromColumn.isNull[0]) {\n+            toColumn.putNulls(0, batchSize);\n+          } else {\n+            DataType type = field.dataType();\n+            if (type instanceof BooleanType) {\n+              toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+            } else if (type instanceof ByteType) {\n+              toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof ShortType) {\n+              toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof IntegerType || type instanceof DateType) {\n+              toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof LongType) {\n+              toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof TimestampType) {\n+              toColumn.putLongs(0, batchSize, fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+            } else if (type instanceof FloatType) {\n+              toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof DoubleType) {\n+              toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+            } else if (type instanceof StringType || type instanceof BinaryType) {\n+              BytesColumnVector data = (BytesColumnVector)fromColumn;\n+              for (int index = 0; index < batchSize; index++) {\n+                toColumn.putByteArray(index, data.vector[0], data.start[0], data.length[0]);\n+              }\n+            } else if (type instanceof DecimalType) {\n+              DecimalType decimalType = (DecimalType)type;\n+              putDecimalWritables(\n+                toColumn,\n+                batchSize,\n+                decimalType.precision(),\n+                decimalType.scale(),\n+                ((DecimalColumnVector)fromColumn).vector[0]);\n+            } else {\n+              throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+            }\n+          }\n+        } else if (fromColumn.noNulls) {\n+          DataType type = field.dataType();\n+          if (type instanceof BooleanType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putBoolean(index, data[index] == 1);\n+            }\n+          } else if (type instanceof ByteType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByte(index, (byte)data[index]);\n+            }\n+          } else if (type instanceof ShortType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putShort(index, (short)data[index]);\n+            }\n+          } else if (type instanceof IntegerType || type instanceof DateType) {\n+            long[] data = ((LongColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putInt(index, (int)data[index]);\n+            }\n+          } else if (type instanceof LongType) {\n+            toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof TimestampType) {\n+            TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+            }\n+          } else if (type instanceof FloatType) {\n+            double[] data = ((DoubleColumnVector)fromColumn).vector;\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putFloat(index, (float)data[index]);\n+            }\n+          } else if (type instanceof DoubleType) {\n+            toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+          } else if (type instanceof StringType || type instanceof BinaryType) {\n+            BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+            for (int index = 0; index < batchSize; index++) {\n+              toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "henrify"
    },
    "body": "Based on recent comments by @cloud-fan regarding the byte[][] structure, it would seem that the vector.start[index] is always 0, and you could save little bit of performance here.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T10:18:02Z",
    "diffHunk": "@@ -0,0 +1,510 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructField[] requiredFields;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructField[] requiredFields,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    this.requiredFields = requiredFields;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredFields.length == requestedColIds.length);\n+\n+    StructType resultSchema = new StructType(requiredFields);\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredFields.length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      StructField field = requiredFields[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+        toColumn.reserve(batchSize);\n+\n+        if (fromColumn.isRepeating) {\n+          putRepeatingValues(batchSize, field, fromColumn, toColumn);\n+        } else if (fromColumn.noNulls) {\n+          putNonNullValues(batchSize, field, fromColumn, toColumn);\n+        } else {\n+          putValues(batchSize, field, fromColumn, toColumn);\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  private void putRepeatingValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    if (fromColumn.isNull[0]) {\n+      toColumn.putNulls(0, batchSize);\n+    } else {\n+      DataType type = field.dataType();\n+      if (type instanceof BooleanType) {\n+        toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+      } else if (type instanceof ByteType) {\n+        toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof ShortType) {\n+        toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof IntegerType || type instanceof DateType) {\n+        toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof LongType) {\n+        toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof TimestampType) {\n+        toColumn.putLongs(0, batchSize,\n+          fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+      } else if (type instanceof FloatType) {\n+        toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof DoubleType) {\n+        toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof StringType || type instanceof BinaryType) {\n+        BytesColumnVector data = (BytesColumnVector)fromColumn;\n+        toColumn.putByteArray(0, data.vector[0]);\n+        for (int index = 0; index < batchSize; index++) {\n+          toColumn.putArray(index, data.start[0], data.length[0]);\n+        }\n+      } else if (type instanceof DecimalType) {\n+        DecimalType decimalType = (DecimalType)type;\n+        putDecimalWritables(\n+          toColumn,\n+          batchSize,\n+          decimalType.precision(),\n+          decimalType.scale(),\n+          ((DecimalColumnVector)fromColumn).vector[0]);\n+      } else {\n+        throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+      }\n+    }\n+  }\n+\n+  private void putNonNullValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    DataType type = field.dataType();\n+    if (type instanceof BooleanType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putBoolean(index, data[index] == 1);\n+      }\n+    } else if (type instanceof ByteType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putByte(index, (byte)data[index]);\n+      }\n+    } else if (type instanceof ShortType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putShort(index, (short)data[index]);\n+      }\n+    } else if (type instanceof IntegerType || type instanceof DateType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putInt(index, (int)data[index]);\n+      }\n+    } else if (type instanceof LongType) {\n+      toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof TimestampType) {\n+      TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+      }\n+    } else if (type instanceof FloatType) {\n+      double[] data = ((DoubleColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putFloat(index, (float)data[index]);\n+      }\n+    } else if (type instanceof DoubleType) {\n+      toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof StringType || type instanceof BinaryType) {\n+      BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putByteArray(index, data.vector[index], data.start[index], data.length[index]);\n+      }\n+    } else if (type instanceof DecimalType) {\n+      DecimalType decimalType = (DecimalType)type;\n+      DecimalColumnVector data = ((DecimalColumnVector)fromColumn);\n+      for (int index = 0; index < batchSize; index++) {\n+        putDecimalWritable(\n+          toColumn,\n+          index,\n+          decimalType.precision(),\n+          decimalType.scale(),\n+          data.vector[index]);\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+    }\n+  }\n+\n+  private void putValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    DataType type = field.dataType();\n+    if (type instanceof BooleanType) {\n+      long[] vector = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putBoolean(index, vector[index] == 1);\n+        }\n+      }\n+    } else if (type instanceof ByteType) {\n+      long[] vector = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putByte(index, (byte)vector[index]);\n+        }\n+      }\n+    } else if (type instanceof ShortType) {\n+      long[] vector = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putShort(index, (short)vector[index]);\n+        }\n+      }\n+    } else if (type instanceof IntegerType || type instanceof DateType) {\n+      long[] vector = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putInt(index, (int)vector[index]);\n+        }\n+      }\n+    } else if (type instanceof LongType) {\n+      long[] vector = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putLong(index, vector[index]);\n+        }\n+      }\n+    } else if (type instanceof TimestampType) {\n+      TimestampColumnVector vector = ((TimestampColumnVector)fromColumn);\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putLong(index, fromTimestampColumnVector(vector, index));\n+        }\n+      }\n+    } else if (type instanceof FloatType) {\n+      double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putFloat(index, (float)vector[index]);\n+        }\n+      }\n+    } else if (type instanceof DoubleType) {\n+      double[] vector = ((DoubleColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putDouble(index, vector[index]);\n+        }\n+      }\n+    } else if (type instanceof StringType || type instanceof BinaryType) {\n+      BytesColumnVector vector = (BytesColumnVector)fromColumn;\n+      for (int index = 0; index < batchSize; index++) {\n+        if (fromColumn.isNull[index]) {\n+          toColumn.putNull(index);\n+        } else {\n+          toColumn.putByteArray(\n+            index, vector.vector[index], vector.start[index], vector.length[index]);"
  }],
  "prId": 19943
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "do you mean `data.vector[0]` contains all the bytes of this `BytesColumnVector`?",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T11:37:46Z",
    "diffHunk": "@@ -0,0 +1,510 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructField[] requiredFields;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructField[] requiredFields,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    this.requiredFields = requiredFields;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredFields.length == requestedColIds.length);\n+\n+    StructType resultSchema = new StructType(requiredFields);\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredFields.length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      StructField field = requiredFields[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          putRepeatingValues(batchSize, field, fromColumn, toColumn);\n+        } else if (fromColumn.noNulls) {\n+          putNonNullValues(batchSize, field, fromColumn, toColumn);\n+        } else {\n+          putValues(batchSize, field, fromColumn, toColumn);\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  private void putRepeatingValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    if (fromColumn.isNull[0]) {\n+      toColumn.putNulls(0, batchSize);\n+    } else {\n+      DataType type = field.dataType();\n+      if (type instanceof BooleanType) {\n+        toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+      } else if (type instanceof ByteType) {\n+        toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof ShortType) {\n+        toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof IntegerType || type instanceof DateType) {\n+        toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof LongType) {\n+        toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof TimestampType) {\n+        toColumn.putLongs(0, batchSize,\n+          fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+      } else if (type instanceof FloatType) {\n+        toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof DoubleType) {\n+        toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof StringType || type instanceof BinaryType) {\n+        BytesColumnVector data = (BytesColumnVector)fromColumn;\n+        toColumn.putByteArray(0, data.vector[0]);\n+        for (int index = 0; index < batchSize; index++) {\n+          toColumn.putArray(index, data.start[0], data.length[0]);\n+        }\n+      } else if (type instanceof DecimalType) {\n+        DecimalType decimalType = (DecimalType)type;\n+        putDecimalWritables(\n+          toColumn,\n+          batchSize,\n+          decimalType.precision(),\n+          decimalType.scale(),\n+          ((DecimalColumnVector)fromColumn).vector[0]);\n+      } else {\n+        throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+      }\n+    }\n+  }\n+\n+  private void putNonNullValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    DataType type = field.dataType();\n+    if (type instanceof BooleanType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putBoolean(index, data[index] == 1);\n+      }\n+    } else if (type instanceof ByteType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putByte(index, (byte)data[index]);\n+      }\n+    } else if (type instanceof ShortType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putShort(index, (short)data[index]);\n+      }\n+    } else if (type instanceof IntegerType || type instanceof DateType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putInt(index, (int)data[index]);\n+      }\n+    } else if (type instanceof LongType) {\n+      toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof TimestampType) {\n+      TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+      }\n+    } else if (type instanceof FloatType) {\n+      double[] data = ((DoubleColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putFloat(index, (float)data[index]);\n+      }\n+    } else if (type instanceof DoubleType) {\n+      toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof StringType || type instanceof BinaryType) {\n+      BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+      toColumn.putByteArray(0, data.vector[0]);"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "It's for testing. Sorry, I'll revert this.",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T11:41:33Z",
    "diffHunk": "@@ -0,0 +1,510 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructField[] requiredFields;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructField[] requiredFields,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    this.requiredFields = requiredFields;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredFields.length == requestedColIds.length);\n+\n+    StructType resultSchema = new StructType(requiredFields);\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredFields.length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      StructField field = requiredFields[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          putRepeatingValues(batchSize, field, fromColumn, toColumn);\n+        } else if (fromColumn.noNulls) {\n+          putNonNullValues(batchSize, field, fromColumn, toColumn);\n+        } else {\n+          putValues(batchSize, field, fromColumn, toColumn);\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  private void putRepeatingValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    if (fromColumn.isNull[0]) {\n+      toColumn.putNulls(0, batchSize);\n+    } else {\n+      DataType type = field.dataType();\n+      if (type instanceof BooleanType) {\n+        toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+      } else if (type instanceof ByteType) {\n+        toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof ShortType) {\n+        toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof IntegerType || type instanceof DateType) {\n+        toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof LongType) {\n+        toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof TimestampType) {\n+        toColumn.putLongs(0, batchSize,\n+          fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+      } else if (type instanceof FloatType) {\n+        toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof DoubleType) {\n+        toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof StringType || type instanceof BinaryType) {\n+        BytesColumnVector data = (BytesColumnVector)fromColumn;\n+        toColumn.putByteArray(0, data.vector[0]);\n+        for (int index = 0; index < batchSize; index++) {\n+          toColumn.putArray(index, data.start[0], data.length[0]);\n+        }\n+      } else if (type instanceof DecimalType) {\n+        DecimalType decimalType = (DecimalType)type;\n+        putDecimalWritables(\n+          toColumn,\n+          batchSize,\n+          decimalType.precision(),\n+          decimalType.scale(),\n+          ((DecimalColumnVector)fromColumn).vector[0]);\n+      } else {\n+        throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+      }\n+    }\n+  }\n+\n+  private void putNonNullValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    DataType type = field.dataType();\n+    if (type instanceof BooleanType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putBoolean(index, data[index] == 1);\n+      }\n+    } else if (type instanceof ByteType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putByte(index, (byte)data[index]);\n+      }\n+    } else if (type instanceof ShortType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putShort(index, (short)data[index]);\n+      }\n+    } else if (type instanceof IntegerType || type instanceof DateType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putInt(index, (int)data[index]);\n+      }\n+    } else if (type instanceof LongType) {\n+      toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof TimestampType) {\n+      TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+      }\n+    } else if (type instanceof FloatType) {\n+      double[] data = ((DoubleColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putFloat(index, (float)data[index]);\n+      }\n+    } else if (type instanceof DoubleType) {\n+      toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof StringType || type instanceof BinaryType) {\n+      BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+      toColumn.putByteArray(0, data.vector[0]);"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "I found this. Previously, I missed this from `BytesColumnVector` doc.\r\n```\r\n * This class supports string and binary data by value reference -- i.e. each field is\r\n * explicitly present, as opposed to provided by a dictionary reference.\r\n * In some cases, all the values will be in the same byte array to begin with,\r\n * but this need not be the case. If each value is in a separate byte\r\n * array to start with, or not all of the values are in the same original\r\n * byte array, you can still assign data by reference into this column vector.\r\n * This gives flexibility to use this in multiple situations.\r\n```",
    "commit": "2cf98b6734c806f66e21df50520a465b03d9f060",
    "createdAt": "2018-01-08T11:42:05Z",
    "diffHunk": "@@ -0,0 +1,510 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.orc.OrcConf;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.mapred.OrcInputFormat;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.*;\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.vectorized.ColumnVectorUtils;\n+import org.apache.spark.sql.execution.vectorized.OffHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.OnHeapColumnVector;\n+import org.apache.spark.sql.execution.vectorized.WritableColumnVector;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+\n+/**\n+ * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n+ * After creating, `initialize` and `initBatch` should be called sequentially.\n+ */\n+public class JavaOrcColumnarBatchReader extends RecordReader<Void, ColumnarBatch> {\n+\n+  // ORC File Reader\n+  private Reader reader;\n+\n+  // Vectorized ORC Row Batch\n+  private VectorizedRowBatch batch;\n+\n+  /**\n+   * The column IDs of the physical ORC file schema which are required by this reader.\n+   * -1 means this required column doesn't exist in the ORC file.\n+   */\n+  private int[] requestedColIds;\n+\n+  // Record reader from ORC row batch.\n+  private org.apache.orc.RecordReader recordReader;\n+\n+  private StructField[] requiredFields;\n+\n+  // The result columnar batch for vectorized execution by whole-stage codegen.\n+  private ColumnarBatch columnarBatch;\n+\n+  // Writable column vectors of the result columnar batch.\n+  private WritableColumnVector[] columnVectors;\n+\n+  // The number of rows read and considered to be returned.\n+  private long rowsReturned = 0L;\n+\n+  private long totalRowCount = 0L;\n+\n+  /**\n+   * The memory mode of the columnarBatch\n+   */\n+  private final MemoryMode MEMORY_MODE;\n+\n+  public JavaOrcColumnarBatchReader(boolean useOffHeap) {\n+    MEMORY_MODE = useOffHeap ? MemoryMode.OFF_HEAP : MemoryMode.ON_HEAP;\n+  }\n+\n+\n+  @Override\n+  public Void getCurrentKey() throws IOException, InterruptedException {\n+    return null;\n+  }\n+\n+  @Override\n+  public ColumnarBatch getCurrentValue() throws IOException, InterruptedException {\n+    return columnarBatch;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return (float) rowsReturned / totalRowCount;\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    return nextBatch();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (columnarBatch != null) {\n+      columnarBatch.close();\n+      columnarBatch = null;\n+    }\n+    if (recordReader != null) {\n+      recordReader.close();\n+      recordReader = null;\n+    }\n+  }\n+\n+  /**\n+   * Initialize ORC file reader and batch record reader.\n+   * Please note that `initBatch` is needed to be called after this.\n+   */\n+  @Override\n+  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n+      throws IOException, InterruptedException {\n+    FileSplit fileSplit = (FileSplit)inputSplit;\n+    Configuration conf = taskAttemptContext.getConfiguration();\n+    reader = OrcFile.createReader(\n+      fileSplit.getPath(),\n+      OrcFile.readerOptions(conf)\n+        .maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf))\n+        .filesystem(fileSplit.getPath().getFileSystem(conf)));\n+\n+    Reader.Options options =\n+      OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());\n+    recordReader = reader.rows(options);\n+    totalRowCount = reader.getNumberOfRows();\n+  }\n+\n+  /**\n+   * Initialize columnar batch by setting required schema and partition information.\n+   * With this information, this creates ColumnarBatch with the full schema.\n+   */\n+  public void initBatch(\n+      TypeDescription orcSchema,\n+      int[] requestedColIds,\n+      StructField[] requiredFields,\n+      StructType partitionSchema,\n+      InternalRow partitionValues) {\n+    batch = orcSchema.createRowBatch(DEFAULT_SIZE);\n+    assert(!batch.selectedInUse); // `selectedInUse` should be initialized with `false`.\n+\n+    this.requiredFields = requiredFields;\n+    this.requestedColIds = requestedColIds;\n+    assert(requiredFields.length == requestedColIds.length);\n+\n+    StructType resultSchema = new StructType(requiredFields);\n+    for (StructField f : partitionSchema.fields()) {\n+      resultSchema = resultSchema.add(f);\n+    }\n+\n+    int capacity = DEFAULT_SIZE;\n+    if (MEMORY_MODE == MemoryMode.OFF_HEAP) {\n+      columnVectors = OffHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    } else {\n+      columnVectors = OnHeapColumnVector.allocateColumns(capacity, resultSchema);\n+    }\n+    columnarBatch = new ColumnarBatch(resultSchema, columnVectors, capacity);\n+\n+    if (partitionValues.numFields() > 0) {\n+      int partitionIdx = requiredFields.length;\n+      for (int i = 0; i < partitionValues.numFields(); i++) {\n+        ColumnVectorUtils.populate(columnVectors[i + partitionIdx], partitionValues, i);\n+        columnVectors[i + partitionIdx].setIsConstant();\n+      }\n+    }\n+\n+    // Initialize the missing columns once.\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      if (requestedColIds[i] == -1) {\n+        columnVectors[i].putNulls(0, columnarBatch.capacity());\n+        columnVectors[i].setIsConstant();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n+   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n+   */\n+  private boolean nextBatch() throws IOException {\n+    if (rowsReturned >= totalRowCount) {\n+      return false;\n+    }\n+\n+    recordReader.nextBatch(batch);\n+    int batchSize = batch.size;\n+    if (batchSize == 0) {\n+      return false;\n+    }\n+    rowsReturned += batchSize;\n+    for (WritableColumnVector vector : columnVectors) {\n+      vector.reset();\n+    }\n+    columnarBatch.setNumRows(batchSize);\n+    for (int i = 0; i < requiredFields.length; i++) {\n+      StructField field = requiredFields[i];\n+      WritableColumnVector toColumn = columnVectors[i];\n+\n+      if (requestedColIds[i] >= 0) {\n+        ColumnVector fromColumn = batch.cols[requestedColIds[i]];\n+\n+        if (fromColumn.isRepeating) {\n+          putRepeatingValues(batchSize, field, fromColumn, toColumn);\n+        } else if (fromColumn.noNulls) {\n+          putNonNullValues(batchSize, field, fromColumn, toColumn);\n+        } else {\n+          putValues(batchSize, field, fromColumn, toColumn);\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  private void putRepeatingValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    if (fromColumn.isNull[0]) {\n+      toColumn.putNulls(0, batchSize);\n+    } else {\n+      DataType type = field.dataType();\n+      if (type instanceof BooleanType) {\n+        toColumn.putBooleans(0, batchSize, ((LongColumnVector)fromColumn).vector[0] == 1);\n+      } else if (type instanceof ByteType) {\n+        toColumn.putBytes(0, batchSize, (byte)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof ShortType) {\n+        toColumn.putShorts(0, batchSize, (short)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof IntegerType || type instanceof DateType) {\n+        toColumn.putInts(0, batchSize, (int)((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof LongType) {\n+        toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof TimestampType) {\n+        toColumn.putLongs(0, batchSize,\n+          fromTimestampColumnVector((TimestampColumnVector)fromColumn, 0));\n+      } else if (type instanceof FloatType) {\n+        toColumn.putFloats(0, batchSize, (float)((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof DoubleType) {\n+        toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector[0]);\n+      } else if (type instanceof StringType || type instanceof BinaryType) {\n+        BytesColumnVector data = (BytesColumnVector)fromColumn;\n+        toColumn.putByteArray(0, data.vector[0]);\n+        for (int index = 0; index < batchSize; index++) {\n+          toColumn.putArray(index, data.start[0], data.length[0]);\n+        }\n+      } else if (type instanceof DecimalType) {\n+        DecimalType decimalType = (DecimalType)type;\n+        putDecimalWritables(\n+          toColumn,\n+          batchSize,\n+          decimalType.precision(),\n+          decimalType.scale(),\n+          ((DecimalColumnVector)fromColumn).vector[0]);\n+      } else {\n+        throw new UnsupportedOperationException(\"Unsupported Data Type: \" + type);\n+      }\n+    }\n+  }\n+\n+  private void putNonNullValues(\n+      int batchSize,\n+      StructField field,\n+      ColumnVector fromColumn,\n+      WritableColumnVector toColumn) {\n+    DataType type = field.dataType();\n+    if (type instanceof BooleanType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putBoolean(index, data[index] == 1);\n+      }\n+    } else if (type instanceof ByteType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putByte(index, (byte)data[index]);\n+      }\n+    } else if (type instanceof ShortType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putShort(index, (short)data[index]);\n+      }\n+    } else if (type instanceof IntegerType || type instanceof DateType) {\n+      long[] data = ((LongColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putInt(index, (int)data[index]);\n+      }\n+    } else if (type instanceof LongType) {\n+      toColumn.putLongs(0, batchSize, ((LongColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof TimestampType) {\n+      TimestampColumnVector data = ((TimestampColumnVector)fromColumn);\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putLong(index, fromTimestampColumnVector(data, index));\n+      }\n+    } else if (type instanceof FloatType) {\n+      double[] data = ((DoubleColumnVector)fromColumn).vector;\n+      for (int index = 0; index < batchSize; index++) {\n+        toColumn.putFloat(index, (float)data[index]);\n+      }\n+    } else if (type instanceof DoubleType) {\n+      toColumn.putDoubles(0, batchSize, ((DoubleColumnVector)fromColumn).vector, 0);\n+    } else if (type instanceof StringType || type instanceof BinaryType) {\n+      BytesColumnVector data = ((BytesColumnVector)fromColumn);\n+      toColumn.putByteArray(0, data.vector[0]);"
  }],
  "prId": 19943
}]