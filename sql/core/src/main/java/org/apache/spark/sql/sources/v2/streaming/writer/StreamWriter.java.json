[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: javadoc typo.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-30T23:06:37Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.",
    "line": 16
  }],
  "prId": 20386
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "What does this mean? It isn't clear to me what \"the number of input partitions\" means, or why it isn't obvious that it is equal to the number of pending `WriterCommitMessage` instances passed to add.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-30T23:07:40Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.",
    "line": 12
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about `the number of data(RDD) partitions to write`?\r\n\r\n> why it isn't obvious ...\r\n\r\nMaybe we can just follow `FileCommitProtocol`, i.e. `commit` and `abort` still takes an array of messages.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T01:31:34Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.",
    "line": 12
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Passing the messages to commit and abort seems simpler and better to me, but that's for the batch side. And, we shouldn't move forward with this unless there's a use case.\r\n\r\nAs for the docs here, what is an implementer intended to understand as a result of this? \"The number of data partitions to write\" is also misleading: weren't these already written and committed by tasks?",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T17:03:45Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.",
    "line": 12
  }],
  "prId": 20386
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Commit messages in flight should be handled and aborted. Otherwise, this isn't a \"best effort\". Best effort means that Spark does everything that is feasible to ensure that commit messages are added before aborting, and that should include race conditions from RPC.\r\n\r\nThe case where \"best effort\" might miss a message is if the message is created, but a node fails before it is sent to the driver.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-30T23:15:11Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch\n    * in some circumstances.\n    */\n-  void commit(long epochId, WriterCommitMessage[] messages);\n+  void commit(long epochId);\n \n   /**\n-   * Aborts this writing job because some data writers are failed and keep failing when retry, or\n-   * the Spark job fails with some unknown reasons, or {@link #commit(WriterCommitMessage[])} fails.\n+   * Aborts this writing job because some data writers are failed and keep failing when retry,\n+   * or the Spark job fails with some unknown reasons,\n+   * or {@link #commit()} / {@link #add(WriterCommitMessage)} fails\n    *\n    * If this method fails (by throwing an exception), the underlying data source may require manual\n    * cleanup.\n    *\n-   * Unless the abort is triggered by the failure of commit, the given messages should have some\n-   * null slots as there maybe only a few data writers that are committed before the abort\n-   * happens, or some data writers were committed but their commit messages haven't reached the\n-   * driver when the abort is triggered. So this is just a \"best effort\" for data sources to\n-   * clean up the data left by data writers.\n+   * Unless the abort is triggered by the failure of commit, the number of commit\n+   * messages added by {@link #add(WriterCommitMessage)} should be smaller than the number\n+   * of input data partitions, as there may be only a few data writers that are committed\n+   * before the abort happens, or some data writers were committed but their commit messages\n+   * haven't reached the driver when the abort is triggered. So this is just a \"best effort\"",
    "line": 44
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think there is no difference between \"the message is created, but a node fails before it is sent\" and \"the message is in flight\". Implementations need to deal with the case when a writer finishes successfully but its message is not available in `abort` anyway.\r\n\r\n`best effort` might not be a good word, do you have a better suggestion?",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T01:39:18Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch\n    * in some circumstances.\n    */\n-  void commit(long epochId, WriterCommitMessage[] messages);\n+  void commit(long epochId);\n \n   /**\n-   * Aborts this writing job because some data writers are failed and keep failing when retry, or\n-   * the Spark job fails with some unknown reasons, or {@link #commit(WriterCommitMessage[])} fails.\n+   * Aborts this writing job because some data writers are failed and keep failing when retry,\n+   * or the Spark job fails with some unknown reasons,\n+   * or {@link #commit()} / {@link #add(WriterCommitMessage)} fails\n    *\n    * If this method fails (by throwing an exception), the underlying data source may require manual\n    * cleanup.\n    *\n-   * Unless the abort is triggered by the failure of commit, the given messages should have some\n-   * null slots as there maybe only a few data writers that are committed before the abort\n-   * happens, or some data writers were committed but their commit messages haven't reached the\n-   * driver when the abort is triggered. So this is just a \"best effort\" for data sources to\n-   * clean up the data left by data writers.\n+   * Unless the abort is triggered by the failure of commit, the number of commit\n+   * messages added by {@link #add(WriterCommitMessage)} should be smaller than the number\n+   * of input data partitions, as there may be only a few data writers that are committed\n+   * before the abort happens, or some data writers were committed but their commit messages\n+   * haven't reached the driver when the abort is triggered. So this is just a \"best effort\"",
    "line": 44
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Best effort is not just how we describe the behavior, it is a requirement of the contract. Spark should not drop commit messages because it is convenient. Spark knows what tasks succeeded and failed and which ones were authorized to commit. That's enough information to provide the best-effort guarantee.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T16:58:14Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch\n    * in some circumstances.\n    */\n-  void commit(long epochId, WriterCommitMessage[] messages);\n+  void commit(long epochId);\n \n   /**\n-   * Aborts this writing job because some data writers are failed and keep failing when retry, or\n-   * the Spark job fails with some unknown reasons, or {@link #commit(WriterCommitMessage[])} fails.\n+   * Aborts this writing job because some data writers are failed and keep failing when retry,\n+   * or the Spark job fails with some unknown reasons,\n+   * or {@link #commit()} / {@link #add(WriterCommitMessage)} fails\n    *\n    * If this method fails (by throwing an exception), the underlying data source may require manual\n    * cleanup.\n    *\n-   * Unless the abort is triggered by the failure of commit, the given messages should have some\n-   * null slots as there maybe only a few data writers that are committed before the abort\n-   * happens, or some data writers were committed but their commit messages haven't reached the\n-   * driver when the abort is triggered. So this is just a \"best effort\" for data sources to\n-   * clean up the data left by data writers.\n+   * Unless the abort is triggered by the failure of commit, the number of commit\n+   * messages added by {@link #add(WriterCommitMessage)} should be smaller than the number\n+   * of input data partitions, as there may be only a few data writers that are committed\n+   * before the abort happens, or some data writers were committed but their commit messages\n+   * haven't reached the driver when the abort is triggered. So this is just a \"best effort\"",
    "line": 44
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "This is a bit of a weird case for API documentation, because the external users of the API will be implementing rather than consuming the interface. We shouldn't drop messages just because we don't want to be bothered, but it's easy to fix that if we make a mistake and there's no serious problem if we miss cases we really could have handled. It's a more serious issue if people misunderstand what Spark can provide, and implement sources which assume any commit message that's been generated will be passed to abort.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T17:25:42Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch\n    * in some circumstances.\n    */\n-  void commit(long epochId, WriterCommitMessage[] messages);\n+  void commit(long epochId);\n \n   /**\n-   * Aborts this writing job because some data writers are failed and keep failing when retry, or\n-   * the Spark job fails with some unknown reasons, or {@link #commit(WriterCommitMessage[])} fails.\n+   * Aborts this writing job because some data writers are failed and keep failing when retry,\n+   * or the Spark job fails with some unknown reasons,\n+   * or {@link #commit()} / {@link #add(WriterCommitMessage)} fails\n    *\n    * If this method fails (by throwing an exception), the underlying data source may require manual\n    * cleanup.\n    *\n-   * Unless the abort is triggered by the failure of commit, the given messages should have some\n-   * null slots as there maybe only a few data writers that are committed before the abort\n-   * happens, or some data writers were committed but their commit messages haven't reached the\n-   * driver when the abort is triggered. So this is just a \"best effort\" for data sources to\n-   * clean up the data left by data writers.\n+   * Unless the abort is triggered by the failure of commit, the number of commit\n+   * messages added by {@link #add(WriterCommitMessage)} should be smaller than the number\n+   * of input data partitions, as there may be only a few data writers that are committed\n+   * before the abort happens, or some data writers were committed but their commit messages\n+   * haven't reached the driver when the abort is triggered. So this is just a \"best effort\"",
    "line": 44
  }],
  "prId": 20386
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I realize this isn't part of this commit, but why would an exactly-once guarantee require idempotent commits? Processing the same data twice with an idempotent guarantee is not the same thing as exactly-once.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-30T23:17:06Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch",
    "line": 19
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "The StreamWriter is responsible for setting up a distributed transaction to commit the data within batch both locally and to the remote system. But the StreamExecution keeps its own log of which batches have been fully completed. (\"Fully completed\" includes things like stateful aggregation commits and progress logging which can't reasonably participate in the StreamWriter's transaction.)\r\n\r\nSo there's a scenario where Spark fails between StreamWriter commit and StreamExecution commit, in which the StreamExecution must re-execute the batch to ensure everything is in the right state. The StreamWriter is responsible for ensuring this doesn't generate duplicate data in the remote system.\r\n\r\nNote that the \"true\" exactly once strategy, where the StreamWriter aborts the retried batch because it was already committed before, is indeed idempotent wrt StreamWriter.commit(epochId). But there are weaker strategies which still provide equivalent semantics.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T00:06:58Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch",
    "line": 19
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Thanks for this explanation, I think I see what you're saying. But I think your statement that refers to \"true\" exactly-once gives away the fact that this does not provide exactly-once semantics.\r\n\r\nMaybe this is a question for the dev list: why the weaker version? Shouldn't this API provide a check to see whether the data was already committed?",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T17:11:40Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch",
    "line": 19
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "What are the exact guarantees you're looking for when calling a system \"exactly-once\"? I worry you're looking for something that isn't possible. In particular, I don't know of any additional guarantee that check would allow us to make.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T17:20:50Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch",
    "line": 19
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "For a commit interface, I expect the guarantee to be that data is committed exactly once. If commits are idempotent, data may be reprocessed, and commits may happen more than once, then that is not an exactly-once commit: that is an at-least-once commit.\r\n\r\nI'm not trying to split hairs. My point is that if there's no difference in behavior between exactly-once and at-least-once because the commit must be idempotent, then you don't actually have a exactly-once guarantee.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T17:43:49Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch",
    "line": 19
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "It's true that there's no exactly-once behavior with respect to StreamWriter.commit(). \"Exactly-once processing\" refers to the promise that the remote sink will contain exactly one committed copy of each processed record.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T18:06:30Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch",
    "line": 19
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "If that's the case, then this interface should be clear about it instead of including wording about exactly-once. For this interface, there is no exactly-once guarantee.",
    "commit": "540ff0631471a27af23abb7e8c034bad1ba27cbc",
    "createdAt": "2018-01-31T18:10:25Z",
    "diffHunk": "@@ -32,40 +32,44 @@\n @InterfaceStability.Evolving\n public interface StreamWriter extends DataSourceWriter {\n   /**\n-   * Commits this writing job for the specified epoch with a list of commit messages. The commit\n-   * messages are collected from successful data writers and are produced by\n-   * {@link DataWriter#commit()}.\n+   * Commits this writing job for the specified epoch.\n    *\n-   * If this method fails (by throwing an exception), this writing job is considered to have been\n-   * failed, and the execution engine will attempt to call {@link #abort(WriterCommitMessage[])}.\n+   * When this method is called, the number of commit messages added by\n+   * {@link #add(WriterCommitMessage)} equals to the number of input data partitions.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort()} would be called. The state of the destination\n+   * is undefined and @{@link #abort()} may not be able to deal with it.\n    *\n    * To support exactly-once processing, writer implementations should ensure that this method is\n    * idempotent. The execution engine may call commit() multiple times for the same epoch",
    "line": 19
  }],
  "prId": 20386
}]