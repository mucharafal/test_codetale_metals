[{
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "this is weird. configure the max capacity (in the batch) and the load factor and size numbuckets to capacity / load_factor. You have dependent constants here.\n",
    "commit": "f11c12f946fc13afcafc99c850d4a3063f032429",
    "createdAt": "2016-03-30T22:52:23Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.util.Arrays;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.types.StructType;\n+\n+import static org.apache.spark.sql.types.DataTypes.LongType;\n+\n+/**\n+ * This is an illustrative implementation of an append-only single-key/single value aggregate hash\n+ * map that can act as a 'cache' for extremely fast key-value lookups while evaluating aggregates\n+ * (and fall back to the `BytesToBytesMap` if a given key isn't found). This can be potentially\n+ * 'codegened' in TungstenAggregate to speed up aggregates w/ key.\n+ *\n+ * It is backed by a power-of-2-sized array for index lookups and a columnar batch that stores the\n+ * key-value pairs. The index lookups in the array rely on linear probing (with a small number of\n+ * maximum tries) and use an inexpensive hash function which makes it really efficient for a\n+ * majority of lookups. However, using linear probing and an inexpensive hash function also makes it\n+ * less robust as compared to the `BytesToBytesMap` (especially for a large number of keys or even\n+ * for certain distribution of keys) and requires us to fall back on the latter for correctness.\n+ */\n+public class AggregateHashMap {\n+  public ColumnarBatch batch;\n+  public int[] buckets;\n+\n+  private int numBuckets;\n+  private int numRows = 0;\n+  private int maxSteps = 3;\n+\n+  private static int DEFAULT_NUM_BUCKETS = 65536 * 4;"
  }, {
    "author": {
      "login": "sameeragarwal"
    },
    "body": "by `capacity` I was implying `numBuckets` instead of the capacity of the batch, but yes, the latter makes more sense.\n",
    "commit": "f11c12f946fc13afcafc99c850d4a3063f032429",
    "createdAt": "2016-03-30T22:58:52Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.util.Arrays;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.types.StructType;\n+\n+import static org.apache.spark.sql.types.DataTypes.LongType;\n+\n+/**\n+ * This is an illustrative implementation of an append-only single-key/single value aggregate hash\n+ * map that can act as a 'cache' for extremely fast key-value lookups while evaluating aggregates\n+ * (and fall back to the `BytesToBytesMap` if a given key isn't found). This can be potentially\n+ * 'codegened' in TungstenAggregate to speed up aggregates w/ key.\n+ *\n+ * It is backed by a power-of-2-sized array for index lookups and a columnar batch that stores the\n+ * key-value pairs. The index lookups in the array rely on linear probing (with a small number of\n+ * maximum tries) and use an inexpensive hash function which makes it really efficient for a\n+ * majority of lookups. However, using linear probing and an inexpensive hash function also makes it\n+ * less robust as compared to the `BytesToBytesMap` (especially for a large number of keys or even\n+ * for certain distribution of keys) and requires us to fall back on the latter for correctness.\n+ */\n+public class AggregateHashMap {\n+  public ColumnarBatch batch;\n+  public int[] buckets;\n+\n+  private int numBuckets;\n+  private int numRows = 0;\n+  private int maxSteps = 3;\n+\n+  private static int DEFAULT_NUM_BUCKETS = 65536 * 4;"
  }],
  "prId": 12055
}]