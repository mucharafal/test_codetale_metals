[{
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "Is it the ID of the task or the ID of one particular attempt of the task? (The target audience here is people who know a reasonable amount about Spark - I think we should just say TaskContext.taskAttemptId() if that's what this is.)",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T17:50:00Z",
    "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer.streaming;\n+\n+import java.io.Serializable;\n+\n+import org.apache.spark.annotation.InterfaceStability;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+\n+/**\n+ * A factory of {@link DataWriter} returned by\n+ * {@link StreamingWriteSupport#createStreamingWriterFactory()}, which is responsible for creating\n+ * and initializing the actual data writer at executor side.\n+ *\n+ * Note that, the writer factory will be serialized and sent to executors, then the data writer\n+ * will be created on executors and do the actual writing. So this interface must be\n+ * serializable and {@link DataWriter} doesn't need to be.\n+ */\n+@InterfaceStability.Evolving\n+public interface StreamingDataWriterFactory extends Serializable {\n+\n+  /**\n+   * Returns a data writer to do the actual writing work. Note that, Spark will reuse the same data\n+   * object instance when sending data to the data writer, for better performance. Data writers\n+   * are responsible for defensive copies if necessary, e.g. copy the data before buffer it in a\n+   * list.\n+   *\n+   * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n+   * submitted.\n+   *\n+   * @param partitionId A unique id of the RDD partition that the returned writer will process.\n+   *                    Usually Spark processes many RDD partitions at the same time,\n+   *                    implementations should use the partition id to distinguish writers for\n+   *                    different partitions.\n+   * @param taskId A unique identifier for a task that is performing the write of the partition\n+   *               data. Spark may run multiple tasks for the same partition (due to speculation\n+   *               or task failures, for example)."
  }],
  "prId": 22009
}]