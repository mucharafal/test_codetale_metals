[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "The proposal removes the `jobId` string and the `SaveMode`. Shouldn't that be done in this PR?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T18:14:16Z",
    "diffHunk": "@@ -21,32 +21,32 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n    * @param jobId A unique string for the writing job. It's possible that there are many writing\n-   *              jobs running at the same time, and the returned {@link DataSourceWriter} can\n+   *              jobs running at the same time, and the returned {@link BatchWriteSupport} can\n    *              use this job id to distinguish itself from other jobs.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n    */\n-  Optional<DataSourceWriter> createWriter(\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n       String jobId, StructType schema, SaveMode mode, DataSourceOptions options);"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "If it is done here, this should no longer return `Optional`.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T18:28:28Z",
    "diffHunk": "@@ -21,32 +21,32 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n    * @param jobId A unique string for the writing job. It's possible that there are many writing\n-   *              jobs running at the same time, and the returned {@link DataSourceWriter} can\n+   *              jobs running at the same time, and the returned {@link BatchWriteSupport} can\n    *              use this job id to distinguish itself from other jobs.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n    */\n-  Optional<DataSourceWriter> createWriter(\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n       String jobId, StructType schema, SaveMode mode, DataSourceOptions options);"
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "@rdblue I'd like to keep the save mode before we finish the new DDL logical plans and DataFrameWriter APIs. We are migrating file source and we still need to support `df.write.mode(\"errorIfExists\").parquet(...)`.\r\n\r\nAfter we finish the new DDL logical plans and DataFrameWriter APIs, we can rename it to `LegacyBatchWriteSupportProvider` and create a new `BatchWriteSupportProvider` for the new DDL logical plans and DataFrameWriter APIs.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T04:22:22Z",
    "diffHunk": "@@ -21,33 +21,39 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n+ *\n+ * This interface is used when end users want to use a data source implementation directly, e.g.\n+ * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n-   * @param writeUUID A unique string for the writing job. It's possible that there are many writing\n-   *                  jobs running at the same time, and the returned {@link DataSourceWriter} can\n-   *                  use this job id to distinguish itself from other jobs.\n+   * @param queryId A unique string for the writing query. It's possible that there are many\n+   *                writing queries running at the same time, and the returned\n+   *                {@link BatchWriteSupport} can use this id to distinguish itself from others.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n-   * @return a writer to append data to this data source\n+   * @return a write support to write data to this data source.\n    */\n-  Optional<DataSourceWriter> createWriter(\n-      String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options);\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n+      String queryId,\n+      StructType schema,\n+      SaveMode mode,",
    "line": 50
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I don't think this is a good idea. Why introduce a legacy API into a new API? If we are moving old sources to the new API, then they should fully implement the new API and should not continue to expose the unpredictable v1 behavior.\r\n\r\nThat said, as long as the `TableCatalog` makes it in, I don't care what anonymous tables do because I don't intend for any of our sources to use this path.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T17:46:15Z",
    "diffHunk": "@@ -21,33 +21,39 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n+ *\n+ * This interface is used when end users want to use a data source implementation directly, e.g.\n+ * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n-   * @param writeUUID A unique string for the writing job. It's possible that there are many writing\n-   *                  jobs running at the same time, and the returned {@link DataSourceWriter} can\n-   *                  use this job id to distinguish itself from other jobs.\n+   * @param queryId A unique string for the writing query. It's possible that there are many\n+   *                writing queries running at the same time, and the returned\n+   *                {@link BatchWriteSupport} can use this id to distinguish itself from others.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n-   * @return a writer to append data to this data source\n+   * @return a write support to write data to this data source.\n    */\n-  Optional<DataSourceWriter> createWriter(\n-      String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options);\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n+      String queryId,\n+      StructType schema,\n+      SaveMode mode,",
    "line": 50
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "The problem here is, if we don't take `saveMode`, the only end-user API to write to a data source is: `df.write.format(...).mode(\"append\").save()`. That makes data source v2 totally unusable before we introduce the new write APIs.\r\n\r\nI hope we can get this in before Spark 2.4, so that some data source projects can start migrating and experimenting.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-10T01:54:18Z",
    "diffHunk": "@@ -21,33 +21,39 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n+ *\n+ * This interface is used when end users want to use a data source implementation directly, e.g.\n+ * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n-   * @param writeUUID A unique string for the writing job. It's possible that there are many writing\n-   *                  jobs running at the same time, and the returned {@link DataSourceWriter} can\n-   *                  use this job id to distinguish itself from other jobs.\n+   * @param queryId A unique string for the writing query. It's possible that there are many\n+   *                writing queries running at the same time, and the returned\n+   *                {@link BatchWriteSupport} can use this id to distinguish itself from others.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n-   * @return a writer to append data to this data source\n+   * @return a write support to write data to this data source.\n    */\n-  Optional<DataSourceWriter> createWriter(\n-      String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options);\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n+      String queryId,\n+      StructType schema,\n+      SaveMode mode,",
    "line": 50
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@cloud-fan: this is a bad idea **because** it enables save modes other than append for DataSourceV2 without using the new logical plans. This leads to undefined behavior and is why we proposed standard logical plans in the first place.\r\n\r\nUsing a data source implementation directly should only support appending and scanning anything more complex must require `DeleteSupport` (see #21308) or `TableCatalog` (see #21306) and the new logical plans. Otherwise, this will allow sources to expose behavior that we are trying to fix.\r\n\r\nI'm -1 on this.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-13T17:48:03Z",
    "diffHunk": "@@ -21,33 +21,39 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n+ *\n+ * This interface is used when end users want to use a data source implementation directly, e.g.\n+ * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n-   * @param writeUUID A unique string for the writing job. It's possible that there are many writing\n-   *                  jobs running at the same time, and the returned {@link DataSourceWriter} can\n-   *                  use this job id to distinguish itself from other jobs.\n+   * @param queryId A unique string for the writing query. It's possible that there are many\n+   *                writing queries running at the same time, and the returned\n+   *                {@link BatchWriteSupport} can use this id to distinguish itself from others.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n-   * @return a writer to append data to this data source\n+   * @return a write support to write data to this data source.\n    */\n-  Optional<DataSourceWriter> createWriter(\n-      String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options);\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n+      String queryId,\n+      StructType schema,\n+      SaveMode mode,",
    "line": 50
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "To clarify, your proposal is that we should block the completion of DataSourceV2 until the new logical plans are in place?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-13T18:18:21Z",
    "diffHunk": "@@ -21,33 +21,39 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n+ *\n+ * This interface is used when end users want to use a data source implementation directly, e.g.\n+ * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n-   * @param writeUUID A unique string for the writing job. It's possible that there are many writing\n-   *                  jobs running at the same time, and the returned {@link DataSourceWriter} can\n-   *                  use this job id to distinguish itself from other jobs.\n+   * @param queryId A unique string for the writing query. It's possible that there are many\n+   *                writing queries running at the same time, and the returned\n+   *                {@link BatchWriteSupport} can use this id to distinguish itself from others.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n-   * @return a writer to append data to this data source\n+   * @return a write support to write data to this data source.\n    */\n-  Optional<DataSourceWriter> createWriter(\n-      String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options);\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n+      String queryId,\n+      StructType schema,\n+      SaveMode mode,",
    "line": 50
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I think that the v2 sources should only use the plans proposed in the SPIP. I also think that the v2 data source API should always tell the data source exactly what to do: for overwrite, what should be deleted and what should be added.\r\n\r\nThat doesn't block fixing the v2 API here and doesn't prevent anyone from using it. But it would prevent people from relying on undefined behavior that results from passing an ambiguous `SaveMode` to a source.\r\n\r\nThe only thing that would not be available by doing this is overwrite support by using the SaveMode, which isn't something anyone should rely on because it doesn't have defined behavior.\r\n\r\nI understand that this may seem like it would block migration from the v1 API to the v2 API. But I think it is the right thing to do so that we have a clear and consistent definition for how v2 sources behave.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-13T18:30:05Z",
    "diffHunk": "@@ -21,33 +21,39 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n+ *\n+ * This interface is used when end users want to use a data source implementation directly, e.g.\n+ * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n-   * @param writeUUID A unique string for the writing job. It's possible that there are many writing\n-   *                  jobs running at the same time, and the returned {@link DataSourceWriter} can\n-   *                  use this job id to distinguish itself from other jobs.\n+   * @param queryId A unique string for the writing query. It's possible that there are many\n+   *                writing queries running at the same time, and the returned\n+   *                {@link BatchWriteSupport} can use this id to distinguish itself from others.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n-   * @return a writer to append data to this data source\n+   * @return a write support to write data to this data source.\n    */\n-  Optional<DataSourceWriter> createWriter(\n-      String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options);\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n+      String queryId,\n+      StructType schema,\n+      SaveMode mode,",
    "line": 50
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I totally agree that `SaveMode` is a bad API which leads to undefined behavior. That's why we start a project to design new DDL logical plans and write APIs. However, I believe we had an agreement before that we can't remove existing APIs, so the `DataFrameWriter` and `SaveMode` will still be there in Spark. If I'm a data source developer, even I've implemented the new write APIs (assuming it's finished), I would still support `SaveMode` to attract more users. `DataFrameWriter` is a very widely used API and the end users may need a long time to migrate to the new write APIs. BTW, file source (without catalog) does have a clearly defined behavior regarding `SaveMode`, we should make it possible to migrate file source to data source v2.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-15T02:13:42Z",
    "diffHunk": "@@ -21,33 +21,39 @@\n \n import org.apache.spark.annotation.InterfaceStability;\n import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n+import org.apache.spark.sql.sources.v2.writer.BatchWriteSupport;\n import org.apache.spark.sql.types.StructType;\n \n /**\n  * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n- * provide data writing ability and save the data to the data source.\n+ * provide data writing ability for batch processing.\n+ *\n+ * This interface is used when end users want to use a data source implementation directly, e.g.\n+ * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @InterfaceStability.Evolving\n-public interface WriteSupport extends DataSourceV2 {\n+public interface BatchWriteSupportProvider extends DataSourceV2 {\n \n   /**\n-   * Creates an optional {@link DataSourceWriter} to save the data to this data source. Data\n+   * Creates an optional {@link BatchWriteSupport} to save the data to this data source. Data\n    * sources can return None if there is no writing needed to be done according to the save mode.\n    *\n    * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n    * submitted.\n    *\n-   * @param writeUUID A unique string for the writing job. It's possible that there are many writing\n-   *                  jobs running at the same time, and the returned {@link DataSourceWriter} can\n-   *                  use this job id to distinguish itself from other jobs.\n+   * @param queryId A unique string for the writing query. It's possible that there are many\n+   *                writing queries running at the same time, and the returned\n+   *                {@link BatchWriteSupport} can use this id to distinguish itself from others.\n    * @param schema the schema of the data to be written.\n    * @param mode the save mode which determines what to do when the data are already in this data\n    *             source, please refer to {@link SaveMode} for more details.\n    * @param options the options for the returned data source writer, which is an immutable\n    *                case-insensitive string-to-string map.\n-   * @return a writer to append data to this data source\n+   * @return a write support to write data to this data source.\n    */\n-  Optional<DataSourceWriter> createWriter(\n-      String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options);\n+  Optional<BatchWriteSupport> createBatchWriteSupport(\n+      String queryId,\n+      StructType schema,\n+      SaveMode mode,",
    "line": 50
  }],
  "prId": 22009
}]