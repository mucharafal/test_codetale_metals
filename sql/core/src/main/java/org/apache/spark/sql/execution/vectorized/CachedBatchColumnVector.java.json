[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "do we support reading values in a random order? e.g. `getBoolean(2), getBoolean(1), getBoolean(2)`?",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-18T12:34:03Z",
    "diffHunk": "@@ -0,0 +1,421 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector implements java.io.Serializable {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // whether a row is already extracted or not. If extractTo() is called, set true\n+  // e.g. when isNullAt() and getInt() ara called, extractTo() must be called only once\n+  private boolean[] calledExtractTo;\n+\n+  // accessor for a column\n+  private transient ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private transient ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+    calledExtractTo = new boolean[capacity];\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (!calledExtractTo[rowId]) {\n+      assert (columnAccessor.hasNext());\n+      columnAccessor.extractTo(columnVector, ROWID);\n+      calledExtractTo[rowId] = true;\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public void putNotNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNotNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.isNullAt(ROWID);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public void putBoolean(int rowId, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBooleans(int rowId, int count, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "We do not support reading values in a random order. This is because implementation of `CompressionScheme` (e.g. `IntDelta`) supports only sequential access.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-18T12:37:58Z",
    "diffHunk": "@@ -0,0 +1,421 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector implements java.io.Serializable {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // whether a row is already extracted or not. If extractTo() is called, set true\n+  // e.g. when isNullAt() and getInt() ara called, extractTo() must be called only once\n+  private boolean[] calledExtractTo;\n+\n+  // accessor for a column\n+  private transient ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private transient ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+    calledExtractTo = new boolean[capacity];\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (!calledExtractTo[rowId]) {\n+      assert (columnAccessor.hasNext());\n+      columnAccessor.extractTo(columnVector, ROWID);\n+      calledExtractTo[rowId] = true;\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public void putNotNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNotNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.isNullAt(ROWID);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public void putBoolean(int rowId, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBooleans(int rowId, int count, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "then we should throw exception for this case instead of returning wrong result.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-18T12:59:36Z",
    "diffHunk": "@@ -0,0 +1,421 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector implements java.io.Serializable {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // whether a row is already extracted or not. If extractTo() is called, set true\n+  // e.g. when isNullAt() and getInt() ara called, extractTo() must be called only once\n+  private boolean[] calledExtractTo;\n+\n+  // accessor for a column\n+  private transient ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private transient ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+    calledExtractTo = new boolean[capacity];\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (!calledExtractTo[rowId]) {\n+      assert (columnAccessor.hasNext());\n+      columnAccessor.extractTo(columnVector, ROWID);\n+      calledExtractTo[rowId] = true;\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public void putNotNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNotNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.isNullAt(ROWID);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public void putBoolean(int rowId, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBooleans(int rowId, int count, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "I see. I will add code to track access order for each getter.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-18T13:08:28Z",
    "diffHunk": "@@ -0,0 +1,421 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector implements java.io.Serializable {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // whether a row is already extracted or not. If extractTo() is called, set true\n+  // e.g. when isNullAt() and getInt() ara called, extractTo() must be called only once\n+  private boolean[] calledExtractTo;\n+\n+  // accessor for a column\n+  private transient ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private transient ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+    calledExtractTo = new boolean[capacity];\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (!calledExtractTo[rowId]) {\n+      assert (columnAccessor.hasNext());\n+      columnAccessor.extractTo(columnVector, ROWID);\n+      calledExtractTo[rowId] = true;\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public void putNotNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNotNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.isNullAt(ROWID);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public void putBoolean(int rowId, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBooleans(int rowId, int count, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We should explicitly say that, this is a wrapper to read compressed data as ColumnVector in table cache.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T03:02:33Z",
    "diffHunk": "@@ -0,0 +1,416 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array."
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "Sure, done",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T04:55:24Z",
    "diffHunk": "@@ -0,0 +1,416 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array."
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can we inline this method?",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T03:15:37Z",
    "diffHunk": "@@ -0,0 +1,416 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId + 1 == rowId) {\n+      assert (columnAccessor.hasNext());\n+      columnAccessor.extractTo(columnVector, ROWID);\n+      previousRowId = rowId;\n+    } else if (previousRowId != rowId) {\n+      throw new UnsupportedOperationException(\"Row access order must be sequentially ascending.\" +\n+        \" Internal row \" + rowId + \"is accessed after internal row \"+ previousRowId + \"was accessed.\");\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public void putNotNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNotNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.isNullAt(ROWID);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public void putBoolean(int rowId, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBooleans(int rowId, int count, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getBoolean(ROWID);\n+  }\n+\n+  @Override\n+  public boolean[] getBooleans(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+\n+  //\n+  // APIs dealing with Bytes\n+  //\n+\n+  @Override\n+  public void putByte(int rowId, byte value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBytes(int rowId, int count, byte value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBytes(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getByte(ROWID);\n+  }\n+\n+  @Override\n+  public byte[] getBytes(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Shorts\n+  //\n+\n+  @Override\n+  public void putShort(int rowId, short value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putShorts(int rowId, int count, short value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putShorts(int rowId, int count, short[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getShort(ROWID);\n+  }\n+\n+  @Override\n+  public short[] getShorts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Ints\n+  //\n+\n+  @Override\n+  public void putInt(int rowId, int value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putInts(int rowId, int count, int value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putInts(int rowId, int count, int[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getInt(ROWID);\n+  }\n+\n+  @Override\n+  public int[] getInts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  /**\n+   * Returns the dictionary Id for rowId.\n+   * This should only be called when the ColumnVector is dictionaryIds.\n+   * We have this separate method for dictionaryIds as per SPARK-16928.\n+   */\n+  public int getDictId(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Longs\n+  //\n+\n+  @Override\n+  public void putLong(int rowId, long value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putLongs(int rowId, int count, long value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putLongs(int rowId, int count, long[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getLong(ROWID);\n+  }\n+\n+  @Override\n+  public long[] getLongs(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with floats\n+  //\n+\n+  @Override\n+  public void putFloat(int rowId, float value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putFloats(int rowId, int count, float value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putFloats(int rowId, int count, float[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putFloats(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getFloat(ROWID);\n+  }\n+\n+  @Override\n+  public float[] getFloats(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with doubles\n+  //\n+\n+  @Override\n+  public void putDouble(int rowId, double value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putDoubles(int rowId, int count, double value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putDoubles(int rowId, int count, double[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putDoubles(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getDouble(ROWID);\n+  }\n+\n+  @Override\n+  public double[] getDoubles(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Arrays\n+  //\n+\n+  @Override\n+  public int getArrayLength(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+  @Override\n+  public int getArrayOffset(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putArray(int rowId, int offset, int length) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void loadBytes(ColumnVector.Array array) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Byte Arrays\n+  //\n+\n+  @Override\n+  public int putByteArray(int rowId, byte[] value, int offset, int length) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  public final UTF8String getUTF8String(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getUTF8String(ROWID);\n+  }\n+\n+  // Spilt this function out since it is the slow path.\n+  @Override\n+  protected void reserveInternal(int newCapacity) {\n+    capacity = newCapacity;\n+  }\n+\n+  private void initialize(byte[] buffer, DataType type) {\n+    this.buffer = buffer;\n+    this.type = type;\n+\n+    if (columnAccessor == null) {\n+      setColumnAccessor();"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "Yes, we can. done",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T04:55:35Z",
    "diffHunk": "@@ -0,0 +1,416 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId + 1 == rowId) {\n+      assert (columnAccessor.hasNext());\n+      columnAccessor.extractTo(columnVector, ROWID);\n+      previousRowId = rowId;\n+    } else if (previousRowId != rowId) {\n+      throw new UnsupportedOperationException(\"Row access order must be sequentially ascending.\" +\n+        \" Internal row \" + rowId + \"is accessed after internal row \"+ previousRowId + \"was accessed.\");\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public void putNotNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNull(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putNotNulls(int rowId, int count) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.isNullAt(ROWID);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public void putBoolean(int rowId, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBooleans(int rowId, int count, boolean value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getBoolean(ROWID);\n+  }\n+\n+  @Override\n+  public boolean[] getBooleans(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+\n+  //\n+  // APIs dealing with Bytes\n+  //\n+\n+  @Override\n+  public void putByte(int rowId, byte value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBytes(int rowId, int count, byte value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putBytes(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getByte(ROWID);\n+  }\n+\n+  @Override\n+  public byte[] getBytes(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Shorts\n+  //\n+\n+  @Override\n+  public void putShort(int rowId, short value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putShorts(int rowId, int count, short value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putShorts(int rowId, int count, short[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getShort(ROWID);\n+  }\n+\n+  @Override\n+  public short[] getShorts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Ints\n+  //\n+\n+  @Override\n+  public void putInt(int rowId, int value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putInts(int rowId, int count, int value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putInts(int rowId, int count, int[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getInt(ROWID);\n+  }\n+\n+  @Override\n+  public int[] getInts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  /**\n+   * Returns the dictionary Id for rowId.\n+   * This should only be called when the ColumnVector is dictionaryIds.\n+   * We have this separate method for dictionaryIds as per SPARK-16928.\n+   */\n+  public int getDictId(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Longs\n+  //\n+\n+  @Override\n+  public void putLong(int rowId, long value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putLongs(int rowId, int count, long value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putLongs(int rowId, int count, long[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getLong(ROWID);\n+  }\n+\n+  @Override\n+  public long[] getLongs(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with floats\n+  //\n+\n+  @Override\n+  public void putFloat(int rowId, float value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putFloats(int rowId, int count, float value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putFloats(int rowId, int count, float[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putFloats(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getFloat(ROWID);\n+  }\n+\n+  @Override\n+  public float[] getFloats(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with doubles\n+  //\n+\n+  @Override\n+  public void putDouble(int rowId, double value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putDoubles(int rowId, int count, double value) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putDoubles(int rowId, int count, double[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putDoubles(int rowId, int count, byte[] src, int srcIndex) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getDouble(ROWID);\n+  }\n+\n+  @Override\n+  public double[] getDoubles(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Arrays\n+  //\n+\n+  @Override\n+  public int getArrayLength(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+  @Override\n+  public int getArrayOffset(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void putArray(int rowId, int offset, int length) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void loadBytes(ColumnVector.Array array) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Byte Arrays\n+  //\n+\n+  @Override\n+  public int putByteArray(int rowId, byte[] value, int offset, int length) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  public final UTF8String getUTF8String(int rowId) {\n+    prepareAccess(rowId);\n+    return columnVector.getUTF8String(ROWID);\n+  }\n+\n+  // Spilt this function out since it is the slow path.\n+  @Override\n+  protected void reserveInternal(int newCapacity) {\n+    capacity = newCapacity;\n+  }\n+\n+  private void initialize(byte[] buffer, DataType type) {\n+    this.buffer = buffer;\n+    this.type = type;\n+\n+    if (columnAccessor == null) {\n+      setColumnAccessor();"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should allow `previousRowId == rowId`, as we are able to support `getInt(1), getInt(1), getInt(2)`",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T03:18:41Z",
    "diffHunk": "@@ -0,0 +1,416 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId + 1 == rowId) {"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "yes, we can do it for now. See line 78. It means that `extractTo()` must be called only once for a given rowId.\r\nNow, we also support that `isNullAt(0), getInt(0)`, too.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T04:21:08Z",
    "diffHunk": "@@ -0,0 +1,416 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by an in memory JVM array.\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // keep compressed data\n+  private byte[] buffer;\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  private void setColumnAccessor() {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);\n+    columnAccessor = ColumnAccessor$.MODULE$.apply(type, byteBuffer);\n+  }\n+\n+  // call extractTo() before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId + 1 == rowId) {"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Can we keep the value in `MutableUnsafeRow` instead of `ColumnVector`? Then we don't need to change `ColumnAccessor` to support `ColumnVector`.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T12:29:25Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (int i = previousRowId + 1; i <= rowId; i++) {\n+        assert (columnAccessor.hasNext());\n+        columnAccessor.extractTo(columnVector, ROWID);"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "TODO: in the future we can defer the decoding like https://github.com/apache/spark/pull/11437",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T12:33:31Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (int i = previousRowId + 1; i <= rowId; i++) {\n+        assert (columnAccessor.hasNext());\n+        columnAccessor.extractTo(columnVector, ROWID);"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "I see. I reverted the recent change.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T14:12:44Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (int i = previousRowId + 1; i <= rowId; i++) {\n+        assert (columnAccessor.hasNext());\n+        columnAccessor.extractTo(columnVector, ROWID);"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit:\r\n```\r\nfor (; previousRowId < rowId; previousRowId++) {\r\n  ...\r\n}\r\n```",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T12:30:30Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (int i = previousRowId + 1; i <= rowId; i++) {"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "Updated.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T14:12:25Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (int i = previousRowId + 1; i <= rowId; i++) {"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "update the error message with the new behavior?",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T12:31:01Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (int i = previousRowId + 1; i <= rowId; i++) {\n+        assert (columnAccessor.hasNext());\n+        columnAccessor.extractTo(columnVector, ROWID);\n+      }\n+      previousRowId = rowId;\n+    } else {\n+      throw new UnsupportedOperationException(\"Row access order must be ascending.\" +"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "OK, done",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T14:12:13Z",
    "diffHunk": "@@ -0,0 +1,407 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private ColumnVector columnVector;\n+\n+  // an accessor uses only row 0 in columnVector\n+  private final int ROWID = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (int i = previousRowId + 1; i <= rowId; i++) {\n+        assert (columnAccessor.hasNext());\n+        columnAccessor.extractTo(columnVector, ROWID);\n+      }\n+      previousRowId = rowId;\n+    } else {\n+      throw new UnsupportedOperationException(\"Row access order must be ascending.\" +"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why `transient`? The `ColumnVector` is not even serializable.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T14:12:08Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private transient UnsafeRow unsafeRow;"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: don't need to say `internal row`, just `row`",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-19T14:15:01Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private transient UnsafeRow unsafeRow;\n+  private transient BufferHolder bufferHolder;\n+  private transient UnsafeRowWriter rowWriter;\n+  private transient MutableUnsafeRow mutableRow;\n+\n+  // an accessor uses only column 0\n+  private final int ORDINAL = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, DataTypes.NullType, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reserveInternal(numRows);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (; previousRowId < rowId; previousRowId++) {\n+        assert (columnAccessor.hasNext());\n+        bufferHolder.reset();\n+        rowWriter.zeroOutNullBytes();\n+        columnAccessor.extractTo(mutableRow, ORDINAL);\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Row access order must be equal or ascending.\" +\n+        \" Internal row \" + rowId + \"is accessed after internal row \"+ previousRowId + \"was accessed.\");"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "after https://github.com/apache/spark/pull/18680 , we have a `ReadOnlyColumnVector` and we can use it here.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-20T13:02:32Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ColumnVector {"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "nit: add a space to \"was accessed\" -> \" was accessed\". Also \"is accessed\" -> \" is accessed\".",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-21T03:26:57Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ReadOnlyColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private UnsafeRow unsafeRow;\n+  private BufferHolder bufferHolder;\n+  private UnsafeRowWriter rowWriter;\n+  private MutableUnsafeRow mutableRow;\n+\n+  // an accessor uses only column 0\n+  private final int ORDINAL = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, type, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (; previousRowId < rowId; previousRowId++) {\n+        assert (columnAccessor.hasNext());\n+        bufferHolder.reset();\n+        rowWriter.zeroOutNullBytes();\n+        columnAccessor.extractTo(mutableRow, ORDINAL);\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Row access order must be equal or ascending.\" +\n+        \" Row \" + rowId + \"is accessed after row \"+ previousRowId + \"was accessed.\");"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: A column vector ?",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-21T03:42:48Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "thanks, updated",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-21T06:50:40Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Is this check needed?\r\nI guess this is called only from constructor. ditto for `mutableRow` below.",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-21T03:53:29Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ReadOnlyColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private UnsafeRow unsafeRow;\n+  private BufferHolder bufferHolder;\n+  private UnsafeRowWriter rowWriter;\n+  private MutableUnsafeRow mutableRow;\n+\n+  // an accessor uses only column 0\n+  private final int ORDINAL = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, type, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (; previousRowId < rowId; previousRowId++) {\n+        assert (columnAccessor.hasNext());\n+        bufferHolder.reset();\n+        rowWriter.zeroOutNullBytes();\n+        columnAccessor.extractTo(mutableRow, ORDINAL);\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Row access order must be equal or ascending.\" +\n+        \" Row \" + rowId + \"is accessed after row \"+ previousRowId + \" was accessed.\");\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.isNullAt(ORDINAL);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getBoolean(ORDINAL);\n+  }\n+\n+  @Override\n+  public boolean[] getBooleans(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+\n+  //\n+  // APIs dealing with Bytes\n+  //\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getByte(ORDINAL);\n+  }\n+\n+  @Override\n+  public byte[] getBytes(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Shorts\n+  //\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getShort(ORDINAL);\n+  }\n+\n+  @Override\n+  public short[] getShorts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Ints\n+  //\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getInt(ORDINAL);\n+  }\n+\n+  @Override\n+  public int[] getInts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  public int getDictId(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Longs\n+  //\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getLong(ORDINAL);\n+  }\n+\n+  @Override\n+  public long[] getLongs(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with floats\n+  //\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getFloat(ORDINAL);\n+  }\n+\n+  @Override\n+  public float[] getFloats(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with doubles\n+  //\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getDouble(ORDINAL);\n+  }\n+\n+  @Override\n+  public double[] getDoubles(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Arrays\n+  //\n+\n+  @Override\n+  public int getArrayLength(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+  @Override\n+  public int getArrayOffset(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void loadBytes(ColumnVector.Array array) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Byte Arrays\n+  //\n+\n+  public final UTF8String getUTF8String(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getUTF8String(ORDINAL);\n+  }\n+\n+  private void initialize(byte[] buffer, DataType type) {\n+    if (columnAccessor == null) {"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "good catch, done",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-21T06:50:25Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ReadOnlyColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private UnsafeRow unsafeRow;\n+  private BufferHolder bufferHolder;\n+  private UnsafeRowWriter rowWriter;\n+  private MutableUnsafeRow mutableRow;\n+\n+  // an accessor uses only column 0\n+  private final int ORDINAL = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, type, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reset();\n+  }\n+\n+  @Override\n+  public long valuesNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+  @Override\n+  public long nullsNativeAddress() {\n+    throw new RuntimeException(\"Cannot get native address for on heap column\");\n+  }\n+\n+  @Override\n+  public void close() {\n+  }\n+\n+  // call extractTo() for rowId only once before getting actual data\n+  private void prepareAccess(int rowId) {\n+    if (previousRowId == rowId) {\n+      // do nothing\n+    } else if (previousRowId < rowId) {\n+      for (; previousRowId < rowId; previousRowId++) {\n+        assert (columnAccessor.hasNext());\n+        bufferHolder.reset();\n+        rowWriter.zeroOutNullBytes();\n+        columnAccessor.extractTo(mutableRow, ORDINAL);\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Row access order must be equal or ascending.\" +\n+        \" Row \" + rowId + \"is accessed after row \"+ previousRowId + \" was accessed.\");\n+    }\n+  }\n+\n+  //\n+  // APIs dealing with nulls\n+  //\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.isNullAt(ORDINAL);\n+  }\n+\n+  //\n+  // APIs dealing with Booleans\n+  //\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getBoolean(ORDINAL);\n+  }\n+\n+  @Override\n+  public boolean[] getBooleans(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+\n+  //\n+  // APIs dealing with Bytes\n+  //\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getByte(ORDINAL);\n+  }\n+\n+  @Override\n+  public byte[] getBytes(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Shorts\n+  //\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getShort(ORDINAL);\n+  }\n+\n+  @Override\n+  public short[] getShorts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Ints\n+  //\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getInt(ORDINAL);\n+  }\n+\n+  @Override\n+  public int[] getInts(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  public int getDictId(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Longs\n+  //\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getLong(ORDINAL);\n+  }\n+\n+  @Override\n+  public long[] getLongs(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with floats\n+  //\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getFloat(ORDINAL);\n+  }\n+\n+  @Override\n+  public float[] getFloats(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with doubles\n+  //\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getDouble(ORDINAL);\n+  }\n+\n+  @Override\n+  public double[] getDoubles(int rowId, int count)  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Arrays\n+  //\n+\n+  @Override\n+  public int getArrayLength(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+  @Override\n+  public int getArrayOffset(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void loadBytes(ColumnVector.Array array) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  //\n+  // APIs dealing with Byte Arrays\n+  //\n+\n+  public final UTF8String getUTF8String(int rowId) {\n+    prepareAccess(rowId);\n+    return unsafeRow.getUTF8String(ORDINAL);\n+  }\n+\n+  private void initialize(byte[] buffer, DataType type) {\n+    if (columnAccessor == null) {"
  }],
  "prId": 18468
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "We can remove this?",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-21T03:55:26Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ReadOnlyColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private UnsafeRow unsafeRow;\n+  private BufferHolder bufferHolder;\n+  private UnsafeRowWriter rowWriter;\n+  private MutableUnsafeRow mutableRow;\n+\n+  // an accessor uses only column 0\n+  private final int ORDINAL = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, type, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reset();"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "Yes, I did",
    "commit": "a26dc150f6b95cc42558561cd2548de04a89f041",
    "createdAt": "2017-07-21T06:50:33Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.vectorized;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.spark.memory.MemoryMode;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;\n+import org.apache.spark.sql.execution.columnar.*;\n+import org.apache.spark.sql.types.*;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A column backed by data compressed thru ColumnAccessor\n+ * this is a wrapper to read compressed data for table cache\n+ */\n+public final class CachedBatchColumnVector extends ReadOnlyColumnVector {\n+\n+  // accessor for a column\n+  private ColumnAccessor columnAccessor;\n+\n+  // a row where the compressed data is extracted\n+  private UnsafeRow unsafeRow;\n+  private BufferHolder bufferHolder;\n+  private UnsafeRowWriter rowWriter;\n+  private MutableUnsafeRow mutableRow;\n+\n+  // an accessor uses only column 0\n+  private final int ORDINAL = 0;\n+\n+  // Keep row id that was previously accessed\n+  private int previousRowId = -1;\n+\n+\n+  public CachedBatchColumnVector(byte[] buffer, int numRows, DataType type) {\n+    super(numRows, type, MemoryMode.ON_HEAP);\n+    initialize(buffer, type);\n+    reset();"
  }],
  "prId": 18468
}]