[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should mention that users can disable this and use their customer commit coordinator.",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-08T10:54:35Z",
    "diffHunk": "@@ -78,10 +78,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the OutputCommitCoordinator to allow only one attempt to commit.\n+   * {@link DataWriterFactory} implementations can disable this behavior. If disabled, multiple"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "It says that already: \"DataWriterFactory implementations can disable this behavior.\"",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-08T16:41:56Z",
    "diffHunk": "@@ -78,10 +78,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the OutputCommitCoordinator to allow only one attempt to commit.\n+   * {@link DataWriterFactory} implementations can disable this behavior. If disabled, multiple"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I clarified this and added a note about how to do it.",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-08T17:34:18Z",
    "diffHunk": "@@ -78,10 +78,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the OutputCommitCoordinator to allow only one attempt to commit.\n+   * {@link DataWriterFactory} implementations can disable this behavior. If disabled, multiple"
  }],
  "prId": 20490
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "don't say `OutputCommitCoordinator` as it's an internal class. We can just say `a commit coordinator`",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-08T10:56:41Z",
    "diffHunk": "@@ -78,10 +78,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the OutputCommitCoordinator to allow only one attempt to commit."
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Fixed.",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-08T17:34:23Z",
    "diffHunk": "@@ -78,10 +78,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the OutputCommitCoordinator to allow only one attempt to commit."
  }],
  "prId": 20490
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "This is actually not a guarantee, is it?\r\n",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-09T04:18:14Z",
    "diffHunk": "@@ -62,6 +62,16 @@\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n+  /**\n+   * Returns whether Spark should use the commit coordinator to ensure that only one attempt for"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Currently, the commit coordinator will only authorize one attempt and only authorize another if the authorized attempt fails, so it does ensure that only one attempt commits. Do you think the wording here needs to change?\r\n\r\nInstead of documenting the behavior of the commit coordinator here, I'd rather point to its docs. Are those written, or is the coordinator an internal class?",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-09T16:40:25Z",
    "diffHunk": "@@ -62,6 +62,16 @@\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n+  /**\n+   * Returns whether Spark should use the commit coordinator to ensure that only one attempt for"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`only one` -> `at most one`? BTW I think we should not link to an internal class.",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-09T18:20:10Z",
    "diffHunk": "@@ -62,6 +62,16 @@\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n+  /**\n+   * Returns whether Spark should use the commit coordinator to ensure that only one attempt for"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I agree we shouldn't link to an internal class, but I don't think this is the place to document the built-in coordinator's behavior. Is there already a doc for that elsewhere that is public?",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-09T19:18:56Z",
    "diffHunk": "@@ -62,6 +62,16 @@\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n+  /**\n+   * Returns whether Spark should use the commit coordinator to ensure that only one attempt for"
  }],
  "prId": 20490
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `only one` -> `at most one`",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-10T02:32:26Z",
    "diffHunk": "@@ -78,10 +88,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the commit coordinator to allow only one attempt to commit. Implementations can"
  }],
  "prId": 20490
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`... committed successfully, and  Spark will pick the commit message that arrives at the driver side first.`",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-10T02:34:19Z",
    "diffHunk": "@@ -78,10 +88,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the commit coordinator to allow only one attempt to commit. Implementations can\n+   * disable this behavior by overriding {@link #useCommitCoordinator()}. If disabled, multiple\n+   * attempts may have committed successfully and all successful commit messages are passed to this"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I think we need to address this guarantee. Spark will just drop commit messages? That seems like a huge problem to me.\r\n\r\ncc @steveloughran ",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-12T18:27:09Z",
    "diffHunk": "@@ -78,10 +88,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the commit coordinator to allow only one attempt to commit. Implementations can\n+   * disable this behavior by overriding {@link #useCommitCoordinator()}. If disabled, multiple\n+   * attempts may have committed successfully and all successful commit messages are passed to this"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I'm changing the wording to this to capture the behavior:\r\n\r\n> If disabled, multiple attempts may have committed successfully and one successful commit message per task will be passed to this commit method. The remaining commit messages are ignored by Spark.\r\n\r\nI think we should fix this for non-coordinated commits, but it doesn't need to block the commit to get coordinator support in.",
    "commit": "538bc864f8ebb8d1b7e63c26806f209f2c3b0fc4",
    "createdAt": "2018-02-12T18:30:37Z",
    "diffHunk": "@@ -78,10 +88,11 @@ default void onDataWriterCommit(WriterCommitMessage message) {}\n    * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n    * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n-   * Note that, one partition may have multiple committed data writers because of speculative tasks.\n-   * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n-   * writer can commit, or have a way to clean up the data of already-committed writers.\n+   * Note that speculative execution may cause multiple tasks to run for a partition. By default,\n+   * Spark uses the commit coordinator to allow only one attempt to commit. Implementations can\n+   * disable this behavior by overriding {@link #useCommitCoordinator()}. If disabled, multiple\n+   * attempts may have committed successfully and all successful commit messages are passed to this"
  }],
  "prId": 20490
}]