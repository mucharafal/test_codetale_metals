[{
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Instead of just being a marker interface, could these be a mixin interface for the Scan (that also defines the respective methods)? like:\r\n```\r\npublic interface SupportsBatchRead extends Scan {\r\n   Batch toBatch();\r\n}\r\n```\r\nand so on. And if a Table supports Read one could query its Scan object to figure out the type if required.",
    "commit": "5a4047e84a1e46a247a962137e77cf83390200aa",
    "createdAt": "2019-01-02T23:28:21Z",
    "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+\n+/**\n+ * An empty mix-in interface for {@link Table}, to indicate this table supports streaming scan with\n+ * micro-batch mode.\n+ * <p>\n+ * If a {@link Table} implements this interface, the\n+ * {@link SupportsRead#newScanBuilder(DataSourceOptions)} must return a {@link ScanBuilder} that\n+ * builds {@link Scan} with {@link Scan#toMicroBatchStream()} implemented.\n+ * </p>\n+ */\n+@Evolving\n+public interface SupportsMicroBatchRead extends SupportsRead { }",
    "line": 34
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "We need to know the capability at the table level. It's too late to do it at the scan level, as creating a scan may be expensive.",
    "commit": "5a4047e84a1e46a247a962137e77cf83390200aa",
    "createdAt": "2019-01-03T02:57:11Z",
    "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+\n+/**\n+ * An empty mix-in interface for {@link Table}, to indicate this table supports streaming scan with\n+ * micro-batch mode.\n+ * <p>\n+ * If a {@link Table} implements this interface, the\n+ * {@link SupportsRead#newScanBuilder(DataSourceOptions)} must return a {@link ScanBuilder} that\n+ * builds {@link Scan} with {@link Scan#toMicroBatchStream()} implemented.\n+ * </p>\n+ */\n+@Evolving\n+public interface SupportsMicroBatchRead extends SupportsRead { }",
    "line": 34
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Only thing is it doesn't enforce anything. A method like `Table.supportedTypes()` might also work.",
    "commit": "5a4047e84a1e46a247a962137e77cf83390200aa",
    "createdAt": "2019-01-03T18:54:33Z",
    "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+\n+/**\n+ * An empty mix-in interface for {@link Table}, to indicate this table supports streaming scan with\n+ * micro-batch mode.\n+ * <p>\n+ * If a {@link Table} implements this interface, the\n+ * {@link SupportsRead#newScanBuilder(DataSourceOptions)} must return a {@link ScanBuilder} that\n+ * builds {@link Scan} with {@link Scan#toMicroBatchStream()} implemented.\n+ * </p>\n+ */\n+@Evolving\n+public interface SupportsMicroBatchRead extends SupportsRead { }",
    "line": 34
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@arunmahadevan, that's similar to the capabilities that we plan to add. Spark will query specific capabilities for a table to make determinations like this to cut down on the number of empty interfaces.",
    "commit": "5a4047e84a1e46a247a962137e77cf83390200aa",
    "createdAt": "2019-01-03T20:49:00Z",
    "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+\n+/**\n+ * An empty mix-in interface for {@link Table}, to indicate this table supports streaming scan with\n+ * micro-batch mode.\n+ * <p>\n+ * If a {@link Table} implements this interface, the\n+ * {@link SupportsRead#newScanBuilder(DataSourceOptions)} must return a {@link ScanBuilder} that\n+ * builds {@link Scan} with {@link Scan#toMicroBatchStream()} implemented.\n+ * </p>\n+ */\n+@Evolving\n+public interface SupportsMicroBatchRead extends SupportsRead { }",
    "line": 34
  }],
  "prId": 23430
}]