[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: text should be \"Batch scans are not supported\". Starting with \"Do not\" makes the sentence a command.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T19:56:54Z",
    "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.sources.v2.SupportsBatchRead;\n+import org.apache.spark.sql.sources.v2.Table;\n+\n+/**\n+ * A logical representation of a data source scan. This interface is used to provide logical\n+ * information, like what the actual read schema is.\n+ *\n+ * This logical representation is shared between batch scan, micro-batch streaming scan and\n+ * continuous streaming scan. Data sources must implement the corresponding methods in this\n+ * interface, to match what the table promises to support. For example, {@link #toBatch()} must be\n+ * implemented, if the {@link Table} that creates this {@link Scan} implements\n+ * {@link SupportsBatchRead}.\n+ */\n+@Evolving\n+public interface Scan {\n+\n+  /**\n+   * Returns the actual schema of this data source scan, which may be different from the physical\n+   * schema of the underlying storage, as column pruning or other optimizations may happen.\n+   */\n+  StructType readSchema();\n+\n+  /**\n+   * Returns the physical representation of this scan for batch query. By default this method throws\n+   * exception, data sources must overwrite this method to provide an implementation, if the\n+   * {@link Table} that creates this scan implements {@link SupportsBatchRead}.\n+   */\n+  default Batch toBatch() {\n+    throw new UnsupportedOperationException(\"Do not support batch scan.\");"
  }],
  "prId": 23086
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I would have expected the default implementation to show both pushed filters and the read schema, along with the implementation class name. Read schema can be accessed by `readSchema`. Should there also be a way to access the pushed filters? `pushedFilters` seems like a good idea to me. (This can be added later)",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-29T21:43:49Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.sources.v2.SupportsBatchRead;\n+import org.apache.spark.sql.sources.v2.Table;\n+\n+/**\n+ * A logical representation of a data source scan. This interface is used to provide logical\n+ * information, like what the actual read schema is.\n+ * <p>\n+ * This logical representation is shared between batch scan, micro-batch streaming scan and\n+ * continuous streaming scan. Data sources must implement the corresponding methods in this\n+ * interface, to match what the table promises to support. For example, {@link #toBatch()} must be\n+ * implemented, if the {@link Table} that creates this {@link Scan} implements\n+ * {@link SupportsBatchRead}.\n+ * </p>\n+ */\n+@Evolving\n+public interface Scan {\n+\n+  /**\n+   * Returns the actual schema of this data source scan, which may be different from the physical\n+   * schema of the underlying storage, as column pruning or other optimizations may happen.\n+   */\n+  StructType readSchema();\n+\n+  /**\n+   * A description string of this scan, which may includes information like: what filters are\n+   * configured for this scan, what's the value of some important options like path, etc. The\n+   * description doesn't need to include {@link #readSchema()}, as Spark already knows it.\n+   * <p>\n+   * By default this returns the class name of the implementation. Please override it to provide a\n+   * meaningful description.\n+   * </p>\n+   */\n+  default String description() {",
    "line": 54
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Since this is an interface, and filter pushdown is optional, I'm not sure how to report `pushedFilters` here.\r\n\r\nThe read schema is always reported, see [`DataSourceV2ScanExec.simpleString`](https://github.com/apache/spark/pull/23086/files#diff-3e1258979e16f72a829abb8a1cd88bdaR39). Maybe we should still keep `pushedFilters` in `DataSourceV2ScanExec`, and display it in the plan string format. What do you think? ",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-30T03:15:34Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.sources.v2.SupportsBatchRead;\n+import org.apache.spark.sql.sources.v2.Table;\n+\n+/**\n+ * A logical representation of a data source scan. This interface is used to provide logical\n+ * information, like what the actual read schema is.\n+ * <p>\n+ * This logical representation is shared between batch scan, micro-batch streaming scan and\n+ * continuous streaming scan. Data sources must implement the corresponding methods in this\n+ * interface, to match what the table promises to support. For example, {@link #toBatch()} must be\n+ * implemented, if the {@link Table} that creates this {@link Scan} implements\n+ * {@link SupportsBatchRead}.\n+ * </p>\n+ */\n+@Evolving\n+public interface Scan {\n+\n+  /**\n+   * Returns the actual schema of this data source scan, which may be different from the physical\n+   * schema of the underlying storage, as column pruning or other optimizations may happen.\n+   */\n+  StructType readSchema();\n+\n+  /**\n+   * A description string of this scan, which may includes information like: what filters are\n+   * configured for this scan, what's the value of some important options like path, etc. The\n+   * description doesn't need to include {@link #readSchema()}, as Spark already knows it.\n+   * <p>\n+   * By default this returns the class name of the implementation. Please override it to provide a\n+   * meaningful description.\n+   * </p>\n+   */\n+  default String description() {",
    "line": 54
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "What about adding `pushedFilters` that defaults to `new Filter[0]`? Then users should override that to add filters to the description, if they are pushed. I think a Scan should be able to report its options, especially those that distinguish it from other scans, like pushed filters.\r\n\r\nI guess we could have some wrapper around the user-provided Scan that holds the Scan options. I would want to standardize that instead of doing it in every scan exec node.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-30T18:58:14Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.sources.v2.SupportsBatchRead;\n+import org.apache.spark.sql.sources.v2.Table;\n+\n+/**\n+ * A logical representation of a data source scan. This interface is used to provide logical\n+ * information, like what the actual read schema is.\n+ * <p>\n+ * This logical representation is shared between batch scan, micro-batch streaming scan and\n+ * continuous streaming scan. Data sources must implement the corresponding methods in this\n+ * interface, to match what the table promises to support. For example, {@link #toBatch()} must be\n+ * implemented, if the {@link Table} that creates this {@link Scan} implements\n+ * {@link SupportsBatchRead}.\n+ * </p>\n+ */\n+@Evolving\n+public interface Scan {\n+\n+  /**\n+   * Returns the actual schema of this data source scan, which may be different from the physical\n+   * schema of the underlying storage, as column pruning or other optimizations may happen.\n+   */\n+  StructType readSchema();\n+\n+  /**\n+   * A description string of this scan, which may includes information like: what filters are\n+   * configured for this scan, what's the value of some important options like path, etc. The\n+   * description doesn't need to include {@link #readSchema()}, as Spark already knows it.\n+   * <p>\n+   * By default this returns the class name of the implementation. Please override it to provide a\n+   * meaningful description.\n+   * </p>\n+   */\n+  default String description() {",
    "line": 54
  }],
  "prId": 23086
}]