[{
  "comments": [{
    "author": {
      "login": "wzhfy"
    },
    "body": "\"an exception\".\r\nBy the way, not related to this PR, shall we also list allowed writing optimizations like [in DataSourceV2Reader](https://github.com/apache/spark/pull/19623/files#diff-f7f970e9684796e2120223d1153302e4R35) ?",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-01T03:43:19Z",
    "diffHunk": "@@ -30,6 +30,9 @@\n  * It can mix in various writing optimization interfaces to speed up the data saving. The actual\n  * writing logic is delegated to {@link DataWriter}.\n  *\n+ * If exception happens when applying any of these writing optimizations, the action would fail and"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "we only have one so I don't know how to category them... We can do this after having more optimizations",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-01T09:41:48Z",
    "diffHunk": "@@ -30,6 +30,9 @@\n  * It can mix in various writing optimization interfaces to speed up the data saving. The actual\n  * writing logic is delegated to {@link DataWriter}.\n  *\n+ * If exception happens when applying any of these writing optimizations, the action would fail and"
  }],
  "prId": 19623
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "bit optimistic on the \"should\". Maybe: \"state of the destination is undefined\". \r\n\r\nThe issue here is what if there's a network failure and abort() fails for that. it hasn't made any changes to the destination. Thus its state is whatever the previous state of the dest was. If no task was committed, nothing should be directly observable. 1+ task committed and there *may* be output. If job commit failed and abort() was called then, nobody is going to have any idea of the final state.\r\n\r\nMaybe: \"Implementations are required to perform best-effort cleanup irrespective of the current state of the destination\".\r\n\r\n",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-01T10:30:21Z",
    "diffHunk": "@@ -75,8 +82,10 @@\n   /**\n    * Aborts this writing job because some data writers are failed to write the records and aborted,\n    * or the Spark job fails with some unknown reasons, or {@link #commit(WriterCommitMessage[])}\n-   * fails. If this method fails(throw exception), the underlying data source may have garbage that\n-   * need to be cleaned manually, but these garbage should not be visible to data source readers.\n+   * fails.\n+   *\n+   * If an exception was thrown, the underlying data source may have garbage that need to be\n+   * cleaned manually, but these garbage should not be visible to data source readers."
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "call it, because commit may fail for other reasons, e.g. the committer permits the job to commit if the output is all to empty partitions, but fails if there is data in any of the dest partitions. the abort() call triggers the cleanup.\r\n\r\n1. exceptions aborts after job commit failure. must of course be caught/swallowed. Hadoop MR doesn't do that properly, which is something someone should fix.  Probably me.\r\n1. ideally the committers should be doing their own cleanup if the job fails, so they can be confident the cleanup always takes place. Again: catch & swallow exceptions.\r\n1. Failure in job commit is very much an \"outcome is unknown\" state.\r\n\r\nCouple of references\r\n* [s3a committer lifecycle tests](https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractITCommitProtocol.java), including OOO workflows (abort before commit, 2x commit task, 2x commit job, commit job -> commit task).\r\n* [SPARK-2984](https://issues.apache.org/jira/browse/SPARK-2984), where underlying problem of recurrent issue is people trying to write to same dest with >1 job, and finding all jobs after first one failing as the `__temporary` dir has been deleted in cleanup(). Arguably, the job commit/cleanup is being overaggressive about cleanup, but given how partition merging isn't atomic with the classic fileoutput committers, its really a manifestation of people trying to do something they shouldn't.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-01T13:46:33Z",
    "diffHunk": "@@ -75,8 +82,10 @@\n   /**\n    * Aborts this writing job because some data writers are failed to write the records and aborted,\n    * or the Spark job fails with some unknown reasons, or {@link #commit(WriterCommitMessage[])}\n-   * fails. If this method fails(throw exception), the underlying data source may have garbage that\n-   * need to be cleaned manually, but these garbage should not be visible to data source readers.\n+   * fails.\n+   *\n+   * If an exception was thrown, the underlying data source may have garbage that need to be\n+   * cleaned manually, but these garbage should not be visible to data source readers."
  }],
  "prId": 19623
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "Having >1 committed writers is a failure of protocol. Speculation & failure handling should allow >1 ready-to-commit writers, but only actually commit one. That's where the stuff about writers reporting ready-to-commit & driver tracking state of active tasks comes in. \r\n\r\nWhat if a commit fails/commit operation times out or raises some other error? Hadoop FileOutputFormat v1 commit can actually recover from a failed task commit, v2 can't [Assumption: rename() is atomic and will fail if dest path exists). Hence the `OutputCommitter.isRecoverySupported()` probe to tell driver whether or not it can recover from a task commit which is perceived as failing.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T11:21:02Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "are you proposing something like 2PC? I wanna keep this write commit API simple that there is only one round trip between driver and executors: \"writer factory sent to executor\" -> \"executor write data and commit\" -> \"commit message sent back to driver\" -> \"driver does job-level commit\". This round trip can easily be implemented by Spark RDD.\r\n\r\nIf implementations wanna something stronger, they can still implement it with their own coordinator, which can probably be more efficient than using Spark driver as coordinator.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T11:32:33Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "@steveloughran it is not really possible to enforce exclusivity this way in a general commit system. That would just be a best effort thing to avoid duplicate work.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T13:12:05Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK, now I'm confused to the extent that I've been using the IDE to trace method calls, and ideally would like to see any TLA+/PlusCan specs of what's going on. Failing that, some .py pseudocode. \r\n\r\nOtherwise, there's setting breakpoints and stepping through the debugger, which what I ended up doing to work out WTF MapReduce did. [Committer Architecture](https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/committer_architecture.md) is essentially the documentation of that debugger-derived analysis.\r\n\r\n### sequence of \r\n1. 1+ task created, scheduled, \r\n1. setupTask(taskContext) called in workers for task before execution\r\n1. task does its work. worker knows task is now ready to be committed\r\n1. 1+ task declares ready to commit to driver \r\n1. driver selects which of these tasks to move to commit\r\n1. driver sends ok-to-commit(task) message request to executor with task.\r\n1. Task committed via some `commitTask()` operation\r\n1. speculative tasks told to abort or left in pending state to see what happens\r\n1. task commit succeeds: job continues\r\n1. task commit fail: best effort attempt abort failed task (Hadoop MR may do this in AM by recreating taskContext of failed task & calling `taskAbort(tCtx)`) on committer created for job. means that if cleanup wants to delete local file:// data, it may not get deleted)\r\n1. if engine supports recovery from failed commits and commit algorithm does: select another of pending speculative tasks or retry entire task, move to step (2) unless #of attempts exceeded.\r\n1. else: abort entire job\r\n\r\nA requirement of speculation is \"speculative work which is only observable after commit\". Its why the direct committers had to run with speculation off, and even there failure handling could only be strictly handled by failing entire job. \r\n\r\nThis is not a full 2PC protocol where the entire job is committed iff all workers are ready to commit. Instead you are allowing >1 task to run in parallel & choose which to commit.\r\n\r\n### if >1 speculated tasks can commit work without coordination, then unless all tasks generate output with same filenames/paths *and the commit of an individual task is atomic*,  you are in danger\r\n\r\n1. Task A and Task B both commit different files: result, dest contains > 1 copy of each file. Task abort() isn't going to help as it's abort of pre-commit state, not rollback of output.\r\n1. Task A and Task B commit different files, but in parallel and with nonatomic commit of files, then you will end up with the output of both.\r\n\r\nTo avoid that you're going to need some for of coordination between tasks s.t. only one actually commits. This can be done implicitly via a filesystem with some atomic operations (rename(src, dest), create(file, overwrite=false)...) or some other mechanism.\r\n\r\nExcept, what about [SPARK-4879](https://issues.apache.org/jira/browse/SPARK-4879) and `OutputCommitCoordinator`. That is asking for permission to commit, and it is, AFAIK, turned on by default from the property \r\n\r\nLooking at `org.apache.spark.sql.execution.datasources.FileFormatWriter.write()`\r\n\r\n```scala\r\n/**\r\n * Basic work flow of this command is:\r\n * 1. Driver side setup, including output committer initialization and data source specific\r\n *    preparation work for the write job to be issued.\r\n * 2. Issues a write job consists of one or more executor side tasks, each of which writes all\r\n *    rows within an RDD partition.\r\n * 3. If no exception is thrown in a task, commits that task, otherwise aborts that task;  If any\r\n *    exception is thrown during task commitment, also aborts that task.\r\n * 4. If all tasks are committed, commit the job, otherwise aborts the job;  If any exception is\r\n *    thrown during job commitment, also aborts the job.\r\n * 5. If the job is successfully committed, perform post-commit operations such as\r\n *    processing statistics.\r\n * @return The set of all partition paths that were updated during this write job.\r\n */\r\n```\r\n\r\nIt's `FileFormatWriter.executeTask`,\r\n1.  does the etupTask/exec/commitTask in sequence, \r\n1. calls `FileCommitProtocol.commitTask`\r\n1. `HadoopMapReduceCommitProtocol.commitTask` calls `SparkHadoopMapRedUtil.commitTask`\r\n1. which, if there's data (`OutputCommitter.needsTaskCommit`) and co-ordination enabled `\"spark.hadoop.outputCommitCoordination.enabled\" == true`, calls `OutputCommitCoordinator.canCommit(stage, part, attempt)`, which then talks to the driver to get permission to commit. The driver-side `OutputCommitCoordinator` instance gets to manage the state machine of task attempts, which is done in `handleAskPermissionToCommit`\r\n\r\nSo, unless I've misunderstood the flow of things. Spark guarantees at most one speculative task attempt for a (stage, part) succeeds, it's done through coordination with the driver inside `FileFormatWriter.executeTask`.\r\n\r\nIf have understood, the key thing is: co-ordination is done in Spark's OutputCommitCoordinator, with that coordinator being called from `FileFormatWriter`, hence the routines to insert into HadoopFS or Hive. \r\n\r\nFor this API here, where is that coordination going to take place? ",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T14:09:15Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "You are not taking into account network partitioning. ",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T14:20:38Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "@steveloughran you can still implement this with the current interface. Like you said, we can use HDFS(or other coordinators) to make sure only one speculative task can commit, but I think there may be other ways to do it, we don't need to restrict us from allowing only one speculative task to commit.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T14:40:30Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "The only way to guarantee no more than one task can commit is if the underlying storage system guarantees that. There is no way to design something generic. It is simply not possible in a distributed system, when network partitioning or message lost.\r\n",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T14:44:00Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "> The only way to guarantee no more than one task can commit is if the underlying storage system guarantees that. There is no way to design something generic. It is simply not possible in a distributed system, when network partitioning or message lost.\r\n\r\nI agree that you cannot guarantee that >1 task has executed. \r\n\r\nI believe that you can stop >1 task committing by having the scheduler decide the exact task to commit & fail the job if it doesn't return within a bounded time saying \"done\". At which point the final state of the destination is unknown & you are into whatever driver-side algorithms chooses to implement there, possibly based on information provided by the committer about its ability to recover/retry. That's where it gets complicated, and where I'd need to do a lot more delving into the spark source with a debugger and some fault injecting committer underneath to really stress things.\r\n\r\nNetwork partitioning\r\n\r\n1.  executor split from driver during execution & does not rejoin: can't request commit, or requests commit & doesn't get a response. Timeout -> failure on executor. Task abort? Driver must eventually consider task failed too. Driver presumably has to consider executor timeout & retry. if speculation enabled & first committer requests first, either let it through or tell it to abort. If !speculation & first committer requests. must abort.\r\n1. executor split from driver after asking for commit & before getting response: same.\r\n1. executor request to commit -> driver, rejection sent & not received. timeout -> failure. task should abort, driver: should it just assume aborted?\r\n1. executor request to commit -> driver, acceptance sent & not received; task times out & doesn't commit, aborts. But what does driver do?\r\n1. executor request to commit -> driver, acceptance sent, received, processed & the response lost. Executor knows the difference: it's finished. Driver cannot differentiate this from the previous one.\r\n1. executor request to commit -> driver, acceptance sent, executor fails before execution. No task abort executed on executor. No direct way for driver to tell. How to interpret? Retry vis job fail?\r\n1. executor request to commit -> driver, acceptance sent, driver fails during commit. Dest state unknown. Unless protocol says it can handle recovery here, -> abort.\r\n1. driver itself fails; job restarted. Maybe: recovery attempted. Hadoop MR v1 FileOutputCommitAlgorithm can do this as it can infer which tasks committed by state of destFS, relying on all state being in consistent FS, task commit atomic. Does make for a slower job commit though...main benefit is that for jobs big & long enough that the probability of AM failure is tangible, it doesn't require a replay of all committed tasks.\r\n\r\nFor the cases where driver is tells executor to commit & never returns , driver has a problem. It can't directly differentiate: (not committed, committed, nonatomic commit-failed-partway)\r\n\r\nLooking into the Hadoop code, `OutputCommiter.recoverTask()` comes out to play here, which is run from the AM on a rebuilt task context. If it can recover: Fine. If not, task is considered dead and needs cleanup (abort?). \r\n\r\nTo recover: \r\n* dest exists ==> old state == committed => no-op + success\r\n* dest !exists, repeat commit through rename(). If that rename [on the AM] fails: retry some more times then abort task & fail job. Lack of meaningful error text in rename hurts here. Because this recovery is done on the AM then there's no partitioning going to happen between scheduler & commitTask() call, just with remote FS/store/DB\r\n\r\nSpark does not use the recovery methods of OutputCommitter, AFAIK.\r\n\r\nThe other way to handle failed/timed-out task commit is: give up, abort the entire job, say \"dest is in unknown state now\". My tests imply that a task commit failure in spark fails the job; I don't know about what happens if the task commit outcome isn't received by the driver & will have to think of a test there. Maybe I'll add another failure mode to the S3A fault injector: \"block\", & see what happens.\r\n\r\nReturning to the point about speculation; its the same as rerunning a task which failed to report in: they both generate output, exactly one must be committed, and the generated output should not (must not?) be manifest at the dest paths until task commit. That was the issue with the DirectOutputCommitter, wasn't it: you couldn't repeat a task whose executor failed as had already modified the output directory.\r\n\r\nFor the spec,I think you should mandate that output MUST NOT be visible until taskCommit; taskAbort MUST be repeatable & best effort; same for jobAbort. I don't know about saying \"Can recreate state of a task attempt and be able to call taskAbort() again. I know Hadoop output committer requires it, but I already know of one committer which will leak file:/tmp data if rerun on a different host, because it can't get at the remote FS.\r\n\r\nNow, should output be visible on taskCommit? It is on Hadoop v2 commit protocol, which moves in all output to outputPath on task commit, downgrading jobCommit to `touchz(\"$dest/_SUCCESS\")`. It's faster, it just means that if job fails before commit, you need to clean up the job by deleting dest/*.  \r\n\r\nThe S3A committer delays all manifestation of MPUs until job commit because we don't want anything to be observable, and cost of commit is a small POST XML doc per file, which can be done in parallel. taskCommits are not repeatable, and at least currently, any partition/timeout of taskCommit() is going to mean the entire job needs rerunning. If spark does have the ability to rerun tasks which didn't return from taskcommit, we could do something here, relying on the fact tha we PUT the .pendingset of a tasks' commit metadata is atomically: cleanup would just mean doing a HEAD of it. Exists -> task was committed. !exists, retry (& rely on job cleanup() to cancel all pending MPUs under the dest path being called in jobCommit()). \r\n\r\n\r\n",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T16:58:03Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "@cloud-fan \r\n\r\n> @steveloughran you can still implement this with the current interface. Like you said, we can use HDFS(or other coordinators) to make sure only one speculative task can commit, but I think there may be other ways to do it, we don't need to restrict us from allowing only one speculative task to commit.\r\n\r\nI guess the main thing is decide who has to make the decision & make clear. what the expectations are and what the driver does for them",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T16:59:41Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "The expectation is well defined, see `DataWriter#commit`",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T17:11:40Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "@steveloughran it is proven to be not possible. Your algorithm doesn't work.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T17:32:22Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "> Your algorithm doesn't work\r\n\r\nto be clear: I know you cant reliably guarantee exactly 1 commit of a task in all conditions, but as long as speculative tasks don't manifest their work in the dest directory until commitTask is called, then you can have a stateful driver call taskCommit exactly once for a task attempt. It just can't be confident that anything received that request.\r\nThe driver needs to as long as it reacts to a failure to return success by failing the job and saying \"something went wrong\" & letting layers above decide how to react. The output should have the results of 0-1 commits of that task, plus whatever other tasks had already committed. \r\n\r\nOf course, the layers above have to somehow be connected to the driver, otherwise they won't be able to distinguish \"job failed\" from \"job completed\", hence the fact that the `_SUCCESS` manifest is considered the marker of a successful job. (which of course means that you need a consistent store so that HEAD of _SUCCESS doesn't return an old copy, so a simple probe isn't actually sufficient for S3)\r\n\r\nOh, and there are a couple of failure modes I forgot to mention: executors doing their work after you think they've stopped. e.g\r\n\r\n* task attempt 1 commitTask called -> timeout, task attempt 2 commitTask called -< success, then task1 actually doing its commit. \r\n* task 1 invoked commitTask -> timeout, job reacts to failure by aborting job, something retrying etc, *then task 1 actually doing the commit*. That is: even after the entire job has aborted and a new job executed, it is possible for a partitioned task for the first attempt to commit its work.\r\n\r\nThat's an interesting one to defend against. \r\n\r\nI do know that the MapReduce AM for retryable jobs tries to minimise this risk by only attempting to commit work if the last time it interacted with the YARN was within\r\n`yarn.app.mapreduce.am.job.committer.commit-window` millis. So a partitioned AM is guaranteed not to attempt to commit its work after that window. Task commit is similar, using the timeout for the communications as the window\r\n\r\n```scala\r\nrequestTaskCommit()\r\nval canCommit = awaitTaskCommit(timeout = jobcommitWindow)\r\nif (canCommit ) doCommit() else doAbort()\r\n```\r\nInstead they just rely on the assumption that once the worker/manager protocol says \"Commit\" then you can go ahead, that is: the interval between retrieving a success from the manager and executing taskCommit() is negligible.\r\n\r\nOf course, timeout logic assumes time goes forward at the same rate everywhere, which in a world of VMs isn't something you can consistently see: either the JVM or the entire VM can block between awaitTaskCommt() returning true and doCommit() being called.\r\n\r\n\r\n",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T19:59:09Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I haven't read Steve's points here entirely, but I agree that Spark should be primarily responsible for task commit coordination. Most implementations would be fine using the current [output commit coordinator](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala), which does a good job balancing the trade-offs that you've been discussing. It ensures that only one task is authorized to commit and has well-defined failure cases (when a network partition prevents the authorized committer from responding before its commit authorization times out).\r\n\r\nI think that Spark should use the current commit coordinator unless an implementation opts out of using it (and I'm not sure that opting out is a use case we care to support at this point). It's fine if Spark documents how its coordinator works and there are some drawbacks, but expecting implementations to handle their own commit coordination (which requires RPC for Spark) is, I think, unreasonable. Let's use the one we have by default, however imperfect.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-03T19:33:50Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "thinking about this some more (and still reserving the right to be provably wrong), I could imagine a job commit protocol which allowed 1+ Task attempt to commit, making the decision about which completed tasks to accept into the final results at job commit.\r\n\r\nThat is, \r\n\r\n1. every task attempt could (atomically) promote its output to the job commit dir\r\n1. the job commit would enumerate all promoted task attempts, and choose which ones to accept.\r\n1. The ones it didn't want would be aborted by the job committer.\r\n\r\nExample: task attempts rename their output to a dir `dest/_tempt/jobId/completed/$taskId_$task_attemptId`; job committer would enum all directories in the completed dir, and for all dirs where the task ID was the same: pick one to commit, abort the others.\r\n\r\nWith that variant, you don't need to coordinate task commit across workers, even with speculation enabled. You'd just need to be confident that the promotion of task commit information was atomic, the view of the output consistent, and that job commit is not initiated until at least one attempt per task has succeeded. Job commit is potentially slower though.\r\nBecause you can turn off use of the output co-ordinator when writing to hive/hadoop tables, then a commit protocol like this could work today. Interestingly, it's not that far off the FileOutputCommitter v1 protocol, you'd just renane the task attempt output dir to the promoted dir & let the job commit filter out duplicates from its directory listing. Be a bit more expensive in terms of storage use between task commit and job commit though.\r\n\r\nMaybe a good policy here is \"a job must not commit 1+ task attempt\", but give committers the option to bypass the output coordinator. if the job committer can handle the conflict resolution & so make that guarantee in its commit phase.\r\n",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-03T20:39:54Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "it's really out of the scope of this PR...\r\n\r\nThis PR is adding documents to describe exception behavior, please send a new PR to update the document for the commit stuff and we can move our discussion there. Thanks!",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-04T01:02:00Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be",
    "line": 33
  }],
  "prId": 19623
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "A single task failure shoudn't abort entire job. \r\nJob abortion is more likely to be triggered by\r\n* Failure count of task exceeds configured limit. \r\n* Non-recoverable failure of the commit() operation of one or more tasks. I don't see spark invoking `OutputCommitter.isRecoverySupported()` as its focusing more on \"faster execution and recovery through retry\".\r\n* pre-emption of job (if engine supports preemption)\r\n\r\nLooking in the code, it's called after `sparkContext.runJob()` throws an exception for any reason, & on fail of `FileFormatWriter.write()`, again, any reason. ",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T11:38:51Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a mechanism to make sure only one data writer\n-   * can commit successfully, or have a way to clean up the data of already-committed writers.\n+   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n+   * writer can commit, or have a way to clean up the data of already-committed writers.\n    */\n   void commit(WriterCommitMessage[] messages);\n \n   /**\n    * Aborts this writing job because some data writers are failed to write the records and aborted,"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I'll update the comment, we do retry failed write tasks before calling this method.",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T13:36:56Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a mechanism to make sure only one data writer\n-   * can commit successfully, or have a way to clean up the data of already-committed writers.\n+   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n+   * writer can commit, or have a way to clean up the data of already-committed writers.\n    */\n   void commit(WriterCommitMessage[] messages);\n \n   /**\n    * Aborts this writing job because some data writers are failed to write the records and aborted,"
  }],
  "prId": 19623
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "\"may require manual cleanup\". It could be more than just \"garbage\", which implies filesystem temp data...it could be tables in a database or similar",
    "commit": "1e7dca3a57582dee4431878f21a95e5c8a28e56b",
    "createdAt": "2017-11-02T11:40:35Z",
    "diffHunk": "@@ -50,28 +53,34 @@\n \n   /**\n    * Creates a writer factory which will be serialized and sent to executors.\n+   *\n+   * If this method fails (by throwing an exception), the action would fail and no Spark job was\n+   * submitted.\n    */\n   DataWriterFactory<Row> createWriterFactory();\n \n   /**\n    * Commits this writing job with a list of commit messages. The commit messages are collected from\n-   * successful data writers and are produced by {@link DataWriter#commit()}. If this method\n-   * fails(throw exception), this writing job is considered to be failed, and\n-   * {@link #abort(WriterCommitMessage[])} will be called. The written data should only be visible\n-   * to data source readers if this method succeeds.\n+   * successful data writers and are produced by {@link DataWriter#commit()}.\n+   *\n+   * If this method fails (by throwing an exception), this writing job is considered to to have been\n+   * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination\n+   * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.\n    *\n    * Note that, one partition may have multiple committed data writers because of speculative tasks.\n    * Spark will pick the first successful one and get its commit message. Implementations should be\n-   * aware of this and handle it correctly, e.g., have a mechanism to make sure only one data writer\n-   * can commit successfully, or have a way to clean up the data of already-committed writers.\n+   * aware of this and handle it correctly, e.g., have a coordinator to make sure only one data\n+   * writer can commit, or have a way to clean up the data of already-committed writers.\n    */\n   void commit(WriterCommitMessage[] messages);\n \n   /**\n    * Aborts this writing job because some data writers are failed to write the records and aborted,\n    * or the Spark job fails with some unknown reasons, or {@link #commit(WriterCommitMessage[])}\n-   * fails. If this method fails(throw exception), the underlying data source may have garbage that\n-   * need to be cleaned manually, but these garbage should not be visible to data source readers.\n+   * fails.\n+   *\n+   * If this method fails (by throwing an exception), the underlying data source may have garbage\n+   * that need to be cleaned manually."
  }],
  "prId": 19623
}]