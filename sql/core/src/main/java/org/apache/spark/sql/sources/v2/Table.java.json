[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "#21306 (TableCatalog support) adds this class as `org.apache.spark.sql.catalog.v2.Table` in the `spark-catalyst` module. I think it needs to be in the catalyst module and should probably be in the `o.a.s.sql.catalog.v2` package as well.\r\n\r\nThe important one is moving this to the catalyst module. The analyzer is in catalyst and all of the v2 logical plans and analysis rules will be in catalyst as well, because we are standardizing behavior. The standard validation rules should be in catalyst, not in a source-specific or hive-specific package in the sql-core or hive modules.\r\n\r\nBecause the logical plans and validation rules are in the catalyst package, the `TableCatalog` API needs to be there as well. For example, when a [catalog table identifier](https://github.com/apache/spark/pull/21978) is resolved for a read query, one of the results is a `TableCatalog` instance for the catalog portion of the identifier. That catalog is used to load the v2 table, which is then wrapped in a v2 relation for further analysis. Similarly, the write path should also validate that the catalog exists during analysis by loading it, and would then pass the catalog in a v2 logical plan for `CreateTable` or `CreateTableAsSelect`.\r\n\r\nI also think that it makes sense to use the `org.apache.spark.sql.catalog.v2` package for `Table` because `Table` is more closely tied to the `TableCatalog` API than to the data source API. The link to DSv2 is that `Table` carries `newScanBuilder`, but the rest of the methods exposed by `Table` are for catalog functions, like inspecting a table's partitioning or table properties.\r\n\r\nMoving this class would make adding `TableCatalog` less intrusive.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T01:08:16Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Moving this to the Catalyst package would set a precedent for user-overridable behavior to live in the catalyst project. I'm not aware of anything in the Catalyst package being considered as public API right now. Are we allowed to start such a convention at this juncture?",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T01:28:53Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Everything in catalyst is considered private (although public visibility for debugging) and it's best to stay that way.\r\n",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T01:38:45Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "why does this `Table` API need to be in catalyst? It's not even a plan. We can define a table LogicalPlan interface in catalyst, and implement it in the SQL module with this `Table` API.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T04:03:34Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I can understand wanting to keep everything in Catalyst private. That's fine with me, but I think that Catalyst does need to be able to interact with tables and catalogs that are supplied by users.\r\n\r\nFor example: Our tables support schema evolution. Specifically, reading files that were written before a column was added. When we add a column, Spark shouldn't start failing in analysis for an AppendData operation in a scheduled job (as it would today). We need to be able to signal to the validation rule that the table supports reading files that are missing columns, so that Spark can do the right validation and allow writes that used to work to continue.\r\n\r\nHow would that information -- support for reading missing columns -- be communicated to the analyzer?\r\n\r\nAlso, what about my example above: how will the analyzer load tables using a user-supplied catalog if catalyst can't use any user-supplied implementations?\r\n\r\nWe could move all of the v2 analysis rules, like ResolveRelations, into the core module, but it seems to me that this requirement is no longer providing value if we have to do that. I think that catalyst is the right place for common plans and analysis rules to live because it is the library of common SQL components.\r\n\r\nWherever the rules and plans end up, they will need to access to the `TableCatalog` API.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T19:39:06Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "It's unclear to me what would be the best choice:\r\n1. move data source API to catalyst module\r\n2. move data source related rules to SQL core module\r\n3. define private catalog related APIs in catalyst module and implement them in SQL core\r\n\r\nCan we delay the discussion when we have a PR to add catalog support after the refactor?",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-28T06:52:01Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "> Can we delay the discussion when we have a PR to add catalog support after the refactor?\r\n\r\nYes, that works.\r\n\r\nBut, can we move `Table` to the `org.apache.spark.sql.catalog.v2` package where `TableCatalog` is defined in the other PR? I think `Table` should be defined with the catalog API and moving that later would require import changes to any file that references `Table`.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-28T16:54:14Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "for other reviewers: in the ds v2 community sync, we decided to move data source v2 into a new module `sql-api`, and make catalyst depends on it. This will be done in a followup.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-29T03:54:57Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I just went to make this change, but it requires moving any SQL class from catalyst referenced by the API into the API module as well... Let's discuss the options more on the dev list thread.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-30T20:38:27Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;",
    "line": 18
  }],
  "prId": 23086
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "`DataSourceOptions` isn't simply a map for two main reasons that I can tell: first, it forces options to be case insensitive, and second, it exposes helper methods to identify tables, like `tableName`, `databaseName`, and `paths`. In the new abstraction, the second use of `DataSourceOptions` is no longer needed. The table is already instantiated by the time that this is called.\r\n\r\nWe should to reconsider `DataSourceOptions`. The `tableName` methods aren't needed and we also no longer need to forward properties from the session config because the way tables are configured has changed (catalogs handle that). I think we should remove this class and instead use the more direct implementation, `CaseInsensitiveStringMap` from #21306. The behavior of that class is obvious from its name and it would be shared between the v2 APIs, both catalog and data source.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T01:32:22Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * An interface representing a logical structured data set of a data source. For example, the\n+ * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n+ * catalog, etc.\n+ *\n+ * This interface can mixin the following interfaces to support different operations:\n+ * <ul>\n+ *   <li>{@link SupportsBatchRead}: this table can be read in batch queries.</li>\n+ * </ul>\n+ */\n+@Evolving\n+public interface Table {\n+\n+  /**\n+   * Returns the schema of this table.\n+   */\n+  StructType schema();\n+\n+  /**\n+   * Returns a {@link ScanBuilder} which can be used to build a {@link Scan} later. Spark will call\n+   * this method for each data scanning query.\n+   *\n+   * The builder can take some query specific information to do operators pushdown, and keep these\n+   * information in the created {@link Scan}.\n+   */\n+  ScanBuilder newScanBuilder(DataSourceOptions options);",
    "line": 58
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Makes sense to me - `DataSourceOptions` was carrying along identifiers that really belong to a table identifier and that should be interpreted at the catalog level, not the data read level. In other words the implementation of this `Table` should already know _what_ locations to look up (e.g. \"files comprising dataset D\"), now it's a matter of _how_ (e.g. pushdown, filter predicates).",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T02:27:54Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * An interface representing a logical structured data set of a data source. For example, the\n+ * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n+ * catalog, etc.\n+ *\n+ * This interface can mixin the following interfaces to support different operations:\n+ * <ul>\n+ *   <li>{@link SupportsBatchRead}: this table can be read in batch queries.</li>\n+ * </ul>\n+ */\n+@Evolving\n+public interface Table {\n+\n+  /**\n+   * Returns the schema of this table.\n+   */\n+  StructType schema();\n+\n+  /**\n+   * Returns a {@link ScanBuilder} which can be used to build a {@link Scan} later. Spark will call\n+   * this method for each data scanning query.\n+   *\n+   * The builder can take some query specific information to do operators pushdown, and keep these\n+   * information in the created {@link Scan}.\n+   */\n+  ScanBuilder newScanBuilder(DataSourceOptions options);",
    "line": 58
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I agree with it. Since `CaseInsensitiveStringMap` is not in the code base yet, shall we do it in the followup?",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T04:06:49Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * An interface representing a logical structured data set of a data source. For example, the\n+ * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n+ * catalog, etc.\n+ *\n+ * This interface can mixin the following interfaces to support different operations:\n+ * <ul>\n+ *   <li>{@link SupportsBatchRead}: this table can be read in batch queries.</li>\n+ * </ul>\n+ */\n+@Evolving\n+public interface Table {\n+\n+  /**\n+   * Returns the schema of this table.\n+   */\n+  StructType schema();\n+\n+  /**\n+   * Returns a {@link ScanBuilder} which can be used to build a {@link Scan} later. Spark will call\n+   * this method for each data scanning query.\n+   *\n+   * The builder can take some query specific information to do operators pushdown, and keep these\n+   * information in the created {@link Scan}.\n+   */\n+  ScanBuilder newScanBuilder(DataSourceOptions options);",
    "line": 58
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Either in a follow-up or you can add the class in this PR. Either way works for me.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T18:44:24Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * An interface representing a logical structured data set of a data source. For example, the\n+ * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n+ * catalog, etc.\n+ *\n+ * This interface can mixin the following interfaces to support different operations:\n+ * <ul>\n+ *   <li>{@link SupportsBatchRead}: this table can be read in batch queries.</li>\n+ * </ul>\n+ */\n+@Evolving\n+public interface Table {\n+\n+  /**\n+   * Returns the schema of this table.\n+   */\n+  StructType schema();\n+\n+  /**\n+   * Returns a {@link ScanBuilder} which can be used to build a {@link Scan} later. Spark will call\n+   * this method for each data scanning query.\n+   *\n+   * The builder can take some query specific information to do operators pushdown, and keep these\n+   * information in the created {@link Scan}.\n+   */\n+  ScanBuilder newScanBuilder(DataSourceOptions options);",
    "line": 58
  }],
  "prId": 23086
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "It would be helpful for a `Table` to also expose a name or identifier of some kind. The `TableIdentifier` passed into `DataSourceV2Relation` is only used in `name` to identify the relation's table. If the name (or location for path-based tables) were supplied by the table instead, it would remove the need to pass it in the relation.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-27T21:19:43Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * An interface representing a logical structured data set of a data source. For example, the\n+ * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n+ * catalog, etc.\n+ *\n+ * This interface can mixin the following interfaces to support different operations:\n+ * <ul>\n+ *   <li>{@link SupportsBatchRead}: this table can be read in batch queries.</li>\n+ * </ul>\n+ */\n+@Evolving\n+public interface Table {",
    "line": 37
  }],
  "prId": 23086
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Do you think it's better to just ask implementations to override `toString`? cc @rdblue ",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-29T03:29:57Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * An interface representing a logical structured data set of a data source. For example, the\n+ * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n+ * catalog, etc.\n+ * <p>\n+ * This interface can mixin the following interfaces to support different operations:\n+ * </p>\n+ * <ul>\n+ *   <li>{@link SupportsBatchRead}: this table can be read in batch queries.</li>\n+ * </ul>\n+ */\n+@Evolving\n+public interface Table {\n+\n+  /**\n+   * A name to identify this table.\n+   * <p>\n+   * By default this returns the class name of this implementation. Please override it to provide a\n+   * meaningful name, like the database and table name from catalog, or the location of files for\n+   * this table.\n+   * </p>\n+   */\n+  default String name() {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I don't think this should have a default. Implementations should definitely implement this.\r\n\r\nI think there is a difference between `toString` and `name`. An implementation may choose to display `name` when showing a table's string representation, but may choose to include extra information to show more about the table state, like Iceberg's snapshot ID.",
    "commit": "eecb161075720aec0c496576fe6b6ad749c3a726",
    "createdAt": "2018-11-29T21:38:52Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.sources.v2.reader.Scan;\n+import org.apache.spark.sql.sources.v2.reader.ScanBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * An interface representing a logical structured data set of a data source. For example, the\n+ * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n+ * catalog, etc.\n+ * <p>\n+ * This interface can mixin the following interfaces to support different operations:\n+ * </p>\n+ * <ul>\n+ *   <li>{@link SupportsBatchRead}: this table can be read in batch queries.</li>\n+ * </ul>\n+ */\n+@Evolving\n+public interface Table {\n+\n+  /**\n+   * A name to identify this table.\n+   * <p>\n+   * By default this returns the class name of this implementation. Please override it to provide a\n+   * meaningful name, like the database and table name from catalog, or the location of files for\n+   * this table.\n+   * </p>\n+   */\n+  default String name() {"
  }],
  "prId": 23086
}]