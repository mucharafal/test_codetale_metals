[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "we need to add license header to this\n",
    "commit": "e0f427f9d4083068fefaed6a29719f0a7eeb22b3",
    "createdAt": "2016-01-15T02:28:47Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.spark.sql.execution.datasources.parquet;",
    "line": 17
  }],
  "prId": 10593
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "add a space here - otherwise this will fail the java style check now it is turned on\n",
    "commit": "e0f427f9d4083068fefaed6a29719f0a7eeb22b3",
    "createdAt": "2016-01-15T05:23:02Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.spark.sql.execution.datasources.parquet;\n+\n+import java.io.IOException;\n+\n+import org.apache.spark.sql.execution.vectorized.ColumnVector;\n+import org.apache.spark.unsafe.Platform;\n+\n+import org.apache.parquet.column.values.ValuesReader;\n+\n+/**\n+ * An implementation of the Parquet PLAIN decoder that supports the vectorized interface.\n+ */\n+public class VectorizedPlainValuesReader extends ValuesReader implements VectorizedValuesReader {\n+  private byte[] buffer;\n+  private int offset;\n+  private final int byteSize;\n+\n+  public VectorizedPlainValuesReader(int byteSize) {\n+    this.byteSize = byteSize;\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, byte[] bytes, int offset) throws IOException {\n+    this.buffer = bytes;\n+    this.offset = offset + Platform.BYTE_ARRAY_OFFSET;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    offset += byteSize;\n+  }\n+\n+  @Override\n+  public void skip(int n) {\n+    offset += n * byteSize;\n+  }\n+\n+  @Override\n+  public void readIntegers(int total, ColumnVector c, int rowId) {\n+    c.putIntsLittleEndian(rowId, total, buffer, offset - Platform.BYTE_ARRAY_OFFSET);\n+    offset += 4 * total;\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    int v = Platform.getInt(buffer, offset);\n+    offset += 4;\n+    return v;\n+  }\n+}",
    "line": 66
  }],
  "prId": 10593
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this is not this pr but i'm confused by putIntsLittleEndian. it just assumes it is using the native endianness, not little?\n\nis an implicit assumption here that \"buffer\" is encoded using the same endianness?\n",
    "commit": "e0f427f9d4083068fefaed6a29719f0a7eeb22b3",
    "createdAt": "2016-01-15T05:25:36Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.spark.sql.execution.datasources.parquet;\n+\n+import java.io.IOException;\n+\n+import org.apache.spark.sql.execution.vectorized.ColumnVector;\n+import org.apache.spark.unsafe.Platform;\n+\n+import org.apache.parquet.column.values.ValuesReader;\n+\n+/**\n+ * An implementation of the Parquet PLAIN decoder that supports the vectorized interface.\n+ */\n+public class VectorizedPlainValuesReader extends ValuesReader implements VectorizedValuesReader {\n+  private byte[] buffer;\n+  private int offset;\n+  private final int byteSize;\n+\n+  public VectorizedPlainValuesReader(int byteSize) {\n+    this.byteSize = byteSize;\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, byte[] bytes, int offset) throws IOException {\n+    this.buffer = bytes;\n+    this.offset = offset + Platform.BYTE_ARRAY_OFFSET;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    offset += byteSize;\n+  }\n+\n+  @Override\n+  public void skip(int n) {\n+    offset += n * byteSize;\n+  }\n+\n+  @Override\n+  public void readIntegers(int total, ColumnVector c, int rowId) {\n+    c.putIntsLittleEndian(rowId, total, buffer, offset - Platform.BYTE_ARRAY_OFFSET);",
    "line": 56
  }, {
    "author": {
      "login": "nongli"
    },
    "body": "I'll update the comment in putIntsLittleEndian. It is assuming that the input byte array contains little endian encoded integers. The API does not care what the host machine's endianness is. i.e. parquet always stores it as little endian, the column vector has to figur eit out.\n",
    "commit": "e0f427f9d4083068fefaed6a29719f0a7eeb22b3",
    "createdAt": "2016-01-15T19:23:43Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.spark.sql.execution.datasources.parquet;\n+\n+import java.io.IOException;\n+\n+import org.apache.spark.sql.execution.vectorized.ColumnVector;\n+import org.apache.spark.unsafe.Platform;\n+\n+import org.apache.parquet.column.values.ValuesReader;\n+\n+/**\n+ * An implementation of the Parquet PLAIN decoder that supports the vectorized interface.\n+ */\n+public class VectorizedPlainValuesReader extends ValuesReader implements VectorizedValuesReader {\n+  private byte[] buffer;\n+  private int offset;\n+  private final int byteSize;\n+\n+  public VectorizedPlainValuesReader(int byteSize) {\n+    this.byteSize = byteSize;\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, byte[] bytes, int offset) throws IOException {\n+    this.buffer = bytes;\n+    this.offset = offset + Platform.BYTE_ARRAY_OFFSET;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    offset += byteSize;\n+  }\n+\n+  @Override\n+  public void skip(int n) {\n+    offset += n * byteSize;\n+  }\n+\n+  @Override\n+  public void readIntegers(int total, ColumnVector c, int rowId) {\n+    c.putIntsLittleEndian(rowId, total, buffer, offset - Platform.BYTE_ARRAY_OFFSET);",
    "line": 56
  }],
  "prId": 10593
}]