[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I don't have a better naming in mind, so I leave it as `WriteSupport` for now. Better naming is welcome to match `Scan`!",
    "commit": "693fb986e54ef8f7d09d6c028d18df50d2db117e",
    "createdAt": "2018-12-04T04:12:51Z",
    "diffHunk": "@@ -25,14 +25,14 @@\n import org.apache.spark.sql.types.StructType;\n \n /**\n- * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n+ * A mix-in interface for {@link Table}. Data sources can implement this interface to\n  * provide data writing ability for batch processing.\n  *\n  * This interface is used to create {@link BatchWriteSupport} instances when end users run"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "What if we just call it BatchWrite",
    "commit": "693fb986e54ef8f7d09d6c028d18df50d2db117e",
    "createdAt": "2018-12-13T04:10:38Z",
    "diffHunk": "@@ -25,14 +25,14 @@\n import org.apache.spark.sql.types.StructType;\n \n /**\n- * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n+ * A mix-in interface for {@link Table}. Data sources can implement this interface to\n  * provide data writing ability for batch processing.\n  *\n  * This interface is used to create {@link BatchWriteSupport} instances when end users run"
  }],
  "prId": 23208
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "To me, it is quite confusing to have `BatchWriteSupport` and  `SupportsBatchWrite`.",
    "commit": "693fb986e54ef8f7d09d6c028d18df50d2db117e",
    "createdAt": "2018-12-06T13:38:30Z",
    "diffHunk": "@@ -25,14 +25,14 @@\n import org.apache.spark.sql.types.StructType;\n \n /**\n- * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n+ * A mix-in interface for {@link Table}. Data sources can implement this interface to\n  * provide data writing ability for batch processing.\n  *\n  * This interface is used to create {@link BatchWriteSupport} instances when end users run\n  * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @Evolving\n-public interface BatchWriteSupportProvider extends DataSourceV2 {\n+public interface SupportsBatchWrite extends Table {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "That's why I left https://github.com/apache/spark/pull/23208#discussion_r238524973 .\r\n\r\nnamings are welcome!",
    "commit": "693fb986e54ef8f7d09d6c028d18df50d2db117e",
    "createdAt": "2018-12-06T14:20:35Z",
    "diffHunk": "@@ -25,14 +25,14 @@\n import org.apache.spark.sql.types.StructType;\n \n /**\n- * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n+ * A mix-in interface for {@link Table}. Data sources can implement this interface to\n  * provide data writing ability for batch processing.\n  *\n  * This interface is used to create {@link BatchWriteSupport} instances when end users run\n  * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @Evolving\n-public interface BatchWriteSupportProvider extends DataSourceV2 {\n+public interface SupportsBatchWrite extends Table {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "`Table` exposes `newScanBuilder` without an interface. Why should the write side be different? Doesn't Spark support sources that are read-only and write-only?\r\n\r\nI think that both reads and writes should use interfaces to mix support into `Table` or both should be exposed by `Table` and throw `UnsupportedOperationException` by default, not a mix of the two options.\r\n\r\nIf `newWriteBuilder` were added to `Table`, then this interface wouldn't be necessary and the name problem goes away.",
    "commit": "693fb986e54ef8f7d09d6c028d18df50d2db117e",
    "createdAt": "2018-12-06T21:05:22Z",
    "diffHunk": "@@ -25,14 +25,14 @@\n import org.apache.spark.sql.types.StructType;\n \n /**\n- * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n+ * A mix-in interface for {@link Table}. Data sources can implement this interface to\n  * provide data writing ability for batch processing.\n  *\n  * This interface is used to create {@link BatchWriteSupport} instances when end users run\n  * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @Evolving\n-public interface BatchWriteSupportProvider extends DataSourceV2 {\n+public interface SupportsBatchWrite extends Table {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I do think read-only or write-only is a necessary feature, according to what I've seen in the dev list. Maybe we should move `newScanBuilder` from `Table` to the mixin traits.",
    "commit": "693fb986e54ef8f7d09d6c028d18df50d2db117e",
    "createdAt": "2018-12-07T03:07:16Z",
    "diffHunk": "@@ -25,14 +25,14 @@\n import org.apache.spark.sql.types.StructType;\n \n /**\n- * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n+ * A mix-in interface for {@link Table}. Data sources can implement this interface to\n  * provide data writing ability for batch processing.\n  *\n  * This interface is used to create {@link BatchWriteSupport} instances when end users run\n  * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @Evolving\n-public interface BatchWriteSupportProvider extends DataSourceV2 {\n+public interface SupportsBatchWrite extends Table {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I'm fine either way, as long as we are consistent between the read and write sides.",
    "commit": "693fb986e54ef8f7d09d6c028d18df50d2db117e",
    "createdAt": "2018-12-07T17:40:47Z",
    "diffHunk": "@@ -25,14 +25,14 @@\n import org.apache.spark.sql.types.StructType;\n \n /**\n- * A mix-in interface for {@link DataSourceV2}. Data sources can implement this interface to\n+ * A mix-in interface for {@link Table}. Data sources can implement this interface to\n  * provide data writing ability for batch processing.\n  *\n  * This interface is used to create {@link BatchWriteSupport} instances when end users run\n  * {@code Dataset.write.format(...).option(...).save()}.\n  */\n @Evolving\n-public interface BatchWriteSupportProvider extends DataSourceV2 {\n+public interface SupportsBatchWrite extends Table {"
  }],
  "prId": 23208
}]