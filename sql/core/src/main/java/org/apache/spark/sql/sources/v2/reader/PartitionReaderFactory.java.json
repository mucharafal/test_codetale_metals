[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: should be \"A factory used to create `{@link PartitionReader}` instances\".",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T18:40:18Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import java.io.Serializable;\n+\n+import org.apache.spark.annotation.InterfaceStability;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * A factory of {@link PartitionReader}s. Implementations can do either row-based scan or columnar"
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Can we update this to accept `InputPartition`? That would make it possible to use columnar scans for some input splits and row-based scans for others. That's helpful when a table has mixed formats, like Hive tables that are converted from Sequence to Parquet and have both formats.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T18:41:50Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import java.io.Serializable;\n+\n+import org.apache.spark.annotation.InterfaceStability;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * A factory of {@link PartitionReader}s. Implementations can do either row-based scan or columnar\n+ * scan, by switching the {@link #supportColumnarReads()} flag.\n+ */\n+@InterfaceStability.Evolving\n+public interface PartitionReaderFactory extends Serializable {\n+\n+  /**\n+   * Returns a row-based partition reader to read data from the given {@link InputPartition}.\n+   *\n+   * Implementations probably need to cast the input partition to the concrete\n+   * {@link InputPartition} class defined for the data source.\n+   *\n+   * If this method fails (by throwing an exception), the corresponding Spark task would fail and\n+   * get retried until hitting the maximum retry times.\n+   */\n+  PartitionReader<InternalRow> createReader(InputPartition partition);\n+\n+  /**\n+   * Returns a columnar partition reader to read data from the given {@link InputPartition}.\n+   *\n+   * Implementations probably need to cast the input partition to the concrete\n+   * {@link InputPartition} class defined for the data source.\n+   *\n+   * If this method fails (by throwing an exception), the corresponding Spark task would fail and\n+   * get retried until hitting the maximum retry times.\n+   */\n+  default PartitionReader<ColumnarBatch> createColumnarReader(InputPartition partition) {\n+    throw new UnsupportedOperationException(\"Cannot create columnar reader.\");\n+  }\n+\n+  /**\n+   * If this method returns true, Spark will call {@link #createColumnarReader(InputPartition)} to\n+   * create the {@link PartitionReader} and scan the data in a columnar way. This means,\n+   * implementations must also implement {@link #createColumnarReader(InputPartition)} when true\n+   * is returned here.\n+   */\n+  default boolean supportColumnarReads() {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "good idea!",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T02:13:37Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import java.io.Serializable;\n+\n+import org.apache.spark.annotation.InterfaceStability;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * A factory of {@link PartitionReader}s. Implementations can do either row-based scan or columnar\n+ * scan, by switching the {@link #supportColumnarReads()} flag.\n+ */\n+@InterfaceStability.Evolving\n+public interface PartitionReaderFactory extends Serializable {\n+\n+  /**\n+   * Returns a row-based partition reader to read data from the given {@link InputPartition}.\n+   *\n+   * Implementations probably need to cast the input partition to the concrete\n+   * {@link InputPartition} class defined for the data source.\n+   *\n+   * If this method fails (by throwing an exception), the corresponding Spark task would fail and\n+   * get retried until hitting the maximum retry times.\n+   */\n+  PartitionReader<InternalRow> createReader(InputPartition partition);\n+\n+  /**\n+   * Returns a columnar partition reader to read data from the given {@link InputPartition}.\n+   *\n+   * Implementations probably need to cast the input partition to the concrete\n+   * {@link InputPartition} class defined for the data source.\n+   *\n+   * If this method fails (by throwing an exception), the corresponding Spark task would fail and\n+   * get retried until hitting the maximum retry times.\n+   */\n+  default PartitionReader<ColumnarBatch> createColumnarReader(InputPartition partition) {\n+    throw new UnsupportedOperationException(\"Cannot create columnar reader.\");\n+  }\n+\n+  /**\n+   * If this method returns true, Spark will call {@link #createColumnarReader(InputPartition)} to\n+   * create the {@link PartitionReader} and scan the data in a columnar way. This means,\n+   * implementations must also implement {@link #createColumnarReader(InputPartition)} when true\n+   * is returned here.\n+   */\n+  default boolean supportColumnarReads() {"
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "nit: may be relaxed, we shouldn't guarantee it",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T17:24:15Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader;\n+\n+import java.io.Serializable;\n+\n+import org.apache.spark.annotation.InterfaceStability;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * A factory used to create {@link PartitionReader} instances.\n+ */\n+@InterfaceStability.Evolving\n+public interface PartitionReaderFactory extends Serializable {\n+\n+  /**\n+   * Returns a row-based partition reader to read data from the given {@link InputPartition}.\n+   *\n+   * Implementations probably need to cast the input partition to the concrete\n+   * {@link InputPartition} class defined for the data source.\n+   *\n+   * If this method fails (by throwing an exception), the corresponding Spark task would fail and\n+   * get retried until hitting the maximum retry times.\n+   */\n+  PartitionReader<InternalRow> createReader(InputPartition partition);\n+\n+  /**\n+   * Returns a columnar partition reader to read data from the given {@link InputPartition}.\n+   *\n+   * Implementations probably need to cast the input partition to the concrete\n+   * {@link InputPartition} class defined for the data source.\n+   *\n+   * If this method fails (by throwing an exception), the corresponding Spark task would fail and\n+   * get retried until hitting the maximum retry times.\n+   */\n+  default PartitionReader<ColumnarBatch> createColumnarReader(InputPartition partition) {\n+    throw new UnsupportedOperationException(\"Cannot create columnar reader.\");\n+  }\n+\n+  /**\n+   * Returns true if the given {@link InputPartition} should be read by Spark in a columnar way.\n+   * This means, implementations must also implement {@link #createColumnarReader(InputPartition)}\n+   * for the input partitions that this method returns true.\n+   *\n+   * As of Spark 2.4, Spark can only read all input partition in a columnar way, or none of them.\n+   * Data source can't mix columnar and row-based partitions. This will be relaxed in future"
  }],
  "prId": 22009
}]