[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why must this be JSON and why must it be a String? Why not byte[] and let the implementation choose the representation it prefers?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:15:29Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "The offsets are ultimately exposed as JSON inside the JSON representation of StreamingQueryProgress. It's important for visibility and debuggability that progress events contain human-readable representations of offsets.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:22:56Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "If Spark uses JSON to serialize, why can't Spark handle deserialization itself? \r\n\r\nWhy not require `Offset` to have a human-readable `toString` and a `toBytes` for serialization? We don't have to conflate serialization with human readability.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T21:24:24Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Currently, there are two representations of any given offset: a connector-defined JVM object and a serialized JSON string.\r\n\r\nSpark can't build the JVM object itself because it doesn't know what the right type is. If you know of some clean way for a connector to declare \"here is the type of my offsets for you to instantiate\", we should do that instead, but I only know how to do it through reflection magic more confusing than the status quo.\r\n\r\nI'd hesitate to introduce a third representation unless there's some concrete use case where JSON serialization won't work well.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T21:33:37Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "If Spark can serialize the offset based on the Offset interface, then it should deserialize to a generic Offset and use that to create a source-specific Offset. But then the Offset can't have any implementation-specific information, so why is the concrete class determined by the source anyway? This seems to me like a concrete case where the abstraction is failing.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T00:17:23Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I think I understand what you're saying. I could get behind a proposal to simply define \"arbitrary JSON string\" as the one and only offset type, with each connector responsible for writing and parsing JSON however it'd like. All the existing offsets are trivial case classes anyway; it'd be a bit of a migration, but nothing architecturally difficult to handle.\r\n\r\nI don't see how a `toBytes` method would help the problem. Neither arbitrary byte arrays nor arbitrary JSON strings let Spark know what type it's supposed to instantiate.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T00:30:46Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I guess my core point is, we should stick with the existing serialization mechanism unless there's some kind of serialization we need to do which only a byte array can express. The serialization mechanism reaches deep into the execution layer, so coupling it with a connector API revamp is awkward.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T00:34:43Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "What I'm trying to say is this: either delegate serialization and deserialization entirely to the source -- in which case it passes you byte[] -- or standardize both the serialized representation and the class you will pass to the source.\r\n\r\nI don't think it makes any sense for Spark to serialize to JSON and require that the source can deserialize it. Alternatively, if the source produces and consumes that JSON why are we forcing the source to use JSON?\r\n\r\nHaving a human-readable representation isn't a good justification for this because we have a ton of other objects that get serialized and aren't required to have a human-readable serialized form. Why is this different? If we care about the serialized representation so much, why allow customization of the Offset classes?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T15:47:22Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "As I said, I'm fine with defining arbitrary JSON strings as the single non-customizable offset type, if you think that would be better. (I think they would have to be strings, because making a JSON object the type would mean packaging some JSON library into the API.) I don't think it would ever be correct to have an Offset implementation which doesn't trivially reduce to a key-value map, so the only thing we'd lose by doing that is type safety within the connectors.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T16:05:04Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@cloud-fan, I think this method should be removed from the API.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T18:53:36Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is an existing API, if we remove it, all the streaming sources can't work...",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-10T01:30:58Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Sorry, to be clear, this method was in the original streaming design doc https://docs.google.com/document/d/1VzxEuvpLfuHKL6vJO9qJ6ug0x9J_gLoLSH_vJL3-Cho which was sent out 2 months ago. If the reworking in this PR has made you realize there's a better way to do things, I think we should absolutely consider the alternative. But we can't just remove random methods without a complete proposal for what should replace them.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-10T15:40:52Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@jose-torres, that's a fair point, this isn't a great place to decide whether serialization should be JSON or not.\r\n\r\nIt also looks like both serialization and deserialization is delegated to the source in that design doc, since Offset implementations must have a `json` method. That was a big part of my concern I pointed out, so requiring a source to provide both should be okay.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-14T16:47:02Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);",
    "line": 42
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Should this be `oldestOffset`?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:16:01Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the",
    "line": 30
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Streaming-centric sources won't always have the initial offset be the oldest offset. In the Kafka source, for instance, the default is actually to start from the newest offset.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:26:20Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the",
    "line": 30
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I was thinking oldest available, but it's a minor point.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T21:24:43Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the",
    "line": 30
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I think this should accept a `ScanConfig`. The read support is general and can create multiple scans. It should not keep state about any one scan. That's something the `ScanConfig` should do.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:25:09Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "For commit, the only thing it's interested in is the end offset. Even we pass in a `ScanConfig`, I think the implementation would just get the end offset from the `ScanConfig` and commit.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T02:23:58Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I'd +1 passing a ScanConfig. I agree that all the existing sources are just going to pull out the offset, but \"Spark is finished with this scan\" is a cleaner semantic than \"Spark is finished with the scan such that it goes up to this offset\".",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T17:44:39Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "In Kafka, isn't the ScanConfig what identifies the consumer group? @jose-torres ",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T18:54:43Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Not in the current implementation.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T20:00:17Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Why not? A `ScanConfig` represents the scan. The thing that is scanned shouldn't store the scan state.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T22:08:23Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I'm not sure what you mean by \"scan state\" here. The thing that is scanned needs to know what offsets are available for scanning, which requires holding a Kafka consumer to read that information.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T22:10:50Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I tried it but this can't work for continuous streaming.\r\n\r\nThe `ScanConfig` in continuous streaming represents the scan from a certain offset, but it doesn't know when it ends. It's the streaming engine that tracks the offset we have committed to the sink and call this method.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-10T16:10:20Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I don't understand what you mean. A scan should be associated with a consumer or consumer group. The offset that is being updated is for that consumer, not the topic. And because the ReadSupport object corresponds to the topic in this abstraction (the thing that can be scanned), it shouldn't be what tracks the consumer or consumer group.\r\n\r\nAll we need to do to fix the API is to pass in ScanConfig here.\r\n\r\n@cloud-fan, I'm not sure what you mean that this doesn't work. Why would tracking the consumer in the ScanConfig for a continuous streaming job not work?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-13T17:43:54Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "There are two consumer groups in streaming:\r\n\r\n1. The one at the driver, which determines what offsets are available to scan.\r\n2. The one distributed across the executors which actually performs the scan.\r\n\r\nThis method is used to commit certain offsets in group 1, based on the offsets which have been logged as processed by group 2. In microbatch mode, this happens to work with ScanConfig, because there is one ScanConfig for each offset log entry. In continuous mode there is one ScanConfig corresponding to an indefinite number of offset log entries, so ScanConfig does not provide the information required to commit any particular entry.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-13T17:58:12Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "> Why would tracking the consumer in the ScanConfig for a continuous streaming job not work?\r\n\r\nIt's spark (the streaming engine) that tracks the consumers, not streaming source. If you insist to pass a `ScanConfig`, the only way I can think of is to make `ScanConfig` mutable, then Spark sets the end offset to the `ScanConfig` and calls this `commit`. This is a really bad API design and we should not do it. I'd like to keep this `commit` method as it is.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-14T13:10:51Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Okay, I think I see where the misunderstanding is coming from: the current Spark implementation keeps track of offsets itself and doesn't commit those offsets in an external system. That makes sense and I would not want Spark to make the ScanConfig mutable to update it with the scan's current offset.\r\n\r\nHowever, as a streaming API I think that it is entirely possible for an implementation to commit offsets to an external system based on a scan, so it makes sense to me to pass ScanConfig in for that purpose, when a ScanConfig does represent a single consumer of a scan.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-14T16:39:32Z",
    "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.reader.streaming;\n+\n+import org.apache.spark.sql.sources.v2.reader.ReadSupport;\n+\n+/**\n+ * A base interface for streaming read support. This is package private and is invisible to data\n+ * sources. Data sources should implement concrete streaming read support interfaces:\n+ * {@link MicroBatchReadSupport} or {@link ContinuousReadSupport}.\n+ */\n+interface StreamingReadSupport extends ReadSupport {\n+\n+  /**\n+   * Returns the initial offset for a streaming query to start reading from. Note that the\n+   * streaming data source should not assume that it will start reading from its\n+   * {@link #initialOffset()} value: if Spark is restarting an existing query, it will restart from\n+   * the check-pointed offset rather than the initial one.\n+   */\n+  Offset initialOffset();\n+\n+  /**\n+   * Deserialize a JSON string into an Offset of the implementation-defined offset type.\n+   *\n+   * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader\n+   */\n+  Offset deserializeOffset(String json);\n+\n+  /**\n+   * Informs the source that Spark has completed processing all data for offsets less than or\n+   * equal to `end` and will only request offsets greater than `end` in the future.\n+   */\n+  void commit(Offset end);",
    "line": 48
  }],
  "prId": 22009
}]