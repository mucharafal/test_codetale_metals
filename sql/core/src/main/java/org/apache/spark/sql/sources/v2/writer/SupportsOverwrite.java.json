[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This reminds me of the SQL MERGE command:\r\nhttps://en.wikipedia.org/wiki/Merge_(SQL)\r\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=sql-server-2017\r\n\r\nSQL has 3 standard simple date changing commands: INSERT, UPDATE, DELETE.\r\nINSERT is Append in DS v2.\r\nUPDATE and DELETE are special. They do not have a source table, they just change the data of the target table directly. I'd say they are not data writing, and we should exclude them from ds v2 write API.\r\n\r\nMERGE(also called UPSERT) is a more powerful version of INSERT. It uses the source table to update the target table. For each row in the target table, find the matched row in the source table w.r.t. a condition, and update the row in the target table according to the matched row.\r\n\r\nThis `SupportsOverwrite` here looks like just a DELETE + INSERT, and I don't know if there is any SQL command has the same semantic. Can you give some use cases of this overwrite?",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-01-22T04:28:41Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "This is needed for a couple use cases:\r\n1. `INSERT OVERWRITE ... PARTITION ...` with static partitions in static overwrite mode, for compatibility with existing queries.\r\n2. Situations where users would currently call `INSERT OVERWRITE`, but should be explicit.\r\n\r\nThis operation is a combined delete and replace, [ReplaceData in the SPIP](https://docs.google.com/document/d/1gYm5Ji2Mge3QBdOliFV5gSPTKlX4q1DCBXIkiyMv62A/edit?ts=5a987801#heading=h.vgt28bdlilsp). This is currently implemented by the existing `INSERT OVERWRITE` statement.\r\n\r\n`INSERT OVERWRITE` (as defined by Hive) is dynamic so what gets deleted is implicit. Implicit deletes lead to confusion, so it would be better to move users to patterns that explicitly replace data.\r\n\r\nAn example of such a pattern is an hourly job that is idempotent and produces a summary for a day. Each hour, it overwrites the summary from the last hour, until the day is finished and the last summary is left. We are moving users to structure this query to explicitly overwrite using an expression, which we can also use to warn if the user writes data that wouldn't be replaced by running the query again.\r\n\r\nA second use for this is static overwrite mode, which is a Spark-only behavior. In static mode, Spark will drop any matching static partitions and then insert data. In that case, we can't use dynamic overwrite because it wouldn't delete all of the static partitions. Instead, we translate the static partitions to an overwrite expression. So using this, we can implement all overwrite behaviors using the v2 API.",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-01-23T01:23:47Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah i see, make senses. So this `SupportsOverwrite` is mostly for partitioned tables, though it doesn't mention partition at all. I think the semantic can also apply to non-partitioned tables but that will be very hard to implement.",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-01-23T02:45:45Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "BTW for a normal INSERT OVERWRITE, which needs to truncate the entire table, the `filters` will be a single true literal?",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-01-23T02:47:12Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "> I think the semantic can also apply to non-partitioned tables but that will be very hard to implement.\r\n\r\nNot necessarily. JDBC sources can implement this fairly easily, and those that support transactions can make it an atomic operation. There are also strategies that can work for unpartitioned tables, like deleting data files using min/max ranges show all rows are matched by a filter.\r\n\r\n> BTW for a normal INSERT OVERWRITE, which needs to truncate the entire table, the filters will be a single true literal?\r\n\r\nI think that truncate is slightly different. Because it is fairly easy to support truncate, but not overwrite by expression, I think that truncate should be a separate operation in the v2 API. I would make `SupportsOverwrite` implement `SupportsTruncate` with a default that calls overwrite with `true` like you suggest, but I think we will need to add a `true` filter.\r\n\r\nAlso, what do you mean by \"normal INSERT OVERWRITE\"? What operation is that? Right now, `INSERT OVERWRITE` is effectively dynamic partition overwrite. Unpartitioned tables are truncated because they have just one \"partition\". Do you agree with that summary?",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-01-23T17:28:38Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "> Unpartitioned tables are truncated because they have just one \"partition\"\r\n\r\nIf we go with this definition, do you mean data sources that do not support partition must implement `SupportsDynamicOverwrite` to support `INSERT OVERWRITE`?",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-01-24T02:32:45Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@cloud-fan, I've added `SupportsTruncate` to address this. Now tables can support truncation to implement `INSERT OVERWRITE` that replaces all data in the table.\r\n\r\nI also ran tests to find out how file system data sources behave with overwrite mode in `DataFrameWriter`. That showed that tables are always truncated and the value of `spark.sql.sources.partitionOverwriteMode` is always ignored.",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-01-27T23:04:10Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder {"
  }],
  "prId": 23606
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can we add it later? This PR does not show how it will be used, and only uses `SupportsTruncate`",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-02-01T02:06:10Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.AlwaysTrue$;\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder, SupportsTruncate {",
    "line": 29
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "nvm, I got it. Both truncate and overwriteByExpression will be handled by one plan: OverwriteByExpression",
    "commit": "0e42cc28b173f55c645c863732fb13d1343d4d75",
    "createdAt": "2019-02-01T02:11:00Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2.writer;\n+\n+import org.apache.spark.sql.sources.AlwaysTrue$;\n+import org.apache.spark.sql.sources.Filter;\n+\n+/**\n+ * Write builder trait for tables that support overwrite by filter.\n+ * <p>\n+ * Overwriting data by filter will delete any data that matches the filter and replace it with data\n+ * that is committed in the write.\n+ */\n+public interface SupportsOverwrite extends WriteBuilder, SupportsTruncate {",
    "line": 29
  }],
  "prId": 23606
}]