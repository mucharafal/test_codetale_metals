[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "we need a specialized accumulator in order to avoid object allocation in +=.\n\n@zsxwing can you just quickly hack together a LongAccumulator? \n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T05:34:00Z",
    "diffHunk": "@@ -34,9 +34,16 @@ private[sql] case class LocalTableScan(\n \n   protected override def doExecute(): RDD[InternalRow] = rdd\n \n+  override lazy val accumulators = Map(\n+    \"numRows\" -> sparkContext.internalAccumulator(0L, \"number of rows\"))\n+\n   override def executeCollect(): Array[Row] = {\n     val converter = CatalystTypeConverters.createToScalaConverter(schema)\n-    rows.map(converter(_).asInstanceOf[Row]).toArray\n+    val numRows = accumulator[Long](\"numRows\")"
  }],
  "prId": 7774
}]