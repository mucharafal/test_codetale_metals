[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This class seems to be copied & pasted version of GraphUIData with addition of a method. Please leave comments about where you copied from in code diff wherever you copied so that reviewers can save their time.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-10-22T22:05:34Z",
    "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.{ArrayBuffer, HashSet}\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[HashSet[(StreamingQuery, Long)]])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/sql/streaming/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/sql/streaming/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = UIUtils.stripXSS(request.getParameter(\"id\"))\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.find(i => i._1.runId.equals(UUID.fromString(parameterId)))\n+        .getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        UIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${UIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{withNoProgress(query, { query.lastProgress.batchId + 1L }, \"NaN\")}</strong>\n+      completed batches, <strong>{query.getTotalInputRecords}</strong> records)\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime = withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime,\n+      0L)\n+    val maxBatchTime = withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime,\n+      0L)\n+    val maxRecordRate = withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max,\n+      0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate = withNoProgress(query,\n+      query.recentProgress.map(_.processedRowsPerSecond).max, 0L)\n+    val minProcessRate = 0L\n+    val maxRows = withNoProgress(query, query.recentProgress.map(_.numInputRows).max, 0L)\n+    val minRows = 0L\n+    val maxBatchDuration = withNoProgress(query, query.recentProgress.map(_.batchDuration).max, 0L)\n+    val minBatchDuration = 0L\n+\n+    val inputRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.inputRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val processRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.processedRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val inputRowsData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.numInputRows })),\n+      Array.empty[(Long, Double)])\n+    val batchDurations = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.batchDuration })),\n+      Array.empty[(Long, Double)])\n+    val operationDurationData = withNoProgress(query, query.recentProgress.map { case p =>\n+      val durationMs = p.durationMs\n+      durationMs.remove(\"triggerExecution\")\n+      (df.parse(p.timestamp).getTime, durationMs)},\n+      Array.empty[(Long, ju.Map[String, JLong])])\n+    val operationLabels = withNoProgress(query, {\n+        val durationKeys = query.lastProgress.durationMs.keySet()\n+        // remove \"triggerExecution\" as it count the other operation duration.\n+        durationKeys.remove(\"triggerExecution\")\n+        durationKeys.asScala.toSeq.sorted\n+      }, Seq.empty[String])\n+\n+    val jsCollector = new JsCollector\n+    val graphUIDataForInputRate =\n+      new GraphUIData(\n+        \"input-rate-timeline\",\n+        \"input-rate-histogram\",\n+        inputRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRecordRate,\n+        maxRecordRate,\n+        \"records/sec\")\n+    graphUIDataForInputRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForProcessRate =\n+      new GraphUIData(\n+        \"process-rate-timeline\",\n+        \"process-rate-histogram\",\n+        processRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minProcessRate,\n+        maxProcessRate,\n+        \"records/sec\")\n+    graphUIDataForProcessRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForInputRows =\n+      new GraphUIData(\n+        \"input-rows-timeline\",\n+        \"input-rows-histogram\",\n+        inputRowsData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRows,\n+        maxRows,\n+        \"records\")\n+    graphUIDataForInputRows.generateDataJs(jsCollector)\n+\n+    val graphUIDataForBatchDuration =\n+      new GraphUIData(\n+        \"batch-duration-timeline\",\n+        \"batch-duration-histogram\",\n+        batchDurations,\n+        minBatchTime,\n+        maxBatchTime,\n+        minBatchDuration,\n+        maxBatchDuration,\n+        \"ms\")\n+    graphUIDataForBatchDuration.generateDataJs(jsCollector)\n+\n+    val graphUIDataForDuration =\n+      new GraphUIData(\n+        \"duration-area-stack\",\n+        \"\",\n+        Seq.empty[(Long, Double)],\n+        0L,\n+        0L,\n+        0L,\n+        0L,\n+        \"ms\")\n+\n+    val table =\n+    // scalastyle:off\n+      <table id=\"stat-table\" class=\"table table-bordered\" style=\"width: auto\">\n+        <thead>\n+          <tr>\n+            <th style=\"width: 160px;\"></th>\n+            <th style=\"width: 492px;\">Timelines</th>\n+            <th style=\"width: 350px;\">Histograms</th></tr>\n+        </thead>\n+        <tbody>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Input Rate {SparkUIUtils.tooltip(\"The aggregate (across all sources) rate of data arriving.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForInputRate.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForInputRate.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Process Rate {SparkUIUtils.tooltip(\"The aggregate (across all sources) rate at which Spark is processing data.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForProcessRate.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForProcessRate.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Input Rows {SparkUIUtils.tooltip(\"The aggregate (across all sources) number of records processed in a trigger.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForInputRows.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForInputRows.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Batch Duration {SparkUIUtils.tooltip(\"The process duration of each batch.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForBatchDuration.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForBatchDuration.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Operation Duration {SparkUIUtils.tooltip(\"The amount of time taken to perform various operations in milliseconds.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"duration-area-stack\" colspan=\"2\">{graphUIDataForDuration.generateAreaStackHtmlWithData(jsCollector, operationDurationData, operationLabels)}</td>\n+          </tr>\n+        </tbody>\n+      </table>\n+    // scalastyle:on\n+\n+    generateVar(operationDurationData) ++ generateTimeMap(batchTimes) ++ table ++ jsCollector.toHtml\n+  }\n+}\n+\n+class JsCollector {\n+\n+  private var variableId = 0\n+\n+  /**\n+   * Return the next unused JavaScript variable name\n+   */\n+  def nextVariableName: String = {\n+    variableId += 1\n+    \"v\" + variableId\n+  }\n+\n+  /**\n+   * JavaScript statements that will execute before `statements`\n+   */\n+  private val preparedStatements = ArrayBuffer[String]()\n+\n+  /**\n+   * JavaScript statements that will execute after `preparedStatements`\n+   */\n+  private val statements = ArrayBuffer[String]()\n+\n+  def addPreparedStatement(js: String): Unit = {\n+    preparedStatements += js\n+  }\n+\n+  def addStatement(js: String): Unit = {\n+    statements += js\n+  }\n+\n+  /**\n+   * Generate a html snippet that will execute all scripts when the DOM has finished loading.\n+   */\n+  def toHtml: Seq[Node] = {\n+    val js =\n+      s\"\"\"\n+         |$$(document).ready(function() {\n+         |    ${preparedStatements.mkString(\"\\n\")}\n+         |    ${statements.mkString(\"\\n\")}\n+         |});\"\"\".stripMargin\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+}\n+\n+class GraphUIData("
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "Moved this and other classes to core package and reused in streaming and sql UI",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-10-23T02:25:09Z",
    "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.{ArrayBuffer, HashSet}\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[HashSet[(StreamingQuery, Long)]])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/sql/streaming/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/sql/streaming/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = UIUtils.stripXSS(request.getParameter(\"id\"))\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.find(i => i._1.runId.equals(UUID.fromString(parameterId)))\n+        .getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        UIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${UIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{withNoProgress(query, { query.lastProgress.batchId + 1L }, \"NaN\")}</strong>\n+      completed batches, <strong>{query.getTotalInputRecords}</strong> records)\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime = withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime,\n+      0L)\n+    val maxBatchTime = withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime,\n+      0L)\n+    val maxRecordRate = withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max,\n+      0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate = withNoProgress(query,\n+      query.recentProgress.map(_.processedRowsPerSecond).max, 0L)\n+    val minProcessRate = 0L\n+    val maxRows = withNoProgress(query, query.recentProgress.map(_.numInputRows).max, 0L)\n+    val minRows = 0L\n+    val maxBatchDuration = withNoProgress(query, query.recentProgress.map(_.batchDuration).max, 0L)\n+    val minBatchDuration = 0L\n+\n+    val inputRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.inputRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val processRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.processedRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val inputRowsData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.numInputRows })),\n+      Array.empty[(Long, Double)])\n+    val batchDurations = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.batchDuration })),\n+      Array.empty[(Long, Double)])\n+    val operationDurationData = withNoProgress(query, query.recentProgress.map { case p =>\n+      val durationMs = p.durationMs\n+      durationMs.remove(\"triggerExecution\")\n+      (df.parse(p.timestamp).getTime, durationMs)},\n+      Array.empty[(Long, ju.Map[String, JLong])])\n+    val operationLabels = withNoProgress(query, {\n+        val durationKeys = query.lastProgress.durationMs.keySet()\n+        // remove \"triggerExecution\" as it count the other operation duration.\n+        durationKeys.remove(\"triggerExecution\")\n+        durationKeys.asScala.toSeq.sorted\n+      }, Seq.empty[String])\n+\n+    val jsCollector = new JsCollector\n+    val graphUIDataForInputRate =\n+      new GraphUIData(\n+        \"input-rate-timeline\",\n+        \"input-rate-histogram\",\n+        inputRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRecordRate,\n+        maxRecordRate,\n+        \"records/sec\")\n+    graphUIDataForInputRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForProcessRate =\n+      new GraphUIData(\n+        \"process-rate-timeline\",\n+        \"process-rate-histogram\",\n+        processRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minProcessRate,\n+        maxProcessRate,\n+        \"records/sec\")\n+    graphUIDataForProcessRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForInputRows =\n+      new GraphUIData(\n+        \"input-rows-timeline\",\n+        \"input-rows-histogram\",\n+        inputRowsData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRows,\n+        maxRows,\n+        \"records\")\n+    graphUIDataForInputRows.generateDataJs(jsCollector)\n+\n+    val graphUIDataForBatchDuration =\n+      new GraphUIData(\n+        \"batch-duration-timeline\",\n+        \"batch-duration-histogram\",\n+        batchDurations,\n+        minBatchTime,\n+        maxBatchTime,\n+        minBatchDuration,\n+        maxBatchDuration,\n+        \"ms\")\n+    graphUIDataForBatchDuration.generateDataJs(jsCollector)\n+\n+    val graphUIDataForDuration =\n+      new GraphUIData(\n+        \"duration-area-stack\",\n+        \"\",\n+        Seq.empty[(Long, Double)],\n+        0L,\n+        0L,\n+        0L,\n+        0L,\n+        \"ms\")\n+\n+    val table =\n+    // scalastyle:off\n+      <table id=\"stat-table\" class=\"table table-bordered\" style=\"width: auto\">\n+        <thead>\n+          <tr>\n+            <th style=\"width: 160px;\"></th>\n+            <th style=\"width: 492px;\">Timelines</th>\n+            <th style=\"width: 350px;\">Histograms</th></tr>\n+        </thead>\n+        <tbody>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Input Rate {SparkUIUtils.tooltip(\"The aggregate (across all sources) rate of data arriving.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForInputRate.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForInputRate.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Process Rate {SparkUIUtils.tooltip(\"The aggregate (across all sources) rate at which Spark is processing data.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForProcessRate.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForProcessRate.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Input Rows {SparkUIUtils.tooltip(\"The aggregate (across all sources) number of records processed in a trigger.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForInputRows.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForInputRows.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Batch Duration {SparkUIUtils.tooltip(\"The process duration of each batch.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForBatchDuration.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForBatchDuration.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Operation Duration {SparkUIUtils.tooltip(\"The amount of time taken to perform various operations in milliseconds.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"duration-area-stack\" colspan=\"2\">{graphUIDataForDuration.generateAreaStackHtmlWithData(jsCollector, operationDurationData, operationLabels)}</td>\n+          </tr>\n+        </tbody>\n+      </table>\n+    // scalastyle:on\n+\n+    generateVar(operationDurationData) ++ generateTimeMap(batchTimes) ++ table ++ jsCollector.toHtml\n+  }\n+}\n+\n+class JsCollector {\n+\n+  private var variableId = 0\n+\n+  /**\n+   * Return the next unused JavaScript variable name\n+   */\n+  def nextVariableName: String = {\n+    variableId += 1\n+    \"v\" + variableId\n+  }\n+\n+  /**\n+   * JavaScript statements that will execute before `statements`\n+   */\n+  private val preparedStatements = ArrayBuffer[String]()\n+\n+  /**\n+   * JavaScript statements that will execute after `preparedStatements`\n+   */\n+  private val statements = ArrayBuffer[String]()\n+\n+  def addPreparedStatement(js: String): Unit = {\n+    preparedStatements += js\n+  }\n+\n+  def addStatement(js: String): Unit = {\n+    statements += js\n+  }\n+\n+  /**\n+   * Generate a html snippet that will execute all scripts when the DOM has finished loading.\n+   */\n+  def toHtml: Seq[Node] = {\n+    val js =\n+      s\"\"\"\n+         |$$(document).ready(function() {\n+         |    ${preparedStatements.mkString(\"\\n\")}\n+         |    ${statements.mkString(\"\\n\")}\n+         |});\"\"\".stripMargin\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+}\n+\n+class GraphUIData("
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Note for reviewers: this is newly added.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-10-22T22:06:25Z",
    "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.{ArrayBuffer, HashSet}\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[HashSet[(StreamingQuery, Long)]])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/sql/streaming/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/sql/streaming/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = UIUtils.stripXSS(request.getParameter(\"id\"))\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.find(i => i._1.runId.equals(UUID.fromString(parameterId)))\n+        .getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        UIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${UIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{withNoProgress(query, { query.lastProgress.batchId + 1L }, \"NaN\")}</strong>\n+      completed batches, <strong>{query.getTotalInputRecords}</strong> records)\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime = withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime,\n+      0L)\n+    val maxBatchTime = withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime,\n+      0L)\n+    val maxRecordRate = withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max,\n+      0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate = withNoProgress(query,\n+      query.recentProgress.map(_.processedRowsPerSecond).max, 0L)\n+    val minProcessRate = 0L\n+    val maxRows = withNoProgress(query, query.recentProgress.map(_.numInputRows).max, 0L)\n+    val minRows = 0L\n+    val maxBatchDuration = withNoProgress(query, query.recentProgress.map(_.batchDuration).max, 0L)\n+    val minBatchDuration = 0L\n+\n+    val inputRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.inputRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val processRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.processedRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val inputRowsData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.numInputRows })),\n+      Array.empty[(Long, Double)])\n+    val batchDurations = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.batchDuration })),\n+      Array.empty[(Long, Double)])\n+    val operationDurationData = withNoProgress(query, query.recentProgress.map { case p =>\n+      val durationMs = p.durationMs\n+      durationMs.remove(\"triggerExecution\")\n+      (df.parse(p.timestamp).getTime, durationMs)},\n+      Array.empty[(Long, ju.Map[String, JLong])])\n+    val operationLabels = withNoProgress(query, {\n+        val durationKeys = query.lastProgress.durationMs.keySet()\n+        // remove \"triggerExecution\" as it count the other operation duration.\n+        durationKeys.remove(\"triggerExecution\")\n+        durationKeys.asScala.toSeq.sorted\n+      }, Seq.empty[String])\n+\n+    val jsCollector = new JsCollector\n+    val graphUIDataForInputRate =\n+      new GraphUIData(\n+        \"input-rate-timeline\",\n+        \"input-rate-histogram\",\n+        inputRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRecordRate,\n+        maxRecordRate,\n+        \"records/sec\")\n+    graphUIDataForInputRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForProcessRate =\n+      new GraphUIData(\n+        \"process-rate-timeline\",\n+        \"process-rate-histogram\",\n+        processRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minProcessRate,\n+        maxProcessRate,\n+        \"records/sec\")\n+    graphUIDataForProcessRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForInputRows =\n+      new GraphUIData(\n+        \"input-rows-timeline\",\n+        \"input-rows-histogram\",\n+        inputRowsData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRows,\n+        maxRows,\n+        \"records\")\n+    graphUIDataForInputRows.generateDataJs(jsCollector)\n+\n+    val graphUIDataForBatchDuration =\n+      new GraphUIData(\n+        \"batch-duration-timeline\",\n+        \"batch-duration-histogram\",\n+        batchDurations,\n+        minBatchTime,\n+        maxBatchTime,\n+        minBatchDuration,\n+        maxBatchDuration,\n+        \"ms\")\n+    graphUIDataForBatchDuration.generateDataJs(jsCollector)\n+\n+    val graphUIDataForDuration =\n+      new GraphUIData(\n+        \"duration-area-stack\",\n+        \"\",\n+        Seq.empty[(Long, Double)],\n+        0L,\n+        0L,\n+        0L,\n+        0L,\n+        \"ms\")\n+\n+    val table =\n+    // scalastyle:off\n+      <table id=\"stat-table\" class=\"table table-bordered\" style=\"width: auto\">\n+        <thead>\n+          <tr>\n+            <th style=\"width: 160px;\"></th>\n+            <th style=\"width: 492px;\">Timelines</th>\n+            <th style=\"width: 350px;\">Histograms</th></tr>\n+        </thead>\n+        <tbody>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Input Rate {SparkUIUtils.tooltip(\"The aggregate (across all sources) rate of data arriving.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForInputRate.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForInputRate.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Process Rate {SparkUIUtils.tooltip(\"The aggregate (across all sources) rate at which Spark is processing data.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForProcessRate.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForProcessRate.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Input Rows {SparkUIUtils.tooltip(\"The aggregate (across all sources) number of records processed in a trigger.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForInputRows.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForInputRows.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Batch Duration {SparkUIUtils.tooltip(\"The process duration of each batch.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"timeline\">{graphUIDataForBatchDuration.generateTimelineHtml(jsCollector)}</td>\n+            <td class=\"histogram\">{graphUIDataForBatchDuration.generateHistogramHtml(jsCollector)}</td>\n+          </tr>\n+          <tr>\n+            <td style=\"vertical-align: middle;\">\n+              <div style=\"width: 160px;\">\n+                <div><strong>Operation Duration {SparkUIUtils.tooltip(\"The amount of time taken to perform various operations in milliseconds.\", \"right\")}</strong></div>\n+              </div>\n+            </td>\n+            <td class=\"duration-area-stack\" colspan=\"2\">{graphUIDataForDuration.generateAreaStackHtmlWithData(jsCollector, operationDurationData, operationLabels)}</td>\n+          </tr>\n+        </tbody>\n+      </table>\n+    // scalastyle:on\n+\n+    generateVar(operationDurationData) ++ generateTimeMap(batchTimes) ++ table ++ jsCollector.toHtml\n+  }\n+}\n+\n+class JsCollector {\n+\n+  private var variableId = 0\n+\n+  /**\n+   * Return the next unused JavaScript variable name\n+   */\n+  def nextVariableName: String = {\n+    variableId += 1\n+    \"v\" + variableId\n+  }\n+\n+  /**\n+   * JavaScript statements that will execute before `statements`\n+   */\n+  private val preparedStatements = ArrayBuffer[String]()\n+\n+  /**\n+   * JavaScript statements that will execute after `preparedStatements`\n+   */\n+  private val statements = ArrayBuffer[String]()\n+\n+  def addPreparedStatement(js: String): Unit = {\n+    preparedStatements += js\n+  }\n+\n+  def addStatement(js: String): Unit = {\n+    statements += js\n+  }\n+\n+  /**\n+   * Generate a html snippet that will execute all scripts when the DOM has finished loading.\n+   */\n+  def toHtml: Seq[Node] = {\n+    val js =\n+      s\"\"\"\n+         |$$(document).ready(function() {\n+         |    ${preparedStatements.mkString(\"\\n\")}\n+         |    ${statements.mkString(\"\\n\")}\n+         |});\"\"\".stripMargin\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+}\n+\n+class GraphUIData(\n+    timelineDivId: String,\n+    histogramDivId: String,\n+    data: Seq[(Long, Double)],\n+    minX: Long,\n+    maxX: Long,\n+    minY: Double,\n+    maxY: Double,\n+    unitY: String,\n+    batchInterval: Option[Double] = None) {\n+\n+  private var dataJavaScriptName: String = _\n+\n+  def generateDataJs(jsCollector: JsCollector): Unit = {\n+    val jsForData = data.map { case (x, y) =>\n+      s\"\"\"{\"x\": $x, \"y\": $y}\"\"\"\n+    }.mkString(\"[\", \",\", \"]\")\n+    dataJavaScriptName = jsCollector.nextVariableName\n+    jsCollector.addPreparedStatement(s\"var $dataJavaScriptName = $jsForData;\")\n+  }\n+\n+  def generateTimelineHtml(jsCollector: JsCollector): Seq[Node] = {\n+    jsCollector.addPreparedStatement(s\"registerTimeline($minY, $maxY);\")\n+    if (batchInterval.isDefined) {\n+      jsCollector.addStatement(\n+        \"drawTimeline(\" +\n+          s\"'#$timelineDivId', $dataJavaScriptName, $minX, $maxX, $minY, $maxY, '$unitY',\" +\n+          s\" ${batchInterval.get}\" +\n+          \");\")\n+    } else {\n+      jsCollector.addStatement(\n+        s\"drawTimeline('#$timelineDivId', $dataJavaScriptName, $minX, $maxX, $minY, $maxY,\" +\n+          s\" '$unitY');\")\n+    }\n+    <div id={timelineDivId}></div>\n+  }\n+\n+  def generateHistogramHtml(jsCollector: JsCollector): Seq[Node] = {\n+    val histogramData = s\"$dataJavaScriptName.map(function(d) { return d.y; })\"\n+    jsCollector.addPreparedStatement(s\"registerHistogram($histogramData, $minY, $maxY);\")\n+    if (batchInterval.isDefined) {\n+      jsCollector.addStatement(\n+        \"drawHistogram(\" +\n+          s\"'#$histogramDivId', $histogramData, $minY, $maxY, '$unitY', ${batchInterval.get}\" +\n+          \");\")\n+    } else {\n+      jsCollector.addStatement(\n+        s\"drawHistogram('#$histogramDivId', $histogramData, $minY, $maxY, '$unitY');\")\n+    }\n+    <div id={histogramDivId}></div>\n+  }\n+\n+  def generateAreaStackHtmlWithData("
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Do we need two line for this?",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-10-24T20:03:05Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.existingStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime = withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime,\n+      0L)\n+    val maxBatchTime = withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime,\n+      0L)"
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "updated to one line",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-10-28T02:43:57Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.existingStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime = withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime,\n+      0L)\n+    val maxBatchTime = withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime,\n+      0L)"
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "`maxProcessRate` can be NaN and affects the result of rendering.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-11T11:44:28Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)",
    "line": 131
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "`maxProcessRate ` is the max of `processedRowsPerSecond` or `0L` if there is not completed batch.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-15T06:06:05Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)",
    "line": 131
  }, {
    "author": {
      "login": "sarutak"
    },
    "body": "I understand what `maxProcessRate` means but I actually met the situation where `maxProcessRate` indicates NaN.\r\nIf there are no operation (just load and then start like `spark.readStream.load(...).writeStream.start`), you will  hit this situation.\r\n<img width=\"1436\" alt=\" 2019-11-15 15 56 38\" src=\"https://user-images.githubusercontent.com/4736016/68923108-d365c980-07c0-11ea-979d-fd0a13db5929.png\">",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-15T06:59:09Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)",
    "line": 131
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "This problem will be discussed and fixed in #26610. Take a review please.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-21T02:58:04Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)",
    "line": 131
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "I think we don't need `case` here.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-11T11:53:44Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)\n+    val minProcessRate = 0L\n+    val maxRows = withNoProgress(query, query.recentProgress.map(_.numInputRows).max, 0L)\n+    val minRows = 0L\n+    val maxBatchDuration = withNoProgress(query, query.recentProgress.map(_.batchDuration).max, 0L)\n+    val minBatchDuration = 0L\n+\n+    val inputRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.inputRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val processRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.processedRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val inputRowsData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.numInputRows })),\n+      Array.empty[(Long, Double)])\n+    val batchDurations = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.batchDuration })),\n+      Array.empty[(Long, Double)])\n+    val operationDurationData = withNoProgress(query, query.recentProgress.map { case p =>"
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "ok",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-15T06:06:14Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)\n+    val minProcessRate = 0L\n+    val maxRows = withNoProgress(query, query.recentProgress.map(_.numInputRows).max, 0L)\n+    val minRows = 0L\n+    val maxBatchDuration = withNoProgress(query, query.recentProgress.map(_.batchDuration).max, 0L)\n+    val minBatchDuration = 0L\n+\n+    val inputRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.inputRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val processRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.processedRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val inputRowsData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.numInputRows })),\n+      Array.empty[(Long, Double)])\n+    val batchDurations = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.batchDuration })),\n+      Array.empty[(Long, Double)])\n+    val operationDurationData = withNoProgress(query, query.recentProgress.map { case p =>"
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Should we use auto width here?\r\n![image](https://user-images.githubusercontent.com/4833765/68624235-595be900-0511-11ea-915d-17b85ad70757.png)\r\n",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-11T21:58:04Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]\n+    </div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)\n+    val minProcessRate = 0L\n+    val maxRows = withNoProgress(query, query.recentProgress.map(_.numInputRows).max, 0L)\n+    val minRows = 0L\n+    val maxBatchDuration = withNoProgress(query, query.recentProgress.map(_.batchDuration).max, 0L)\n+    val minBatchDuration = 0L\n+\n+    val inputRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.inputRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val processRateData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.processedRowsPerSecond })),\n+      Array.empty[(Long, Double)])\n+    val inputRowsData = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.numInputRows })),\n+      Array.empty[(Long, Double)])\n+    val batchDurations = withNoProgress(query, query.recentProgress.map(p =>\n+      (df.parse(p.timestamp).getTime, withNumberInvalid { p.batchDuration })),\n+      Array.empty[(Long, Double)])\n+    val operationDurationData = withNoProgress(query, query.recentProgress.map { case p =>\n+      val durationMs = p.durationMs\n+      durationMs.remove(\"triggerExecution\")\n+      (df.parse(p.timestamp).getTime, durationMs)},\n+      Array.empty[(Long, ju.Map[String, JLong])])\n+    val operationLabels = withNoProgress(query, {\n+        val durationKeys = query.lastProgress.durationMs.keySet()\n+        // remove \"triggerExecution\" as it count the other operation duration.\n+        durationKeys.remove(\"triggerExecution\")\n+        durationKeys.asScala.toSeq.sorted\n+      }, Seq.empty[String])\n+\n+    val jsCollector = new JsCollector\n+    val graphUIDataForInputRate =\n+      new GraphUIData(\n+        \"input-rate-timeline\",\n+        \"input-rate-histogram\",\n+        inputRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRecordRate,\n+        maxRecordRate,\n+        \"records/sec\")\n+    graphUIDataForInputRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForProcessRate =\n+      new GraphUIData(\n+        \"process-rate-timeline\",\n+        \"process-rate-histogram\",\n+        processRateData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minProcessRate,\n+        maxProcessRate,\n+        \"records/sec\")\n+    graphUIDataForProcessRate.generateDataJs(jsCollector)\n+\n+    val graphUIDataForInputRows =\n+      new GraphUIData(\n+        \"input-rows-timeline\",\n+        \"input-rows-histogram\",\n+        inputRowsData,\n+        minBatchTime,\n+        maxBatchTime,\n+        minRows,\n+        maxRows,\n+        \"records\")\n+    graphUIDataForInputRows.generateDataJs(jsCollector)\n+\n+    val graphUIDataForBatchDuration =\n+      new GraphUIData(\n+        \"batch-duration-timeline\",\n+        \"batch-duration-histogram\",\n+        batchDurations,\n+        minBatchTime,\n+        maxBatchTime,\n+        minBatchDuration,\n+        maxBatchDuration,\n+        \"ms\")\n+    graphUIDataForBatchDuration.generateDataJs(jsCollector)\n+\n+    val graphUIDataForDuration =\n+      new GraphUIData(\n+        \"duration-area-stack\",\n+        \"\",\n+        Seq.empty[(Long, Double)],\n+        0L,\n+        0L,\n+        0L,\n+        0L,\n+        \"ms\")\n+\n+    val table =\n+    // scalastyle:off\n+      <table id=\"stat-table\" class=\"table table-bordered\" style=\"width: auto\">\n+        <thead>\n+          <tr>\n+            <th style=\"width: 160px;\"></th>\n+            <th style=\"width: 492px;\">Timelines</th>\n+            <th style=\"width: 350px;\">Histograms</th></tr>",
    "line": 229
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "How about itemize these properties?\r\nThis is an example from Stage page.\r\n<img width=\"405\" alt=\" 2019-11-11 20 46 21\" src=\"https://user-images.githubusercontent.com/4736016/68634411-bb7c1480-0538-11ea-82e1-edcf13adc7db.png\">\r\n\r\n",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-12T01:39:46Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]"
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "ok",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-15T06:31:31Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\"\n+    } else {\n+      query.name\n+    }\n+\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div>\n+      [name = <strong>{name}</strong>,\n+       id = <strong>{query.id}</strong>,\n+       runId = <strong>{query.runId}</strong>]"
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Let's throw more specific exception; `IllegalArgumentException` in this case. And we can flatten the if statement via:\r\n\r\n```\r\nval (query, timeSinceStart) = store.flatMap { s =>\r\n  s.allStreamQueries.find(_._1.runId.equals(UUID.fromString(parameterId)))\r\n}.getOrElse(throw new IllegalArgumentException(s\"Can not find streaming query $parameterId\"))\r\n```",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-20T08:31:43Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))"
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Same here; as I commented, it will not work as expected and you should clone the necessary information in prior and only rely on that.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-20T08:32:12Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent"
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "removed",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-21T07:15:55Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent"
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Same here; let's give a better representation.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-20T08:35:52Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = if (query.name == null || query.name.isEmpty) {\n+      \"null\""
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "You can wrap `link` tag and `scalastyle:off` is not needed.",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-20T10:41:03Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off",
    "line": 44
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "use `scalastyle:off` to disable `line length 100` check",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-21T07:43:25Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off",
    "line": 44
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "Same as I commented above.\r\n",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-20T11:07:07Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")"
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "ok",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-21T07:57:20Z",
    "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.execution.ui.SQLTab\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: SQLTab,\n+    store: Option[StreamQueryStore])\n+  extends WebUIPage(\"streaming/statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = if (store.nonEmpty) {\n+      store.get.allStreamQueries.find { case (query, _) =>\n+        query.runId.equals(UUID.fromString(parameterId))\n+      }.getOrElse(throw new Exception(s\"Can not find streaming query $parameterId\"))\n+    } else {\n+      throw new Exception(s\"Can not find streaming query $parameterId\")\n+    }\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      store.synchronized { // make sure all parts in this page are consistent\n+        resources ++\n+          basicInfo ++\n+          generateStatTable(query)\n+      }\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime =\n+        SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s = y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\")\n+        .mkString(\"[\", \",\", \"]\")"
  }],
  "prId": 26201
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "`operationLabels` should not be determined based on only the **last** progress.\r\nIf we use some v2-writers, one micro-batch contains >1 jobs.\r\nOne is an [actual data processing job](https://github.com/apache/spark/blob/54c5087a3ae306ad766df81a5a6279f219b2ea47/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala#L376).\r\nAnd another job is a [dummy](https://github.com/apache/spark/blob/54c5087a3ae306ad766df81a5a6279f219b2ea47/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala#L412).\r\nSo, in such case, the `last` progress can be the dummy job and `durationMs` should not be set.\r\n\r\nAs the result, if we see like a following graph,\r\n<img width=\"1069\" alt=\" 2019-11-25 20 34 03\" src=\"https://user-images.githubusercontent.com/4736016/69539856-b1dabe00-0fc8-11ea-9b66-5767358dfa75.png\">\r\n\r\nand then reload, the graph disappears because labels are not set for the dummy job.\r\n<img width=\"1062\" alt=\" 2019-11-25 20 34 19\" src=\"https://user-images.githubusercontent.com/4736016/69540060-244b9e00-0fc9-11ea-9c59-e2b60ee0ec87.png\">\r\n\r\nYou can reproduce this issue easily by setting `writeStream.format(\"console\")`.\r\n",
    "commit": "6de18cc2e20bd8ef0167a52c869c7706f67014a2",
    "createdAt": "2019-11-25T12:19:49Z",
    "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.ui\n+\n+import java.{util => ju}\n+import java.lang.{Long => JLong}\n+import java.text.SimpleDateFormat\n+import java.util.UUID\n+import javax.servlet.http.HttpServletRequest\n+\n+import scala.collection.JavaConverters._\n+import scala.xml.{Node, Unparsed}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.getTimeZone\n+import org.apache.spark.sql.execution.streaming.{QuerySummary, StreamQueryStore}\n+import org.apache.spark.sql.streaming.StreamingQuery\n+import org.apache.spark.sql.streaming.ui.UIUtils._\n+import org.apache.spark.ui.{GraphUIData, JsCollector, UIUtils => SparkUIUtils, WebUIPage}\n+\n+class StreamingQueryStatisticsPage(\n+    parent: StreamingQueryTab,\n+    store: StreamQueryStore)\n+  extends WebUIPage(\"statistics\") with Logging {\n+  val df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n+  df.setTimeZone(getTimeZone(\"UTC\"))\n+\n+  def generateLoadResources(request: HttpServletRequest): Seq[Node] = {\n+    // scalastyle:off\n+    <script src={SparkUIUtils.prependBaseUri(request, \"/static/d3.min.js\")}></script>\n+        <link rel=\"stylesheet\" href={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.css\")} type=\"text/css\"/>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/streaming-page.js\")}></script>\n+      <script src={SparkUIUtils.prependBaseUri(request, \"/static/structured-streaming-page.js\")}></script>\n+    // scalastyle:on\n+  }\n+\n+  override def render(request: HttpServletRequest): Seq[Node] = {\n+    val parameterId = request.getParameter(\"id\")\n+    require(parameterId != null && parameterId.nonEmpty, \"Missing id parameter\")\n+\n+    val (query, timeSinceStart) = store.allStreamQueries.find { case (q, _) =>\n+      q.runId.equals(UUID.fromString(parameterId))\n+    }.getOrElse(throw new IllegalArgumentException(s\"Failed to find streaming query $parameterId\"))\n+\n+    val resources = generateLoadResources(request)\n+    val basicInfo = generateBasicInfo(query, timeSinceStart)\n+    val content =\n+      resources ++\n+        basicInfo ++\n+        generateStatTable(query)\n+    SparkUIUtils.headerSparkPage(request, \"Streaming Query Statistics\", content, parent)\n+  }\n+\n+  def generateTimeMap(times: Seq[Long]): Seq[Node] = {\n+    val js = \"var timeFormat = {};\\n\" + times.map { time =>\n+      val formattedTime = SparkUIUtils.formatBatchTime(time, 1, showYYYYMMSS = false)\n+      s\"timeFormat[$time] = '$formattedTime';\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateVar(values: Array[(Long, ju.Map[String, JLong])]): Seq[Node] = {\n+    val js = \"var timeToValues = {};\\n\" + values.map { case (x, y) =>\n+      val s =\n+        y.asScala.toSeq.sortBy(_._1).map(e => s\"\"\"\"${e._2.toDouble}\"\"\"\").mkString(\"[\", \",\", \"]\")\n+      s\"\"\"timeToValues[\"${SparkUIUtils.formatBatchTime(x, 1, showYYYYMMSS = false)}\"] = $s;\"\"\"\n+    }.mkString(\"\\n\")\n+\n+    <script>{Unparsed(js)}</script>\n+  }\n+\n+  def generateBasicInfo(query: StreamingQuery, timeSinceStart: Long): Seq[Node] = {\n+    val duration = if (query.isActive) {\n+      SparkUIUtils.formatDurationVerbose(System.currentTimeMillis() - timeSinceStart)\n+    } else {\n+      withNoProgress(query, {\n+        val end = query.lastProgress.timestamp\n+        val start = query.recentProgress.head.timestamp\n+        SparkUIUtils.formatDurationVerbose(\n+          df.parse(end).getTime - df.parse(start).getTime)\n+      }, \"-\")\n+    }\n+\n+    val name = UIUtils.getQueryName(query)\n+    val numBatches = withNoProgress(query, { query.lastProgress.batchId + 1L }, 0)\n+    val totalRecords = query.getQuerySummary.getMetric(QuerySummary.TOTAL_INPUT_RECORDS, 0L)\n+    <div>Running batches for\n+      <strong>\n+        {duration}\n+      </strong>\n+      since\n+      <strong>\n+        {SparkUIUtils.formatDate(timeSinceStart)}\n+      </strong>\n+      (<strong>{numBatches}</strong> completed batches, <strong>{totalRecords}</strong> records)\n+    </div>\n+    <br />\n+    <div><strong>Name: </strong>{name}</div>\n+    <div><strong>Id: </strong>{query.id}</div>\n+    <div><strong>RunId: </strong>{query.runId}</div>\n+    <br />\n+  }\n+\n+  def generateStatTable(query: StreamingQuery): Seq[Node] = {\n+    val batchTimes = withNoProgress(query,\n+      query.recentProgress.map(p => df.parse(p.timestamp).getTime), Array.empty[Long])\n+    val minBatchTime =\n+      withNoProgress(query, df.parse(query.recentProgress.head.timestamp).getTime, 0L)\n+    val maxBatchTime =\n+      withNoProgress(query, df.parse(query.lastProgress.timestamp).getTime, 0L)\n+    val maxRecordRate =\n+      withNoProgress(query, query.recentProgress.map(_.inputRowsPerSecond).max, 0L)\n+    val minRecordRate = 0L\n+    val maxProcessRate =\n+      withNoProgress(query, query.recentProgress.map(_.processedRowsPerSecond).max, 0L)\n+\n+    val minProcessRate = 0L\n+    val maxRows = withNoProgress(query, query.recentProgress.map(_.numInputRows).max, 0L)\n+    val minRows = 0L\n+    val maxBatchDuration = withNoProgress(query, query.recentProgress.map(_.batchDuration).max, 0L)\n+    val minBatchDuration = 0L\n+\n+    val inputRateData = withNoProgress(query,\n+      query.recentProgress.map(p => (df.parse(p.timestamp).getTime,\n+        withNumberInvalid { p.inputRowsPerSecond })), Array.empty[(Long, Double)])\n+    val processRateData = withNoProgress(query,\n+      query.recentProgress.map(p => (df.parse(p.timestamp).getTime,\n+        withNumberInvalid { p.processedRowsPerSecond })), Array.empty[(Long, Double)])\n+    val inputRowsData = withNoProgress(query,\n+      query.recentProgress.map(p => (df.parse(p.timestamp).getTime,\n+        withNumberInvalid { p.numInputRows })), Array.empty[(Long, Double)])\n+    val batchDurations = withNoProgress(query,\n+      query.recentProgress.map(p => (df.parse(p.timestamp).getTime,\n+        withNumberInvalid { p.batchDuration })), Array.empty[(Long, Double)])\n+    val operationDurationData = withNoProgress(query, query.recentProgress.map { p =>\n+      val durationMs = p.durationMs\n+      // remove \"triggerExecution\" as it count the other operation duration.\n+      durationMs.remove(\"triggerExecution\")\n+      (df.parse(p.timestamp).getTime, durationMs)}, Array.empty[(Long, ju.Map[String, JLong])])\n+    val operationLabels = withNoProgress(query, {\n+      val durationKeys = query.lastProgress.durationMs.keySet()",
    "line": 157
  }],
  "prId": 26201
}]