[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "this method doesn't follow the java Iterator next contract:\r\n```\r\nNoSuchElementException if the iteration has no more elements\r\n```\r\n\r\nYou can extend `org.apache.spark.util.NextIterator` to fix it.",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T21:03:04Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {\n+          // epoch boundary marker\n+          case (null, null) =>\n+            epochEndpoint.send(ReportPartitionOffset(\n+              context.partitionId(),\n+              currentEpoch,\n+              currentOffset))\n+            currentEpoch += 1\n+            false\n+          // real row\n+          case (row, offset) =>\n+            currentRow = row\n+            currentOffset = offset\n+            true\n+        }\n+      }\n+\n+      override def next(): UnsafeRow = {",
    "line": 124
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "NextIterator won't quite work, because we need to be able to start going again after the iterator is \"finished\". I'll clean it up a bit to comply with the contract.",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T23:02:08Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {\n+          // epoch boundary marker\n+          case (null, null) =>\n+            epochEndpoint.send(ReportPartitionOffset(\n+              context.partitionId(),\n+              currentEpoch,\n+              currentOffset))\n+            currentEpoch += 1\n+            false\n+          // real row\n+          case (row, offset) =>\n+            currentRow = row\n+            currentOffset = offset\n+            true\n+        }\n+      }\n+\n+      override def next(): UnsafeRow = {",
    "line": 124
  }],
  "prId": 19984
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "switch the order so that the other thread can definitely see `failureReason` if `failedFlag` is `true`. ",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T21:05:02Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {\n+          // epoch boundary marker\n+          case (null, null) =>\n+            epochEndpoint.send(ReportPartitionOffset(\n+              context.partitionId(),\n+              currentEpoch,\n+              currentOffset))\n+            currentEpoch += 1\n+            false\n+          // real row\n+          case (row, offset) =>\n+            currentRow = row\n+            currentOffset = offset\n+            true\n+        }\n+      }\n+\n+      override def next(): UnsafeRow = {\n+        val r = currentRow\n+        currentRow = null\n+        r\n+      }\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    split.asInstanceOf[DataSourceRDDPartition].readTask.preferredLocations()\n+  }\n+}\n+\n+case class EpochPackedPartitionOffset(epoch: Long) extends PartitionOffset\n+\n+class EpochPollRunnable(\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean)\n+  extends Thread with Logging {\n+  private[continuous] var failureReason: Throwable = _\n+\n+  private val epochEndpoint = EpochCoordinatorRef.get(\n+    context.getLocalProperty(ContinuousExecution.RUN_ID_KEY), SparkEnv.get)\n+  private var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+  override def run(): Unit = {\n+    try {\n+      val newEpoch = epochEndpoint.askSync[Long](GetCurrentEpoch())\n+      for (i <- currentEpoch to newEpoch - 1) {\n+        queue.put((null, null))\n+        logDebug(s\"Sent marker to start epoch ${i + 1}\")\n+      }\n+      currentEpoch = newEpoch\n+    } catch {\n+      case t: Throwable =>\n+        failedFlag.set(true)\n+        failureReason = t"
  }],
  "prId": 19984
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "ditto",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T21:05:16Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {\n+          // epoch boundary marker\n+          case (null, null) =>\n+            epochEndpoint.send(ReportPartitionOffset(\n+              context.partitionId(),\n+              currentEpoch,\n+              currentOffset))\n+            currentEpoch += 1\n+            false\n+          // real row\n+          case (row, offset) =>\n+            currentRow = row\n+            currentOffset = offset\n+            true\n+        }\n+      }\n+\n+      override def next(): UnsafeRow = {\n+        val r = currentRow\n+        currentRow = null\n+        r\n+      }\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    split.asInstanceOf[DataSourceRDDPartition].readTask.preferredLocations()\n+  }\n+}\n+\n+case class EpochPackedPartitionOffset(epoch: Long) extends PartitionOffset\n+\n+class EpochPollRunnable(\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean)\n+  extends Thread with Logging {\n+  private[continuous] var failureReason: Throwable = _\n+\n+  private val epochEndpoint = EpochCoordinatorRef.get(\n+    context.getLocalProperty(ContinuousExecution.RUN_ID_KEY), SparkEnv.get)\n+  private var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+  override def run(): Unit = {\n+    try {\n+      val newEpoch = epochEndpoint.askSync[Long](GetCurrentEpoch())\n+      for (i <- currentEpoch to newEpoch - 1) {\n+        queue.put((null, null))\n+        logDebug(s\"Sent marker to start epoch ${i + 1}\")\n+      }\n+      currentEpoch = newEpoch\n+    } catch {\n+      case t: Throwable =>\n+        failedFlag.set(true)\n+        failureReason = t\n+        throw t\n+    }\n+  }\n+}\n+\n+class DataReaderThread(\n+    reader: DataReader[UnsafeRow],\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean) extends Thread {\n+  private[continuous] var failureReason: Throwable = _\n+\n+  override def run(): Unit = {\n+    val baseReader = ContinuousDataSourceRDD.getBaseReader(reader)\n+    try {\n+      while (!context.isInterrupted && !context.isCompleted()) {\n+        if (!reader.next()) {\n+          // Check again, since reader.next() might have blocked through an incoming interrupt.\n+          if (!context.isInterrupted && !context.isCompleted()) {\n+            throw new IllegalStateException(\n+              \"Continuous reader reported no elements! Reader should have blocked waiting.\")\n+          } else {\n+            return\n+          }\n+        }\n+\n+        queue.put((reader.get().copy(), baseReader.getOffset))\n+      }\n+    } catch {\n+      case _: InterruptedException if context.isInterrupted() =>\n+        // Continuous shutdown always involves an interrupt; shut down quietly.\n+        return\n+\n+      case t: Throwable =>\n+        failedFlag.set(true)\n+        failureReason = t"
  }],
  "prId": 19984
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "What if `dataReaderFailed` is set when a thread is blocking here? Seems the task will block forever.",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T21:12:21Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {"
  }],
  "prId": 19984
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "set a proper thread name for this thread",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T21:14:17Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {\n+          // epoch boundary marker\n+          case (null, null) =>\n+            epochEndpoint.send(ReportPartitionOffset(\n+              context.partitionId(),\n+              currentEpoch,\n+              currentOffset))\n+            currentEpoch += 1\n+            false\n+          // real row\n+          case (row, offset) =>\n+            currentRow = row\n+            currentOffset = offset\n+            true\n+        }\n+      }\n+\n+      override def next(): UnsafeRow = {\n+        val r = currentRow\n+        currentRow = null\n+        r\n+      }\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    split.asInstanceOf[DataSourceRDDPartition].readTask.preferredLocations()\n+  }\n+}\n+\n+case class EpochPackedPartitionOffset(epoch: Long) extends PartitionOffset\n+\n+class EpochPollRunnable(\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean)\n+  extends Thread with Logging {\n+  private[continuous] var failureReason: Throwable = _\n+\n+  private val epochEndpoint = EpochCoordinatorRef.get(\n+    context.getLocalProperty(ContinuousExecution.RUN_ID_KEY), SparkEnv.get)\n+  private var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+  override def run(): Unit = {\n+    try {\n+      val newEpoch = epochEndpoint.askSync[Long](GetCurrentEpoch())\n+      for (i <- currentEpoch to newEpoch - 1) {\n+        queue.put((null, null))\n+        logDebug(s\"Sent marker to start epoch ${i + 1}\")\n+      }\n+      currentEpoch = newEpoch\n+    } catch {\n+      case t: Throwable =>\n+        failedFlag.set(true)\n+        failureReason = t\n+        throw t\n+    }\n+  }\n+}\n+\n+class DataReaderThread(\n+    reader: DataReader[UnsafeRow],\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean) extends Thread {"
  }],
  "prId": 19984
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "We cannot throw `t` here as it will kill the executor. See org.apache.spark.util.SparkUncaughtExceptionHandler",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T21:21:49Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {\n+          // epoch boundary marker\n+          case (null, null) =>\n+            epochEndpoint.send(ReportPartitionOffset(\n+              context.partitionId(),\n+              currentEpoch,\n+              currentOffset))\n+            currentEpoch += 1\n+            false\n+          // real row\n+          case (row, offset) =>\n+            currentRow = row\n+            currentOffset = offset\n+            true\n+        }\n+      }\n+\n+      override def next(): UnsafeRow = {\n+        val r = currentRow\n+        currentRow = null\n+        r\n+      }\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    split.asInstanceOf[DataSourceRDDPartition].readTask.preferredLocations()\n+  }\n+}\n+\n+case class EpochPackedPartitionOffset(epoch: Long) extends PartitionOffset\n+\n+class EpochPollRunnable(\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean)\n+  extends Thread with Logging {\n+  private[continuous] var failureReason: Throwable = _\n+\n+  private val epochEndpoint = EpochCoordinatorRef.get(\n+    context.getLocalProperty(ContinuousExecution.RUN_ID_KEY), SparkEnv.get)\n+  private var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+  override def run(): Unit = {\n+    try {\n+      val newEpoch = epochEndpoint.askSync[Long](GetCurrentEpoch())\n+      for (i <- currentEpoch to newEpoch - 1) {\n+        queue.put((null, null))\n+        logDebug(s\"Sent marker to start epoch ${i + 1}\")\n+      }\n+      currentEpoch = newEpoch\n+    } catch {\n+      case t: Throwable =>\n+        failedFlag.set(true)\n+        failureReason = t\n+        throw t\n+    }\n+  }\n+}\n+\n+class DataReaderThread(\n+    reader: DataReader[UnsafeRow],\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean) extends Thread {\n+  private[continuous] var failureReason: Throwable = _\n+\n+  override def run(): Unit = {\n+    val baseReader = ContinuousDataSourceRDD.getBaseReader(reader)\n+    try {\n+      while (!context.isInterrupted && !context.isCompleted()) {\n+        if (!reader.next()) {\n+          // Check again, since reader.next() might have blocked through an incoming interrupt.\n+          if (!context.isInterrupted && !context.isCompleted()) {\n+            throw new IllegalStateException(\n+              \"Continuous reader reported no elements! Reader should have blocked waiting.\")\n+          } else {\n+            return\n+          }\n+        }\n+\n+        queue.put((reader.get().copy(), baseReader.getOffset))\n+      }\n+    } catch {\n+      case _: InterruptedException if context.isInterrupted() =>\n+        // Continuous shutdown always involves an interrupt; shut down quietly.\n+        return\n+\n+      case t: Throwable =>\n+        failedFlag.set(true)\n+        failureReason = t\n+        throw t"
  }],
  "prId": 19984
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "ditto",
    "commit": "b4f79762c083735011bf98250c39c263876c8cc8",
    "createdAt": "2017-12-20T21:22:27Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.concurrent.{ArrayBlockingQueue, BlockingQueue, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, RowToUnsafeDataReader}\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous._\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.streaming.ProcessingTime\n+import org.apache.spark.util.{SystemClock, ThreadUtils}\n+\n+class ContinuousDataSourceRDD(\n+    sc: SparkContext,\n+    sqlContext: SQLContext,\n+    @transient private val readTasks: java.util.List[ReadTask[UnsafeRow]])\n+  extends RDD[UnsafeRow](sc, Nil) {\n+\n+  private val dataQueueSize = sqlContext.conf.continuousStreamingExecutorQueueSize\n+  private val epochPollIntervalMs = sqlContext.conf.continuousStreamingExecutorPollIntervalMs\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    readTasks.asScala.zipWithIndex.map {\n+      case (readTask, index) => new DataSourceRDDPartition(index, readTask)\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val reader = split.asInstanceOf[DataSourceRDDPartition].readTask.createDataReader()\n+\n+    val runId = context.getLocalProperty(ContinuousExecution.RUN_ID_KEY)\n+\n+    // This queue contains two types of messages:\n+    // * (null, null) representing an epoch boundary.\n+    // * (row, off) containing a data row and its corresponding PartitionOffset.\n+    val queue = new ArrayBlockingQueue[(UnsafeRow, PartitionOffset)](dataQueueSize)\n+\n+    val epochPollFailed = new AtomicBoolean(false)\n+    val epochPollExecutor = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+      s\"epoch-poll--${runId}--${context.partitionId()}\")\n+    val epochPollRunnable = new EpochPollRunnable(queue, context, epochPollFailed)\n+    epochPollExecutor.scheduleWithFixedDelay(\n+      epochPollRunnable, 0, epochPollIntervalMs, TimeUnit.MILLISECONDS)\n+\n+    // Important sequencing - we must get start offset before the data reader thread begins\n+    val startOffset = ContinuousDataSourceRDD.getBaseReader(reader).getOffset\n+\n+    val dataReaderFailed = new AtomicBoolean(false)\n+    val dataReaderThread = new DataReaderThread(reader, queue, context, dataReaderFailed)\n+    dataReaderThread.setDaemon(true)\n+    dataReaderThread.start()\n+\n+    context.addTaskCompletionListener(_ => {\n+      reader.close()\n+      dataReaderThread.interrupt()\n+      epochPollExecutor.shutdown()\n+    })\n+\n+    val epochEndpoint = EpochCoordinatorRef.get(runId, SparkEnv.get)\n+    new Iterator[UnsafeRow] {\n+      private var currentRow: UnsafeRow = _\n+      private var currentOffset: PartitionOffset = startOffset\n+      private var currentEpoch =\n+        context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+      override def hasNext(): Boolean = {\n+        if (dataReaderFailed.get()) {\n+          throw new SparkException(\"data read failed\", dataReaderThread.failureReason)\n+        }\n+        if (epochPollFailed.get()) {\n+          throw new SparkException(\"epoch poll failed\", epochPollRunnable.failureReason)\n+        }\n+\n+        queue.take() match {\n+          // epoch boundary marker\n+          case (null, null) =>\n+            epochEndpoint.send(ReportPartitionOffset(\n+              context.partitionId(),\n+              currentEpoch,\n+              currentOffset))\n+            currentEpoch += 1\n+            false\n+          // real row\n+          case (row, offset) =>\n+            currentRow = row\n+            currentOffset = offset\n+            true\n+        }\n+      }\n+\n+      override def next(): UnsafeRow = {\n+        val r = currentRow\n+        currentRow = null\n+        r\n+      }\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    split.asInstanceOf[DataSourceRDDPartition].readTask.preferredLocations()\n+  }\n+}\n+\n+case class EpochPackedPartitionOffset(epoch: Long) extends PartitionOffset\n+\n+class EpochPollRunnable(\n+    queue: BlockingQueue[(UnsafeRow, PartitionOffset)],\n+    context: TaskContext,\n+    failedFlag: AtomicBoolean)\n+  extends Thread with Logging {\n+  private[continuous] var failureReason: Throwable = _\n+\n+  private val epochEndpoint = EpochCoordinatorRef.get(\n+    context.getLocalProperty(ContinuousExecution.RUN_ID_KEY), SparkEnv.get)\n+  private var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+  override def run(): Unit = {\n+    try {\n+      val newEpoch = epochEndpoint.askSync[Long](GetCurrentEpoch())\n+      for (i <- currentEpoch to newEpoch - 1) {\n+        queue.put((null, null))\n+        logDebug(s\"Sent marker to start epoch ${i + 1}\")\n+      }\n+      currentEpoch = newEpoch\n+    } catch {\n+      case t: Throwable =>\n+        failedFlag.set(true)\n+        failureReason = t\n+        throw t"
  }],
  "prId": 19984
}]