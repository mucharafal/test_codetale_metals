[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "Can you document the high level approach in classdoc?\n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-25T19:44:33Z",
    "diffHunk": "@@ -17,16 +17,69 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+private[spark]\n+class UnsafeCartesianRDD(rdd1 : RDD[UnsafeRow], rdd2 : RDD[UnsafeRow])"
  }],
  "prId": 9969
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "does the `UnsafeExternalSorter` preserve records order if it spills? \n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-26T01:49:01Z",
    "diffHunk": "@@ -17,16 +17,69 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+private[spark]\n+class UnsafeCartesianRDD(rdd1 : RDD[UnsafeRow], rdd2 : RDD[UnsafeRow])\n+  extends CartesianRDD[UnsafeRow, UnsafeRow](rdd1.sparkContext, rdd1, rdd2) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[(UnsafeRow, UnsafeRow)] = {\n+    val sorter = UnsafeExternalSorter.create(\n+      context.taskMemoryManager(),\n+      SparkEnv.get.blockManager,\n+      context,\n+      null,\n+      null,\n+      1024,\n+      SparkEnv.get.memoryManager.pageSizeBytes)\n+\n+    val currSplit = split.asInstanceOf[CartesianPartition]\n+    var numFields = 0\n+    for (y <- rdd2.iterator(currSplit.s2, context)) {\n+      numFields = y.numFields()\n+      sorter.insertRecord(y.getBaseObject, y.getBaseOffset, y.getSizeInBytes, 0)\n+    }\n+\n+    def createIter(): Iterator[UnsafeRow] = {\n+      val iter = sorter.getIterator\n+      val unsafeRow = new UnsafeRow\n+      new Iterator[UnsafeRow] {\n+        override def hasNext: Boolean = {\n+          iter.hasNext\n+        }\n+        override def next(): UnsafeRow = {\n+          iter.loadNext()\n+          unsafeRow.pointTo(iter.getBaseObject, iter.getBaseOffset, numFields, iter.getRecordLength)\n+          unsafeRow\n+        }\n+      }\n+    }\n+\n+    val resultIter =\n+      for (x <- rdd1.iterator(currSplit.s1, context);\n+           y <- createIter()) yield (x, y)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "and we may also need to update `CartesianProduct` strategy to put smaller child at right side.\n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-26T01:51:16Z",
    "diffHunk": "@@ -17,16 +17,69 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+private[spark]\n+class UnsafeCartesianRDD(rdd1 : RDD[UnsafeRow], rdd2 : RDD[UnsafeRow])\n+  extends CartesianRDD[UnsafeRow, UnsafeRow](rdd1.sparkContext, rdd1, rdd2) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[(UnsafeRow, UnsafeRow)] = {\n+    val sorter = UnsafeExternalSorter.create(\n+      context.taskMemoryManager(),\n+      SparkEnv.get.blockManager,\n+      context,\n+      null,\n+      null,\n+      1024,\n+      SparkEnv.get.memoryManager.pageSizeBytes)\n+\n+    val currSplit = split.asInstanceOf[CartesianPartition]\n+    var numFields = 0\n+    for (y <- rdd2.iterator(currSplit.s2, context)) {\n+      numFields = y.numFields()\n+      sorter.insertRecord(y.getBaseObject, y.getBaseOffset, y.getSizeInBytes, 0)\n+    }\n+\n+    def createIter(): Iterator[UnsafeRow] = {\n+      val iter = sorter.getIterator\n+      val unsafeRow = new UnsafeRow\n+      new Iterator[UnsafeRow] {\n+        override def hasNext: Boolean = {\n+          iter.hasNext\n+        }\n+        override def next(): UnsafeRow = {\n+          iter.loadNext()\n+          unsafeRow.pointTo(iter.getBaseObject, iter.getBaseOffset, numFields, iter.getRecordLength)\n+          unsafeRow\n+        }\n+      }\n+    }\n+\n+    val resultIter =\n+      for (x <- rdd1.iterator(currSplit.s1, context);\n+           y <- createIter()) yield (x, y)"
  }, {
    "author": {
      "login": "davies"
    },
    "body": "As we discussed it in #7417, right now it's not clear that which metric could be used as the `size` of table, that could be another story.\n\nEven the right table is larger than left, this approach is still much better than current one (building the partition is usually much expensive than loading them from memory or disk), it also fix another problem that the right table could be nondeterministic. \n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-26T07:02:04Z",
    "diffHunk": "@@ -17,16 +17,69 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+private[spark]\n+class UnsafeCartesianRDD(rdd1 : RDD[UnsafeRow], rdd2 : RDD[UnsafeRow])\n+  extends CartesianRDD[UnsafeRow, UnsafeRow](rdd1.sparkContext, rdd1, rdd2) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[(UnsafeRow, UnsafeRow)] = {\n+    val sorter = UnsafeExternalSorter.create(\n+      context.taskMemoryManager(),\n+      SparkEnv.get.blockManager,\n+      context,\n+      null,\n+      null,\n+      1024,\n+      SparkEnv.get.memoryManager.pageSizeBytes)\n+\n+    val currSplit = split.asInstanceOf[CartesianPartition]\n+    var numFields = 0\n+    for (y <- rdd2.iterator(currSplit.s2, context)) {\n+      numFields = y.numFields()\n+      sorter.insertRecord(y.getBaseObject, y.getBaseOffset, y.getSizeInBytes, 0)\n+    }\n+\n+    def createIter(): Iterator[UnsafeRow] = {\n+      val iter = sorter.getIterator\n+      val unsafeRow = new UnsafeRow\n+      new Iterator[UnsafeRow] {\n+        override def hasNext: Boolean = {\n+          iter.hasNext\n+        }\n+        override def next(): UnsafeRow = {\n+          iter.loadNext()\n+          unsafeRow.pointTo(iter.getBaseObject, iter.getBaseOffset, numFields, iter.getRecordLength)\n+          unsafeRow\n+        }\n+      }\n+    }\n+\n+    val resultIter =\n+      for (x <- rdd1.iterator(currSplit.s1, context);\n+           y <- createIter()) yield (x, y)"
  }, {
    "author": {
      "login": "davies"
    },
    "body": "@cloud-fan For the first question, yes.\n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-26T07:02:24Z",
    "diffHunk": "@@ -17,16 +17,69 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+private[spark]\n+class UnsafeCartesianRDD(rdd1 : RDD[UnsafeRow], rdd2 : RDD[UnsafeRow])\n+  extends CartesianRDD[UnsafeRow, UnsafeRow](rdd1.sparkContext, rdd1, rdd2) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[(UnsafeRow, UnsafeRow)] = {\n+    val sorter = UnsafeExternalSorter.create(\n+      context.taskMemoryManager(),\n+      SparkEnv.get.blockManager,\n+      context,\n+      null,\n+      null,\n+      1024,\n+      SparkEnv.get.memoryManager.pageSizeBytes)\n+\n+    val currSplit = split.asInstanceOf[CartesianPartition]\n+    var numFields = 0\n+    for (y <- rdd2.iterator(currSplit.s2, context)) {\n+      numFields = y.numFields()\n+      sorter.insertRecord(y.getBaseObject, y.getBaseOffset, y.getSizeInBytes, 0)\n+    }\n+\n+    def createIter(): Iterator[UnsafeRow] = {\n+      val iter = sorter.getIterator\n+      val unsafeRow = new UnsafeRow\n+      new Iterator[UnsafeRow] {\n+        override def hasNext: Boolean = {\n+          iter.hasNext\n+        }\n+        override def next(): UnsafeRow = {\n+          iter.loadNext()\n+          unsafeRow.pointTo(iter.getBaseObject, iter.getBaseOffset, numFields, iter.getRecordLength)\n+          unsafeRow\n+        }\n+      }\n+    }\n+\n+    val resultIter =\n+      for (x <- rdd1.iterator(currSplit.s1, context);\n+           y <- createIter()) yield (x, y)"
  }],
  "prId": 9969
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "Does it make sense to move this into sorter? Seems other calls might want this.\n\nSomething like UnsafeRowSorter.unsafeRowIterator().\n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-30T18:10:23Z",
    "diffHunk": "@@ -17,16 +17,75 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+/**\n+  * An optimized CartesianRDD for UnsafeRow, which will cache the rows from second child RDD,\n+  * will be much faster than building the right partition for every row in left RDD, it also\n+  * materialize the right RDD (in case of the right RDD is nondeterministic).\n+  */\n+private[spark]\n+class UnsafeCartesianRDD(left : RDD[UnsafeRow], right : RDD[UnsafeRow], numFieldsOfRight: Int)\n+  extends CartesianRDD[UnsafeRow, UnsafeRow](left.sparkContext, left, right) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[(UnsafeRow, UnsafeRow)] = {\n+    // We will not sort the rows, so prefixComparator and recordComparator are null.\n+    val sorter = UnsafeExternalSorter.create(\n+      context.taskMemoryManager(),\n+      SparkEnv.get.blockManager,\n+      context,\n+      null,\n+      null,\n+      1024,\n+      SparkEnv.get.memoryManager.pageSizeBytes)\n+\n+    val partition = split.asInstanceOf[CartesianPartition]\n+    for (y <- rdd2.iterator(partition.s2, context)) {\n+      sorter.insertRecord(y.getBaseObject, y.getBaseOffset, y.getSizeInBytes, 0)\n+    }\n+\n+    // Create an iterator from sorter and wrapper it as Iterator[UnsafeRow]\n+    def createIter(): Iterator[UnsafeRow] = {",
    "line": 44
  }, {
    "author": {
      "login": "davies"
    },
    "body": "We use `UnsafeExternalSorter` here, it does not known the meaning of pointers. Or we should use UnsafeExternalRowSorter, it need more changes.\n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-30T18:22:49Z",
    "diffHunk": "@@ -17,16 +17,75 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+/**\n+  * An optimized CartesianRDD for UnsafeRow, which will cache the rows from second child RDD,\n+  * will be much faster than building the right partition for every row in left RDD, it also\n+  * materialize the right RDD (in case of the right RDD is nondeterministic).\n+  */\n+private[spark]\n+class UnsafeCartesianRDD(left : RDD[UnsafeRow], right : RDD[UnsafeRow], numFieldsOfRight: Int)\n+  extends CartesianRDD[UnsafeRow, UnsafeRow](left.sparkContext, left, right) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[(UnsafeRow, UnsafeRow)] = {\n+    // We will not sort the rows, so prefixComparator and recordComparator are null.\n+    val sorter = UnsafeExternalSorter.create(\n+      context.taskMemoryManager(),\n+      SparkEnv.get.blockManager,\n+      context,\n+      null,\n+      null,\n+      1024,\n+      SparkEnv.get.memoryManager.pageSizeBytes)\n+\n+    val partition = split.asInstanceOf[CartesianPartition]\n+    for (y <- rdd2.iterator(partition.s2, context)) {\n+      sorter.insertRecord(y.getBaseObject, y.getBaseOffset, y.getSizeInBytes, 0)\n+    }\n+\n+    // Create an iterator from sorter and wrapper it as Iterator[UnsafeRow]\n+    def createIter(): Iterator[UnsafeRow] = {",
    "line": 44
  }, {
    "author": {
      "login": "nongli"
    },
    "body": "I see. Ok.\n",
    "commit": "91c782444b847df8a84d5b9496db20fc9f6edb7e",
    "createdAt": "2015-11-30T18:25:17Z",
    "diffHunk": "@@ -17,16 +17,75 @@\n \n package org.apache.spark.sql.execution.joins\n \n-import org.apache.spark.rdd.RDD\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CartesianPartition, CartesianRDD, RDD}\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.catalyst.expressions.{Attribute, JoinedRow}\n-import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeRow}\n import org.apache.spark.sql.execution.metric.SQLMetrics\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.CompletionIterator\n+import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n+\n+\n+/**\n+  * An optimized CartesianRDD for UnsafeRow, which will cache the rows from second child RDD,\n+  * will be much faster than building the right partition for every row in left RDD, it also\n+  * materialize the right RDD (in case of the right RDD is nondeterministic).\n+  */\n+private[spark]\n+class UnsafeCartesianRDD(left : RDD[UnsafeRow], right : RDD[UnsafeRow], numFieldsOfRight: Int)\n+  extends CartesianRDD[UnsafeRow, UnsafeRow](left.sparkContext, left, right) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[(UnsafeRow, UnsafeRow)] = {\n+    // We will not sort the rows, so prefixComparator and recordComparator are null.\n+    val sorter = UnsafeExternalSorter.create(\n+      context.taskMemoryManager(),\n+      SparkEnv.get.blockManager,\n+      context,\n+      null,\n+      null,\n+      1024,\n+      SparkEnv.get.memoryManager.pageSizeBytes)\n+\n+    val partition = split.asInstanceOf[CartesianPartition]\n+    for (y <- rdd2.iterator(partition.s2, context)) {\n+      sorter.insertRecord(y.getBaseObject, y.getBaseOffset, y.getSizeInBytes, 0)\n+    }\n+\n+    // Create an iterator from sorter and wrapper it as Iterator[UnsafeRow]\n+    def createIter(): Iterator[UnsafeRow] = {",
    "line": 44
  }],
  "prId": 9969
}]