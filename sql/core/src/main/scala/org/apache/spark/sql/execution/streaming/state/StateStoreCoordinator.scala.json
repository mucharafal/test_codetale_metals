[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "remove extra line.\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-15T19:42:42Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEnv}\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator.StateStoreCoordinatorEndpoint\n+\n+private sealed trait StateStoreCoordinatorMessage extends Serializable\n+private case class ReportActiveInstance(storeId: StateStoreId, host: String, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+private case class VerifyIfInstanceActive(storeId: StateStoreId, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+private object StopCoordinator extends StateStoreCoordinatorMessage\n+\n+\n+class StateStoreCoordinator(rpcEnv: RpcEnv) {\n+  private val coordinatorRef = rpcEnv.setupEndpoint(\n+    \"StateStoreCoordinator\", new StateStoreCoordinatorEndpoint(rpcEnv, this))\n+  private val instances = new mutable.HashMap[StateStoreId, ExecutorCacheTaskLocation]\n+\n+  def reportActiveInstance(storeId: StateStoreId, host: String, executorId: String): Boolean = {\n+    instances.synchronized { instances.put(storeId, ExecutorCacheTaskLocation(host, executorId)) }\n+    true\n+  }\n+\n+  def verifyIfInstanceActive(storeId: StateStoreId, executorId: String): Boolean = {\n+    instances.synchronized {\n+      instances.get(storeId).forall(_.executorId == executorId)\n+    }\n+  }\n+\n+  def getLocation(storeId: StateStoreId): Option[String] = {\n+    instances.synchronized { instances.get(storeId).map(_.toString) }\n+  }\n+\n+  def makeInstancesInactive(operatorIds: Set[Long]): Unit = {\n+    instances.synchronized {\n+      val instancesToRemove =\n+        instances.keys.filter(id => operatorIds.contains(id.operatorId)).toSeq\n+      instances --= instancesToRemove\n+    }\n+  }\n+}\n+\n+private[spark] object StateStoreCoordinator {\n+\n+  private[spark] class StateStoreCoordinatorEndpoint(\n+    override val rpcEnv: RpcEnv, coordinator: StateStoreCoordinator)\n+    extends RpcEndpoint with Logging {\n+\n+    override def receive: PartialFunction[Any, Unit] = {\n+      case StopCoordinator =>\n+        logInfo(\"StateStoreCoordinator stopped!\")\n+        stop()\n+    }\n+\n+    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+      case ReportActiveInstance(id, host, executorId) =>\n+        context.reply(coordinator.reportActiveInstance(id, host, executorId))\n+      case VerifyIfInstanceActive(id, executor) =>\n+        context.reply(coordinator.verifyIfInstanceActive(id, executor))\n+    }\n+  }\n+}\n+\n+"
  }],
  "prId": 11645
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "This is a pretty general exception to catch.  Can we not check to see if it already exists instead of relying on an exception to get the already existing coordinator.\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-22T00:11:58Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEndpointRef, RpcEnv}\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.util.RpcUtils\n+\n+/** Trait representing all messages to [[StateStoreCoordinator]] */\n+private sealed trait StateStoreCoordinatorMessage extends Serializable\n+\n+/** Classes representing messages */\n+private case class ReportActiveInstance(storeId: StateStoreId, host: String, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class VerifyIfInstanceActive(storeId: StateStoreId, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class GetLocation(storeId: StateStoreId)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class DeactivateInstances(storeRootLocation: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private object StopCoordinator\n+  extends StateStoreCoordinatorMessage\n+\n+/** Helper object used to create reference to [[StateStoreCoordinator]]. */\n+private[sql] object StateStoreCoordinatorRef extends Logging {\n+\n+  private val endpointName = \"StateStoreCoordinator\"\n+\n+  /**\n+   * Create a reference to a [[StateStoreCoordinator]], This can be called from driver as well as\n+   * executors.\n+   */\n+  def apply(env: SparkEnv): StateStoreCoordinatorRef = synchronized {\n+    try {\n+      val coordinator = new StateStoreCoordinator(env.rpcEnv)\n+      val coordinatorRef = env.rpcEnv.setupEndpoint(endpointName, coordinator)\n+      logInfo(\"Registered StateStoreCoordinator endpoint\")\n+      new StateStoreCoordinatorRef(coordinatorRef)\n+    } catch {\n+      case e: IllegalArgumentException =>",
    "line": 63
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "There isnt any API in the RPC stuff to do that at this point, definitely not atomically. @zsxwing \n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-22T00:22:26Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEndpointRef, RpcEnv}\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.util.RpcUtils\n+\n+/** Trait representing all messages to [[StateStoreCoordinator]] */\n+private sealed trait StateStoreCoordinatorMessage extends Serializable\n+\n+/** Classes representing messages */\n+private case class ReportActiveInstance(storeId: StateStoreId, host: String, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class VerifyIfInstanceActive(storeId: StateStoreId, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class GetLocation(storeId: StateStoreId)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class DeactivateInstances(storeRootLocation: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private object StopCoordinator\n+  extends StateStoreCoordinatorMessage\n+\n+/** Helper object used to create reference to [[StateStoreCoordinator]]. */\n+private[sql] object StateStoreCoordinatorRef extends Logging {\n+\n+  private val endpointName = \"StateStoreCoordinator\"\n+\n+  /**\n+   * Create a reference to a [[StateStoreCoordinator]], This can be called from driver as well as\n+   * executors.\n+   */\n+  def apply(env: SparkEnv): StateStoreCoordinatorRef = synchronized {\n+    try {\n+      val coordinator = new StateStoreCoordinator(env.rpcEnv)\n+      val coordinatorRef = env.rpcEnv.setupEndpoint(endpointName, coordinator)\n+      logInfo(\"Registered StateStoreCoordinator endpoint\")\n+      new StateStoreCoordinatorRef(coordinatorRef)\n+    } catch {\n+      case e: IllegalArgumentException =>",
    "line": 63
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "I think we can add a new API to RPC. This can be done later.\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-22T20:52:22Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEndpointRef, RpcEnv}\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.util.RpcUtils\n+\n+/** Trait representing all messages to [[StateStoreCoordinator]] */\n+private sealed trait StateStoreCoordinatorMessage extends Serializable\n+\n+/** Classes representing messages */\n+private case class ReportActiveInstance(storeId: StateStoreId, host: String, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class VerifyIfInstanceActive(storeId: StateStoreId, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class GetLocation(storeId: StateStoreId)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class DeactivateInstances(storeRootLocation: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private object StopCoordinator\n+  extends StateStoreCoordinatorMessage\n+\n+/** Helper object used to create reference to [[StateStoreCoordinator]]. */\n+private[sql] object StateStoreCoordinatorRef extends Logging {\n+\n+  private val endpointName = \"StateStoreCoordinator\"\n+\n+  /**\n+   * Create a reference to a [[StateStoreCoordinator]], This can be called from driver as well as\n+   * executors.\n+   */\n+  def apply(env: SparkEnv): StateStoreCoordinatorRef = synchronized {\n+    try {\n+      val coordinator = new StateStoreCoordinator(env.rpcEnv)\n+      val coordinatorRef = env.rpcEnv.setupEndpoint(endpointName, coordinator)\n+      logInfo(\"Registered StateStoreCoordinator endpoint\")\n+      new StateStoreCoordinatorRef(coordinatorRef)\n+    } catch {\n+      case e: IllegalArgumentException =>",
    "line": 63
  }],
  "prId": 11645
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: You can use one line code here: `instances.retain((storeId, _) => storeId.checkpointLocation != loc)`\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-22T20:44:37Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEndpointRef, RpcEnv}\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.util.RpcUtils\n+\n+/** Trait representing all messages to [[StateStoreCoordinator]] */\n+private sealed trait StateStoreCoordinatorMessage extends Serializable\n+\n+/** Classes representing messages */\n+private case class ReportActiveInstance(storeId: StateStoreId, host: String, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class VerifyIfInstanceActive(storeId: StateStoreId, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class GetLocation(storeId: StateStoreId)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class DeactivateInstances(storeRootLocation: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private object StopCoordinator\n+  extends StateStoreCoordinatorMessage\n+\n+/** Helper object used to create reference to [[StateStoreCoordinator]]. */\n+private[sql] object StateStoreCoordinatorRef extends Logging {\n+\n+  private val endpointName = \"StateStoreCoordinator\"\n+\n+  /**\n+   * Create a reference to a [[StateStoreCoordinator]], This can be called from driver as well as\n+   * executors.\n+   */\n+  def apply(env: SparkEnv): StateStoreCoordinatorRef = synchronized {\n+    try {\n+      val coordinator = new StateStoreCoordinator(env.rpcEnv)\n+      val coordinatorRef = env.rpcEnv.setupEndpoint(endpointName, coordinator)\n+      logInfo(\"Registered StateStoreCoordinator endpoint\")\n+      new StateStoreCoordinatorRef(coordinatorRef)\n+    } catch {\n+      case e: IllegalArgumentException =>\n+        logDebug(\"Retrieving existing StateStoreCoordinator endpoint\")\n+        val rpcEndpointRef = RpcUtils.makeDriverRef(endpointName, env.conf, env.rpcEnv)\n+        new StateStoreCoordinatorRef(rpcEndpointRef)\n+    }\n+  }\n+}\n+\n+/**\n+ * Reference to a [[StateStoreCoordinator]] that can be used to coordinator instances of\n+ * [[StateStore]]s across all the executors, and get their locations for job scheduling.\n+ */\n+private[sql] class StateStoreCoordinatorRef private(rpcEndpointRef: RpcEndpointRef) {\n+\n+  private[state] def reportActiveInstance(\n+      storeId: StateStoreId,\n+      host: String,\n+      executorId: String): Unit = {\n+    rpcEndpointRef.send(ReportActiveInstance(storeId, host, executorId))\n+  }\n+\n+  /** Verify whether the given executor has the active instance of a state store */\n+  private[state] def verifyIfInstanceActive(storeId: StateStoreId, executorId: String): Boolean = {\n+    rpcEndpointRef.askWithRetry[Boolean](VerifyIfInstanceActive(storeId, executorId))\n+  }\n+\n+  /** Get the location of the state store */\n+  private[state] def getLocation(storeId: StateStoreId): Option[String] = {\n+    rpcEndpointRef.askWithRetry[Option[String]](GetLocation(storeId))\n+  }\n+\n+  /** Deactivate instances related to a set of operator */\n+  private[state] def deactivateInstances(storeRootLocation: String): Unit = {\n+    rpcEndpointRef.askWithRetry[Boolean](DeactivateInstances(storeRootLocation))\n+  }\n+\n+  private[state] def stop(): Unit = {\n+    rpcEndpointRef.askWithRetry[Boolean](StopCoordinator)\n+  }\n+}\n+\n+\n+/**\n+ * Class for coordinating instances of [[StateStore]]s loaded in executors across the cluster,\n+ * and get their locations for job scheduling.\n+ */\n+private class StateStoreCoordinator(override val rpcEnv: RpcEnv) extends RpcEndpoint {\n+  private val instances = new mutable.HashMap[StateStoreId, ExecutorCacheTaskLocation]\n+\n+  override def receive: PartialFunction[Any, Unit] = {\n+    case ReportActiveInstance(id, host, executorId) =>\n+      instances.put(id, ExecutorCacheTaskLocation(host, executorId))\n+  }\n+\n+  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n+    case VerifyIfInstanceActive(id, execId) =>\n+      val response = instances.get(id) match {\n+        case Some(location) => location.executorId == execId\n+        case None => false\n+      }\n+      context.reply(response)\n+\n+    case GetLocation(id) =>\n+      context.reply(instances.get(id).map(_.toString))\n+\n+    case DeactivateInstances(loc) =>\n+      val storeIdsToRemove =\n+        instances.keys.filter(_.checkpointLocation == loc).toSeq\n+      instances --= storeIdsToRemove",
    "line": 137
  }],
  "prId": 11645
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "This can not be called from executors as creating `StateStoreCoordinator` will succeed even if there is one `StateStoreCoordinator` in driver.\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-22T20:49:15Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEndpointRef, RpcEnv}\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.util.RpcUtils\n+\n+/** Trait representing all messages to [[StateStoreCoordinator]] */\n+private sealed trait StateStoreCoordinatorMessage extends Serializable\n+\n+/** Classes representing messages */\n+private case class ReportActiveInstance(storeId: StateStoreId, host: String, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class VerifyIfInstanceActive(storeId: StateStoreId, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class GetLocation(storeId: StateStoreId)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class DeactivateInstances(storeRootLocation: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private object StopCoordinator\n+  extends StateStoreCoordinatorMessage\n+\n+/** Helper object used to create reference to [[StateStoreCoordinator]]. */\n+private[sql] object StateStoreCoordinatorRef extends Logging {\n+\n+  private val endpointName = \"StateStoreCoordinator\"\n+\n+  /**\n+   * Create a reference to a [[StateStoreCoordinator]], This can be called from driver as well as\n+   * executors.",
    "line": 54
  }],
  "prId": 11645
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "We need two methods: one for driver, another for executor.\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-23T00:00:19Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEndpointRef, RpcEnv}\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.util.RpcUtils\n+\n+/** Trait representing all messages to [[StateStoreCoordinator]] */\n+private sealed trait StateStoreCoordinatorMessage extends Serializable\n+\n+/** Classes representing messages */\n+private case class ReportActiveInstance(storeId: StateStoreId, host: String, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class VerifyIfInstanceActive(storeId: StateStoreId, executorId: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class GetLocation(storeId: StateStoreId)\n+  extends StateStoreCoordinatorMessage\n+\n+private case class DeactivateInstances(storeRootLocation: String)\n+  extends StateStoreCoordinatorMessage\n+\n+private object StopCoordinator\n+  extends StateStoreCoordinatorMessage\n+\n+/** Helper object used to create reference to [[StateStoreCoordinator]]. */\n+private[sql] object StateStoreCoordinatorRef extends Logging {\n+\n+  private val endpointName = \"StateStoreCoordinator\"\n+\n+  /**\n+   * Create a reference to a [[StateStoreCoordinator]], This can be called from driver as well as\n+   * executors.\n+   */\n+  def apply(env: SparkEnv): StateStoreCoordinatorRef = synchronized {"
  }],
  "prId": 11645
}]