[{
  "comments": [{
    "author": {
      "login": "wangxiaojing"
    },
    "body": "Space after //\n",
    "commit": "444258dbd98207552b3eea3bb8c47bc8137ea11e",
    "createdAt": "2014-10-29T09:55:27Z",
    "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.AllTuples\n+import org.apache.spark.sql.catalyst.plans.physical.ClusteredDistribution\n+import org.apache.spark.sql.catalyst.errors._\n+import scala.collection.mutable.ArrayBuffer\n+import org.apache.spark.util.collection.CompactBuffer\n+import org.apache.spark.sql.catalyst.plans.physical.ClusteredDistribution\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.SortPartitions\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Groups input data by `partitionExpressions` and computes the `computeExpressions` for each\n+ * group.\n+ * @param partitionExpressions expressions that are evaluated to determine partition.\n+ * @param functionExpressions expressions that are computed for each partition.\n+ * @param child the input data source.\n+ */\n+@DeveloperApi\n+case class WindowFunction(\n+  partitionExpressions: Seq[Expression],\n+  functionExpressions: Seq[NamedExpression],\n+  child: SparkPlan)\n+  extends UnaryNode {\n+\n+  override def requiredChildDistribution =\n+    if (partitionExpressions == Nil) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(partitionExpressions) :: Nil\n+    }\n+\n+  // HACK: Generators don't correctly preserve their output through serializations so we grab\n+  // out child's output attributes statically here.\n+  private[this] val childOutput = child.output\n+\n+  override def output = functionExpressions.map(_.toAttribute)\n+\n+  /** A list of functions that need to be computed for each partition. */\n+  private[this] val computeExpressions = new ArrayBuffer[AggregateExpression]\n+\n+  private[this] val otherExpressions = new ArrayBuffer[NamedExpression]\n+\n+  functionExpressions.foreach { sel =>\n+    sel.collect {\n+      case func: AggregateExpression => computeExpressions += func\n+      case other: NamedExpression if (!other.isInstanceOf[Alias]) => otherExpressions += other\n+    }\n+  }\n+\n+  private[this] val functionAttributes = computeExpressions.map { func =>\n+    func -> AttributeReference(s\"funcResult:$func\", func.dataType, func.nullable)()}\n+\n+  /** The schema of the result of all evaluations */\n+  private[this] val resultAttributes =\n+    otherExpressions.map(_.toAttribute) ++ functionAttributes.map(_._2)\n+\n+  private[this] val resultMap =\n+    (otherExpressions.map { other => other -> other.toAttribute } ++ functionAttributes\n+    ).toMap\n+\n+\n+  private[this] val resultExpressions = functionExpressions.map { sel =>\n+    sel.transform {\n+      case e: Expression if resultMap.contains(e) => resultMap(e)\n+    }\n+  }\n+\n+  private[this] val sortExpressions =\n+    if (child.isInstanceOf[SortPartitions]) {\n+      child.asInstanceOf[SortPartitions].sortExpressions\n+    }\n+    else if (child.isInstanceOf[Sort]) {\n+      child.asInstanceOf[Sort].sortOrder\n+    }\n+    else null\n+\n+  /** Creates a new function buffer for a partition. */\n+  private[this] def newFunctionBuffer(): Array[AggregateFunction] = {\n+    val buffer = new Array[AggregateFunction](computeExpressions.length)\n+    var i = 0\n+    while (i < computeExpressions.length) {\n+      val baseExpr = BindReferences.bindReference(computeExpressions(i), childOutput)\n+      baseExpr.windowRange = computeExpressions(i).windowRange\n+      buffer(i) = baseExpr.newInstance()\n+      i += 1\n+    }\n+    buffer\n+  }\n+\n+  private[this] def computeFunctions(rows: CompactBuffer[Row]): Array[Iterator[Any]] = {\n+    val aggrFunctions = newFunctionBuffer()\n+    val functionResults = new Array[Iterator[Any]](aggrFunctions.length)\n+    var i = 0\n+    while (i < aggrFunctions.length) {\n+      val aggrFunction = aggrFunctions(i)\n+      val base = aggrFunction.base\n+      if (base.windowRange == null) {\n+        if (sortExpressions != null) {\n+          if (aggrFunction.dataType.isInstanceOf[ArrayType]) {\n+            rows.foreach(aggrFunction.update)\n+            functionResults(i) = aggrFunction.eval(EmptyRow).asInstanceOf[Seq[Any]].iterator\n+          } else {\n+            functionResults(i) = rows.map(row => {\n+              aggrFunction.update(row)\n+              aggrFunction.eval(EmptyRow)\n+            }).iterator\n+          }\n+        } else {\n+          rows.foreach(aggrFunction.update)\n+          functionResults(i) = aggrFunction.eval(EmptyRow) match {\n+            case r: Seq[_] => r.iterator\n+            case other => (0 to rows.size - 1).map(r => other).iterator\n+          }\n+        }\n+\n+      } else {\n+        functionResults(i) =\n+          if (base.windowRange.windowType == \"ROWS_RANGE\") rowsWindowFunction(base, rows).iterator\n+          else valueWindowFunction(base, rows).iterator\n+      }\n+      i += 1\n+    }\n+    functionResults\n+  }\n+\n+  private[this] def rowsWindowFunction(base: AggregateExpression,\n+    rows: CompactBuffer[Row]): CompactBuffer[Any] = {\n+\n+    val rangeResults = new CompactBuffer[Any]()\n+    var rowIndex = 0\n+    while (rowIndex < rows.size) {\n+\n+      val windowRange = base.windowRange\n+      var start =\n+        if (windowRange.preceding == Int.MaxValue) 0\n+        else rowIndex - windowRange.preceding\n+      if (start < 0) start = 0\n+      var end =\n+        if (windowRange.following == Int.MaxValue) {\n+          rows.size - 1\n+        } else {\n+          rowIndex + windowRange.following\n+        }\n+      if (end > rows.size - 1) end = rows.size - 1\n+\n+      //new aggregate function"
  }],
  "prId": 2953
}, {
  "comments": [{
    "author": {
      "login": "wangxiaojing"
    },
    "body": "Space after //\n",
    "commit": "444258dbd98207552b3eea3bb8c47bc8137ea11e",
    "createdAt": "2014-10-29T09:55:34Z",
    "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.HashMap\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.AllTuples\n+import org.apache.spark.sql.catalyst.plans.physical.ClusteredDistribution\n+import org.apache.spark.sql.catalyst.errors._\n+import scala.collection.mutable.ArrayBuffer\n+import org.apache.spark.util.collection.CompactBuffer\n+import org.apache.spark.sql.catalyst.plans.physical.ClusteredDistribution\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.dsl.plans._\n+import org.apache.spark.sql.catalyst.dsl.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.SortPartitions\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Groups input data by `partitionExpressions` and computes the `computeExpressions` for each\n+ * group.\n+ * @param partitionExpressions expressions that are evaluated to determine partition.\n+ * @param functionExpressions expressions that are computed for each partition.\n+ * @param child the input data source.\n+ */\n+@DeveloperApi\n+case class WindowFunction(\n+  partitionExpressions: Seq[Expression],\n+  functionExpressions: Seq[NamedExpression],\n+  child: SparkPlan)\n+  extends UnaryNode {\n+\n+  override def requiredChildDistribution =\n+    if (partitionExpressions == Nil) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(partitionExpressions) :: Nil\n+    }\n+\n+  // HACK: Generators don't correctly preserve their output through serializations so we grab\n+  // out child's output attributes statically here.\n+  private[this] val childOutput = child.output\n+\n+  override def output = functionExpressions.map(_.toAttribute)\n+\n+  /** A list of functions that need to be computed for each partition. */\n+  private[this] val computeExpressions = new ArrayBuffer[AggregateExpression]\n+\n+  private[this] val otherExpressions = new ArrayBuffer[NamedExpression]\n+\n+  functionExpressions.foreach { sel =>\n+    sel.collect {\n+      case func: AggregateExpression => computeExpressions += func\n+      case other: NamedExpression if (!other.isInstanceOf[Alias]) => otherExpressions += other\n+    }\n+  }\n+\n+  private[this] val functionAttributes = computeExpressions.map { func =>\n+    func -> AttributeReference(s\"funcResult:$func\", func.dataType, func.nullable)()}\n+\n+  /** The schema of the result of all evaluations */\n+  private[this] val resultAttributes =\n+    otherExpressions.map(_.toAttribute) ++ functionAttributes.map(_._2)\n+\n+  private[this] val resultMap =\n+    (otherExpressions.map { other => other -> other.toAttribute } ++ functionAttributes\n+    ).toMap\n+\n+\n+  private[this] val resultExpressions = functionExpressions.map { sel =>\n+    sel.transform {\n+      case e: Expression if resultMap.contains(e) => resultMap(e)\n+    }\n+  }\n+\n+  private[this] val sortExpressions =\n+    if (child.isInstanceOf[SortPartitions]) {\n+      child.asInstanceOf[SortPartitions].sortExpressions\n+    }\n+    else if (child.isInstanceOf[Sort]) {\n+      child.asInstanceOf[Sort].sortOrder\n+    }\n+    else null\n+\n+  /** Creates a new function buffer for a partition. */\n+  private[this] def newFunctionBuffer(): Array[AggregateFunction] = {\n+    val buffer = new Array[AggregateFunction](computeExpressions.length)\n+    var i = 0\n+    while (i < computeExpressions.length) {\n+      val baseExpr = BindReferences.bindReference(computeExpressions(i), childOutput)\n+      baseExpr.windowRange = computeExpressions(i).windowRange\n+      buffer(i) = baseExpr.newInstance()\n+      i += 1\n+    }\n+    buffer\n+  }\n+\n+  private[this] def computeFunctions(rows: CompactBuffer[Row]): Array[Iterator[Any]] = {\n+    val aggrFunctions = newFunctionBuffer()\n+    val functionResults = new Array[Iterator[Any]](aggrFunctions.length)\n+    var i = 0\n+    while (i < aggrFunctions.length) {\n+      val aggrFunction = aggrFunctions(i)\n+      val base = aggrFunction.base\n+      if (base.windowRange == null) {\n+        if (sortExpressions != null) {\n+          if (aggrFunction.dataType.isInstanceOf[ArrayType]) {\n+            rows.foreach(aggrFunction.update)\n+            functionResults(i) = aggrFunction.eval(EmptyRow).asInstanceOf[Seq[Any]].iterator\n+          } else {\n+            functionResults(i) = rows.map(row => {\n+              aggrFunction.update(row)\n+              aggrFunction.eval(EmptyRow)\n+            }).iterator\n+          }\n+        } else {\n+          rows.foreach(aggrFunction.update)\n+          functionResults(i) = aggrFunction.eval(EmptyRow) match {\n+            case r: Seq[_] => r.iterator\n+            case other => (0 to rows.size - 1).map(r => other).iterator\n+          }\n+        }\n+\n+      } else {\n+        functionResults(i) =\n+          if (base.windowRange.windowType == \"ROWS_RANGE\") rowsWindowFunction(base, rows).iterator\n+          else valueWindowFunction(base, rows).iterator\n+      }\n+      i += 1\n+    }\n+    functionResults\n+  }\n+\n+  private[this] def rowsWindowFunction(base: AggregateExpression,\n+    rows: CompactBuffer[Row]): CompactBuffer[Any] = {\n+\n+    val rangeResults = new CompactBuffer[Any]()\n+    var rowIndex = 0\n+    while (rowIndex < rows.size) {\n+\n+      val windowRange = base.windowRange\n+      var start =\n+        if (windowRange.preceding == Int.MaxValue) 0\n+        else rowIndex - windowRange.preceding\n+      if (start < 0) start = 0\n+      var end =\n+        if (windowRange.following == Int.MaxValue) {\n+          rows.size - 1\n+        } else {\n+          rowIndex + windowRange.following\n+        }\n+      if (end > rows.size - 1) end = rows.size - 1\n+\n+      //new aggregate function\n+      val aggr = base.newInstance()\n+      (start to end).foreach(i => aggr.update(rows(i)))\n+\n+      rangeResults += aggr.eval(EmptyRow)\n+      rowIndex += 1\n+    }\n+    rangeResults\n+  }\n+\n+  private[this] def valueWindowFunction(base: AggregateExpression,\n+    rows: CompactBuffer[Row]): CompactBuffer[Any] = {\n+\n+    val windowRange = base.windowRange\n+\n+    // rande only support 1 order\n+    val sortExpression = BindReferences.bindReference(sortExpressions.head, childOutput)\n+\n+    val preceding = sortExpression.child.dataType match {\n+      case IntegerType => Literal(windowRange.preceding)\n+      case LongType => Literal(windowRange.preceding.toLong)\n+      case DoubleType => Literal(windowRange.preceding.toDouble)\n+      case FloatType => Literal(windowRange.preceding.toFloat)\n+      case ShortType => Literal(windowRange.preceding.toShort)\n+      case DecimalType => Literal(BigDecimal(windowRange.preceding))\n+      case _=> throw new Exception(s\"not support dataType \")\n+    }\n+    val following = sortExpression.child.dataType match {\n+      case IntegerType => Literal(windowRange.following)\n+      case LongType => Literal(windowRange.following.toLong)\n+      case DoubleType => Literal(windowRange.following.toDouble)\n+      case FloatType => Literal(windowRange.following.toFloat)\n+      case ShortType => Literal(windowRange.following.toShort)\n+      case DecimalType => Literal(BigDecimal(windowRange.following))\n+      case _=> throw new Exception(s\"not support dataType \")\n+    }\n+\n+    val rangeResults = new CompactBuffer[Any]()\n+    var rowIndex = 0\n+    while (rowIndex < rows.size) {\n+      val currentRow = rows(rowIndex)\n+      val precedingExpr =\n+        if (sortExpression.direction == Ascending) {\n+          Literal(sortExpression.child.eval(currentRow)) - sortExpression.child <= preceding\n+        } else {\n+          sortExpression.child - Literal(sortExpression.child.eval(currentRow)) <= preceding\n+        }\n+\n+\n+      val followingExpr =\n+        if (sortExpression.direction == Ascending) {\n+          sortExpression.child - Literal(sortExpression.child.eval(currentRow)) <= following\n+        } else {\n+          Literal(sortExpression.child.eval(currentRow)) - sortExpression.child <= following\n+        }\n+\n+      var precedingIndex = 0\n+      var followingIndex = rows.size - 1\n+      if (sortExpression != null) {\n+\n+        if (windowRange.preceding != Int.MaxValue) precedingIndex = rowIndex\n+        while (precedingIndex > 0 &&\n+          precedingExpr.eval(rows(precedingIndex - 1)).asInstanceOf[Boolean]) {\n+          precedingIndex -= 1\n+        }\n+\n+        if (windowRange.following != Int.MaxValue) followingIndex = rowIndex\n+        while (followingIndex < rows.size - 1 &&\n+          followingExpr.eval(rows(followingIndex + 1)).asInstanceOf[Boolean]) {\n+          followingIndex += 1\n+        }\n+      }\n+      //new aggregate function"
  }],
  "prId": 2953
}]