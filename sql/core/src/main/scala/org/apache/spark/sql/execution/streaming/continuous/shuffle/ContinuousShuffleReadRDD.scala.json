[{
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "incomplete comment?\r\n\r\nBottom is a bit ambiguous. Can we explicitly state whats at the bottom (reader or writer) and if this receiving the shuffle data from downstream ?",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-15T18:35:41Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Well, ContinuousShuffleReadRDD is a bit self-documenting as a reader. Added that it's receiving shuffle data from upstream tasks.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-15T22:34:36Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the"
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "\"Bottom is a bit ambiguous\" +1 for this. ",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-16T12:23:37Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "If my understanding is right, this (bottom) will be the RDD which will be just injected before shuffling, so that would be neither reader nor writer.\r\n\r\n`first` and `last` would be good alternative for me if bottom looks like ambiguous. \r\n\r\nAs @arunmahadevan stated, comment looks like incomplete.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-16T13:40:40Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Clarified and completed the comment.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-17T03:24:05Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "shouldn't this wait for epoch markers from all child tasks ?",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-15T18:46:21Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the\n+ */\n+class ContinuousShuffleReadRDD(sc: SparkContext, numPartitions: Int)\n+    extends RDD[UnsafeRow](sc, Nil) {\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    (0 until numPartitions).map(ContinuousShuffleReadPartition).toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val receiver = split.asInstanceOf[ContinuousShuffleReadPartition].receiver\n+\n+    new NextIterator[UnsafeRow] {\n+      override def getNext(): UnsafeRow = receiver.poll() match {\n+        case ReceiverRow(r) => r\n+        case ReceiverEpochMarker() =>"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "It should, but I think that's significant enough to justify its own PR. Added an explicit TODO to be safe.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-15T22:35:31Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the\n+ */\n+class ContinuousShuffleReadRDD(sc: SparkContext, numPartitions: Int)\n+    extends RDD[UnsafeRow](sc, Nil) {\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    (0 until numPartitions).map(ContinuousShuffleReadPartition).toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val receiver = split.asInstanceOf[ContinuousShuffleReadPartition].receiver\n+\n+    new NextIterator[UnsafeRow] {\n+      override def getNext(): UnsafeRow = receiver.poll() match {\n+        case ReceiverRow(r) => r\n+        case ReceiverEpochMarker() =>"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Does this ensure at-least-once? Then we could start from this, and improve it from another PR as @jose-torres stated.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-16T13:51:02Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the\n+ */\n+class ContinuousShuffleReadRDD(sc: SparkContext, numPartitions: Int)\n+    extends RDD[UnsafeRow](sc, Nil) {\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    (0 until numPartitions).map(ContinuousShuffleReadPartition).toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val receiver = split.asInstanceOf[ContinuousShuffleReadPartition].receiver\n+\n+    new NextIterator[UnsafeRow] {\n+      override def getNext(): UnsafeRow = receiver.poll() match {\n+        case ReceiverRow(r) => r\n+        case ReceiverEpochMarker() =>"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "If the task does not wait for markers from all its children, it would not guarantee at-least once.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-16T16:03:29Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the\n+ */\n+class ContinuousShuffleReadRDD(sc: SparkContext, numPartitions: Int)\n+    extends RDD[UnsafeRow](sc, Nil) {\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    (0 until numPartitions).map(ContinuousShuffleReadPartition).toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val receiver = split.asInstanceOf[ContinuousShuffleReadPartition].receiver\n+\n+    new NextIterator[UnsafeRow] {\n+      override def getNext(): UnsafeRow = receiver.poll() match {\n+        case ReceiverRow(r) => r\n+        case ReceiverEpochMarker() =>"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "The way I see it is that the current implementation only supports one UnsafeRow source, and the followup PR will extend it to multiple.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-17T03:25:08Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)\n+    TaskContext.get().addTaskCompletionListener { ctx =>\n+      env.stop(endpoint)\n+    }\n+    (receiver, endpoint)\n+  }\n+}\n+\n+/**\n+ * RDD at the bottom of each continuous processing shuffle task, reading from the\n+ */\n+class ContinuousShuffleReadRDD(sc: SparkContext, numPartitions: Int)\n+    extends RDD[UnsafeRow](sc, Nil) {\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    (0 until numPartitions).map(ContinuousShuffleReadPartition).toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[UnsafeRow] = {\n+    val receiver = split.asInstanceOf[ContinuousShuffleReadPartition].receiver\n+\n+    new NextIterator[UnsafeRow] {\n+      override def getNext(): UnsafeRow = receiver.poll() match {\n+        case ReceiverRow(r) => r\n+        case ReceiverEpochMarker() =>"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "It might be more easier to understand the information flow if the queue size value is directly passed through the RDD as opposed to magically getting it through `SQLConf.get` (hard to debug issues like why is my conf from my session not being used).",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-17T23:02:17Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(SQLConf.get.continuousStreamingExecutorQueueSize, env)"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "would be better for debugging if the endpoint name had a prefix like \"UnsafeRowReceiver\" that distinguishes it from other endpoints.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-17T23:25:22Z",
    "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (receiver, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(SQLConf.get.continuousStreamingExecutorQueueSize, env)\n+    val endpoint = env.setupEndpoint(UUID.randomUUID().toString, receiver)"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: dont need`toString` when its interpolated in the string.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-18T22:50:09Z",
    "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import java.util.UUID\n+\n+import org.apache.spark.{Partition, SparkContext, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.NextIterator\n+\n+case class ContinuousShuffleReadPartition(index: Int, queueSize: Int) extends Partition {\n+  // Initialized only on the executor, and only once even as we call compute() multiple times.\n+  lazy val (reader: ContinuousShuffleReader, endpoint) = {\n+    val env = SparkEnv.get.rpcEnv\n+    val receiver = new UnsafeRowReceiver(queueSize, env)\n+    val endpoint = env.setupEndpoint(s\"UnsafeRowReceiver-${UUID.randomUUID().toString}\", receiver)"
  }],
  "prId": 21337
}]