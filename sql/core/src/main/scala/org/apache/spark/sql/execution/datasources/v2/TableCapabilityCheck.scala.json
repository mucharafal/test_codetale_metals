[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Here I add the batch scan check. It's possible that a table implements `SupportsRead` without reporting `BATCH_READ` capability. For example, a steaming table which doesn't support batch scan. We must check the `BATCH_READ` capability here, instead of relying on the `.isInstaceOf[SupportsRead]` check at the planner side.",
    "commit": "a25f9df12bdc8c05552ae22700bf8ff6cb9bdb50",
    "createdAt": "2019-09-04T13:45:04Z",
    "diffHunk": "@@ -18,25 +18,59 @@\n package org.apache.spark.sql.execution.datasources.v2\n \n import org.apache.spark.sql.AnalysisException\n-import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.logical.{AppendData, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic}\n import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n-import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+import org.apache.spark.sql.sources.v2.TableCapability._\n+import org.apache.spark.sql.types.BooleanType\n \n /**\n- * This rules adds some basic table capability check for streaming scan, without knowing the actual\n- * streaming execution mode.\n+ * Checks the capabilities of Data Source V2 tables, and fail problematic queries earlier.\n  */\n-object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+object TableCapabilityCheck extends (LogicalPlan => Unit) {\n   import DataSourceV2Implicits._\n \n+  private def failAnalysis(msg: String): Unit = throw new AnalysisException(msg)\n+\n   override def apply(plan: LogicalPlan): Unit = {\n-    plan.foreach {\n+    plan foreach {\n+      case r: DataSourceV2Relation if !r.table.supports(BATCH_READ) =>",
    "line": 26
  }],
  "prId": 25679
}]