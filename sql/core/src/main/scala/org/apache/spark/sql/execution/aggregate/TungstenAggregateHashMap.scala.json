[{
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "Let's simplify this. The generated code only needs findOrInsert() and doesn't need find.\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-06T21:52:23Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(key => (key.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(key => (ctx.freshName(\"value\"), key.dataType.typeName))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String =\n+      s\"\"\"\n+         |new org.apache.spark.sql.types.StructType()\n+         |${(groupingKeySchema ++ bufferSchema).map(key =>\n+            s\"\"\".add(\"${key.name}\", org.apache.spark.sql.types.DataTypes.${key.dataType})\"\"\")\n+            .mkString(\"\\n\")};\n+       \"\"\".stripMargin\n+\n+    s\"\"\"\n+       |  private org.apache.spark.sql.execution.vectorized.ColumnarBatch batch;\n+       |  private int[] buckets;\n+       |  private int numBuckets;\n+       |  private int maxSteps;\n+       |  private int numRows = 0;\n+       |  private org.apache.spark.sql.types.StructType schema = $generatedSchema\n+       |\n+       |  public $generatedClassName(int capacity, double loadFactor, int maxSteps) {\n+       |    assert (capacity > 0 && ((capacity & (capacity - 1)) == 0));\n+       |    this.maxSteps = maxSteps;\n+       |    numBuckets = (int) (capacity / loadFactor);\n+       |    batch = org.apache.spark.sql.execution.vectorized.ColumnarBatch.allocate(schema,\n+       |      org.apache.spark.memory.MemoryMode.ON_HEAP, capacity);\n+       |    buckets = new int[numBuckets];\n+       |    java.util.Arrays.fill(buckets, -1);\n+       |  }\n+       |\n+       |  public $generatedClassName() {\n+       |    new $generatedClassName(1 << 16, 0.25, 5);\n+       |  }\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateHashFunction(): String = {\n+    s\"\"\"\n+       |// TODO: Improve this Hash Function\n+       |private long hash($groupingKeySignature) {\n+       |  return ${groupingKeys.map(_._2).mkString(\" & \")};\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateEquals(): String = {\n+    s\"\"\"\n+       |private boolean equals(int idx, $groupingKeySignature) {\n+       |  return ${groupingKeys.zipWithIndex.map(key =>\n+            s\"batch.column(${key._2}).getLong(buckets[idx]) == ${key._1._2}\").mkString(\" && \")};\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateFindOrInsert(): String = {\n+    s\"\"\"\n+       |public org.apache.spark.sql.execution.vectorized.ColumnarBatch.Row findOrInsert(${\n+          groupingKeySignature}) {\n+       |  int idx = find(${groupingKeys.map(_._2).mkString(\", \")});\n+       |  if (idx != -1 && buckets[idx] == -1) {\n+       |    ${groupingKeys.zipWithIndex.map(key =>\n+              s\"batch.column(${key._2}).putLong(numRows, ${key._1._2});\").mkString(\"\\n\")}\n+       |    ${bufferValues.zipWithIndex.map(key =>\n+              s\"batch.column(${groupingKeys.length + key._2}).putLong(numRows, 0);\")\n+              .mkString(\"\\n\")}\n+       |    buckets[idx] = numRows++;\n+       |  }\n+       |  return batch.getRow(buckets[idx]);\n+       |}\n+       |\n+       |private int find($groupingKeySignature) {"
  }],
  "prId": 12161
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "I don't htink this should be generated. I think the generated ctor should take a schema and we should get that from the non-generated code if possible.\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-06T21:53:48Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(key => (key.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(key => (ctx.freshName(\"value\"), key.dataType.typeName))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String ="
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Yea I agree it is weird to generate the schema on the fly. We should just pass the value in.\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:08:25Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(key => (key.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(key => (ctx.freshName(\"value\"), key.dataType.typeName))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String ="
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "@nongli how come you asked him to revert to generated schema? it looks pretty weird to generate code to create the schema when it is already available outside codegen.\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:25:19Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(key => (key.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(key => (ctx.freshName(\"value\"), key.dataType.typeName))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String ="
  }, {
    "author": {
      "login": "nongli"
    },
    "body": "that was my initial thought too but this generated class only works for one schema due to the specialized equals/hash/find signatures. It's not particularly useful to pass in a schema if only one works.\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T15:33:08Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(key => (key.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(key => (ctx.freshName(\"value\"), key.dataType.typeName))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String ="
  }],
  "prId": 12161
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "we should document how this thing works in the classdoc (i.e. explain the physical layout).\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:17:38Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap("
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "also maybe this should be called ColumnarAggMapCodeGenerator?\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:19:37Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap("
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "and this should be an object with a single public function, generate, which takes the constructor arguments?\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:20:26Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap("
  }, {
    "author": {
      "login": "sameeragarwal"
    },
    "body": "Added docs, renames etc. The reason I made it a class was because there was a lot of shared state that'd otherwise have to be passed around in all the functions (`groupingKeys`, `bufferValues`, `groupingKeySignature` etc.).\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T05:15:54Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap("
  }],
  "prId": 12161
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "it'd be great to document the hash function (since it is more difficult to read the generated code)\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:22:23Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(k => (k.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(k => (k.dataType.typeName, ctx.freshName(\"value\")))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String =\n+      s\"\"\"\n+         |new org.apache.spark.sql.types.StructType()\n+         |${(groupingKeySchema ++ bufferSchema).map(key =>\n+          s\"\"\".add(\"${key.name}\", org.apache.spark.sql.types.DataTypes.${key.dataType})\"\"\")\n+          .mkString(\"\\n\")};\n+      \"\"\".stripMargin\n+\n+    s\"\"\"\n+       |  private org.apache.spark.sql.execution.vectorized.ColumnarBatch batch;\n+       |  private int[] buckets;\n+       |  private int numBuckets;\n+       |  private int maxSteps;\n+       |  private int numRows = 0;\n+       |  private org.apache.spark.sql.types.StructType schema = $generatedSchema\n+       |\n+       |  public $generatedClassName(int capacity, double loadFactor, int maxSteps) {\n+       |    assert (capacity > 0 && ((capacity & (capacity - 1)) == 0));\n+       |    this.maxSteps = maxSteps;\n+       |    numBuckets = (int) (capacity / loadFactor);\n+       |    batch = org.apache.spark.sql.execution.vectorized.ColumnarBatch.allocate(schema,\n+       |      org.apache.spark.memory.MemoryMode.ON_HEAP, capacity);\n+       |    buckets = new int[numBuckets];\n+       |    java.util.Arrays.fill(buckets, -1);\n+       |  }\n+       |\n+       |  public $generatedClassName() {\n+       |    new $generatedClassName(1 << 16, 0.25, 5);\n+       |  }\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateHashFunction(): String = {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "one thing that might be useful is to put the generated code actually in as comments. \n\nsame for the generateEquals and generateFindOrInsert\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:23:22Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(k => (k.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(k => (k.dataType.typeName, ctx.freshName(\"value\")))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String =\n+      s\"\"\"\n+         |new org.apache.spark.sql.types.StructType()\n+         |${(groupingKeySchema ++ bufferSchema).map(key =>\n+          s\"\"\".add(\"${key.name}\", org.apache.spark.sql.types.DataTypes.${key.dataType})\"\"\")\n+          .mkString(\"\\n\")};\n+      \"\"\".stripMargin\n+\n+    s\"\"\"\n+       |  private org.apache.spark.sql.execution.vectorized.ColumnarBatch batch;\n+       |  private int[] buckets;\n+       |  private int numBuckets;\n+       |  private int maxSteps;\n+       |  private int numRows = 0;\n+       |  private org.apache.spark.sql.types.StructType schema = $generatedSchema\n+       |\n+       |  public $generatedClassName(int capacity, double loadFactor, int maxSteps) {\n+       |    assert (capacity > 0 && ((capacity & (capacity - 1)) == 0));\n+       |    this.maxSteps = maxSteps;\n+       |    numBuckets = (int) (capacity / loadFactor);\n+       |    batch = org.apache.spark.sql.execution.vectorized.ColumnarBatch.allocate(schema,\n+       |      org.apache.spark.memory.MemoryMode.ON_HEAP, capacity);\n+       |    buckets = new int[numBuckets];\n+       |    java.util.Arrays.fill(buckets, -1);\n+       |  }\n+       |\n+       |  public $generatedClassName() {\n+       |    new $generatedClassName(1 << 16, 0.25, 5);\n+       |  }\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateHashFunction(): String = {"
  }],
  "prId": 12161
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "indent is off here?\n",
    "commit": "ec74328ab73766481d3aa7e566fe592bbde747eb",
    "createdAt": "2016-04-08T04:23:45Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.aggregate\n+\n+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n+import org.apache.spark.sql.types.StructType\n+\n+class TungstenAggregateHashMap(\n+    ctx: CodegenContext,\n+    generatedClassName: String,\n+    groupingKeySchema: StructType,\n+    bufferSchema: StructType) {\n+  val groupingKeys = groupingKeySchema.map(k => (k.dataType.typeName, ctx.freshName(\"key\")))\n+  val bufferValues = bufferSchema.map(k => (k.dataType.typeName, ctx.freshName(\"value\")))\n+  val groupingKeySignature = groupingKeys.map(_.productIterator.toList.mkString(\" \")).mkString(\", \")\n+\n+  def generate(): String = {\n+    s\"\"\"\n+       |public class $generatedClassName {\n+       |${initializeAggregateHashMap()}\n+       |\n+       |${generateFindOrInsert()}\n+       |\n+       |${generateEquals()}\n+       |\n+       |${generateHashFunction()}\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def initializeAggregateHashMap(): String = {\n+    val generatedSchema: String =\n+      s\"\"\"\n+         |new org.apache.spark.sql.types.StructType()\n+         |${(groupingKeySchema ++ bufferSchema).map(key =>\n+          s\"\"\".add(\"${key.name}\", org.apache.spark.sql.types.DataTypes.${key.dataType})\"\"\")\n+          .mkString(\"\\n\")};\n+      \"\"\".stripMargin\n+\n+    s\"\"\"\n+       |  private org.apache.spark.sql.execution.vectorized.ColumnarBatch batch;\n+       |  private int[] buckets;\n+       |  private int numBuckets;\n+       |  private int maxSteps;\n+       |  private int numRows = 0;\n+       |  private org.apache.spark.sql.types.StructType schema = $generatedSchema\n+       |\n+       |  public $generatedClassName(int capacity, double loadFactor, int maxSteps) {\n+       |    assert (capacity > 0 && ((capacity & (capacity - 1)) == 0));\n+       |    this.maxSteps = maxSteps;\n+       |    numBuckets = (int) (capacity / loadFactor);\n+       |    batch = org.apache.spark.sql.execution.vectorized.ColumnarBatch.allocate(schema,\n+       |      org.apache.spark.memory.MemoryMode.ON_HEAP, capacity);\n+       |    buckets = new int[numBuckets];\n+       |    java.util.Arrays.fill(buckets, -1);\n+       |  }\n+       |\n+       |  public $generatedClassName() {\n+       |    new $generatedClassName(1 << 16, 0.25, 5);\n+       |  }\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateHashFunction(): String = {\n+    s\"\"\"\n+       |// TODO: Improve this Hash Function\n+       |private long hash($groupingKeySignature) {\n+       |  return ${groupingKeys.map(_._2).mkString(\" ^ \")};\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateEquals(): String = {\n+    s\"\"\"\n+       |private boolean equals(int idx, $groupingKeySignature) {\n+       |  return ${groupingKeys.zipWithIndex.map(k =>\n+            s\"batch.column(${k._2}).getLong(buckets[idx]) == ${k._1._2}\").mkString(\" && \")};\n+       |}\n+     \"\"\".stripMargin\n+  }\n+\n+  def generateFindOrInsert(): String = {\n+    s\"\"\"\n+       |public org.apache.spark.sql.execution.vectorized.ColumnarBatch.Row findOrInsert(${\n+          groupingKeySignature}) {\n+       |  long h = hash(${groupingKeys.map(_._2).mkString(\", \")});\n+       |  int step = 0;\n+       |  int idx = (int) h & (numBuckets - 1);\n+       |  while (step < maxSteps) {\n+       |    // Return bucket index if it's either an empty slot or already contains the key\n+       |    if (buckets[idx] == -1) {\n+       |      ${groupingKeys.zipWithIndex.map(k =>\n+                s\"batch.column(${k._2}).putLong(numRows, ${k._1._2});\").mkString(\"\\n\")}\n+       |      ${bufferValues.zipWithIndex.map(k =>\n+                s\"batch.column(${groupingKeys.length + k._2}).putLong(numRows, 0);\")\n+                .mkString(\"\\n\")}\n+       |      buckets[idx] = numRows++;\n+       |      return batch.getRow(buckets[idx]);\n+       |    } else if (equals(idx, ${groupingKeys.map(_._2).mkString(\", \")})) {\n+       |      return batch.getRow(buckets[idx]);\n+       |    }\n+       |    idx = (idx + 1) & (numBuckets - 1);\n+       |    step++;\n+       |  }\n+       |// Didn't find it\n+       |return null;"
  }],
  "prId": 12161
}]