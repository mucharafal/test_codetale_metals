[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "but we should not create a set inside loop body. How about\r\n```\r\ndef createRequiredNameset ...\r\n...\r\nval requiredNameSet = createRequiredNameset..\r\nval fields = partitionSchema.fields.filter { field => ... }\r\n```",
    "commit": "b6dab15db00ecfb11f23e6c138ee1d23ec942c49",
    "createdAt": "2019-04-05T08:14:41Z",
    "diffHunk": "@@ -16,15 +16,43 @@\n  */\n package org.apache.spark.sql.execution.datasources.v2\n \n-import org.apache.spark.sql.sources.v2.reader.{ScanBuilder, SupportsPushDownFilters, SupportsPushDownRequiredColumns}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.{PartitioningAwareFileIndex, PartitioningUtils}\n+import org.apache.spark.sql.sources.v2.reader.{ScanBuilder, SupportsPushDownRequiredColumns}\n import org.apache.spark.sql.types.StructType\n \n-abstract class FileScanBuilder(schema: StructType)\n-  extends ScanBuilder\n-  with SupportsPushDownRequiredColumns {\n-  protected var readSchema = schema\n+abstract class FileScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    dataSchema: StructType) extends ScanBuilder with SupportsPushDownRequiredColumns {\n+  private val partitionSchema = fileIndex.partitionSchema\n+  private val isCaseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis\n+  protected var requiredSchema = StructType(dataSchema.fields ++ partitionSchema.fields)\n \n   override def pruneColumns(requiredSchema: StructType): Unit = {\n-    this.readSchema = requiredSchema\n+    this.requiredSchema = requiredSchema\n   }\n+\n+  protected def readDataSchema: StructType = {\n+    val fields = dataSchema.fields.filter { field =>\n+      val colName = PartitioningUtils.getColName(field, isCaseSensitive)\n+      requiredNameSet.contains(colName) && !partitionNameSet.contains(colName)\n+    }\n+    StructType(fields)\n+  }\n+\n+  protected def readPartitionSchema: StructType = {\n+    val fields = partitionSchema.fields.filter { field =>\n+      val colName = PartitioningUtils.getColName(field, isCaseSensitive)\n+      requiredNameSet.contains(colName)\n+    }\n+    StructType(fields)\n+  }\n+\n+  // Define as method instead of value, since `requiredSchema` is mutable."
  }],
  "prId": 24296
}]