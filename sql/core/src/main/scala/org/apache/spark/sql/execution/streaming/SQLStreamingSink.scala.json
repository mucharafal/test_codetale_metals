[{
  "comments": [{
    "author": {
      "login": "sujith71955"
    },
    "body": "do we require micro seconds unit here? milliseconds/seconds will do i guess.the lowest latency supported by structured stream is 100 ms.",
    "commit": "45acfb5a171979ad0d16871034f98236237a57d4",
    "createdAt": "2018-12-03T16:20:06Z",
    "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils\n+import org.apache.spark.sql.sources.v2.StreamingWriteSupportProvider\n+import org.apache.spark.sql.streaming.Trigger\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The basic RunnableCommand for SQLStreaming, using Command.run to start a streaming query.\n+ *\n+ * @param sparkSession\n+ * @param extraOptions\n+ * @param partitionColumnNames\n+ * @param child\n+ */\n+case class SQLStreamingSink(sparkSession: SparkSession,\n+    table: CatalogTable,\n+    child: LogicalPlan)\n+  extends RunnableCommand {\n+\n+  private val sqlConf = sparkSession.sqlContext.conf\n+\n+  /**\n+   * The given column name may not be equal to any of the existing column names if we were in\n+   * case-insensitive context. Normalize the given column name to the real one so that we don't\n+   * need to care about case sensitivity afterwards.\n+   */\n+  private def normalize(df: DataFrame, columnName: String, columnType: String): String = {\n+    val validColumnNames = df.logicalPlan.output.map(_.name)\n+    validColumnNames.find(sparkSession.sessionState.analyzer.resolver(_, columnName))\n+      .getOrElse(throw new AnalysisException(s\"$columnType column $columnName not found in \" +\n+        s\"existing columns (${validColumnNames.mkString(\", \")})\"))\n+  }\n+\n+  /**\n+   * Parse spark.sqlstreaming.trigger.seconds to Trigger\n+   */\n+  private def parseTrigger(): Trigger = {\n+    val trigger = Utils.timeStringAsMs(sqlConf.sqlStreamTrigger)\n+    Trigger.ProcessingTime(trigger, TimeUnit.MICROSECONDS)"
  }, {
    "author": {
      "login": "stczwd"
    },
    "body": "Yeah, I will change it to milliseconds.",
    "commit": "45acfb5a171979ad0d16871034f98236237a57d4",
    "createdAt": "2018-12-05T15:23:05Z",
    "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils\n+import org.apache.spark.sql.sources.v2.StreamingWriteSupportProvider\n+import org.apache.spark.sql.streaming.Trigger\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The basic RunnableCommand for SQLStreaming, using Command.run to start a streaming query.\n+ *\n+ * @param sparkSession\n+ * @param extraOptions\n+ * @param partitionColumnNames\n+ * @param child\n+ */\n+case class SQLStreamingSink(sparkSession: SparkSession,\n+    table: CatalogTable,\n+    child: LogicalPlan)\n+  extends RunnableCommand {\n+\n+  private val sqlConf = sparkSession.sqlContext.conf\n+\n+  /**\n+   * The given column name may not be equal to any of the existing column names if we were in\n+   * case-insensitive context. Normalize the given column name to the real one so that we don't\n+   * need to care about case sensitivity afterwards.\n+   */\n+  private def normalize(df: DataFrame, columnName: String, columnType: String): String = {\n+    val validColumnNames = df.logicalPlan.output.map(_.name)\n+    validColumnNames.find(sparkSession.sessionState.analyzer.resolver(_, columnName))\n+      .getOrElse(throw new AnalysisException(s\"$columnType column $columnName not found in \" +\n+        s\"existing columns (${validColumnNames.mkString(\", \")})\"))\n+  }\n+\n+  /**\n+   * Parse spark.sqlstreaming.trigger.seconds to Trigger\n+   */\n+  private def parseTrigger(): Trigger = {\n+    val trigger = Utils.timeStringAsMs(sqlConf.sqlStreamTrigger)\n+    Trigger.ProcessingTime(trigger, TimeUnit.MICROSECONDS)"
  }],
  "prId": 22575
}, {
  "comments": [{
    "author": {
      "login": "KevinZwx"
    },
    "body": "Continuous processing mode is supported now, do you plan to support it? If so I think we can traverse the logical plan to find out whether this is a continuous query and create a ContinuousTrigger",
    "commit": "45acfb5a171979ad0d16871034f98236237a57d4",
    "createdAt": "2019-04-16T03:54:53Z",
    "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils\n+import org.apache.spark.sql.sources.v2.StreamingWriteSupportProvider\n+import org.apache.spark.sql.streaming.Trigger\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The basic RunnableCommand for SQLStreaming, using Command.run to start a streaming query.\n+ *\n+ * @param sparkSession\n+ * @param extraOptions\n+ * @param partitionColumnNames\n+ * @param child\n+ */\n+case class SQLStreamingSink(sparkSession: SparkSession,\n+    table: CatalogTable,\n+    child: LogicalPlan)\n+  extends RunnableCommand {\n+\n+  private val sqlConf = sparkSession.sqlContext.conf\n+\n+  /**\n+   * The given column name may not be equal to any of the existing column names if we were in\n+   * case-insensitive context. Normalize the given column name to the real one so that we don't\n+   * need to care about case sensitivity afterwards.\n+   */\n+  private def normalize(df: DataFrame, columnName: String, columnType: String): String = {\n+    val validColumnNames = df.logicalPlan.output.map(_.name)\n+    validColumnNames.find(sparkSession.sessionState.analyzer.resolver(_, columnName))\n+      .getOrElse(throw new AnalysisException(s\"$columnType column $columnName not found in \" +\n+        s\"existing columns (${validColumnNames.mkString(\", \")})\"))\n+  }\n+\n+  /**\n+   * Parse spark.sqlstreaming.trigger.seconds to Trigger\n+   */\n+  private def parseTrigger(): Trigger = {\n+    val trigger = Utils.timeStringAsMs(sqlConf.sqlStreamTrigger)\n+    Trigger.ProcessingTime(trigger, TimeUnit.MILLISECONDS)",
    "line": 65
  }, {
    "author": {
      "login": "uncleGen"
    },
    "body": "This feature is under discussion: https://docs.google.com/document/d/19degwnIIcuMSELv6BQ_1VQI5AIVcvGeqOm5xE2-aRA0",
    "commit": "45acfb5a171979ad0d16871034f98236237a57d4",
    "createdAt": "2019-04-17T01:48:49Z",
    "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils\n+import org.apache.spark.sql.sources.v2.StreamingWriteSupportProvider\n+import org.apache.spark.sql.streaming.Trigger\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The basic RunnableCommand for SQLStreaming, using Command.run to start a streaming query.\n+ *\n+ * @param sparkSession\n+ * @param extraOptions\n+ * @param partitionColumnNames\n+ * @param child\n+ */\n+case class SQLStreamingSink(sparkSession: SparkSession,\n+    table: CatalogTable,\n+    child: LogicalPlan)\n+  extends RunnableCommand {\n+\n+  private val sqlConf = sparkSession.sqlContext.conf\n+\n+  /**\n+   * The given column name may not be equal to any of the existing column names if we were in\n+   * case-insensitive context. Normalize the given column name to the real one so that we don't\n+   * need to care about case sensitivity afterwards.\n+   */\n+  private def normalize(df: DataFrame, columnName: String, columnType: String): String = {\n+    val validColumnNames = df.logicalPlan.output.map(_.name)\n+    validColumnNames.find(sparkSession.sessionState.analyzer.resolver(_, columnName))\n+      .getOrElse(throw new AnalysisException(s\"$columnType column $columnName not found in \" +\n+        s\"existing columns (${validColumnNames.mkString(\", \")})\"))\n+  }\n+\n+  /**\n+   * Parse spark.sqlstreaming.trigger.seconds to Trigger\n+   */\n+  private def parseTrigger(): Trigger = {\n+    val trigger = Utils.timeStringAsMs(sqlConf.sqlStreamTrigger)\n+    Trigger.ProcessingTime(trigger, TimeUnit.MILLISECONDS)",
    "line": 65
  }],
  "prId": 22575
}]