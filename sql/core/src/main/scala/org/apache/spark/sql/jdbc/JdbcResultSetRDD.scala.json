[{
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "This will cause object created for every input row, it's better to move out of function `asRow`, e.g. put it in the function `inferSchema`.\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-29T04:43:35Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)"
  }, {
    "author": {
      "login": "chutium"
    },
    "body": "yep, thanks, good idea, this whole RDD is actually based on @yhuai 's JsonRDD https://github.com/apache/spark/tree/master/sql/core/src/main/scala/org/apache/spark/sql/json\n\nso maybe here https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/json/JsonRDD.scala#L348\nshould also be changed?\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-29T13:24:43Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)"
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "OK, I see, @yhuai actually added a TODO in https://github.com/apache/spark/pull/1346.\nhttps://github.com/yhuai/spark/blob/dataTypeAndSchema/sql/core/src/main/scala/org/apache/spark/sql/json/JsonRDD.scala#L360\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-30T02:24:18Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)"
  }],
  "prId": 1612
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Use `while` loop instead of the `zipWithIndex` and `foreach`.\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-29T04:44:03Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)\n+    schema.zipWithIndex.foreach {"
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "This is actually improvement for performance, it probably be merged very quickly if you do that in a new PR.\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-30T02:26:10Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)\n+    schema.zipWithIndex.foreach {"
  }],
  "prId": 1612
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "This probably contains 2 bugs:\n- Not consistent with `JdbcTypes.toPrimitiveDataType`\n\nAs `java.sql.Types.DATE`, `java.sql.Types.TIME` and `java.sql.Types.CLOB` all of them will cast into `StringType` in  `JdbcTypes.toPrimitiveDataType`, `rs.getString` for `java.sql.Types.TIME` here may cause exception.\n- Null value checking\n\nAccording to the document of ResultSet, the primitive type getter methods will return a default value if null value found. For example the API `int getInt(int columnIndex)` will returns `0` if the actual value is `null`.\nWe can write the code like:\n\n```\nval iVal = rs.getInt(i+1)\nif (rs.wasNull()) {\n  row.update(i, null)\n} else {\n  row.update(i, iVal)\n}\n```\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-29T07:31:55Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)\n+    schema.zipWithIndex.foreach {\n+      case (StructField(name, dataType, nullable), i) => {\n+        dataType match {\n+          case StringType  => row.update(i, rs.getString(i+1))"
  }, {
    "author": {
      "login": "chutium"
    },
    "body": "oh, this is serious, thanks a lot\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-29T13:35:42Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)\n+    schema.zipWithIndex.foreach {\n+      case (StructField(name, dataType, nullable), i) => {\n+        dataType match {\n+          case StringType  => row.update(i, rs.getString(i+1))"
  }, {
    "author": {
      "login": "chutium"
    },
    "body": "i find a easier way, we can check rs.wasNull after the dataType match {}, then just once\nif (rs.wasNull) row.update(i, null)\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-29T15:08:46Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)\n+    schema.zipWithIndex.foreach {\n+      case (StructField(name, dataType, nullable), i) => {\n+        dataType match {\n+          case StringType  => row.update(i, rs.getString(i+1))"
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "If we reused the row object, this is a bug here.\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-07-30T02:27:32Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): LogicalPlan = {\n+    val schema = createSchema(jdbcResultSet.getSchema)\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), jdbcResultSet.map(asRow(_, schema))))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private def asRow(rs: ResultSet, schema: Seq[StructField]): Row = {\n+    val row = new GenericMutableRow(schema.length)\n+    schema.zipWithIndex.foreach {\n+      case (StructField(name, dataType, nullable), i) => {\n+        dataType match {\n+          case StringType  => row.update(i, rs.getString(i+1))"
  }],
  "prId": 1612
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "`zipWithIndex` will cause some overhead, let's use `while` loop instead.\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-08-25T04:59:18Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): StructType = {\n+    StructType(createSchema(jdbcResultSet.getSchema))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private[sql] def jdbcResultSetToRow(\n+      jdbcResultSet: JdbcRDD[ResultSet],\n+      schema: StructType) : RDD[Row] = {\n+    val row = new GenericMutableRow(schema.fields.length)\n+    jdbcResultSet.map(asRow(_, row, schema.fields))\n+  }\n+\n+  private def asRow(rs: ResultSet, row: GenericMutableRow, schema: Seq[StructField]): Row = {\n+    schema.zipWithIndex.foreach {"
  }, {
    "author": {
      "login": "chutium"
    },
    "body": "changed to while, learned to write from your `fillObject` in hive `TableReader` :D https://github.com/apache/spark/blob/2b8d89e30ebfe2272229a1eddd7542d7437c9924/sql/hive/src/main/scala/org/apache/spark/sql/hive/TableReader.scala#L279-L288\n\nthanks.\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-09-04T08:41:07Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): StructType = {\n+    StructType(createSchema(jdbcResultSet.getSchema))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private[sql] def jdbcResultSetToRow(\n+      jdbcResultSet: JdbcRDD[ResultSet],\n+      schema: StructType) : RDD[Row] = {\n+    val row = new GenericMutableRow(schema.fields.length)\n+    jdbcResultSet.map(asRow(_, row, schema.fields))\n+  }\n+\n+  private def asRow(rs: ResultSet, row: GenericMutableRow, schema: Seq[StructField]): Row = {\n+    schema.zipWithIndex.foreach {"
  }, {
    "author": {
      "login": "chutium"
    },
    "body": "Hi, @chenghao-intel how about use `for (i <- 1 to schemaFields.length) { ... }` here? better than `while (i < schemaFields.length) { ... i+=1 }` ?\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-09-05T23:03:58Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): StructType = {\n+    StructType(createSchema(jdbcResultSet.getSchema))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private[sql] def jdbcResultSetToRow(\n+      jdbcResultSet: JdbcRDD[ResultSet],\n+      schema: StructType) : RDD[Row] = {\n+    val row = new GenericMutableRow(schema.fields.length)\n+    jdbcResultSet.map(asRow(_, row, schema.fields))\n+  }\n+\n+  private def asRow(rs: ResultSet, row: GenericMutableRow, schema: Seq[StructField]): Row = {\n+    schema.zipWithIndex.foreach {"
  }],
  "prId": 1612
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Would be good to print what the unsupported type is.  Also, try to wrap at the highest syntatic level, for example:\n\n``` scala\ncase unsupportedType =>\n  sys.error(s\"Unsupported jdbc datatype: $unsupportedType\")\n```\n\n(Though actually in this case I think it'll all fit on one line).\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-09-09T02:17:38Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): StructType = {\n+    StructType(createSchema(jdbcResultSet.getSchema))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private[sql] def jdbcResultSetToRow(\n+      jdbcResultSet: JdbcRDD[ResultSet],\n+      schema: StructType) : RDD[Row] = {\n+    val row = new GenericMutableRow(schema.fields.length)\n+    jdbcResultSet.map(asRow(_, row, schema.fields))\n+  }\n+\n+  private def asRow(rs: ResultSet, row: GenericMutableRow, schemaFields: Seq[StructField]): Row = {\n+    var i = 0\n+    while (i < schemaFields.length) {\n+      schemaFields(i).dataType match {\n+        case StringType  => row.update(i, rs.getString(i + 1))\n+        case DecimalType => row.update(i, rs.getBigDecimal(i + 1))\n+        case BooleanType => row.update(i, rs.getBoolean(i + 1))\n+        case ByteType    => row.update(i, rs.getByte(i + 1))\n+        case ShortType   => row.update(i, rs.getShort(i + 1))\n+        case IntegerType => row.update(i, rs.getInt(i + 1))\n+        case LongType    => row.update(i, rs.getLong(i + 1))\n+        case FloatType   => row.update(i, rs.getFloat(i + 1))\n+        case DoubleType  => row.update(i, rs.getDouble(i + 1))\n+        case BinaryType  => row.update(i, rs.getBytes(i + 1))\n+        case TimestampType => row.update(i, rs.getTimestamp(i + 1))\n+        case _ => sys.error(\n+          s\"Unsupported jdbc datatype\")",
    "line": 64
  }],
  "prId": 1612
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "If you are are going to reuse the row object (which is a good idea), I'd use `mapPartitions` instead and create the object inside of the closure.\n",
    "commit": "2013303c37eb8408ea31d285d2d3ac23ded7b3e9",
    "createdAt": "2014-09-09T02:18:20Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.ResultSet\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.JdbcRDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.Logging\n+\n+private[sql] object JdbcResultSetRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      jdbcResultSet: JdbcRDD[ResultSet]): StructType = {\n+    StructType(createSchema(jdbcResultSet.getSchema))\n+  }\n+\n+  private def createSchema(metaSchema: Seq[(String, Int, Boolean)]): Seq[StructField] = {\n+    metaSchema.map(e => StructField(e._1, JdbcTypes.toPrimitiveDataType(e._2), e._3))\n+  }\n+\n+  private[sql] def jdbcResultSetToRow(\n+      jdbcResultSet: JdbcRDD[ResultSet],\n+      schema: StructType) : RDD[Row] = {\n+    val row = new GenericMutableRow(schema.fields.length)\n+    jdbcResultSet.map(asRow(_, row, schema.fields))",
    "line": 45
  }],
  "prId": 1612
}]