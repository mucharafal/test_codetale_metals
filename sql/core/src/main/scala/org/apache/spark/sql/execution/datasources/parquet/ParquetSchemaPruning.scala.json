[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Hmmmm .. shouldn't we exclude this only for filters?",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-07T06:27:39Z",
    "diffHunk": "@@ -196,6 +196,7 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n    */\n   private def getRootFields(expr: Expression): Seq[RootField] = {\n     expr match {\n+      case IsNotNull(_: Attribute) | IsNull(_: Attribute) => Seq.empty"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "If this is in projects, I think we also don't need to include all nested fields?",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-07T07:06:27Z",
    "diffHunk": "@@ -196,6 +196,7 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n    */\n   private def getRootFields(expr: Expression): Seq[RootField] = {\n     expr match {\n+      case IsNotNull(_: Attribute) | IsNull(_: Attribute) => Seq.empty"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "But the case mentioned here looks specific to the pushed filter itself. Can we add a simple test for project case as well?",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-07T07:15:27Z",
    "diffHunk": "@@ -196,6 +196,7 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n    */\n   private def getRootFields(expr: Expression): Seq[RootField] = {\n     expr match {\n+      case IsNotNull(_: Attribute) | IsNull(_: Attribute) => Seq.empty"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I mean, for instance, this case `select address is not null, name.last from contacts` it wouldn't work. I thought this is a quick bandaid fix to resolve a basic case.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-07T08:05:10Z",
    "diffHunk": "@@ -196,6 +196,7 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n    */\n   private def getRootFields(expr: Expression): Seq[RootField] = {\n     expr match {\n+      case IsNotNull(_: Attribute) | IsNull(_: Attribute) => Seq.empty"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Thanks. This was a case I didn't test. Fixed it and added test case.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-07T09:26:37Z",
    "diffHunk": "@@ -196,6 +196,7 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n    */\n   private def getRootFields(expr: Expression): Seq[RootField] = {\n     expr match {\n+      case IsNotNull(_: Attribute) | IsNull(_: Attribute) => Seq.empty"
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "How about \r\n\r\n```scala\r\n      case IsNotNull(_: Attribute) | IsNull(_: Attribute) =>\r\n        expr.children.flatMap(getRootFields).map(_.copy(contentAccessed = false))\r\n      case _ =>\r\n        expr.children.flatMap(getRootFields)\r\n```",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T05:01:26Z",
    "diffHunk": "@@ -196,6 +201,9 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n    */\n   private def getRootFields(expr: Expression): Seq[RootField] = {\n     expr match {\n+      // Those expressions don't really use the nested fields of a root field.\n+      case i@(IsNotNull(_: Attribute) | IsNull(_: Attribute)) =>\n+        getRootFields(i.children(0)).map(_.copy(contentAccessed = false))\n       case att: Attribute =>\n         RootField(StructField(att.name, att.dataType, att.nullable), derivedFromAtt = true) :: Nil\n       case SelectedField(field) => RootField(field, derivedFromAtt = false) :: Nil"
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Formatting and please elaborate the comment",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T05:12:23Z",
    "diffHunk": "@@ -250,8 +258,9 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     }\n \n   /**\n-   * A \"root\" schema field (aka top-level, no-parent) and whether it was derived from\n-   * an attribute or had a proper child.\n+   * A \"root\" schema field (aka top-level, no-parent), whether it was derived from\n+   * an attribute or had a proper child, and whether it was accessed with its content.\n    */\n-  private case class RootField(field: StructField, derivedFromAtt: Boolean)\n+  private case class RootField(field: StructField, derivedFromAtt: Boolean,\n+                               contentAccessed: Boolean = true)"
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "line too long.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T05:13:17Z",
    "diffHunk": "@@ -17,7 +17,7 @@\n \n package org.apache.spark.sql.execution.datasources.parquet\n \n-import org.apache.spark.sql.catalyst.expressions.{And, Attribute, AttributeReference, Expression, NamedExpression}\n+import org.apache.spark.sql.catalyst.expressions.{And, Attribute, AttributeReference, Expression, IsNotNull, IsNull, NamedExpression}"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "which can be wildcard when there are more than 6 entities per https://github.com/databricks/scala-style-guide#imports",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T05:22:00Z",
    "diffHunk": "@@ -17,7 +17,7 @@\n \n package org.apache.spark.sql.execution.datasources.parquet\n \n-import org.apache.spark.sql.catalyst.expressions.{And, Attribute, AttributeReference, Expression, NamedExpression}\n+import org.apache.spark.sql.catalyst.expressions.{And, Attribute, AttributeReference, Expression, IsNotNull, IsNull, NamedExpression}"
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Some comments here please. ",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T05:42:23Z",
    "diffHunk": "@@ -110,7 +110,12 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    val (rootFields, optRootFields) = (projectionRootFields ++ filterRootFields)\n+      .distinct.partition(_.contentAccessed)"
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "nit: -> `i @ (IsNotNull(_: ...`",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T07:17:11Z",
    "diffHunk": "@@ -196,6 +201,9 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n    */\n   private def getRootFields(expr: Expression): Seq[RootField] = {\n     expr match {\n+      // Those expressions don't really use the nested fields of a root field.\n+      case i@(IsNotNull(_: Attribute) | IsNull(_: Attribute)) =>"
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Not a big deal but `.map { root: RootField => StructType(Array(root.field)) }` per https://github.com/databricks/scala-style-guide#pattern-matching",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T07:19:24Z",
    "diffHunk": "@@ -156,7 +161,7 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     // in the resulting schema may differ from their ordering in the logical relation's\n     // original schema\n     val mergedSchema = requestedRootFields\n-      .map { case RootField(field, _) => StructType(Array(field)) }\n+      .map { case RootField(field, _, _) => StructType(Array(field)) }"
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "@dbtsai The question you mentioned at https://github.com/apache/spark/pull/22357/files#r216204022 was addressed by this.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-10T09:32:49Z",
    "diffHunk": "@@ -199,6 +209,15 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n       case att: Attribute =>\n         RootField(StructField(att.name, att.dataType, att.nullable), derivedFromAtt = true) :: Nil\n       case SelectedField(field) => RootField(field, derivedFromAtt = false) :: Nil\n+      // Root field accesses by `IsNotNull` and `IsNull` are special cases as the expressions\n+      // don't actually use any nested fields. These root field accesses might be excluded later\n+      // if there are any nested fields accesses in the query plan.\n+      case IsNotNull(SelectedField(field)) =>\n+        RootField(field, derivedFromAtt = false, contentAccessed = false) :: Nil\n+      case IsNull(SelectedField(field)) =>\n+        RootField(field, derivedFromAtt = false, contentAccessed = false) :: Nil",
    "line": 47
  }],
  "prId": 22357
}, {
  "comments": [{
    "author": {
      "login": "mallman"
    },
    "body": "I'm having trouble accepting this statement, but perhaps I'm reading too much into it (or not enough). Let me illustrate with a couple of queries and their physical plans.\r\n\r\nAssuming the data model in `ParquetSchemaPruningSuite.scala`, the physical plan for the query\r\n\r\n    select employer.id from contacts where employer is not null\r\n\r\nis\r\n\r\n```\r\n== Physical Plan ==\r\n*(1) Project [employer#36.id AS id#46]\r\n+- *(1) Filter isnotnull(employer#36)\r\n   +- *(1) FileScan parquet [employer#36,p#37] Batched: false, Format: Parquet,\r\n    PartitionCount: 2, PartitionFilters: [], PushedFilters: [IsNotNull(employer)],\r\n    ReadSchema: struct<employer:struct<id:int>>\r\n```\r\n\r\nThe physical plan for the query\r\n\r\n    select employer.id from contacts where employer.id is not null\r\n\r\nis\r\n\r\n```\r\n== Physical Plan ==\r\n*(1) Project [employer#36.id AS id#47]\r\n+- *(1) Filter (isnotnull(employer#36) && isnotnull(employer#36.id))\r\n   +- *(1) FileScan parquet [employer#36,p#37] Batched: false, Format: Parquet,\r\n    PartitionCount: 2, PartitionFilters: [], PushedFilters: [IsNotNull(employer)],\r\n    ReadSchema: struct<employer:struct<id:int>>\r\n```\r\n\r\nThe read schemata are the same, but the query filters are not. The file scan for the second query looks as I would expect, but the scan for the first query appears to only read `employer.id` even though it needs to check `employer is not null`. If it only reads `employer.id`, how does it check that `employer.company` is not null? Perhaps `employer.id` is null but `employer.company` is not null for some row...\r\n\r\nI have run some tests to validate that this PR is returning the correct results for both queries, and it is. But I don't understand why.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T04:54:28Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "For the first query, the constrain is `employer is not null`. \r\n\r\nWhen `employer.id` is not `null`, `employer` will always  not be `null`; as a result, this PR will work. \r\n\r\nHowever, when `employer.id` is `null`, `employer` can be `null` or `something`, so we need to check if `employer` is `something` to return a null of `employer.id`.\r\n\r\nI checked in the `ParquetFilter`, `IsNotNull(employer)` will be ignored since it's not a valid parquet filter as parquet doesn't support pushdown on the struct; thus, with this PR, this query will return wrong answer. \r\n\r\nI think in this scenario, as @mallman suggested, we might need to read the full data. ",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T06:37:45Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "> I checked in the ParquetFilter, IsNotNull(employer) will be ignored since it's not a valid parquet filter as parquet doesn't support pushdown on the struct; thus, with this PR, this query will return wrong answer.\r\n\r\nWe may not worry about wrong answer from datasource like Parquet in predicate pushdown. As not all predicates are supported by pushdown, we always have a SparkSQL Filter on top of scan node to make sure to receive correct answer.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T07:12:23Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "A complex column is null and its fields are null are different. I think we don't need to read all the fields to check if the complex column is null. In other words, in above case, when we only read `employer.id` and it is null, the predicate `employer is not null` will still be true because it is a complex column containing a null field.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T09:21:10Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "mallman"
    },
    "body": "@viirya, I see your point about the difference between a complex type being null and a subfield being null. So to answer the following query\r\n\r\n    select address from contacts where name is not null\r\n\r\ndo we need to read any of the fields in `name`? Or perhaps just read one arbitrary field of simple type, like `name.first`? That's surprising, but I'm starting to believe it's true.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T14:08:26Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Currently under this PR, `name` will be fully read. This is not perfect. However, to pick one arbitrary field from `name` sounds a little bit hacky to me. WDYT? cc @dbtsai @cloud-fan ",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T14:58:37Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Btw, I think this is not seen as schema pruning case in the sense of original PR, so maybe we can leave it as it for now.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T15:14:57Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "mallman"
    },
    "body": "Yeah, I'm okay with leaving it as-is.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T15:32:30Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Instead of reading any arbitrary field of simple type (which may not exist if it's a deeply nested struct), I think we should implement the pushdown with complex type in parquet with similar logic, and let parquet reader handle it. \r\n\r\n@viirya Can you create a followup JIRA for this? \r\n\r\nThanks.",
    "commit": "03bf5406f990db21a980a3c1d86cf962e5e51258",
    "createdAt": "2018-09-11T18:35:32Z",
    "diffHunk": "@@ -110,7 +110,17 @@ private[sql] object ParquetSchemaPruning extends Rule[LogicalPlan] {\n     val projectionRootFields = projects.flatMap(getRootFields)\n     val filterRootFields = filters.flatMap(getRootFields)\n \n-    (projectionRootFields ++ filterRootFields).distinct\n+    // Kind of expressions don't need to access any fields of a root fields, e.g., `IsNotNull`.\n+    // For them, if there are any nested fields accessed in the query, we don't need to add root\n+    // field access of above expressions.\n+    // For example, for a query `SELECT name.first FROM contacts WHERE name IS NOT NULL`,\n+    // we don't need to read nested fields of `name` struct other than `first` field.",
    "line": 18
  }],
  "prId": 22357
}]