[{
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "nit: rename to `partitionId`?",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T00:30:50Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I worry that partitionId is ambiguous with the partition to which the shuffle data is being written.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T01:06:38Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "ok makes sense.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T01:10:09Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,"
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "What about the case where the send fails? the result seem to be ignored here..",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T00:54:33Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {\n+    throw new IllegalArgumentException(\"multiple readers not yet supported\")\n+  }\n+\n+  if (outputPartitioner.numPartitions != endpoints.length) {\n+    throw new IllegalArgumentException(s\"partitioner size ${outputPartitioner.numPartitions} did \" +\n+      s\"not match endpoint count ${endpoints.length}\")\n+  }\n+\n+  def write(epoch: Iterator[UnsafeRow]): Unit = {\n+    while (epoch.hasNext) {\n+      val row = epoch.next()\n+      endpoints(outputPartitioner.getPartition(row)).ask[Unit](ReceiverRow(writerId, row))"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "cc @zsxwing \r\n\r\nIt's my understanding that the RPC framework guarantees messages will be sent in the order that they're ask()ed, and that it's therefore not possible for a single row to fail to be sent while the ones before and after it succeed. If this is the case, then we don't need to handle it here - the query will just start failing to make progress.\r\n\r\nIf it's not the case, we'll need a more clever solution. Maybe have the epoch marker message contain a count for the number of rows that are supposed to be in the epoch?",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T01:10:12Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {\n+    throw new IllegalArgumentException(\"multiple readers not yet supported\")\n+  }\n+\n+  if (outputPartitioner.numPartitions != endpoints.length) {\n+    throw new IllegalArgumentException(s\"partitioner size ${outputPartitioner.numPartitions} did \" +\n+      s\"not match endpoint count ${endpoints.length}\")\n+  }\n+\n+  def write(epoch: Iterator[UnsafeRow]): Unit = {\n+    while (epoch.hasNext) {\n+      val row = epoch.next()\n+      endpoints(outputPartitioner.getPartition(row)).ask[Unit](ReceiverRow(writerId, row))"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "A reliable channel (first case) seems like a requirement for correctness. In that case I think the query can just be restarted from the last successful epoch as soon as a failure is detected.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T15:52:51Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {\n+    throw new IllegalArgumentException(\"multiple readers not yet supported\")\n+  }\n+\n+  if (outputPartitioner.numPartitions != endpoints.length) {\n+    throw new IllegalArgumentException(s\"partitioner size ${outputPartitioner.numPartitions} did \" +\n+      s\"not match endpoint count ${endpoints.length}\")\n+  }\n+\n+  def write(epoch: Iterator[UnsafeRow]): Unit = {\n+    while (epoch.hasNext) {\n+      val row = epoch.next()\n+      endpoints(outputPartitioner.getPartition(row)).ask[Unit](ReceiverRow(writerId, row))"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Discussed offline with @zsxwing. It's actually not valid to be sending these async at all - the framework will retry e.g. connection failures on the next row, so we can end up committing an epoch before we detect that a row within it has failed to send. We need to just make these synchronous.\r\n\r\nThis will incur a slight round-trip latency penalty for now, but as mentioned earlier the TCP-based shuffle is what we actually plan to be production quality. I'm hoping to begin work on it after I finish one more PR on top of this. So I think the latency should be fine for now.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-31T22:35:31Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {\n+    throw new IllegalArgumentException(\"multiple readers not yet supported\")\n+  }\n+\n+  if (outputPartitioner.numPartitions != endpoints.length) {\n+    throw new IllegalArgumentException(s\"partitioner size ${outputPartitioner.numPartitions} did \" +\n+      s\"not match endpoint count ${endpoints.length}\")\n+  }\n+\n+  def write(epoch: Iterator[UnsafeRow]): Unit = {\n+    while (epoch.hasNext) {\n+      val row = epoch.next()\n+      endpoints(outputPartitioner.getPartition(row)).ask[Unit](ReceiverRow(writerId, row))"
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "any reason to disable it ? this should work rt?",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T01:02:32Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {",
    "line": 41
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I believe so, but there's no way to test whether it will work until we implement the scheduling support for distributing the addresses of each of the multiple readers.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T01:11:26Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {",
    "line": 41
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: we don't use vertical alignment as they will introduce unnecessary changes in future.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-06-12T22:27:26Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer."
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "you can use `Future.sequence` to send messages in parallel, such as\r\n```scala\r\n    import scala.concurrent.Future\r\n    import scala.concurrent.duration.Duration\r\n    import org.apache.spark.util.ThreadUtils\r\n\r\n    val futures = endpoints.map(_.ask[Unit](ReceiverEpochMarker(writerId)))\r\n    implicit val ec = ThreadUtils.sameThread\r\n    ThreadUtils.awaitResult(Future.sequence(futures), Duration.Inf)\r\n```",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-06-12T22:47:20Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {\n+    throw new IllegalArgumentException(\"multiple readers not yet supported\")\n+  }\n+\n+  if (outputPartitioner.numPartitions != endpoints.length) {\n+    throw new IllegalArgumentException(s\"partitioner size ${outputPartitioner.numPartitions} did \" +\n+      s\"not match endpoint count ${endpoints.length}\")\n+  }\n+\n+  def write(epoch: Iterator[UnsafeRow]): Unit = {\n+    while (epoch.hasNext) {\n+      val row = epoch.next()\n+      endpoints(outputPartitioner.getPartition(row)).askSync[Unit](ReceiverRow(writerId, row))\n+    }\n+\n+    endpoints.foreach(_.askSync[Unit](ReceiverEpochMarker(writerId)))"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Sure, but I don't think there's any benefit to doing so. We need to sequence the messages across epochs too, so there's little parallelization available that way.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-06-13T03:56:53Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {\n+    throw new IllegalArgumentException(\"multiple readers not yet supported\")\n+  }\n+\n+  if (outputPartitioner.numPartitions != endpoints.length) {\n+    throw new IllegalArgumentException(s\"partitioner size ${outputPartitioner.numPartitions} did \" +\n+      s\"not match endpoint count ${endpoints.length}\")\n+  }\n+\n+  def write(epoch: Iterator[UnsafeRow]): Unit = {\n+    while (epoch.hasNext) {\n+      val row = epoch.next()\n+      endpoints(outputPartitioner.getPartition(row)).askSync[Unit](ReceiverRow(writerId, row))\n+    }\n+\n+    endpoints.foreach(_.askSync[Unit](ReceiverEpochMarker(writerId)))"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "As far as I understand, the code here is to send a ReceiverEpochMarker to each endpoint and wait for all of them to response. You can send `ReceiverEpochMarker`s in parallel rather than send and wait one by one.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-06-13T05:18:04Z",
    "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+/**\n+ * A [[ContinuousShuffleWriter]] sending data to [[RPCContinuousShuffleReader]] instances.\n+ *\n+ * @param writerId          The partition ID of this writer.\n+ * @param outputPartitioner The partitioner on the reader side of the shuffle.\n+ * @param endpoints         The [[RPCContinuousShuffleReader]] endpoints to write to. Indexed by\n+ *                          partition ID within outputPartitioner.\n+ */\n+class RPCContinuousShuffleWriter(\n+    writerId: Int,\n+    outputPartitioner: Partitioner,\n+    endpoints: Array[RpcEndpointRef]) extends ContinuousShuffleWriter {\n+\n+  if (outputPartitioner.numPartitions != 1) {\n+    throw new IllegalArgumentException(\"multiple readers not yet supported\")\n+  }\n+\n+  if (outputPartitioner.numPartitions != endpoints.length) {\n+    throw new IllegalArgumentException(s\"partitioner size ${outputPartitioner.numPartitions} did \" +\n+      s\"not match endpoint count ${endpoints.length}\")\n+  }\n+\n+  def write(epoch: Iterator[UnsafeRow]): Unit = {\n+    while (epoch.hasNext) {\n+      val row = epoch.next()\n+      endpoints(outputPartitioner.getPartition(row)).askSync[Unit](ReceiverRow(writerId, row))\n+    }\n+\n+    endpoints.foreach(_.askSync[Unit](ReceiverEpochMarker(writerId)))"
  }],
  "prId": 21428
}]