[{
  "comments": [{
    "author": {
      "login": "gczsjdy"
    },
    "body": ":nit traverse",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-03-17T14:46:35Z",
    "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, ExecutionContextExecutorService}\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{CollapseCodegenStages, SparkPlan}\n+import org.apache.spark.sql.execution.adaptive.rule._\n+import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, Exchange, ShuffleExchangeExec}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{EventLoop, ThreadUtils}\n+\n+/**\n+ * This class inserts [[QueryStageExec]] into the query plan in a bottom-up fashion, and\n+ * materializes the query stages asynchronously as soon as they are created.\n+ *\n+ * When one query stage finishes materialization, a list of adaptive optimizer rules will be\n+ * executed, trying to optimize the query plan with the data statistics collected from the the\n+ * materialized data. Then we travers the query plan again and try to insert more query stages."
  }],
  "prId": 20303
}, {
  "comments": [{
    "author": {
      "login": "gczsjdy"
    },
    "body": "The three createQueryStage(s) brought some confusion...",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-03-17T16:15:27Z",
    "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, ExecutionContextExecutorService}\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{CollapseCodegenStages, SparkPlan}\n+import org.apache.spark.sql.execution.adaptive.rule._\n+import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, Exchange, ShuffleExchangeExec}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{EventLoop, ThreadUtils}\n+\n+/**\n+ * This class inserts [[QueryStageExec]] into the query plan in a bottom-up fashion, and\n+ * materializes the query stages asynchronously as soon as they are created.\n+ *\n+ * When one query stage finishes materialization, a list of adaptive optimizer rules will be\n+ * executed, trying to optimize the query plan with the data statistics collected from the the\n+ * materialized data. Then we travers the query plan again and try to insert more query stages.\n+ *\n+ * To create query stages, we traverse the query tree bottom up. When we hit an exchange node,\n+ * and all the child query stages of this exchange node are materialized, we create a new\n+ * query stage for this exchange node.\n+ *\n+ * Right before the stage creation, a list of query stage optimizer rules will be executed. These\n+ * optimizer rules are different from the adaptive optimizer rules. Query stage optimizer rules only\n+ * focus on a plan sub-tree of a specific query stage, and they will be executed only after all the\n+ * child stages are materialized.\n+ */\n+class QueryStageManager(\n+    initialPlan: SparkPlan,\n+    session: SparkSession,\n+    callback: QueryStageManagerCallback)\n+  extends EventLoop[QueryStageManagerEvent](\"QueryStageCreator\") {\n+\n+  private def conf = session.sessionState.conf\n+\n+  private val readyStages = mutable.HashSet.empty[Int]\n+\n+  private var currentStageId = 0\n+\n+  private val stageCache =\n+    mutable.HashMap.empty[StructType, mutable.Buffer[(Exchange, QueryStageExec)]]\n+\n+  private var currentPlan = initialPlan\n+\n+  private val localProperties = session.sparkContext.getLocalProperties\n+\n+  private implicit def executionContext: ExecutionContextExecutorService = {\n+    QueryStageManager.executionContext\n+  }\n+\n+  // A list of optimizer rules that will be applied when a query stage finishes materialization.\n+  // These rules need to travers the entire query plan, and find chances to optimize the query plan\n+  // with the data statistics collected from materialized query stage's output.\n+  private val adaptiveOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    RemoveRedundantShuffles)\n+\n+  // A list of optimizer rules that will be applied right before a query stage is created.\n+  // These rules need to traverse the plan sub-tree of the query stage to be created, and find\n+  // chances to optimize this query stage given the all its child query stages.\n+  private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    AssertChildStagesMaterialized,\n+    ReduceNumShufflePartitions(conf),\n+    CollapseCodegenStages(conf))\n+\n+  private def optimizeEntirePlan(plan: SparkPlan): SparkPlan = {\n+    adaptiveOptimizerRules.foldLeft(plan) {\n+      case (current, rule) => rule(current)\n+    }\n+  }\n+\n+  private def optimizeQueryStage(plan: SparkPlan): SparkPlan = {\n+    queryStageOptimizerRules.foldLeft(plan) {\n+      case (current, rule) => rule(current)\n+    }\n+  }\n+\n+  override protected def onReceive(event: QueryStageManagerEvent): Unit = event match {\n+    case Start =>\n+      // set active session and local properties for the event loop thread.\n+      SparkSession.setActiveSession(session)\n+      session.sparkContext.setLocalProperties(localProperties)\n+      currentPlan = createQueryStages(initialPlan)\n+\n+    case MaterializeStage(stage) =>\n+      stage.materialize().onComplete { res =>\n+        if (res.isSuccess) {\n+          post(StageReady(stage))\n+        } else {\n+          callback.onStageMaterializationFailed(stage, res.failed.get)\n+          stop()\n+        }\n+      }\n+\n+    case StageReady(stage) =>\n+      readyStages += stage.id\n+      currentPlan = optimizeEntirePlan(currentPlan)\n+      currentPlan = createQueryStages(currentPlan)\n+  }\n+\n+  override protected def onStart(): Unit = {\n+    post(Start)\n+  }\n+\n+  /**\n+   * Traverse the query plan bottom-up, and creates query stages as many as possible.\n+   */\n+  private def createQueryStages(plan: SparkPlan): SparkPlan = {\n+    val result = createQueryStages0(plan)\n+    if (result.allChildStagesReady) {\n+      val finalPlan = optimizeQueryStage(result.newPlan)\n+      callback.onFinalPlan(finalPlan)\n+      finalPlan\n+    } else {\n+      callback.onPlanUpdate(result.newPlan)\n+      result.newPlan\n+    }\n+  }\n+\n+  /**\n+   * This method is called recursively to traverse the plan tree bottom-up. This method returns two\n+   * information: 1) the new plan after we insert query stages. 2) whether or not the child query\n+   * stages of the new plan are all ready.\n+   *\n+   * if the current plan is an exchange node, and all its child query stages are ready, we create\n+   * a new query stage.\n+   */\n+  private def createQueryStages0(plan: SparkPlan): CreateStageResult = plan match {\n+    case e: Exchange =>\n+      val similarStages = stageCache.getOrElseUpdate(e.schema, mutable.Buffer.empty)\n+      similarStages.find(_._1.sameResult(e)) match {\n+        case Some((_, existingStage)) if conf.exchangeReuseEnabled =>\n+          CreateStageResult(\n+            newPlan = ReusedQueryStageExec(existingStage, e.output),\n+            allChildStagesReady = readyStages.contains(existingStage.id))\n+\n+        case _ =>\n+          val result = createQueryStages0(e.child)\n+          val newPlan = e.withNewChildren(Seq(result.newPlan)).asInstanceOf[Exchange]\n+          // Create a query stage only when all the child query stages are ready.\n+          if (result.allChildStagesReady) {\n+            val queryStage = createQueryStage(newPlan)\n+            similarStages.append(e -> queryStage)\n+            // We've created a new stage, which is obviously not ready yet.\n+            CreateStageResult(newPlan = queryStage, allChildStagesReady = false)\n+          } else {\n+            CreateStageResult(newPlan = newPlan, allChildStagesReady = false)\n+          }\n+      }\n+\n+    case q: QueryStageExec =>\n+      CreateStageResult(newPlan = q, allChildStagesReady = readyStages.contains(q.id))\n+\n+    case _ =>\n+      if (plan.children.isEmpty) {\n+        CreateStageResult(newPlan = plan, allChildStagesReady = true)\n+      } else {\n+        val results = plan.children.map(createQueryStages0)\n+        CreateStageResult(\n+          newPlan = plan.withNewChildren(results.map(_.newPlan)),\n+          allChildStagesReady = results.forall(_.allChildStagesReady))\n+      }\n+  }\n+\n+  private def createQueryStage(e: Exchange): QueryStageExec = {",
    "line": 184
  }],
  "prId": 20303
}]