[{
  "comments": [{
    "author": {
      "login": "sameeragarwal"
    },
    "body": "Should we leave this out from this refactoring?",
    "commit": "9a53219e569dd1cbd05f3c0a1920840d53736c12",
    "createdAt": "2017-01-17T22:33:56Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution\r\n+\r\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\r\n+import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\r\n+import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\r\n+import org.apache.spark.sql.execution.metric.SQLMetrics\r\n+import org.apache.spark.sql.execution.vectorized.{ColumnarBatch, ColumnVector}\r\n+import org.apache.spark.sql.types.DataType\r\n+\r\n+\r\n+/**\r\n+ * Helper trait for abstracting scan functionality using\r\n+ * [[org.apache.spark.sql.execution.vectorized.ColumnarBatch]]es.\r\n+ */\r\n+private[sql] trait ColumnarBatchScan extends CodegenSupport {\r\n+\r\n+  val columnIndexes: Array[Int] = null\r\n+\r\n+  val inMemoryTableScan: InMemoryTableScanExec = null\r\n+\r\n+  override lazy val metrics = Map(\r\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\r\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\"))\r\n+\r\n+  lazy val enableScanStatistics: Boolean =\r\n+    sqlContext.getConf(\"spark.sql.inMemoryTableScanStatistics.enable\", \"false\").toBoolean\r\n+\r\n+  /**\r\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\r\n+   * This is called once per [[ColumnarBatch]].\r\n+   */\r\n+  private def genCodeColumnVector(\r\n+      ctx: CodegenContext,\r\n+      columnVar: String,\r\n+      ordinal: String,\r\n+      dataType: DataType,\r\n+      nullable: Boolean): ExprCode = {\r\n+    val javaType = ctx.javaType(dataType)\r\n+    val value = ctx.getValue(columnVar, dataType, ordinal)\r\n+    val isNullVar = if (nullable) { ctx.freshName(\"isNull\") } else { \"false\" }\r\n+    val valueVar = ctx.freshName(\"value\")\r\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\r\n+    val code = s\"${ctx.registerComment(str)}\\n\" + (if (nullable) {\r\n+      s\"\"\"\r\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\r\n+        $javaType $valueVar = $isNullVar ? ${ctx.defaultValue(dataType)} : ($value);\r\n+      \"\"\"\r\n+    } else {\r\n+      s\"$javaType $valueVar = $value;\"\r\n+    }).trim\r\n+    ExprCode(code, isNullVar, valueVar)\r\n+  }\r\n+\r\n+  /**\r\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\r\n+   * This produces an [[UnsafeRow]] for each row in each batch.\r\n+   */\r\n+  // TODO: return ColumnarBatch.Rows instead\r\n+  override protected def doProduce(ctx: CodegenContext): String = {\r\n+    val input = ctx.freshName(\"input\")\r\n+    // PhysicalRDD always just has one input\r\n+    ctx.addMutableState(\"scala.collection.Iterator\", input, s\"$input = inputs[0];\")\r\n+\r\n+    // metrics\r\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\r\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\r\n+    val scanTimeTotalNs = ctx.freshName(\"scanTime\")\r\n+    ctx.addMutableState(\"long\", scanTimeTotalNs, s\"$scanTimeTotalNs = 0;\")\r\n+    val incReadBatches = if (!enableScanStatistics) \"\" else {\r"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "sure, I did. Even if it does not exists, it works for now.",
    "commit": "9a53219e569dd1cbd05f3c0a1920840d53736c12",
    "createdAt": "2017-01-18T10:28:06Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution\r\n+\r\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\r\n+import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\r\n+import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\r\n+import org.apache.spark.sql.execution.metric.SQLMetrics\r\n+import org.apache.spark.sql.execution.vectorized.{ColumnarBatch, ColumnVector}\r\n+import org.apache.spark.sql.types.DataType\r\n+\r\n+\r\n+/**\r\n+ * Helper trait for abstracting scan functionality using\r\n+ * [[org.apache.spark.sql.execution.vectorized.ColumnarBatch]]es.\r\n+ */\r\n+private[sql] trait ColumnarBatchScan extends CodegenSupport {\r\n+\r\n+  val columnIndexes: Array[Int] = null\r\n+\r\n+  val inMemoryTableScan: InMemoryTableScanExec = null\r\n+\r\n+  override lazy val metrics = Map(\r\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\r\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\"))\r\n+\r\n+  lazy val enableScanStatistics: Boolean =\r\n+    sqlContext.getConf(\"spark.sql.inMemoryTableScanStatistics.enable\", \"false\").toBoolean\r\n+\r\n+  /**\r\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\r\n+   * This is called once per [[ColumnarBatch]].\r\n+   */\r\n+  private def genCodeColumnVector(\r\n+      ctx: CodegenContext,\r\n+      columnVar: String,\r\n+      ordinal: String,\r\n+      dataType: DataType,\r\n+      nullable: Boolean): ExprCode = {\r\n+    val javaType = ctx.javaType(dataType)\r\n+    val value = ctx.getValue(columnVar, dataType, ordinal)\r\n+    val isNullVar = if (nullable) { ctx.freshName(\"isNull\") } else { \"false\" }\r\n+    val valueVar = ctx.freshName(\"value\")\r\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\r\n+    val code = s\"${ctx.registerComment(str)}\\n\" + (if (nullable) {\r\n+      s\"\"\"\r\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\r\n+        $javaType $valueVar = $isNullVar ? ${ctx.defaultValue(dataType)} : ($value);\r\n+      \"\"\"\r\n+    } else {\r\n+      s\"$javaType $valueVar = $value;\"\r\n+    }).trim\r\n+    ExprCode(code, isNullVar, valueVar)\r\n+  }\r\n+\r\n+  /**\r\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\r\n+   * This produces an [[UnsafeRow]] for each row in each batch.\r\n+   */\r\n+  // TODO: return ColumnarBatch.Rows instead\r\n+  override protected def doProduce(ctx: CodegenContext): String = {\r\n+    val input = ctx.freshName(\"input\")\r\n+    // PhysicalRDD always just has one input\r\n+    ctx.addMutableState(\"scala.collection.Iterator\", input, s\"$input = inputs[0];\")\r\n+\r\n+    // metrics\r\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\r\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\r\n+    val scanTimeTotalNs = ctx.freshName(\"scanTime\")\r\n+    ctx.addMutableState(\"long\", scanTimeTotalNs, s\"$scanTimeTotalNs = 0;\")\r\n+    val incReadBatches = if (!enableScanStatistics) \"\" else {\r"
  }],
  "prId": 15467
}, {
  "comments": [{
    "author": {
      "login": "sameeragarwal"
    },
    "body": "same comment: maybe we can not introduce `columnIndexes` for now.",
    "commit": "9a53219e569dd1cbd05f3c0a1920840d53736c12",
    "createdAt": "2017-01-17T22:34:37Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution\r\n+\r\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\r\n+import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\r\n+import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\r\n+import org.apache.spark.sql.execution.metric.SQLMetrics\r\n+import org.apache.spark.sql.execution.vectorized.{ColumnarBatch, ColumnVector}\r\n+import org.apache.spark.sql.types.DataType\r\n+\r\n+\r\n+/**\r\n+ * Helper trait for abstracting scan functionality using\r\n+ * [[org.apache.spark.sql.execution.vectorized.ColumnarBatch]]es.\r\n+ */\r\n+private[sql] trait ColumnarBatchScan extends CodegenSupport {\r\n+\r\n+  val columnIndexes: Array[Int] = null\r\n+\r\n+  val inMemoryTableScan: InMemoryTableScanExec = null\r\n+\r\n+  override lazy val metrics = Map(\r\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\r\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\"))\r\n+\r\n+  lazy val enableScanStatistics: Boolean =\r\n+    sqlContext.getConf(\"spark.sql.inMemoryTableScanStatistics.enable\", \"false\").toBoolean\r\n+\r\n+  /**\r\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\r\n+   * This is called once per [[ColumnarBatch]].\r\n+   */\r\n+  private def genCodeColumnVector(\r\n+      ctx: CodegenContext,\r\n+      columnVar: String,\r\n+      ordinal: String,\r\n+      dataType: DataType,\r\n+      nullable: Boolean): ExprCode = {\r\n+    val javaType = ctx.javaType(dataType)\r\n+    val value = ctx.getValue(columnVar, dataType, ordinal)\r\n+    val isNullVar = if (nullable) { ctx.freshName(\"isNull\") } else { \"false\" }\r\n+    val valueVar = ctx.freshName(\"value\")\r\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\r\n+    val code = s\"${ctx.registerComment(str)}\\n\" + (if (nullable) {\r\n+      s\"\"\"\r\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\r\n+        $javaType $valueVar = $isNullVar ? ${ctx.defaultValue(dataType)} : ($value);\r\n+      \"\"\"\r\n+    } else {\r\n+      s\"$javaType $valueVar = $value;\"\r\n+    }).trim\r\n+    ExprCode(code, isNullVar, valueVar)\r\n+  }\r\n+\r\n+  /**\r\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\r\n+   * This produces an [[UnsafeRow]] for each row in each batch.\r\n+   */\r\n+  // TODO: return ColumnarBatch.Rows instead\r\n+  override protected def doProduce(ctx: CodegenContext): String = {\r\n+    val input = ctx.freshName(\"input\")\r\n+    // PhysicalRDD always just has one input\r\n+    ctx.addMutableState(\"scala.collection.Iterator\", input, s\"$input = inputs[0];\")\r\n+\r\n+    // metrics\r\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\r\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\r\n+    val scanTimeTotalNs = ctx.freshName(\"scanTime\")\r\n+    ctx.addMutableState(\"long\", scanTimeTotalNs, s\"$scanTimeTotalNs = 0;\")\r\n+    val incReadBatches = if (!enableScanStatistics) \"\" else {\r\n+      val readPartitions = ctx.addReferenceObj(\"readPartitions\", inMemoryTableScan.readPartitions)\r\n+      val readBatches = ctx.addReferenceObj(\"readBatches\", inMemoryTableScan.readBatches)\r\n+      ctx.addMutableState(\"int\", \"initializeInMemoryTableScanStatistics\",\r\n+        s\"\"\"\r\n+           |$readPartitions.setValue(0);\r\n+           |$readBatches.setValue(0);\r\n+           |if ($input.hasNext()) { $readPartitions.add(1); }\r\n+       \"\"\".stripMargin)\r\n+      s\"$readBatches.add(1);\"\r\n+    }\r\n+\r\n+    val columnarBatchClz = \"org.apache.spark.sql.execution.vectorized.ColumnarBatch\"\r\n+    val batch = ctx.freshName(\"batch\")\r\n+    ctx.addMutableState(columnarBatchClz, batch, s\"$batch = null;\")\r\n+\r\n+    val columnVectorClz = \"org.apache.spark.sql.execution.vectorized.ColumnVector\"\r\n+    val idx = ctx.freshName(\"batchIdx\")\r\n+    ctx.addMutableState(\"int\", idx, s\"$idx = 0;\")\r\n+    val colVars = output.indices.map(i => ctx.freshName(\"colInstance\" + i))\r\n+    val columnAssigns = colVars.zipWithIndex.map { case (name, i) =>\r\n+      ctx.addMutableState(columnVectorClz, name, s\"$name = null;\")\r\n+      val index = if (columnIndexes == null) i else columnIndexes(i)\r"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "Done",
    "commit": "9a53219e569dd1cbd05f3c0a1920840d53736c12",
    "createdAt": "2017-01-18T10:28:11Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution\r\n+\r\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\r\n+import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\r\n+import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\r\n+import org.apache.spark.sql.execution.metric.SQLMetrics\r\n+import org.apache.spark.sql.execution.vectorized.{ColumnarBatch, ColumnVector}\r\n+import org.apache.spark.sql.types.DataType\r\n+\r\n+\r\n+/**\r\n+ * Helper trait for abstracting scan functionality using\r\n+ * [[org.apache.spark.sql.execution.vectorized.ColumnarBatch]]es.\r\n+ */\r\n+private[sql] trait ColumnarBatchScan extends CodegenSupport {\r\n+\r\n+  val columnIndexes: Array[Int] = null\r\n+\r\n+  val inMemoryTableScan: InMemoryTableScanExec = null\r\n+\r\n+  override lazy val metrics = Map(\r\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\r\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\"))\r\n+\r\n+  lazy val enableScanStatistics: Boolean =\r\n+    sqlContext.getConf(\"spark.sql.inMemoryTableScanStatistics.enable\", \"false\").toBoolean\r\n+\r\n+  /**\r\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\r\n+   * This is called once per [[ColumnarBatch]].\r\n+   */\r\n+  private def genCodeColumnVector(\r\n+      ctx: CodegenContext,\r\n+      columnVar: String,\r\n+      ordinal: String,\r\n+      dataType: DataType,\r\n+      nullable: Boolean): ExprCode = {\r\n+    val javaType = ctx.javaType(dataType)\r\n+    val value = ctx.getValue(columnVar, dataType, ordinal)\r\n+    val isNullVar = if (nullable) { ctx.freshName(\"isNull\") } else { \"false\" }\r\n+    val valueVar = ctx.freshName(\"value\")\r\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\r\n+    val code = s\"${ctx.registerComment(str)}\\n\" + (if (nullable) {\r\n+      s\"\"\"\r\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\r\n+        $javaType $valueVar = $isNullVar ? ${ctx.defaultValue(dataType)} : ($value);\r\n+      \"\"\"\r\n+    } else {\r\n+      s\"$javaType $valueVar = $value;\"\r\n+    }).trim\r\n+    ExprCode(code, isNullVar, valueVar)\r\n+  }\r\n+\r\n+  /**\r\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\r\n+   * This produces an [[UnsafeRow]] for each row in each batch.\r\n+   */\r\n+  // TODO: return ColumnarBatch.Rows instead\r\n+  override protected def doProduce(ctx: CodegenContext): String = {\r\n+    val input = ctx.freshName(\"input\")\r\n+    // PhysicalRDD always just has one input\r\n+    ctx.addMutableState(\"scala.collection.Iterator\", input, s\"$input = inputs[0];\")\r\n+\r\n+    // metrics\r\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\r\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\r\n+    val scanTimeTotalNs = ctx.freshName(\"scanTime\")\r\n+    ctx.addMutableState(\"long\", scanTimeTotalNs, s\"$scanTimeTotalNs = 0;\")\r\n+    val incReadBatches = if (!enableScanStatistics) \"\" else {\r\n+      val readPartitions = ctx.addReferenceObj(\"readPartitions\", inMemoryTableScan.readPartitions)\r\n+      val readBatches = ctx.addReferenceObj(\"readBatches\", inMemoryTableScan.readBatches)\r\n+      ctx.addMutableState(\"int\", \"initializeInMemoryTableScanStatistics\",\r\n+        s\"\"\"\r\n+           |$readPartitions.setValue(0);\r\n+           |$readBatches.setValue(0);\r\n+           |if ($input.hasNext()) { $readPartitions.add(1); }\r\n+       \"\"\".stripMargin)\r\n+      s\"$readBatches.add(1);\"\r\n+    }\r\n+\r\n+    val columnarBatchClz = \"org.apache.spark.sql.execution.vectorized.ColumnarBatch\"\r\n+    val batch = ctx.freshName(\"batch\")\r\n+    ctx.addMutableState(columnarBatchClz, batch, s\"$batch = null;\")\r\n+\r\n+    val columnVectorClz = \"org.apache.spark.sql.execution.vectorized.ColumnVector\"\r\n+    val idx = ctx.freshName(\"batchIdx\")\r\n+    ctx.addMutableState(\"int\", idx, s\"$idx = 0;\")\r\n+    val colVars = output.indices.map(i => ctx.freshName(\"colInstance\" + i))\r\n+    val columnAssigns = colVars.zipWithIndex.map { case (name, i) =>\r\n+      ctx.addMutableState(columnVectorClz, name, s\"$name = null;\")\r\n+      val index = if (columnIndexes == null) i else columnIndexes(i)\r"
  }],
  "prId": 15467
}, {
  "comments": [{
    "author": {
      "login": "sameeragarwal"
    },
    "body": "nit: this is unused right?",
    "commit": "9a53219e569dd1cbd05f3c0a1920840d53736c12",
    "createdAt": "2017-01-18T18:01:48Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution\r\n+\r\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\r\n+import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\r\n+import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\r\n+import org.apache.spark.sql.execution.metric.SQLMetrics\r\n+import org.apache.spark.sql.execution.vectorized.{ColumnarBatch, ColumnVector}\r\n+import org.apache.spark.sql.types.DataType\r\n+\r\n+\r\n+/**\r\n+ * Helper trait for abstracting scan functionality using\r\n+ * [[org.apache.spark.sql.execution.vectorized.ColumnarBatch]]es.\r\n+ */\r\n+private[sql] trait ColumnarBatchScan extends CodegenSupport {\r\n+\r\n+  val inMemoryTableScan: InMemoryTableScanExec = null\r",
    "line": 34
  }],
  "prId": 15467
}]