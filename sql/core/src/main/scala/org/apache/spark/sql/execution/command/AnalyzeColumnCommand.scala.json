[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Since `getTableMetadata` and `alterTableStats` already have the above logic, shall we remove the above two redundant lines?\r\n```scala\r\n-    val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase)\r\n-    val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db))\r\n-    val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB)\r\n+    val tableMeta = sessionState.catalog.getTableMetadata(tableIdent)\r\n```",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-12T05:14:12Z",
    "diffHunk": "@@ -89,6 +89,37 @@ case class AnalyzeColumnCommand(\n     columnsToAnalyze\n   }\n \n+  private def analyzeColumnInCatalog(sparkSession: SparkSession): Unit = {\n+    val sessionState = sparkSession.sessionState\n+    val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase)\n+    val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db))"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "ditto.\r\n```scala\r\n-    sessionState.catalog.alterTableStats(tableIdentWithDB, Some(statistics))\r\n+    sessionState.catalog.alterTableStats(tableIdent, Some(statistics))\r\n```",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-12T05:15:17Z",
    "diffHunk": "@@ -89,6 +89,37 @@ case class AnalyzeColumnCommand(\n     columnsToAnalyze\n   }\n \n+  private def analyzeColumnInCatalog(sparkSession: SparkSession): Unit = {\n+    val sessionState = sparkSession.sessionState\n+    val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase)\n+    val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db))\n+    val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n+    if (tableMeta.tableType == CatalogTableType.VIEW) {\n+      throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")\n+    }\n+    val sizeInBytes = CommandUtils.calculateTotalSize(sparkSession, tableMeta)\n+    val relation = sparkSession.table(tableIdent).logicalPlan\n+    val columnsToAnalyze = getColumnsToAnalyze(tableIdent, relation, columnNames, allColumns)\n+\n+    // Compute stats for the computed list of columns.\n+    val (rowCount, newColStats) =\n+      CommandUtils.computeColumnStats(sparkSession, relation, columnsToAnalyze)\n+\n+    val newColCatalogStats = newColStats.map {\n+      case (attr, columnStat) =>\n+        attr.name -> columnStat.toCatalogColumnStat(attr.name, attr.dataType)\n+    }\n+\n+    // We also update table-level stats in order to keep them consistent with column-level stats.\n+    val statistics = CatalogStatistics(\n+      sizeInBytes = sizeInBytes,\n+      rowCount = Some(rowCount),\n+      // Newly computed column stats should override the existing ones.\n+      colStats = tableMeta.stats.map(_.colStats).getOrElse(Map.empty) ++ newColCatalogStats)\n+\n+    sessionState.catalog.alterTableStats(tableIdentWithDB, Some(statistics))"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Since this PR can analyze all cached plans, we need to remove this exception logically.\r\nFor example, how do you think about the following case? Currently, this case fails due to this exception. It would be great if we can handle this too.\r\n```scala\r\n  test(\"database view\") {\r\n    withDatabase(\"db_v\") {\r\n      sql(\"CREATE DATABASE db_v\")\r\n      sql(\"CREATE VIEW db_v.v1 AS SELECT 1 c1\")\r\n      sql(\"CACHE TABLE db_v.v1\")\r\n      sql(\"ANALYZE TABLE db_v.v1 COMPUTE STATISTICS FOR COLUMNS c1\")\r\n      sql(\"SELECT * FROM db_v.v1\").show\r\n    }\r\n  }\r\n```",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-18T02:36:23Z",
    "diffHunk": "@@ -89,6 +97,35 @@ case class AnalyzeColumnCommand(\n     columnsToAnalyze\n   }\n \n+  private def analyzeColumnInCatalog(sparkSession: SparkSession): Unit = {\n+    val sessionState = sparkSession.sessionState\n+    val tableMeta = sessionState.catalog.getTableMetadata(tableIdent)\n+    if (tableMeta.tableType == CatalogTableType.VIEW) {\n+      throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "It seems we can do the same fix for AnalyzeTableCommand: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala#L38\r\nIf no problem, I'll do in a separate follow-up pr.",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-19T12:38:25Z",
    "diffHunk": "@@ -89,6 +97,35 @@ case class AnalyzeColumnCommand(\n     columnsToAnalyze\n   }\n \n+  private def analyzeColumnInCatalog(sparkSession: SparkSession): Unit = {\n+    val sessionState = sparkSession.sessionState\n+    val tableMeta = sessionState.catalog.getTableMetadata(tableIdent)\n+    if (tableMeta.tableType == CatalogTableType.VIEW) {\n+      throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "This can be `ANALYZE TABLE is not supported on views`, but I'm fine with the current one too because it's too specific case.",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-20T22:33:19Z",
    "diffHunk": "@@ -89,6 +97,46 @@ case class AnalyzeColumnCommand(\n     columnsToAnalyze\n   }\n \n+  private def analyzeColumnInCatalog(sparkSession: SparkSession): Unit = {\n+    val sessionState = sparkSession.sessionState\n+    val tableMeta = sessionState.catalog.getTableMetadata(tableIdent)\n+    if (tableMeta.tableType == CatalogTableType.VIEW) {\n+      // Analyzes a catalog view if the view is cached\n+      val cacheManager = sparkSession.sharedState.cacheManager\n+      val plan = sparkSession.table(tableIdent.quotedString).logicalPlan\n+      cacheManager.lookupCachedData(plan) match {\n+        case Some(cachedData) =>\n+          val columnsToAnalyze = getColumnsToAnalyze(\n+            tableIdent, cachedData.plan, columnNames, allColumns)\n+          cacheManager.analyzeColumnCacheQuery(sparkSession, cachedData, columnsToAnalyze)\n+        case _ =>\n+          throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "The above logic 105 ~ 111 is identical with line 63 ~ 68.",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-20T22:35:34Z",
    "diffHunk": "@@ -89,6 +97,46 @@ case class AnalyzeColumnCommand(\n     columnsToAnalyze\n   }\n \n+  private def analyzeColumnInCatalog(sparkSession: SparkSession): Unit = {\n+    val sessionState = sparkSession.sessionState\n+    val tableMeta = sessionState.catalog.getTableMetadata(tableIdent)\n+    if (tableMeta.tableType == CatalogTableType.VIEW) {\n+      // Analyzes a catalog view if the view is cached\n+      val cacheManager = sparkSession.sharedState.cacheManager\n+      val plan = sparkSession.table(tableIdent.quotedString).logicalPlan\n+      cacheManager.lookupCachedData(plan) match {\n+        case Some(cachedData) =>\n+          val columnsToAnalyze = getColumnsToAnalyze(\n+            tableIdent, cachedData.plan, columnNames, allColumns)\n+          cacheManager.analyzeColumnCacheQuery(sparkSession, cachedData, columnsToAnalyze)"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Can we have `AnalysisException` with a better exception message? Currently, this seems to be `Table or view 'tempView' not found in database 'default'`?",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-20T22:42:54Z",
    "diffHunk": "@@ -39,32 +40,39 @@ case class AnalyzeColumnCommand(\n     require(columnNames.isDefined ^ allColumns, \"Parameter `columnNames` or `allColumns` are \" +\n       \"mutually exclusive. Only one of them should be specified.\")\n     val sessionState = sparkSession.sessionState\n-    val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase)\n-    val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db))\n-    val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n-    if (tableMeta.tableType == CatalogTableType.VIEW) {\n-      throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")\n-    }\n-    val sizeInBytes = CommandUtils.calculateTotalSize(sparkSession, tableMeta)\n-    val relation = sparkSession.table(tableIdent).logicalPlan\n-    val columnsToAnalyze = getColumnsToAnalyze(tableIdent, relation, columnNames, allColumns)\n-\n-    // Compute stats for the computed list of columns.\n-    val (rowCount, newColStats) =\n-      CommandUtils.computeColumnStats(sparkSession, relation, columnsToAnalyze)\n \n-    // We also update table-level stats in order to keep them consistent with column-level stats.\n-    val statistics = CatalogStatistics(\n-      sizeInBytes = sizeInBytes,\n-      rowCount = Some(rowCount),\n-      // Newly computed column stats should override the existing ones.\n-      colStats = tableMeta.stats.map(_.colStats).getOrElse(Map.empty) ++ newColStats)\n-\n-    sessionState.catalog.alterTableStats(tableIdentWithDB, Some(statistics))\n+    tableIdent.database match {\n+      case Some(db) if db == sparkSession.sharedState.globalTempViewManager.database =>\n+        val plan = sessionState.catalog.getGlobalTempView(tableIdent.identifier).getOrElse {\n+          throw new NoSuchTableException(db = db, table = tableIdent.identifier)\n+        }\n+        analyzeColumnInTempView(plan, sparkSession)\n+      case Some(_) =>\n+        analyzeColumnInCatalog(sparkSession)\n+      case None =>\n+        sessionState.catalog.getTempView(tableIdent.identifier) match {\n+          case Some(tempView) => analyzeColumnInTempView(tempView, sparkSession)\n+          case _ => analyzeColumnInCatalog(sparkSession)\n+        }\n+    }\n \n     Seq.empty[Row]\n   }\n \n+  private def analyzeColumnInTempView(plan: LogicalPlan, sparkSession: SparkSession): Unit = {\n+    val cacheManager = sparkSession.sharedState.cacheManager\n+    cacheManager.lookupCachedData(plan) match {\n+      case Some(cachedData) =>\n+        val columnsToAnalyze = getColumnsToAnalyze(\n+          tableIdent, cachedData.plan, columnNames, allColumns)\n+        cacheManager.analyzeColumnCacheQuery(sparkSession, cachedData, columnsToAnalyze)\n+      case _ =>\n+        val catalog = sparkSession.sessionState.catalog\n+        val db = tableIdent.database.getOrElse(catalog.getCurrentDatabase)\n+        throw new NoSuchTableException(db = db, table = tableIdent.identifier)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Since this function is always called after we check the existence of global or temp views, it seems that we can give some directional exception message like `To analyze temp views, cache them first`.",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-20T22:46:26Z",
    "diffHunk": "@@ -39,32 +40,39 @@ case class AnalyzeColumnCommand(\n     require(columnNames.isDefined ^ allColumns, \"Parameter `columnNames` or `allColumns` are \" +\n       \"mutually exclusive. Only one of them should be specified.\")\n     val sessionState = sparkSession.sessionState\n-    val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase)\n-    val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db))\n-    val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n-    if (tableMeta.tableType == CatalogTableType.VIEW) {\n-      throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")\n-    }\n-    val sizeInBytes = CommandUtils.calculateTotalSize(sparkSession, tableMeta)\n-    val relation = sparkSession.table(tableIdent).logicalPlan\n-    val columnsToAnalyze = getColumnsToAnalyze(tableIdent, relation, columnNames, allColumns)\n-\n-    // Compute stats for the computed list of columns.\n-    val (rowCount, newColStats) =\n-      CommandUtils.computeColumnStats(sparkSession, relation, columnsToAnalyze)\n \n-    // We also update table-level stats in order to keep them consistent with column-level stats.\n-    val statistics = CatalogStatistics(\n-      sizeInBytes = sizeInBytes,\n-      rowCount = Some(rowCount),\n-      // Newly computed column stats should override the existing ones.\n-      colStats = tableMeta.stats.map(_.colStats).getOrElse(Map.empty) ++ newColStats)\n-\n-    sessionState.catalog.alterTableStats(tableIdentWithDB, Some(statistics))\n+    tableIdent.database match {\n+      case Some(db) if db == sparkSession.sharedState.globalTempViewManager.database =>\n+        val plan = sessionState.catalog.getGlobalTempView(tableIdent.identifier).getOrElse {\n+          throw new NoSuchTableException(db = db, table = tableIdent.identifier)\n+        }\n+        analyzeColumnInTempView(plan, sparkSession)\n+      case Some(_) =>\n+        analyzeColumnInCatalog(sparkSession)\n+      case None =>\n+        sessionState.catalog.getTempView(tableIdent.identifier) match {\n+          case Some(tempView) => analyzeColumnInTempView(tempView, sparkSession)\n+          case _ => analyzeColumnInCatalog(sparkSession)\n+        }\n+    }\n \n     Seq.empty[Row]\n   }\n \n+  private def analyzeColumnInTempView(plan: LogicalPlan, sparkSession: SparkSession): Unit = {\n+    val cacheManager = sparkSession.sharedState.cacheManager\n+    cacheManager.lookupCachedData(plan) match {\n+      case Some(cachedData) =>\n+        val columnsToAnalyze = getColumnsToAnalyze(\n+          tableIdent, cachedData.plan, columnNames, allColumns)\n+        cacheManager.analyzeColumnCacheQuery(sparkSession, cachedData, columnsToAnalyze)\n+      case _ =>\n+        val catalog = sparkSession.sessionState.catalog\n+        val db = tableIdent.database.getOrElse(catalog.getCurrentDatabase)\n+        throw new NoSuchTableException(db = db, table = tableIdent.identifier)"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "Since the existing tests expect this exception, I kept as it is:\r\nhttps://github.com/apache/spark/blob/d6ee2f331db461c1f7a25e0ef17901f53d8b707e/sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala#L163\r\n",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-21T02:12:07Z",
    "diffHunk": "@@ -39,32 +40,39 @@ case class AnalyzeColumnCommand(\n     require(columnNames.isDefined ^ allColumns, \"Parameter `columnNames` or `allColumns` are \" +\n       \"mutually exclusive. Only one of them should be specified.\")\n     val sessionState = sparkSession.sessionState\n-    val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase)\n-    val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db))\n-    val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n-    if (tableMeta.tableType == CatalogTableType.VIEW) {\n-      throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")\n-    }\n-    val sizeInBytes = CommandUtils.calculateTotalSize(sparkSession, tableMeta)\n-    val relation = sparkSession.table(tableIdent).logicalPlan\n-    val columnsToAnalyze = getColumnsToAnalyze(tableIdent, relation, columnNames, allColumns)\n-\n-    // Compute stats for the computed list of columns.\n-    val (rowCount, newColStats) =\n-      CommandUtils.computeColumnStats(sparkSession, relation, columnsToAnalyze)\n \n-    // We also update table-level stats in order to keep them consistent with column-level stats.\n-    val statistics = CatalogStatistics(\n-      sizeInBytes = sizeInBytes,\n-      rowCount = Some(rowCount),\n-      // Newly computed column stats should override the existing ones.\n-      colStats = tableMeta.stats.map(_.colStats).getOrElse(Map.empty) ++ newColStats)\n-\n-    sessionState.catalog.alterTableStats(tableIdentWithDB, Some(statistics))\n+    tableIdent.database match {\n+      case Some(db) if db == sparkSession.sharedState.globalTempViewManager.database =>\n+        val plan = sessionState.catalog.getGlobalTempView(tableIdent.identifier).getOrElse {\n+          throw new NoSuchTableException(db = db, table = tableIdent.identifier)\n+        }\n+        analyzeColumnInTempView(plan, sparkSession)\n+      case Some(_) =>\n+        analyzeColumnInCatalog(sparkSession)\n+      case None =>\n+        sessionState.catalog.getTempView(tableIdent.identifier) match {\n+          case Some(tempView) => analyzeColumnInTempView(tempView, sparkSession)\n+          case _ => analyzeColumnInCatalog(sparkSession)\n+        }\n+    }\n \n     Seq.empty[Row]\n   }\n \n+  private def analyzeColumnInTempView(plan: LogicalPlan, sparkSession: SparkSession): Unit = {\n+    val cacheManager = sparkSession.sharedState.cacheManager\n+    cacheManager.lookupCachedData(plan) match {\n+      case Some(cachedData) =>\n+        val columnsToAnalyze = getColumnsToAnalyze(\n+          tableIdent, cachedData.plan, columnNames, allColumns)\n+        cacheManager.analyzeColumnCacheQuery(sparkSession, cachedData, columnsToAnalyze)\n+      case _ =>\n+        val catalog = sparkSession.sessionState.catalog\n+        val db = tableIdent.database.getOrElse(catalog.getCurrentDatabase)\n+        throw new NoSuchTableException(db = db, table = tableIdent.identifier)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Got it~",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-21T03:22:25Z",
    "diffHunk": "@@ -39,32 +40,39 @@ case class AnalyzeColumnCommand(\n     require(columnNames.isDefined ^ allColumns, \"Parameter `columnNames` or `allColumns` are \" +\n       \"mutually exclusive. Only one of them should be specified.\")\n     val sessionState = sparkSession.sessionState\n-    val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase)\n-    val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db))\n-    val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n-    if (tableMeta.tableType == CatalogTableType.VIEW) {\n-      throw new AnalysisException(\"ANALYZE TABLE is not supported on views.\")\n-    }\n-    val sizeInBytes = CommandUtils.calculateTotalSize(sparkSession, tableMeta)\n-    val relation = sparkSession.table(tableIdent).logicalPlan\n-    val columnsToAnalyze = getColumnsToAnalyze(tableIdent, relation, columnNames, allColumns)\n-\n-    // Compute stats for the computed list of columns.\n-    val (rowCount, newColStats) =\n-      CommandUtils.computeColumnStats(sparkSession, relation, columnsToAnalyze)\n \n-    // We also update table-level stats in order to keep them consistent with column-level stats.\n-    val statistics = CatalogStatistics(\n-      sizeInBytes = sizeInBytes,\n-      rowCount = Some(rowCount),\n-      // Newly computed column stats should override the existing ones.\n-      colStats = tableMeta.stats.map(_.colStats).getOrElse(Map.empty) ++ newColStats)\n-\n-    sessionState.catalog.alterTableStats(tableIdentWithDB, Some(statistics))\n+    tableIdent.database match {\n+      case Some(db) if db == sparkSession.sharedState.globalTempViewManager.database =>\n+        val plan = sessionState.catalog.getGlobalTempView(tableIdent.identifier).getOrElse {\n+          throw new NoSuchTableException(db = db, table = tableIdent.identifier)\n+        }\n+        analyzeColumnInTempView(plan, sparkSession)\n+      case Some(_) =>\n+        analyzeColumnInCatalog(sparkSession)\n+      case None =>\n+        sessionState.catalog.getTempView(tableIdent.identifier) match {\n+          case Some(tempView) => analyzeColumnInTempView(tempView, sparkSession)\n+          case _ => analyzeColumnInCatalog(sparkSession)\n+        }\n+    }\n \n     Seq.empty[Row]\n   }\n \n+  private def analyzeColumnInTempView(plan: LogicalPlan, sparkSession: SparkSession): Unit = {\n+    val cacheManager = sparkSession.sharedState.cacheManager\n+    cacheManager.lookupCachedData(plan) match {\n+      case Some(cachedData) =>\n+        val columnsToAnalyze = getColumnsToAnalyze(\n+          tableIdent, cachedData.plan, columnNames, allColumns)\n+        cacheManager.analyzeColumnCacheQuery(sparkSession, cachedData, columnsToAnalyze)\n+      case _ =>\n+        val catalog = sparkSession.sessionState.catalog\n+        val db = tableIdent.database.getOrElse(catalog.getCurrentDatabase)\n+        throw new NoSuchTableException(db = db, table = tableIdent.identifier)"
  }],
  "prId": 24047
}]