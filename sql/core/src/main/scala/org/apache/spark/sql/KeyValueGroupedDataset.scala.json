[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Can you remove this?",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-09-27T18:09:36Z",
    "diffHunk": "@@ -54,6 +55,14 @@ class KeyValueGroupedDataset[K, V] private[sql](\n   private def sparkSession = queryExecution.sparkSession\n \n   /**\n+   * Returns the schema of this Dataset.\n+   *\n+   * @group basic\n+   * @since 2.3.0\n+   */\n+  def schema: StructType = queryExecution.analyzed.schema"
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "ok",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-09-28T01:41:49Z",
    "diffHunk": "@@ -54,6 +55,14 @@ class KeyValueGroupedDataset[K, V] private[sql](\n   private def sparkSession = queryExecution.sparkSession\n \n   /**\n+   * Returns the schema of this Dataset.\n+   *\n+   * @group basic\n+   * @since 2.3.0\n+   */\n+  def schema: StructType = queryExecution.analyzed.schema"
  }],
  "prId": 19363
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "When we will encounter this error?",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-10-06T01:32:00Z",
    "diffHunk": "@@ -564,4 +565,30 @@ class KeyValueGroupedDataset[K, V] private[sql](\n       encoder: Encoder[R]): Dataset[R] = {\n     cogroup(other)((key, left, right) => f.call(key, left.asJava, right.asJava).asScala)(encoder)\n   }\n+\n+  override def toString: String = {\n+    try {\n+      val builder = new StringBuilder\n+      val kFields = kExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      val vFields = vExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      builder.append(\"[key: [\")\n+      builder.append(kFields.take(2).mkString(\", \"))\n+      if (kFields.length > 2) {\n+        builder.append(\" ... \" + (kFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"], value: [\")\n+      builder.append(vFields.take(2).mkString(\", \"))\n+      if (vFields.length > 2) {\n+        builder.append(\" ... \" + (vFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"]]\").toString()\n+    } catch {\n+      case NonFatal(e) =>"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "same question",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-10-10T12:54:21Z",
    "diffHunk": "@@ -564,4 +565,30 @@ class KeyValueGroupedDataset[K, V] private[sql](\n       encoder: Encoder[R]): Dataset[R] = {\n     cogroup(other)((key, left, right) => f.call(key, left.asJava, right.asJava).asScala)(encoder)\n   }\n+\n+  override def toString: String = {\n+    try {\n+      val builder = new StringBuilder\n+      val kFields = kExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      val vFields = vExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      builder.append(\"[key: [\")\n+      builder.append(kFields.take(2).mkString(\", \"))\n+      if (kFields.length > 2) {\n+        builder.append(\" ... \" + (kFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"], value: [\")\n+      builder.append(vFields.take(2).mkString(\", \"))\n+      if (vFields.length > 2) {\n+        builder.append(\" ... \" + (vFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"]]\").toString()\n+    } catch {\n+      case NonFatal(e) =>"
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "I once thought you would answer that for me https://github.com/apache/spark/blame/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L374, LOL",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-10-11T03:21:16Z",
    "diffHunk": "@@ -564,4 +565,30 @@ class KeyValueGroupedDataset[K, V] private[sql](\n       encoder: Encoder[R]): Dataset[R] = {\n     cogroup(other)((key, left, right) => f.call(key, left.asJava, right.asJava).asScala)(encoder)\n   }\n+\n+  override def toString: String = {\n+    try {\n+      val builder = new StringBuilder\n+      val kFields = kExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      val vFields = vExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      builder.append(\"[key: [\")\n+      builder.append(kFields.take(2).mkString(\", \"))\n+      if (kFields.length > 2) {\n+        builder.append(\" ... \" + (kFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"], value: [\")\n+      builder.append(vFields.take(2).mkString(\", \"))\n+      if (vFields.length > 2) {\n+        builder.append(\" ... \" + (vFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"]]\").toString()\n+    } catch {\n+      case NonFatal(e) =>"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Although some methods of `StringBuilder` can throw exceptions, I didn't see simple `append` can throw exception. Access of `schema` should not throw exception at all. So I don't get why it is.",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-10-13T09:00:45Z",
    "diffHunk": "@@ -564,4 +565,30 @@ class KeyValueGroupedDataset[K, V] private[sql](\n       encoder: Encoder[R]): Dataset[R] = {\n     cogroup(other)((key, left, right) => f.call(key, left.asJava, right.asJava).asScala)(encoder)\n   }\n+\n+  override def toString: String = {\n+    try {\n+      val builder = new StringBuilder\n+      val kFields = kExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      val vFields = vExprEnc.schema.map {\n+        case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+      }\n+      builder.append(\"[key: [\")\n+      builder.append(kFields.take(2).mkString(\", \"))\n+      if (kFields.length > 2) {\n+        builder.append(\" ... \" + (kFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"], value: [\")\n+      builder.append(vFields.take(2).mkString(\", \"))\n+      if (vFields.length > 2) {\n+        builder.append(\" ... \" + (vFields.length - 2) + \" more field(s)\")\n+      }\n+      builder.append(\"]]\").toString()\n+    } catch {\n+      case NonFatal(e) =>"
  }],
  "prId": 19363
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Why import `StructType`? I didn't see you use it.",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-10-06T01:34:23Z",
    "diffHunk": "@@ -18,16 +18,17 @@\n package org.apache.spark.sql\n \n import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import org.apache.spark.annotation.{Experimental, InterfaceStability}\n import org.apache.spark.api.java.function._\n import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}\n import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, CreateStruct}\n import org.apache.spark.sql.catalyst.plans.logical._\n-import org.apache.spark.sql.catalyst.streaming.InternalOutputModes\n import org.apache.spark.sql.execution.QueryExecution\n import org.apache.spark.sql.expressions.ReduceAggregator\n import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout, OutputMode}\n+import org.apache.spark.sql.types.StructType"
  }],
  "prId": 19363
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "unnecessary import",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-10-16T04:29:16Z",
    "diffHunk": "@@ -18,13 +18,13 @@\n package org.apache.spark.sql\n \n import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal"
  }],
  "prId": 19363
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we include `df` here?",
    "commit": "955e3c9a560e4b4cda3b73533654949f78cb6ec9",
    "createdAt": "2017-10-16T04:31:43Z",
    "diffHunk": "@@ -564,4 +564,25 @@ class KeyValueGroupedDataset[K, V] private[sql](\n       encoder: Encoder[R]): Dataset[R] = {\n     cogroup(other)((key, left, right) => f.call(key, left.asJava, right.asJava).asScala)(encoder)\n   }\n+\n+  override def toString: String = {\n+    val builder = new StringBuilder\n+    val kFields = kExprEnc.schema.map {\n+      case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+    }\n+    val vFields = vExprEnc.schema.map {\n+      case f => s\"${f.name}: ${f.dataType.simpleString(2)}\"\n+    }\n+    builder.append(\"KeyValueGroupedDataset: [key: [\")\n+    builder.append(kFields.take(2).mkString(\", \"))\n+    if (kFields.length > 2) {\n+      builder.append(\" ... \" + (kFields.length - 2) + \" more field(s)\")\n+    }\n+    builder.append(\"], value: [\")\n+    builder.append(vFields.take(2).mkString(\", \"))\n+    if (vFields.length > 2) {\n+      builder.append(\" ... \" + (vFields.length - 2) + \" more field(s)\")\n+    }\n+    builder.append(\"]]\").toString()",
    "line": 31
  }],
  "prId": 19363
}]