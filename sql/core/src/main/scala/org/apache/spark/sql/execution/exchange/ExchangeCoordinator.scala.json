[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should remove the exchange being reused.",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T09:44:07Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()",
    "line": 10
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "In the current fix, this `exchanges` doesn't already have reused exchanges, e.g., in the example case described above, `exchanges.size` already has been `1`.",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T10:33:50Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()",
    "line": 10
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah you are right",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:16:21Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()",
    "line": 10
  }],
  "prId": 21754
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can we make it a `def`?",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:17:09Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()\n \n+  private[this] lazy val numExchanges = exchanges.size"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "also we can remove `assert(exchanges.length == numExchanges)`",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:20:17Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()\n \n+  private[this] lazy val numExchanges = exchanges.size"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "ya, we can do so, But, I used `lazy val` there because IMO that made us easily notice some bugs about an illegal method call order. For example, all the exchange should be registered before `ExchangeCoordinator.postShuffleRDD` called first time. If a new exchange is wrongly registered after  `ExchangeCoordinator.postShuffleRDD` called, the assertion fails. WDYT?",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:43:13Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()\n \n+  private[this] lazy val numExchanges = exchanges.size"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "makes sense, let's document it.",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:57:31Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()\n \n+  private[this] lazy val numExchanges = exchanges.size"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "ok, I'll update",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T12:29:10Z",
    "diffHunk": "@@ -83,16 +83,17 @@ import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan}\n  *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)\n  */\n class ExchangeCoordinator(\n-    numExchanges: Int,\n     advisoryTargetPostShuffleInputSize: Long,\n     minNumPostShufflePartitions: Option[Int] = None)\n   extends Logging {\n \n   // The registered Exchange operators.\n   private[this] val exchanges = ArrayBuffer[ShuffleExchangeExec]()\n \n+  private[this] lazy val numExchanges = exchanges.size"
  }],
  "prId": 21754
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why move this check?",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:19:03Z",
    "diffHunk": "@@ -117,10 +118,6 @@ class ExchangeCoordinator(\n    */\n   def estimatePartitionStartIndices(\n       mapOutputStatistics: Array[MapOutputStatistics]): Array[Int] = {\n-    // If we have mapOutputStatistics.length < numExchange, it is because we do not submit\n-    // a stage when the number of partitions of this dependency is 0.\n-    assert(mapOutputStatistics.length <= numExchanges)",
    "line": 30
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "To pass the existing tests for `estimatePartitionStartIndices`, e.g., https://github.com/apache/spark/pull/21754/files#diff-3cd46a3f60c5352282bd3f2c9efff7fcR61.\r\nAs another approach, we might add a dummy `ShuffleExchange` in `ExhcnageCoordinator` there. But, building `ShuffleExchange` is some troublesome in the test suite without `SharedSparkSession`.",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:33:20Z",
    "diffHunk": "@@ -117,10 +118,6 @@ class ExchangeCoordinator(\n    */\n   def estimatePartitionStartIndices(\n       mapOutputStatistics: Array[MapOutputStatistics]): Array[Int] = {\n-    // If we have mapOutputStatistics.length < numExchange, it is because we do not submit\n-    // a stage when the number of partitions of this dependency is 0.\n-    assert(mapOutputStatistics.length <= numExchanges)",
    "line": 30
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ok",
    "commit": "5dfd94843ff776e75e0c0fb5198f36bfebf94288",
    "createdAt": "2018-08-02T11:59:16Z",
    "diffHunk": "@@ -117,10 +118,6 @@ class ExchangeCoordinator(\n    */\n   def estimatePartitionStartIndices(\n       mapOutputStatistics: Array[MapOutputStatistics]): Array[Int] = {\n-    // If we have mapOutputStatistics.length < numExchange, it is because we do not submit\n-    // a stage when the number of partitions of this dependency is 0.\n-    assert(mapOutputStatistics.length <= numExchanges)",
    "line": 30
  }],
  "prId": 21754
}]