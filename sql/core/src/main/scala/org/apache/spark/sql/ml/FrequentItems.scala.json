[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "let's put this in execution.stat?\n\nIt's annoying to add a top level package because we have rules to specifically exclude existing packages.\n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T05:37:45Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml"
  }],
  "prId": 5799
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "If multiple columns are provided, shall we search the combination of them instead of each individually? For example, if I call\n\n``` scala\nfreqItems(Array(\"gender\", \"title\"), 0.01)\n```\n\nI'm expecting the frequent combinations instead of each of them. The current implementation is more flexible because users can create a struct from multiple columns, and this allows to find frequent items on multiple columns in parallel. But I'm a little worried about what users expect when they call `freqItems(Array(\"gender\", \"title\"))` @rxin \n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T05:56:34Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml\n+\n+\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.types.{StructType, ArrayType, StructField}\n+\n+import scala.collection.mutable.{Map => MutableMap}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.{Row, DataFrame, functions}\n+\n+private[sql] object FrequentItems extends Logging {\n+\n+  /**\n+   * Merge two maps of counts. Subtracts the sum of `otherMap` from `baseMap`, and fills in\n+   * any emptied slots with the most frequent of `otherMap`.\n+   * @param baseMap The map containing the global counts\n+   * @param otherMap The map containing the counts for that partition\n+   * @param maxSize The maximum number of counts to keep in memory\n+   */\n+  private def mergeCounts[A](\n+      baseMap: MutableMap[A, Long],\n+      otherMap: MutableMap[A, Long],\n+      maxSize: Int): Unit = {\n+    val otherSum = otherMap.foldLeft(0L) { case (sum, (k, v)) =>\n+      if (!baseMap.contains(k)) sum + v else sum\n+    }\n+    baseMap.retain((k, v) => v > otherSum)\n+    // sort in decreasing order, so that we will add the most frequent items first\n+    val sorted = otherMap.toSeq.sortBy(-_._2)\n+    var i = 0\n+    val otherSize = sorted.length\n+    while (i < otherSize && baseMap.size < maxSize) {\n+      val keyVal = sorted(i)\n+      baseMap += keyVal._1 -> keyVal._2\n+      i += 1\n+    }\n+  }\n+  \n+\n+  /**\n+   * Finding frequent items for columns, possibly with false positives. Using the algorithm \n+   * described in `http://www.cs.umd.edu/~samir/498/karp.pdf`.\n+   * For Internal use only.\n+   *\n+   * @param df The input DataFrame\n+   * @param cols the names of the columns to search frequent items in\n+   * @param support The minimum frequency for an item to be considered `frequent`\n+   * @return A Local DataFrame with the Array of frequent items for each column.\n+   */\n+  private[sql] def singlePassFreqItems(\n+      df: DataFrame, \n+      cols: Array[String], "
  }],
  "prId": 5799
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Check the range of `support`. Warn if the it is too small (e.g., 1e-6).\n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T05:56:37Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml\n+\n+\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.types.{StructType, ArrayType, StructField}\n+\n+import scala.collection.mutable.{Map => MutableMap}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.{Row, DataFrame, functions}\n+\n+private[sql] object FrequentItems extends Logging {\n+\n+  /**\n+   * Merge two maps of counts. Subtracts the sum of `otherMap` from `baseMap`, and fills in\n+   * any emptied slots with the most frequent of `otherMap`.\n+   * @param baseMap The map containing the global counts\n+   * @param otherMap The map containing the counts for that partition\n+   * @param maxSize The maximum number of counts to keep in memory\n+   */\n+  private def mergeCounts[A](\n+      baseMap: MutableMap[A, Long],\n+      otherMap: MutableMap[A, Long],\n+      maxSize: Int): Unit = {\n+    val otherSum = otherMap.foldLeft(0L) { case (sum, (k, v)) =>\n+      if (!baseMap.contains(k)) sum + v else sum\n+    }\n+    baseMap.retain((k, v) => v > otherSum)\n+    // sort in decreasing order, so that we will add the most frequent items first\n+    val sorted = otherMap.toSeq.sortBy(-_._2)\n+    var i = 0\n+    val otherSize = sorted.length\n+    while (i < otherSize && baseMap.size < maxSize) {\n+      val keyVal = sorted(i)\n+      baseMap += keyVal._1 -> keyVal._2\n+      i += 1\n+    }\n+  }\n+  \n+\n+  /**\n+   * Finding frequent items for columns, possibly with false positives. Using the algorithm \n+   * described in `http://www.cs.umd.edu/~samir/498/karp.pdf`.\n+   * For Internal use only.\n+   *\n+   * @param df The input DataFrame\n+   * @param cols the names of the columns to search frequent items in\n+   * @param support The minimum frequency for an item to be considered `frequent`\n+   * @return A Local DataFrame with the Array of frequent items for each column.\n+   */\n+  private[sql] def singlePassFreqItems(\n+      df: DataFrame, \n+      cols: Array[String], \n+      support: Double): DataFrame = {\n+    val numCols = cols.length"
  }],
  "prId": 5799
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`math.floor` is not necessary: `(1.0 / support).toInt`\n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T05:56:39Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml\n+\n+\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.types.{StructType, ArrayType, StructField}\n+\n+import scala.collection.mutable.{Map => MutableMap}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.{Row, DataFrame, functions}\n+\n+private[sql] object FrequentItems extends Logging {\n+\n+  /**\n+   * Merge two maps of counts. Subtracts the sum of `otherMap` from `baseMap`, and fills in\n+   * any emptied slots with the most frequent of `otherMap`.\n+   * @param baseMap The map containing the global counts\n+   * @param otherMap The map containing the counts for that partition\n+   * @param maxSize The maximum number of counts to keep in memory\n+   */\n+  private def mergeCounts[A](\n+      baseMap: MutableMap[A, Long],\n+      otherMap: MutableMap[A, Long],\n+      maxSize: Int): Unit = {\n+    val otherSum = otherMap.foldLeft(0L) { case (sum, (k, v)) =>\n+      if (!baseMap.contains(k)) sum + v else sum\n+    }\n+    baseMap.retain((k, v) => v > otherSum)\n+    // sort in decreasing order, so that we will add the most frequent items first\n+    val sorted = otherMap.toSeq.sortBy(-_._2)\n+    var i = 0\n+    val otherSize = sorted.length\n+    while (i < otherSize && baseMap.size < maxSize) {\n+      val keyVal = sorted(i)\n+      baseMap += keyVal._1 -> keyVal._2\n+      i += 1\n+    }\n+  }\n+  \n+\n+  /**\n+   * Finding frequent items for columns, possibly with false positives. Using the algorithm \n+   * described in `http://www.cs.umd.edu/~samir/498/karp.pdf`.\n+   * For Internal use only.\n+   *\n+   * @param df The input DataFrame\n+   * @param cols the names of the columns to search frequent items in\n+   * @param support The minimum frequency for an item to be considered `frequent`\n+   * @return A Local DataFrame with the Array of frequent items for each column.\n+   */\n+  private[sql] def singlePassFreqItems(\n+      df: DataFrame, \n+      cols: Array[String], \n+      support: Double): DataFrame = {\n+    val numCols = cols.length\n+    // number of max items to keep counts for\n+    val sizeOfMap = math.floor(1 / support).toInt"
  }],
  "prId": 5799
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`df.select(cols).rdd.aggregate` (then you don't need to skip elements)\n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T05:58:22Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml\n+\n+\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.types.{StructType, ArrayType, StructField}\n+\n+import scala.collection.mutable.{Map => MutableMap}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.{Row, DataFrame, functions}\n+\n+private[sql] object FrequentItems extends Logging {\n+\n+  /**\n+   * Merge two maps of counts. Subtracts the sum of `otherMap` from `baseMap`, and fills in\n+   * any emptied slots with the most frequent of `otherMap`.\n+   * @param baseMap The map containing the global counts\n+   * @param otherMap The map containing the counts for that partition\n+   * @param maxSize The maximum number of counts to keep in memory\n+   */\n+  private def mergeCounts[A](\n+      baseMap: MutableMap[A, Long],\n+      otherMap: MutableMap[A, Long],\n+      maxSize: Int): Unit = {\n+    val otherSum = otherMap.foldLeft(0L) { case (sum, (k, v)) =>\n+      if (!baseMap.contains(k)) sum + v else sum\n+    }\n+    baseMap.retain((k, v) => v > otherSum)\n+    // sort in decreasing order, so that we will add the most frequent items first\n+    val sorted = otherMap.toSeq.sortBy(-_._2)\n+    var i = 0\n+    val otherSize = sorted.length\n+    while (i < otherSize && baseMap.size < maxSize) {\n+      val keyVal = sorted(i)\n+      baseMap += keyVal._1 -> keyVal._2\n+      i += 1\n+    }\n+  }\n+  \n+\n+  /**\n+   * Finding frequent items for columns, possibly with false positives. Using the algorithm \n+   * described in `http://www.cs.umd.edu/~samir/498/karp.pdf`.\n+   * For Internal use only.\n+   *\n+   * @param df The input DataFrame\n+   * @param cols the names of the columns to search frequent items in\n+   * @param support The minimum frequency for an item to be considered `frequent`\n+   * @return A Local DataFrame with the Array of frequent items for each column.\n+   */\n+  private[sql] def singlePassFreqItems(\n+      df: DataFrame, \n+      cols: Array[String], \n+      support: Double): DataFrame = {\n+    val numCols = cols.length\n+    // number of max items to keep counts for\n+    val sizeOfMap = math.floor(1 / support).toInt\n+    val countMaps = Array.tabulate(numCols)(i => MutableMap.empty[Any, Long])\n+    val originalSchema = df.schema\n+    val colInfo = cols.map { name =>\n+      val index = originalSchema.fieldIndex(name)\n+      val dataType = originalSchema.fields(index)\n+      (index, dataType.dataType)\n+    }\n+    val colIndices = colInfo.map(_._1)\n+    \n+    val freqItems: Array[MutableMap[Any, Long]] = df.rdd.aggregate(countMaps)("
  }],
  "prId": 5799
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`1` -> `1L`\n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T05:59:29Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml\n+\n+\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.types.{StructType, ArrayType, StructField}\n+\n+import scala.collection.mutable.{Map => MutableMap}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.{Row, DataFrame, functions}\n+\n+private[sql] object FrequentItems extends Logging {\n+\n+  /**\n+   * Merge two maps of counts. Subtracts the sum of `otherMap` from `baseMap`, and fills in\n+   * any emptied slots with the most frequent of `otherMap`.\n+   * @param baseMap The map containing the global counts\n+   * @param otherMap The map containing the counts for that partition\n+   * @param maxSize The maximum number of counts to keep in memory\n+   */\n+  private def mergeCounts[A](\n+      baseMap: MutableMap[A, Long],\n+      otherMap: MutableMap[A, Long],\n+      maxSize: Int): Unit = {\n+    val otherSum = otherMap.foldLeft(0L) { case (sum, (k, v)) =>\n+      if (!baseMap.contains(k)) sum + v else sum\n+    }\n+    baseMap.retain((k, v) => v > otherSum)\n+    // sort in decreasing order, so that we will add the most frequent items first\n+    val sorted = otherMap.toSeq.sortBy(-_._2)\n+    var i = 0\n+    val otherSize = sorted.length\n+    while (i < otherSize && baseMap.size < maxSize) {\n+      val keyVal = sorted(i)\n+      baseMap += keyVal._1 -> keyVal._2\n+      i += 1\n+    }\n+  }\n+  \n+\n+  /**\n+   * Finding frequent items for columns, possibly with false positives. Using the algorithm \n+   * described in `http://www.cs.umd.edu/~samir/498/karp.pdf`.\n+   * For Internal use only.\n+   *\n+   * @param df The input DataFrame\n+   * @param cols the names of the columns to search frequent items in\n+   * @param support The minimum frequency for an item to be considered `frequent`\n+   * @return A Local DataFrame with the Array of frequent items for each column.\n+   */\n+  private[sql] def singlePassFreqItems(\n+      df: DataFrame, \n+      cols: Array[String], \n+      support: Double): DataFrame = {\n+    val numCols = cols.length\n+    // number of max items to keep counts for\n+    val sizeOfMap = math.floor(1 / support).toInt\n+    val countMaps = Array.tabulate(numCols)(i => MutableMap.empty[Any, Long])\n+    val originalSchema = df.schema\n+    val colInfo = cols.map { name =>\n+      val index = originalSchema.fieldIndex(name)\n+      val dataType = originalSchema.fields(index)\n+      (index, dataType.dataType)\n+    }\n+    val colIndices = colInfo.map(_._1)\n+    \n+    val freqItems: Array[MutableMap[Any, Long]] = df.rdd.aggregate(countMaps)(\n+      seqOp = (counts, row) => {\n+        var i = 0\n+        colIndices.foreach { index =>\n+          val thisMap = counts(i)\n+          val key = row.get(index)\n+          if (thisMap.contains(key))  {\n+            thisMap(key) += 1"
  }],
  "prId": 5799
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`-freqItems` -> `_freqItems`\n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T06:05:09Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml\n+\n+\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.types.{StructType, ArrayType, StructField}\n+\n+import scala.collection.mutable.{Map => MutableMap}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.{Row, DataFrame, functions}\n+\n+private[sql] object FrequentItems extends Logging {\n+\n+  /**\n+   * Merge two maps of counts. Subtracts the sum of `otherMap` from `baseMap`, and fills in\n+   * any emptied slots with the most frequent of `otherMap`.\n+   * @param baseMap The map containing the global counts\n+   * @param otherMap The map containing the counts for that partition\n+   * @param maxSize The maximum number of counts to keep in memory\n+   */\n+  private def mergeCounts[A](\n+      baseMap: MutableMap[A, Long],\n+      otherMap: MutableMap[A, Long],\n+      maxSize: Int): Unit = {\n+    val otherSum = otherMap.foldLeft(0L) { case (sum, (k, v)) =>\n+      if (!baseMap.contains(k)) sum + v else sum\n+    }\n+    baseMap.retain((k, v) => v > otherSum)\n+    // sort in decreasing order, so that we will add the most frequent items first\n+    val sorted = otherMap.toSeq.sortBy(-_._2)\n+    var i = 0\n+    val otherSize = sorted.length\n+    while (i < otherSize && baseMap.size < maxSize) {\n+      val keyVal = sorted(i)\n+      baseMap += keyVal._1 -> keyVal._2\n+      i += 1\n+    }\n+  }\n+  \n+\n+  /**\n+   * Finding frequent items for columns, possibly with false positives. Using the algorithm \n+   * described in `http://www.cs.umd.edu/~samir/498/karp.pdf`.\n+   * For Internal use only.\n+   *\n+   * @param df The input DataFrame\n+   * @param cols the names of the columns to search frequent items in\n+   * @param support The minimum frequency for an item to be considered `frequent`\n+   * @return A Local DataFrame with the Array of frequent items for each column.\n+   */\n+  private[sql] def singlePassFreqItems(\n+      df: DataFrame, \n+      cols: Array[String], \n+      support: Double): DataFrame = {\n+    val numCols = cols.length\n+    // number of max items to keep counts for\n+    val sizeOfMap = math.floor(1 / support).toInt\n+    val countMaps = Array.tabulate(numCols)(i => MutableMap.empty[Any, Long])\n+    val originalSchema = df.schema\n+    val colInfo = cols.map { name =>\n+      val index = originalSchema.fieldIndex(name)\n+      val dataType = originalSchema.fields(index)\n+      (index, dataType.dataType)\n+    }\n+    val colIndices = colInfo.map(_._1)\n+    \n+    val freqItems: Array[MutableMap[Any, Long]] = df.rdd.aggregate(countMaps)(\n+      seqOp = (counts, row) => {\n+        var i = 0\n+        colIndices.foreach { index =>\n+          val thisMap = counts(i)\n+          val key = row.get(index)\n+          if (thisMap.contains(key))  {\n+            thisMap(key) += 1\n+          } else {\n+            if (thisMap.size < sizeOfMap) {\n+              thisMap += key -> 1\n+            } else {\n+              // TODO: Make this more efficient... A flatMap?\n+              thisMap.retain((k, v) => v > 1)\n+              thisMap.transform((k, v) => v - 1)\n+            }\n+          }\n+          i += 1\n+        }\n+        counts\n+      },\n+      combOp = (baseCounts, counts) => {\n+        var i = 0\n+        while (i < numCols) {\n+          mergeCounts(baseCounts(i), counts(i), sizeOfMap)\n+          i += 1\n+        }\n+        baseCounts\n+      }\n+    )\n+    //\n+    val justItems = freqItems.map(m => m.keys.toSeq)\n+    val resultRow = Row(justItems:_*)\n+    // append frequent Items to the column name for easy debugging\n+    val outputCols = cols.zip(colInfo).map{ v =>\n+      StructField(v._1 + \"-freqItems\", ArrayType(v._2._2, false))"
  }],
  "prId": 5799
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I think the implementation could be cleaner if we wrap `MutableMap[A, Long]` with a utility class:\n\n``` scala\nclass FreqItemCounter(size: k) {\n  def add(any: Any, count: Long = 1L): this.type\n  def merge(other: FreqItemCounter): this.type = {\n    other.toSeq.foreach { case (k, c) =>\n      add(k, c)\n    }\n  }\n  def freqItems: Array[Any]\n  def toSeq: Seq[(Any, Long)]\n}\n```\n",
    "commit": "a6ec82cef528c22eff00cf8294d92798c6d2aa9d",
    "createdAt": "2015-04-30T06:10:21Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.ml\n+\n+\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.types.{StructType, ArrayType, StructField}\n+\n+import scala.collection.mutable.{Map => MutableMap}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.sql.{Row, DataFrame, functions}\n+\n+private[sql] object FrequentItems extends Logging {\n+\n+  /**\n+   * Merge two maps of counts. Subtracts the sum of `otherMap` from `baseMap`, and fills in\n+   * any emptied slots with the most frequent of `otherMap`.\n+   * @param baseMap The map containing the global counts\n+   * @param otherMap The map containing the counts for that partition\n+   * @param maxSize The maximum number of counts to keep in memory\n+   */\n+  private def mergeCounts[A]("
  }],
  "prId": 5799
}]