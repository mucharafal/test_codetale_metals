[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "so i think the general approach is that the final implementation should add serializable, and the trait shouldn't ...",
    "commit": "e8884b76a03873f35b0fe313dc05315f96acd96f",
    "createdAt": "2017-08-08T19:19:52Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+\n+/**\n+ * To be implemented by classes that represent data statistics collected during a Write Task.\n+ * It is important that instances of this type are [[Serializable]], as they will be gathered\n+ * on the driver from all executors.\n+ */\n+trait WriteTaskStats\n+  extends Serializable\n+\n+/**\n+ * A trait for classes that are capable of collecting statistics on data that's being processed by\n+ * a single write task in [[FileFormatWriter]] - i.e. there should be one instance per executor.\n+ *\n+ * This trait is coupled with the way [[FileFormatWriter]] works, in the sense that its methods\n+ * will be called according to how tuples are being written out to disk, namely in sorted order\n+ * according to partitionValue(s), then bucketId.\n+ *\n+ * As such, a typical call scenario is:\n+ *\n+ * newPartition -> newBucket -> newFile -> newRow -.\n+ *    ^        |______^___________^ ^         ^____|\n+ *    |               |             |______________|\n+ *    |               |____________________________|\n+ *    |____________________________________________|\n+ *\n+ * newPartition and newBucket events are only triggered if the relation to be written out is\n+ * partitioned and/or bucketed, respectively.\n+ */\n+trait WriteTaskStatsTracker {\n+\n+  /**\n+   * Process the fact that a new partition is about to be written.\n+   * Only triggered when the relation is partitioned by a (non-empty) sequence of columns.\n+   * @param partitionValues The values that define this new partition.\n+   */\n+  def newPartition(partitionValues: InternalRow): Unit\n+\n+  /**\n+   * Process the fact that a new bucket is about to written.\n+   * Only triggered when the relation is bucketed by a (non-empty) sequence of columns.\n+   * @param bucketId The bucket number.\n+   */\n+  def newBucket(bucketId: Int): Unit\n+\n+  /**\n+   * Process the fact that a new file is about to be written.\n+   * @param filePath Path of the file into which future rows will be written.\n+   */\n+  def newFile(filePath: String): Unit\n+\n+  /**\n+   * Process the fact that a new row to update the tracked statistics accordingly.\n+   * The row will be written to the most recently witnessed file (via `newFile`).\n+   * @note Keep in mind that any overhead here is per-row, obviously,\n+   *       so implementations should be as lightweight as possible.\n+   * @param row Current data row to be processed.\n+   */\n+  def newRow(row: InternalRow): Unit\n+\n+  /**\n+   * Returns the final statistics computed so far.\n+   * @note This may only be called once. Further use of the object may lead to undefined behavior.\n+   * @return An object of subtype of [[WriteTaskStats]], to be sent to the driver.\n+   */\n+  def getFinalStats(): WriteTaskStats\n+}\n+\n+/**\n+ * A class implementing this trait is basically a collection of parameters that are necessary\n+ * for instantiating a (derived type of) [[WriteTaskStatsTracker]] on all executors and then\n+ * process the statistics produced by them (e.g. save them to memory/disk, issue warnings, etc).\n+ * It is therefore important that such an objects is [[Serializable]], as it will be sent\n+ * from the driver to all executors.\n+ */\n+trait WriteJobStatsTracker"
  }, {
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "No strong preference, just curious.. why is that preferable?\r\nThe way I see it, this way you're sure to get it; otherwise you might forget to mix it in and then you'll only realize it at runtime when faced with a \"task not serializable\" exception.\r\nIs there some disadvantage to mixing it into the trait?",
    "commit": "e8884b76a03873f35b0fe313dc05315f96acd96f",
    "createdAt": "2017-08-09T10:09:30Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+\n+/**\n+ * To be implemented by classes that represent data statistics collected during a Write Task.\n+ * It is important that instances of this type are [[Serializable]], as they will be gathered\n+ * on the driver from all executors.\n+ */\n+trait WriteTaskStats\n+  extends Serializable\n+\n+/**\n+ * A trait for classes that are capable of collecting statistics on data that's being processed by\n+ * a single write task in [[FileFormatWriter]] - i.e. there should be one instance per executor.\n+ *\n+ * This trait is coupled with the way [[FileFormatWriter]] works, in the sense that its methods\n+ * will be called according to how tuples are being written out to disk, namely in sorted order\n+ * according to partitionValue(s), then bucketId.\n+ *\n+ * As such, a typical call scenario is:\n+ *\n+ * newPartition -> newBucket -> newFile -> newRow -.\n+ *    ^        |______^___________^ ^         ^____|\n+ *    |               |             |______________|\n+ *    |               |____________________________|\n+ *    |____________________________________________|\n+ *\n+ * newPartition and newBucket events are only triggered if the relation to be written out is\n+ * partitioned and/or bucketed, respectively.\n+ */\n+trait WriteTaskStatsTracker {\n+\n+  /**\n+   * Process the fact that a new partition is about to be written.\n+   * Only triggered when the relation is partitioned by a (non-empty) sequence of columns.\n+   * @param partitionValues The values that define this new partition.\n+   */\n+  def newPartition(partitionValues: InternalRow): Unit\n+\n+  /**\n+   * Process the fact that a new bucket is about to written.\n+   * Only triggered when the relation is bucketed by a (non-empty) sequence of columns.\n+   * @param bucketId The bucket number.\n+   */\n+  def newBucket(bucketId: Int): Unit\n+\n+  /**\n+   * Process the fact that a new file is about to be written.\n+   * @param filePath Path of the file into which future rows will be written.\n+   */\n+  def newFile(filePath: String): Unit\n+\n+  /**\n+   * Process the fact that a new row to update the tracked statistics accordingly.\n+   * The row will be written to the most recently witnessed file (via `newFile`).\n+   * @note Keep in mind that any overhead here is per-row, obviously,\n+   *       so implementations should be as lightweight as possible.\n+   * @param row Current data row to be processed.\n+   */\n+  def newRow(row: InternalRow): Unit\n+\n+  /**\n+   * Returns the final statistics computed so far.\n+   * @note This may only be called once. Further use of the object may lead to undefined behavior.\n+   * @return An object of subtype of [[WriteTaskStats]], to be sent to the driver.\n+   */\n+  def getFinalStats(): WriteTaskStats\n+}\n+\n+/**\n+ * A class implementing this trait is basically a collection of parameters that are necessary\n+ * for instantiating a (derived type of) [[WriteTaskStatsTracker]] on all executors and then\n+ * process the statistics produced by them (e.g. save them to memory/disk, issue warnings, etc).\n+ * It is therefore important that such an objects is [[Serializable]], as it will be sent\n+ * from the driver to all executors.\n+ */\n+trait WriteJobStatsTracker"
  }],
  "prId": 18884
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: should be only one space before `@note`",
    "commit": "e8884b76a03873f35b0fe313dc05315f96acd96f",
    "createdAt": "2017-08-10T13:08:21Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+\n+\n+/**\n+ * To be implemented by classes that represent data statistics collected during a Write Task.\n+ * It is important that instances of this type are [[Serializable]], as they will be gathered\n+ * on the driver from all executors.\n+ */\n+trait WriteTaskStats\n+  extends Serializable\n+\n+\n+/**\n+ * A trait for classes that are capable of collecting statistics on data that's being processed by\n+ * a single write task in [[FileFormatWriter]] - i.e. there should be one instance per executor.\n+ *\n+ * This trait is coupled with the way [[FileFormatWriter]] works, in the sense that its methods\n+ * will be called according to how tuples are being written out to disk, namely in sorted order\n+ * according to partitionValue(s), then bucketId.\n+ *\n+ * As such, a typical call scenario is:\n+ *\n+ * newPartition -> newBucket -> newFile -> newRow -.\n+ *    ^        |______^___________^ ^         ^____|\n+ *    |               |             |______________|\n+ *    |               |____________________________|\n+ *    |____________________________________________|\n+ *\n+ * newPartition and newBucket events are only triggered if the relation to be written out is\n+ * partitioned and/or bucketed, respectively.\n+ */\n+trait WriteTaskStatsTracker {\n+\n+  /**\n+   * Process the fact that a new partition is about to be written.\n+   * Only triggered when the relation is partitioned by a (non-empty) sequence of columns.\n+   * @param partitionValues The values that define this new partition.\n+   */\n+  def newPartition(partitionValues: InternalRow): Unit\n+\n+  /**\n+   * Process the fact that a new bucket is about to written.\n+   * Only triggered when the relation is bucketed by a (non-empty) sequence of columns.\n+   * @param bucketId The bucket number.\n+   */\n+  def newBucket(bucketId: Int): Unit\n+\n+  /**\n+   * Process the fact that a new file is about to be written.\n+   * @param filePath Path of the file into which future rows will be written.\n+   */\n+  def newFile(filePath: String): Unit\n+\n+  /**\n+   * Process the fact that a new row to update the tracked statistics accordingly.\n+   * The row will be written to the most recently witnessed file (via `newFile`).\n+   * @note Keep in mind that any overhead here is per-row, obviously,\n+   *       so implementations should be as lightweight as possible.\n+   * @param row Current data row to be processed.\n+   */\n+  def newRow(row: InternalRow): Unit\n+\n+  /**\n+   * Returns the final statistics computed so far.\n+   * @note This may only be called once. Further use of the object may lead to undefined behavior.\n+   * @return An object of subtype of [[WriteTaskStats]], to be sent to the driver.\n+   */\n+  def getFinalStats(): WriteTaskStats\n+}\n+\n+\n+/**\n+ * A class implementing this trait is basically a collection of parameters that are necessary\n+ * for instantiating a (derived type of) [[WriteTaskStatsTracker]] on all executors and then\n+ * process the statistics produced by them (e.g. save them to memory/disk, issue warnings, etc).\n+ * It is therefore important that such an objects is [[Serializable]], as it will be sent\n+ * from the driver to all executors.\n+ */\n+trait WriteJobStatsTracker\n+  extends Serializable {\n+\n+  /**\n+   * Instantiates a [[WriteTaskStatsTracker]], based on (non-transient) members of this class.\n+   * To be called by executors.\n+   * @return A [[WriteTaskStatsTracker]] instance to be used for computing stats during a write task\n+   */\n+  def newTaskInstance(): WriteTaskStatsTracker\n+\n+  /**\n+   * Process the given collection of stats computed during this job.\n+   * E.g. aggregate them, write them to memory / disk, issue warnings, whatever.\n+   * @param stats One [[WriteTaskStats]] object from each successful write task.\n+   *              @note The type here is too generic. These classes should probably be parametrized:"
  }],
  "prId": 18884
}]