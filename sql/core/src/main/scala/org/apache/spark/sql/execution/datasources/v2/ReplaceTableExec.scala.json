[{
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "The `try...catch` here is more for flavor and consistency - since @cloud-fan suggested that `StagingTableCatalog#stageReplace` should be able to throw `NoSuchTableException`, which could theoretically happen if the table is dropped between the above `tableExists` call and `catalog.stageReplace` calls. This ensures that the same type of exception is thrown from the code path for the same kind of illegal state.",
    "commit": "05a827df7094b07a492add875c6e649df52db41f",
    "createdAt": "2019-07-19T19:11:49Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, StagingTableCatalog, TableCatalog}\n+import org.apache.spark.sql.catalog.v2.expressions.Transform\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException}\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.StagedTable\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+case class ReplaceTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    tableSchema: StructType,\n+    partitioning: Seq[Transform],\n+    tableProperties: Map[String, String],\n+    orCreate: Boolean) extends LeafExecNode {\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    if (catalog.tableExists(ident)) {\n+      catalog.dropTable(ident)\n+    } else if (!orCreate) {\n+      throw new CannotReplaceMissingTableException(ident)\n+    }\n+    catalog.createTable(ident, tableSchema, partitioning.toArray, tableProperties.asJava)\n+    sqlContext.sparkContext.parallelize(Seq.empty, 1)\n+  }\n+\n+  override def output: Seq[Attribute] = Seq.empty\n+}\n+\n+case class AtomicReplaceTableExec(\n+    catalog: StagingTableCatalog,\n+    identifier: Identifier,\n+    tableSchema: StructType,\n+    partitioning: Seq[Transform],\n+    tableProperties: Map[String, String],\n+    orCreate: Boolean) extends LeafExecNode {\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val staged = if (orCreate) {\n+      catalog.stageCreateOrReplace(\n+        identifier, tableSchema, partitioning.toArray, tableProperties.asJava)\n+    } else if (catalog.tableExists(identifier)) {\n+      try {\n+        catalog.stageReplace(\n+          identifier, tableSchema, partitioning.toArray, tableProperties.asJava)\n+      } catch {",
    "line": 70
  }],
  "prId": 24798
}]