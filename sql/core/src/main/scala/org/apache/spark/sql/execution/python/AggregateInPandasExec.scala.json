[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "indentation nit",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-04T13:05:57Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingAttributes: Seq[Attribute],\n+    func: Seq[Expression],\n+    output: Seq[Attribute],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+  private val udfs = func.map(expr => expr.asInstanceOf[PythonUDF])\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingAttributes.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingAttributes) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingAttributes.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    // val argOffsets = Array((0 until (child.output.length - groupingAttributes.length)).toArray)\n+    val schema = StructType(child.schema.drop(groupingAttributes.length))\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfs.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+          allInputs += e"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Fixed. Thanks!",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-19T22:42:07Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingAttributes: Seq[Attribute],\n+    func: Seq[Expression],\n+    output: Seq[Attribute],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+  private val udfs = func.map(expr => expr.asInstanceOf[PythonUDF])\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingAttributes.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingAttributes) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingAttributes.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    // val argOffsets = Array((0 until (child.output.length - groupingAttributes.length)).toArray)\n+    val schema = StructType(child.schema.drop(groupingAttributes.length))\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfs.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+          allInputs += e"
  }],
  "prId": 19872
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `columnarBatchIter.flatMap(_.rowIterator)`?\r\nnit: style, add a space between `map` and `{ outputRow =>`.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-11T10:32:36Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+          allInputs += e\n+          dataTypes += e.dataType\n+          allInputs.length - 1 - groupingExpressions.length\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    inputRDD.mapPartitionsInternal { iter =>\n+      val grouped = if (groupingExpressions.isEmpty) {\n+        Iterator((null, iter))\n+      } else {\n+        val groupedIter = GroupedIterator(iter, groupingExpressions, child.output)\n+\n+        val dropGrouping =\n+          UnsafeProjection.create(allInputs.drop(groupingExpressions.length), child.output)\n+\n+        groupedIter.map {\n+          case (k, groupedRowIter) => (k, groupedRowIter.map(dropGrouping))\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // The queue used to buffer input rows so we can drain it to\n+      // combine input with output from Python.\n+      val queue = HybridRowQueue(context.taskMemoryManager(),\n+        new File(Utils.getLocalDir(SparkEnv.get.conf)), groupingExpressions.length)\n+      context.addTaskCompletionListener { _ =>\n+        queue.close()\n+      }\n+\n+      // Add rows to queue to join later with the result.\n+      val projectedRowIter = grouped.map { case (groupingKey, rows) =>\n+        queue.add(groupingKey.asInstanceOf[UnsafeRow])\n+        rows\n+      }\n+\n+      val columnarBatchIter = new ArrowPythonRunner(\n+        pyFuncs, bufferSize, reuseWorker,\n+        PythonEvalType.SQL_PANDAS_GROUP_AGG_UDF, argOffsets, schema,\n+        sessionLocalTimeZone, pandasRespectSessionTimeZone)\n+        .compute(projectedRowIter, context.partitionId(), context)\n+\n+      val joined = new JoinedRow\n+      val resultProj = UnsafeProjection.create(output, output)\n+\n+      columnarBatchIter.map(_.rowIterator.next()).map{ outputRow =>"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "```\r\ncolumnarBatchIter.flatMap(_.rowIterator)\r\n```\r\nDoesn't work because rowIterator is a java iterator not a scala iterator, we can convert it, but I am not sure it's better though. @ueshin if you prefer the flatMap one I can change it.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-19T17:41:54Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+          allInputs += e\n+          dataTypes += e.dataType\n+          allInputs.length - 1 - groupingExpressions.length\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    inputRDD.mapPartitionsInternal { iter =>\n+      val grouped = if (groupingExpressions.isEmpty) {\n+        Iterator((null, iter))\n+      } else {\n+        val groupedIter = GroupedIterator(iter, groupingExpressions, child.output)\n+\n+        val dropGrouping =\n+          UnsafeProjection.create(allInputs.drop(groupingExpressions.length), child.output)\n+\n+        groupedIter.map {\n+          case (k, groupedRowIter) => (k, groupedRowIter.map(dropGrouping))\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // The queue used to buffer input rows so we can drain it to\n+      // combine input with output from Python.\n+      val queue = HybridRowQueue(context.taskMemoryManager(),\n+        new File(Utils.getLocalDir(SparkEnv.get.conf)), groupingExpressions.length)\n+      context.addTaskCompletionListener { _ =>\n+        queue.close()\n+      }\n+\n+      // Add rows to queue to join later with the result.\n+      val projectedRowIter = grouped.map { case (groupingKey, rows) =>\n+        queue.add(groupingKey.asInstanceOf[UnsafeRow])\n+        rows\n+      }\n+\n+      val columnarBatchIter = new ArrowPythonRunner(\n+        pyFuncs, bufferSize, reuseWorker,\n+        PythonEvalType.SQL_PANDAS_GROUP_AGG_UDF, argOffsets, schema,\n+        sessionLocalTimeZone, pandasRespectSessionTimeZone)\n+        .compute(projectedRowIter, context.partitionId(), context)\n+\n+      val joined = new JoinedRow\n+      val resultProj = UnsafeProjection.create(output, output)\n+\n+      columnarBatchIter.map(_.rowIterator.next()).map{ outputRow =>"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "Sorry, I meant `columnarBatchIter.flatMap(_.rowIterator.asScala)`. I'd prefer this one.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-20T03:45:29Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+          allInputs += e\n+          dataTypes += e.dataType\n+          allInputs.length - 1 - groupingExpressions.length\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    inputRDD.mapPartitionsInternal { iter =>\n+      val grouped = if (groupingExpressions.isEmpty) {\n+        Iterator((null, iter))\n+      } else {\n+        val groupedIter = GroupedIterator(iter, groupingExpressions, child.output)\n+\n+        val dropGrouping =\n+          UnsafeProjection.create(allInputs.drop(groupingExpressions.length), child.output)\n+\n+        groupedIter.map {\n+          case (k, groupedRowIter) => (k, groupedRowIter.map(dropGrouping))\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // The queue used to buffer input rows so we can drain it to\n+      // combine input with output from Python.\n+      val queue = HybridRowQueue(context.taskMemoryManager(),\n+        new File(Utils.getLocalDir(SparkEnv.get.conf)), groupingExpressions.length)\n+      context.addTaskCompletionListener { _ =>\n+        queue.close()\n+      }\n+\n+      // Add rows to queue to join later with the result.\n+      val projectedRowIter = grouped.map { case (groupingKey, rows) =>\n+        queue.add(groupingKey.asInstanceOf[UnsafeRow])\n+        rows\n+      }\n+\n+      val columnarBatchIter = new ArrowPythonRunner(\n+        pyFuncs, bufferSize, reuseWorker,\n+        PythonEvalType.SQL_PANDAS_GROUP_AGG_UDF, argOffsets, schema,\n+        sessionLocalTimeZone, pandasRespectSessionTimeZone)\n+        .compute(projectedRowIter, context.partitionId(), context)\n+\n+      val joined = new JoinedRow\n+      val resultProj = UnsafeProjection.create(output, output)\n+\n+      columnarBatchIter.map(_.rowIterator.next()).map{ outputRow =>"
  }],
  "prId": 19872
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "I guess we don't need to append `groupingExpressions`. Seems like they are dropped later.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-20T06:21:10Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "This is fixed.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-27T21:58:19Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)"
  }],
  "prId": 19872
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "We need to handle `resultExpressions` for the following cases:\r\n\r\n```python\r\n    def test_result_expressions(self):\r\n        import numpy as np\r\n        from pyspark.sql.functions import mean, pandas_udf, PandasUDFType\r\n\r\n        df = self.data\r\n\r\n        @pandas_udf('double', PandasUDFType.GROUP_AGG)\r\n        def mean_udf(v, w):\r\n            return np.average(v, weights=w)\r\n\r\n        result1 = (df.groupby('id')\r\n                   .agg(mean_udf(df.v, lit(1.0)) + 1)\r\n                   .sort('id')\r\n                   .toPandas())\r\n\r\n        expected1 = (df.groupby('id')\r\n                     .agg(mean(df.v) + 1)\r\n                     .sort('id')\r\n                     .toPandas())\r\n\r\n        self.assertPandasEqual(expected1, result1)\r\n```\r\n",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-20T06:24:40Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        allInputs += e\n+        dataTypes += e.dataType\n+        allInputs.length - 1 - groupingExpressions.length\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    inputRDD.mapPartitionsInternal { iter =>\n+      val grouped = if (groupingExpressions.isEmpty) {\n+        Iterator((null, iter))\n+      } else {\n+        val groupedIter = GroupedIterator(iter, groupingExpressions, child.output)\n+\n+        val dropGrouping =\n+          UnsafeProjection.create(allInputs.drop(groupingExpressions.length), child.output)\n+\n+        groupedIter.map {\n+          case (k, groupedRowIter) => (k, groupedRowIter.map(dropGrouping))\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // The queue used to buffer input rows so we can drain it to\n+      // combine input with output from Python.\n+      val queue = HybridRowQueue(context.taskMemoryManager(),\n+        new File(Utils.getLocalDir(SparkEnv.get.conf)), groupingExpressions.length)\n+      context.addTaskCompletionListener { _ =>\n+        queue.close()\n+      }\n+\n+      // Add rows to queue to join later with the result.\n+      val projectedRowIter = grouped.map { case (groupingKey, rows) =>\n+        queue.add(groupingKey.asInstanceOf[UnsafeRow])\n+        rows\n+      }\n+\n+      val columnarBatchIter = new ArrowPythonRunner(\n+        pyFuncs, bufferSize, reuseWorker,\n+        PythonEvalType.SQL_PANDAS_GROUP_AGG_UDF, argOffsets, schema,\n+        sessionLocalTimeZone, pandasRespectSessionTimeZone)\n+        .compute(projectedRowIter, context.partitionId(), context)\n+\n+      val joined = new JoinedRow\n+      val resultProj = UnsafeProjection.create(output, output)"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Thanks @ueshin for reminding me of this. Just want to clarify the semantics:\r\n\r\nDoes\r\n```\r\n .agg(mean(df.v) + 1)\r\n```\r\nmean \"compute mean of df.v and plus the mean by one as output\", i.e, same as\r\n\r\n```\r\n.agg(mean(df.v).alias('mean'))\r\n.withColumn('mean', col('mean') + 1)\r\n```\r\n?\r\n",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-21T15:20:09Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        allInputs += e\n+        dataTypes += e.dataType\n+        allInputs.length - 1 - groupingExpressions.length\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    inputRDD.mapPartitionsInternal { iter =>\n+      val grouped = if (groupingExpressions.isEmpty) {\n+        Iterator((null, iter))\n+      } else {\n+        val groupedIter = GroupedIterator(iter, groupingExpressions, child.output)\n+\n+        val dropGrouping =\n+          UnsafeProjection.create(allInputs.drop(groupingExpressions.length), child.output)\n+\n+        groupedIter.map {\n+          case (k, groupedRowIter) => (k, groupedRowIter.map(dropGrouping))\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // The queue used to buffer input rows so we can drain it to\n+      // combine input with output from Python.\n+      val queue = HybridRowQueue(context.taskMemoryManager(),\n+        new File(Utils.getLocalDir(SparkEnv.get.conf)), groupingExpressions.length)\n+      context.addTaskCompletionListener { _ =>\n+        queue.close()\n+      }\n+\n+      // Add rows to queue to join later with the result.\n+      val projectedRowIter = grouped.map { case (groupingKey, rows) =>\n+        queue.add(groupingKey.asInstanceOf[UnsafeRow])\n+        rows\n+      }\n+\n+      val columnarBatchIter = new ArrowPythonRunner(\n+        pyFuncs, bufferSize, reuseWorker,\n+        PythonEvalType.SQL_PANDAS_GROUP_AGG_UDF, argOffsets, schema,\n+        sessionLocalTimeZone, pandasRespectSessionTimeZone)\n+        .compute(projectedRowIter, context.partitionId(), context)\n+\n+      val joined = new JoinedRow\n+      val resultProj = UnsafeProjection.create(output, output)"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "Yes, I think so about the behavior. I guess the plan could be different, though.\r\nWe can compare the behavior with non-udf aggregation and let's follow the behavior.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-22T03:16:55Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        allInputs += e\n+        dataTypes += e.dataType\n+        allInputs.length - 1 - groupingExpressions.length\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    inputRDD.mapPartitionsInternal { iter =>\n+      val grouped = if (groupingExpressions.isEmpty) {\n+        Iterator((null, iter))\n+      } else {\n+        val groupedIter = GroupedIterator(iter, groupingExpressions, child.output)\n+\n+        val dropGrouping =\n+          UnsafeProjection.create(allInputs.drop(groupingExpressions.length), child.output)\n+\n+        groupedIter.map {\n+          case (k, groupedRowIter) => (k, groupedRowIter.map(dropGrouping))\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // The queue used to buffer input rows so we can drain it to\n+      // combine input with output from Python.\n+      val queue = HybridRowQueue(context.taskMemoryManager(),\n+        new File(Utils.getLocalDir(SparkEnv.get.conf)), groupingExpressions.length)\n+      context.addTaskCompletionListener { _ =>\n+        queue.close()\n+      }\n+\n+      // Add rows to queue to join later with the result.\n+      val projectedRowIter = grouped.map { case (groupingKey, rows) =>\n+        queue.add(groupingKey.asInstanceOf[UnsafeRow])\n+        rows\n+      }\n+\n+      val columnarBatchIter = new ArrowPythonRunner(\n+        pyFuncs, bufferSize, reuseWorker,\n+        PythonEvalType.SQL_PANDAS_GROUP_AGG_UDF, argOffsets, schema,\n+        sessionLocalTimeZone, pandasRespectSessionTimeZone)\n+        .compute(projectedRowIter, context.partitionId(), context)\n+\n+      val joined = new JoinedRow\n+      val resultProj = UnsafeProjection.create(output, output)"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "I added `ExtractGroupAggPandasUDFFromAggregate` rule to deal with this",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-27T21:56:23Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, AttributeSet, Expression, JoinedRow, NamedExpression, PythonUDF, SortOrder, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[Expression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    allInputs.appendAll(groupingExpressions)\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        allInputs += e\n+        dataTypes += e.dataType\n+        allInputs.length - 1 - groupingExpressions.length\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    inputRDD.mapPartitionsInternal { iter =>\n+      val grouped = if (groupingExpressions.isEmpty) {\n+        Iterator((null, iter))\n+      } else {\n+        val groupedIter = GroupedIterator(iter, groupingExpressions, child.output)\n+\n+        val dropGrouping =\n+          UnsafeProjection.create(allInputs.drop(groupingExpressions.length), child.output)\n+\n+        groupedIter.map {\n+          case (k, groupedRowIter) => (k, groupedRowIter.map(dropGrouping))\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // The queue used to buffer input rows so we can drain it to\n+      // combine input with output from Python.\n+      val queue = HybridRowQueue(context.taskMemoryManager(),\n+        new File(Utils.getLocalDir(SparkEnv.get.conf)), groupingExpressions.length)\n+      context.addTaskCompletionListener { _ =>\n+        queue.close()\n+      }\n+\n+      // Add rows to queue to join later with the result.\n+      val projectedRowIter = grouped.map { case (groupingKey, rows) =>\n+        queue.add(groupingKey.asInstanceOf[UnsafeRow])\n+        rows\n+      }\n+\n+      val columnarBatchIter = new ArrowPythonRunner(\n+        pyFuncs, bufferSize, reuseWorker,\n+        PythonEvalType.SQL_PANDAS_GROUP_AGG_UDF, argOffsets, schema,\n+        sessionLocalTimeZone, pandasRespectSessionTimeZone)\n+        .compute(projectedRowIter, context.partitionId(), context)\n+\n+      val joined = new JoinedRow\n+      val resultProj = UnsafeProjection.create(output, output)"
  }],
  "prId": 19872
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Don't we need to exclude the semantically same input like `EvalPythonExec` is doing?",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-28T06:11:34Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[NamedExpression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override val output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        allInputs += e"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Ah good point. Let me fix that.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-28T14:38:43Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[NamedExpression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override val output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        allInputs += e"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Fixed.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2017-12-29T22:36:51Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[NamedExpression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override val output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        allInputs += e"
  }],
  "prId": 19872
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: maybe this name `input` is confusing.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2018-01-10T08:47:48Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Physical node for aggregation with group aggregate Pandas UDF.\n+ *\n+ * This plan works by sending the necessary (projected) input grouped data as Arrow record batches\n+ * to the python worker, the python worker invokes the UDF and sends the results to the executor,\n+ * finally the executor evaluates any post-aggregation expressions and join the result with the\n+ * grouped key.\n+ */\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[NamedExpression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override val output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        if (allInputs.exists(_.semanticEquals(e))) {\n+          allInputs.indexWhere(_.semanticEquals(e))\n+        } else {\n+          allInputs += e\n+          dataTypes += e.dataType\n+          allInputs.length - 1\n+        }\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    val input = groupingExpressions.map(_.toAttribute) ++ udfExpressions.map(_.resultAttribute)"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "I have changed the variable naming to be more meaningful and added some comments.",
    "commit": "cc659bc2487d81a9497bd032049c2c4272660716",
    "createdAt": "2018-01-10T19:40:51Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, ClusteredDistribution, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{GroupedIterator, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Physical node for aggregation with group aggregate Pandas UDF.\n+ *\n+ * This plan works by sending the necessary (projected) input grouped data as Arrow record batches\n+ * to the python worker, the python worker invokes the UDF and sends the results to the executor,\n+ * finally the executor evaluates any post-aggregation expressions and join the result with the\n+ * grouped key.\n+ */\n+case class AggregateInPandasExec(\n+    groupingExpressions: Seq[NamedExpression],\n+    udfExpressions: Seq[PythonUDF],\n+    resultExpressions: Seq[NamedExpression],\n+    child: SparkPlan)\n+  extends UnaryExecNode {\n+\n+  override val output: Seq[Attribute] = resultExpressions.map(_.toAttribute)\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def producedAttributes: AttributeSet = AttributeSet(output)\n+\n+  override def requiredChildDistribution: Seq[Distribution] = {\n+    if (groupingExpressions.isEmpty) {\n+      AllTuples :: Nil\n+    } else {\n+      ClusteredDistribution(groupingExpressions) :: Nil\n+    }\n+  }\n+\n+  private def collectFunctions(udf: PythonUDF): (ChainedPythonFunctions, Seq[Expression]) = {\n+    udf.children match {\n+      case Seq(u: PythonUDF) =>\n+        val (chained, children) = collectFunctions(u)\n+        (ChainedPythonFunctions(chained.funcs ++ Seq(udf.func)), children)\n+      case children =>\n+        // There should not be any other UDFs, or the children can't be evaluated directly.\n+        assert(children.forall(_.find(_.isInstanceOf[PythonUDF]).isEmpty))\n+        (ChainedPythonFunctions(Seq(udf.func)), udf.children)\n+    }\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    Seq(groupingExpressions.map(SortOrder(_, Ascending)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute()\n+\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val sessionLocalTimeZone = conf.sessionLocalTimeZone\n+    val pandasRespectSessionTimeZone = conf.pandasRespectSessionTimeZone\n+\n+    val (pyFuncs, inputs) = udfExpressions.map(collectFunctions).unzip\n+\n+    val allInputs = new ArrayBuffer[Expression]\n+    val dataTypes = new ArrayBuffer[DataType]\n+    val argOffsets = inputs.map { input =>\n+      input.map { e =>\n+        if (allInputs.exists(_.semanticEquals(e))) {\n+          allInputs.indexWhere(_.semanticEquals(e))\n+        } else {\n+          allInputs += e\n+          dataTypes += e.dataType\n+          allInputs.length - 1\n+        }\n+      }.toArray\n+    }.toArray\n+\n+    val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>\n+      StructField(s\"_$i\", dt)\n+    })\n+\n+    val input = groupingExpressions.map(_.toAttribute) ++ udfExpressions.map(_.resultAttribute)"
  }],
  "prId": 19872
}]