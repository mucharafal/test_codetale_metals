[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should not check the number of leaf nodes and shuffle query stages, but to check the number of query stages. It's ok we have some broadcast query stages.\r\n\r\n```\r\nval hasNonStageLeaf = plan.exists {\r\n  case _: QueryStage => false\r\n  case s: SparkPlan => s.children.isEmpty\r\n}\r\n```",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-01-31T06:13:46Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive.rule\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.MapOutputStatistics\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.adaptive.ShuffleQueryStage\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * A rule to adjust the post shuffle partitions based on the map output statistics.\n+ *\n+ * The strategy used to determine the number of post-shuffle partitions is described as follows.\n+ * To determine the number of post-shuffle partitions, we have a target input size for a\n+ * post-shuffle partition. Once we have size statistics of all pre-shuffle partitions, we will do\n+ * a pass of those statistics and pack pre-shuffle partitions with continuous indices to a single\n+ * post-shuffle partition until adding another pre-shuffle partition would cause the size of a\n+ * post-shuffle partition to be greater than the target size.\n+ *\n+ * For example, we have two stages with the following pre-shuffle partition size statistics:\n+ * stage 1: [100 MiB, 20 MiB, 100 MiB, 10MiB, 30 MiB]\n+ * stage 2: [10 MiB,  10 MiB, 70 MiB,  5 MiB, 5 MiB]\n+ * assuming the target input size is 128 MiB, we will have four post-shuffle partitions,\n+ * which are:\n+ *  - post-shuffle partition 0: pre-shuffle partition 0 (size 110 MiB)\n+ *  - post-shuffle partition 1: pre-shuffle partition 1 (size 30 MiB)\n+ *  - post-shuffle partition 2: pre-shuffle partition 2 (size 170 MiB)\n+ *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MiB)\n+ */\n+case class ReduceNumShufflePartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    val shuffleMetrics: Seq[MapOutputStatistics] = plan.collect {\n+      case stage: ShuffleQueryStage =>\n+        val metricsFuture = stage.mapOutputStatisticsFuture\n+        assert(metricsFuture.isCompleted, \"ShuffleQueryStage should already be ready\")\n+        ThreadUtils.awaitResult(metricsFuture, Duration.Zero)\n+    }\n+\n+    val leafNodes = plan.collect {\n+      case s: SparkPlan if s.children.isEmpty => s\n+    }"
  }, {
    "author": {
      "login": "carsonwang"
    },
    "body": "Good catch.",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-02-01T09:37:47Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive.rule\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.MapOutputStatistics\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.adaptive.ShuffleQueryStage\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * A rule to adjust the post shuffle partitions based on the map output statistics.\n+ *\n+ * The strategy used to determine the number of post-shuffle partitions is described as follows.\n+ * To determine the number of post-shuffle partitions, we have a target input size for a\n+ * post-shuffle partition. Once we have size statistics of all pre-shuffle partitions, we will do\n+ * a pass of those statistics and pack pre-shuffle partitions with continuous indices to a single\n+ * post-shuffle partition until adding another pre-shuffle partition would cause the size of a\n+ * post-shuffle partition to be greater than the target size.\n+ *\n+ * For example, we have two stages with the following pre-shuffle partition size statistics:\n+ * stage 1: [100 MiB, 20 MiB, 100 MiB, 10MiB, 30 MiB]\n+ * stage 2: [10 MiB,  10 MiB, 70 MiB,  5 MiB, 5 MiB]\n+ * assuming the target input size is 128 MiB, we will have four post-shuffle partitions,\n+ * which are:\n+ *  - post-shuffle partition 0: pre-shuffle partition 0 (size 110 MiB)\n+ *  - post-shuffle partition 1: pre-shuffle partition 1 (size 30 MiB)\n+ *  - post-shuffle partition 2: pre-shuffle partition 2 (size 170 MiB)\n+ *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MiB)\n+ */\n+case class ReduceNumShufflePartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    val shuffleMetrics: Seq[MapOutputStatistics] = plan.collect {\n+      case stage: ShuffleQueryStage =>\n+        val metricsFuture = stage.mapOutputStatisticsFuture\n+        assert(metricsFuture.isCompleted, \"ShuffleQueryStage should already be ready\")\n+        ThreadUtils.awaitResult(metricsFuture, Duration.Zero)\n+    }\n+\n+    val leafNodes = plan.collect {\n+      case s: SparkPlan if s.children.isEmpty => s\n+    }"
  }],
  "prId": 20303
}, {
  "comments": [{
    "author": {
      "login": "gczsjdy"
    },
    "body": "In most cases this has to be 1, but in cases when the children QueryStages are not in one whole-stage code generation block, we can still do adaptive execution even if the distinct value is larger than 1. For example, the root operator is a Union(it doesn't support codegen), and the two children are both ShuffleExchanges. In this case, the 2 ShuffleExchanges don't have to share the same number of pre shuffle partitions. They can reduce post shuffle partitions separately. I am not sure if I think it right. cc @cloud-fan ",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-03-17T16:30:51Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive.rule\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.MapOutputStatistics\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.adaptive.{QueryStageExec, ShuffleQueryStageExec}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * A rule to adjust the post shuffle partitions based on the map output statistics.\n+ *\n+ * The strategy used to determine the number of post-shuffle partitions is described as follows.\n+ * To determine the number of post-shuffle partitions, we have a target input size for a\n+ * post-shuffle partition. Once we have size statistics of all pre-shuffle partitions, we will do\n+ * a pass of those statistics and pack pre-shuffle partitions with continuous indices to a single\n+ * post-shuffle partition until adding another pre-shuffle partition would cause the size of a\n+ * post-shuffle partition to be greater than the target size.\n+ *\n+ * For example, we have two stages with the following pre-shuffle partition size statistics:\n+ * stage 1: [100 MiB, 20 MiB, 100 MiB, 10MiB, 30 MiB]\n+ * stage 2: [10 MiB,  10 MiB, 70 MiB,  5 MiB, 5 MiB]\n+ * assuming the target input size is 128 MiB, we will have four post-shuffle partitions,\n+ * which are:\n+ *  - post-shuffle partition 0: pre-shuffle partition 0 (size 110 MiB)\n+ *  - post-shuffle partition 1: pre-shuffle partition 1 (size 30 MiB)\n+ *  - post-shuffle partition 2: pre-shuffle partition 2 (size 170 MiB)\n+ *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MiB)\n+ */\n+case class ReduceNumShufflePartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    val shuffleMetrics: Seq[MapOutputStatistics] = plan.collect {\n+      case stage: ShuffleQueryStageExec =>\n+        val metricsFuture = stage.mapOutputStatisticsFuture\n+        assert(metricsFuture.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+        ThreadUtils.awaitResult(metricsFuture, Duration.Zero)\n+    }\n+\n+    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {\n+      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n+      // shuffle partitions, because we may break the assumption that all children of a spark plan\n+      // have same number of output partitions.\n+      plan\n+    } else {\n+      // `ShuffleQueryStageExec` gives null mapOutputStatistics when the input RDD has 0 partitions,\n+      // we should skip it when calculating the `partitionStartIndices`.\n+      val validMetrics = shuffleMetrics.filter(_ != null)\n+      if (validMetrics.nonEmpty) {\n+        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n+        // This transformation adds new nodes, so we must use `transformUp` here.\n+        plan.transformUp {\n+          // even for shuffle exchange whose input RDD has 0 partition, we should still update its\n+          // `partitionStartIndices`, so that all the leaf shuffles in a stage have the same\n+          // number of output partitions.\n+          case stage: ShuffleQueryStageExec =>\n+            CoalescedShuffleReaderExec(stage, partitionStartIndices)\n+        }\n+      } else {\n+        plan\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Estimates partition start indices for post-shuffle partitions based on\n+   * mapOutputStatistics provided by all pre-shuffle stages.\n+   */\n+  // visible for testing.\n+  private[sql] def estimatePartitionStartIndices(\n+      mapOutputStatistics: Array[MapOutputStatistics]): Array[Int] = {\n+    val minNumPostShufflePartitions = conf.minNumPostShufflePartitions\n+    val advisoryTargetPostShuffleInputSize = conf.targetPostShuffleInputSize\n+    // If minNumPostShufflePartitions is defined, it is possible that we need to use a\n+    // value less than advisoryTargetPostShuffleInputSize as the target input size of\n+    // a post shuffle task.\n+    val totalPostShuffleInputSize = mapOutputStatistics.map(_.bytesByPartitionId.sum).sum\n+    // The max at here is to make sure that when we have an empty table, we\n+    // only have a single post-shuffle partition.\n+    // There is no particular reason that we pick 16. We just need a number to\n+    // prevent maxPostShuffleInputSize from being set to 0.\n+    val maxPostShuffleInputSize = math.max(\n+      math.ceil(totalPostShuffleInputSize / minNumPostShufflePartitions.toDouble).toLong, 16)\n+    val targetPostShuffleInputSize =\n+      math.min(maxPostShuffleInputSize, advisoryTargetPostShuffleInputSize)\n+\n+    logInfo(\n+      s\"advisoryTargetPostShuffleInputSize: $advisoryTargetPostShuffleInputSize, \" +\n+        s\"targetPostShuffleInputSize $targetPostShuffleInputSize.\")\n+\n+    // Make sure we do get the same number of pre-shuffle partitions for those stages.\n+    val distinctNumPreShufflePartitions =\n+      mapOutputStatistics.map(stats => stats.bytesByPartitionId.length).distinct\n+    // The reason that we are expecting a single value of the number of pre-shuffle partitions\n+    // is that when we add Exchanges, we set the number of pre-shuffle partitions\n+    // (i.e. map output partitions) using a static setting, which is the value of\n+    // spark.sql.shuffle.partitions. Even if two input RDDs are having different\n+    // number of partitions, they will have the same number of pre-shuffle partitions\n+    // (i.e. map output partitions).\n+    assert(\n+      distinctNumPreShufflePartitions.length == 1,",
    "line": 125
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think this is a long-standing issue and I don't have a good idea to deal with Union and Join differently.",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-03-18T06:53:55Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive.rule\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.MapOutputStatistics\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{ShuffledRowRDD, SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.adaptive.{QueryStageExec, ShuffleQueryStageExec}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * A rule to adjust the post shuffle partitions based on the map output statistics.\n+ *\n+ * The strategy used to determine the number of post-shuffle partitions is described as follows.\n+ * To determine the number of post-shuffle partitions, we have a target input size for a\n+ * post-shuffle partition. Once we have size statistics of all pre-shuffle partitions, we will do\n+ * a pass of those statistics and pack pre-shuffle partitions with continuous indices to a single\n+ * post-shuffle partition until adding another pre-shuffle partition would cause the size of a\n+ * post-shuffle partition to be greater than the target size.\n+ *\n+ * For example, we have two stages with the following pre-shuffle partition size statistics:\n+ * stage 1: [100 MiB, 20 MiB, 100 MiB, 10MiB, 30 MiB]\n+ * stage 2: [10 MiB,  10 MiB, 70 MiB,  5 MiB, 5 MiB]\n+ * assuming the target input size is 128 MiB, we will have four post-shuffle partitions,\n+ * which are:\n+ *  - post-shuffle partition 0: pre-shuffle partition 0 (size 110 MiB)\n+ *  - post-shuffle partition 1: pre-shuffle partition 1 (size 30 MiB)\n+ *  - post-shuffle partition 2: pre-shuffle partition 2 (size 170 MiB)\n+ *  - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MiB)\n+ */\n+case class ReduceNumShufflePartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    val shuffleMetrics: Seq[MapOutputStatistics] = plan.collect {\n+      case stage: ShuffleQueryStageExec =>\n+        val metricsFuture = stage.mapOutputStatisticsFuture\n+        assert(metricsFuture.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+        ThreadUtils.awaitResult(metricsFuture, Duration.Zero)\n+    }\n+\n+    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {\n+      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n+      // shuffle partitions, because we may break the assumption that all children of a spark plan\n+      // have same number of output partitions.\n+      plan\n+    } else {\n+      // `ShuffleQueryStageExec` gives null mapOutputStatistics when the input RDD has 0 partitions,\n+      // we should skip it when calculating the `partitionStartIndices`.\n+      val validMetrics = shuffleMetrics.filter(_ != null)\n+      if (validMetrics.nonEmpty) {\n+        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n+        // This transformation adds new nodes, so we must use `transformUp` here.\n+        plan.transformUp {\n+          // even for shuffle exchange whose input RDD has 0 partition, we should still update its\n+          // `partitionStartIndices`, so that all the leaf shuffles in a stage have the same\n+          // number of output partitions.\n+          case stage: ShuffleQueryStageExec =>\n+            CoalescedShuffleReaderExec(stage, partitionStartIndices)\n+        }\n+      } else {\n+        plan\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Estimates partition start indices for post-shuffle partitions based on\n+   * mapOutputStatistics provided by all pre-shuffle stages.\n+   */\n+  // visible for testing.\n+  private[sql] def estimatePartitionStartIndices(\n+      mapOutputStatistics: Array[MapOutputStatistics]): Array[Int] = {\n+    val minNumPostShufflePartitions = conf.minNumPostShufflePartitions\n+    val advisoryTargetPostShuffleInputSize = conf.targetPostShuffleInputSize\n+    // If minNumPostShufflePartitions is defined, it is possible that we need to use a\n+    // value less than advisoryTargetPostShuffleInputSize as the target input size of\n+    // a post shuffle task.\n+    val totalPostShuffleInputSize = mapOutputStatistics.map(_.bytesByPartitionId.sum).sum\n+    // The max at here is to make sure that when we have an empty table, we\n+    // only have a single post-shuffle partition.\n+    // There is no particular reason that we pick 16. We just need a number to\n+    // prevent maxPostShuffleInputSize from being set to 0.\n+    val maxPostShuffleInputSize = math.max(\n+      math.ceil(totalPostShuffleInputSize / minNumPostShufflePartitions.toDouble).toLong, 16)\n+    val targetPostShuffleInputSize =\n+      math.min(maxPostShuffleInputSize, advisoryTargetPostShuffleInputSize)\n+\n+    logInfo(\n+      s\"advisoryTargetPostShuffleInputSize: $advisoryTargetPostShuffleInputSize, \" +\n+        s\"targetPostShuffleInputSize $targetPostShuffleInputSize.\")\n+\n+    // Make sure we do get the same number of pre-shuffle partitions for those stages.\n+    val distinctNumPreShufflePartitions =\n+      mapOutputStatistics.map(stats => stats.bytesByPartitionId.length).distinct\n+    // The reason that we are expecting a single value of the number of pre-shuffle partitions\n+    // is that when we add Exchanges, we set the number of pre-shuffle partitions\n+    // (i.e. map output partitions) using a static setting, which is the value of\n+    // spark.sql.shuffle.partitions. Even if two input RDDs are having different\n+    // number of partitions, they will have the same number of pre-shuffle partitions\n+    // (i.e. map output partitions).\n+    assert(\n+      distinctNumPreShufflePartitions.length == 1,",
    "line": 125
  }],
  "prId": 20303
}]