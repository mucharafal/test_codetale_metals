[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this is mostly moved from the other file\n",
    "commit": "1caae348801fb3f6d5dec4ce44cb4664a526d595",
    "createdAt": "2016-04-21T21:06:47Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, CatalogTable}\n+\n+\n+/**\n+ * Analyzes the given table in the current database to generate statistics, which will be\n+ * used in query optimizations.\n+ *\n+ * Right now, it only supports Hive tables and it only updates the size of a Hive table\n+ * in the Hive metastore.\n+ */\n+case class AnalyzeTable(tableName: String) extends RunnableCommand {",
    "line": 36
  }],
  "prId": 12584
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i changed this to use a sql conf rather than hive conf\n",
    "commit": "1caae348801fb3f6d5dec4ce44cb4664a526d595",
    "createdAt": "2016-04-21T21:06:56Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, CatalogTable}\n+\n+\n+/**\n+ * Analyzes the given table in the current database to generate statistics, which will be\n+ * used in query optimizations.\n+ *\n+ * Right now, it only supports Hive tables and it only updates the size of a Hive table\n+ * in the Hive metastore.\n+ */\n+case class AnalyzeTable(tableName: String) extends RunnableCommand {\n+\n+  override def run(sqlContext: SQLContext): Seq[Row] = {\n+    val sessionState = sqlContext.sessionState\n+    val tableIdent = sessionState.sqlParser.parseTableIdentifier(tableName)\n+    val relation = EliminateSubqueryAliases(sessionState.catalog.lookupRelation(tableIdent))\n+\n+    relation match {\n+      case relation: CatalogRelation =>\n+        val catalogTable: CatalogTable = relation.catalogTable\n+        // This method is mainly based on\n+        // org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForTable(HiveConf, Table)\n+        // in Hive 0.13 (except that we do not use fs.getContentSummary).\n+        // TODO: Generalize statistics collection.\n+        // TODO: Why fs.getContentSummary returns wrong size on Jenkins?\n+        // Can we use fs.getContentSummary in future?\n+        // Seems fs.getContentSummary returns wrong table size on Jenkins. So we use\n+        // countFileSize to count the table size.\n+        val stagingDir = sessionState.conf.getConfString(\"hive.exec.stagingdir\", \".hive-staging\")",
    "line": 54
  }],
  "prId": 12584
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this was using some hive constant -- changed to use an internal constant.\n",
    "commit": "1caae348801fb3f6d5dec4ce44cb4664a526d595",
    "createdAt": "2016-04-21T21:07:11Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, CatalogTable}\n+\n+\n+/**\n+ * Analyzes the given table in the current database to generate statistics, which will be\n+ * used in query optimizations.\n+ *\n+ * Right now, it only supports Hive tables and it only updates the size of a Hive table\n+ * in the Hive metastore.\n+ */\n+case class AnalyzeTable(tableName: String) extends RunnableCommand {\n+\n+  override def run(sqlContext: SQLContext): Seq[Row] = {\n+    val sessionState = sqlContext.sessionState\n+    val tableIdent = sessionState.sqlParser.parseTableIdentifier(tableName)\n+    val relation = EliminateSubqueryAliases(sessionState.catalog.lookupRelation(tableIdent))\n+\n+    relation match {\n+      case relation: CatalogRelation =>\n+        val catalogTable: CatalogTable = relation.catalogTable\n+        // This method is mainly based on\n+        // org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForTable(HiveConf, Table)\n+        // in Hive 0.13 (except that we do not use fs.getContentSummary).\n+        // TODO: Generalize statistics collection.\n+        // TODO: Why fs.getContentSummary returns wrong size on Jenkins?\n+        // Can we use fs.getContentSummary in future?\n+        // Seems fs.getContentSummary returns wrong table size on Jenkins. So we use\n+        // countFileSize to count the table size.\n+        val stagingDir = sessionState.conf.getConfString(\"hive.exec.stagingdir\", \".hive-staging\")\n+\n+        def calculateTableSize(fs: FileSystem, path: Path): Long = {\n+          val fileStatus = fs.getFileStatus(path)\n+          val size = if (fileStatus.isDirectory) {\n+            fs.listStatus(path)\n+              .map { status =>\n+                if (!status.getPath.getName.startsWith(stagingDir)) {\n+                  calculateTableSize(fs, status.getPath)\n+                } else {\n+                  0L\n+                }\n+              }.sum\n+          } else {\n+            fileStatus.getLen\n+          }\n+\n+          size\n+        }\n+\n+        val tableParameters = catalogTable.properties\n+        val oldTotalSize = tableParameters.get(\"totalSize\").map(_.toLong).getOrElse(0L)\n+        val newTotalSize =\n+          catalogTable.storage.locationUri.map { p =>\n+            val path = new Path(p)\n+            try {\n+              val fs = path.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+              calculateTableSize(fs, path)\n+            } catch {\n+              case NonFatal(e) =>\n+                logWarning(\n+                  s\"Failed to get the size of table ${catalogTable.identifier.table} in the \" +\n+                    s\"database ${catalogTable.identifier.database} because of ${e.toString}\", e)\n+                0L\n+            }\n+          }.getOrElse(0L)\n+\n+        // Update the Hive metastore if the total size of the table is different than the size\n+        // recorded in the Hive metastore.\n+        // This logic is based on org.apache.hadoop.hive.ql.exec.StatsTask.aggregateStats().\n+        if (newTotalSize > 0 && newTotalSize != oldTotalSize) {\n+          sessionState.catalog.alterTable(\n+            catalogTable.copy(\n+              properties = relation.catalogTable.properties +\n+                (AnalyzeTable.TOTAL_SIZE_FIELD -> newTotalSize.toString)))",
    "line": 98
  }],
  "prId": 12584
}]