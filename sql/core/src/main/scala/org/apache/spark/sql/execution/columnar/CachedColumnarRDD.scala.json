[{
  "comments": [{
    "author": {
      "login": "eyalfa"
    },
    "body": "could these be moved to become a member of the RDD class? seems like a map of this->some.property, in this case can be made an instance member.",
    "commit": "9d450addd095e2ac78770bdd03c3bf77817379e8",
    "createdAt": "2018-09-19T08:35:18Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.columnar\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.storage.{RDDPartitionMetadataBlockId, StorageLevel}\n+\n+private[columnar] class CachedColumnarRDD(\n+    @transient private var _sc: SparkContext,\n+    private var dataRDD: RDD[CachedBatch],\n+    private[columnar] val containsPartitionMetadata: Boolean,\n+    expectedStorageLevel: StorageLevel)\n+  extends RDD[CachedBatch](_sc, Seq(new OneToOneDependency(dataRDD))) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[CachedBatch] = {\n+    firstParent.iterator(split, context)\n+  }\n+\n+  override def unpersist(blocking: Boolean = true): this.type = {\n+    CachedColumnarRDD.allMetadataFetched.remove(id)\n+    CachedColumnarRDD.rddIdToMetadata.remove(id)\n+    super.unpersist(blocking)\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = dataRDD.partitions\n+\n+  override private[spark] def getOrCompute(split: Partition, context: TaskContext):\n+      Iterator[CachedBatch] = {\n+    val metadataBlockId = RDDPartitionMetadataBlockId(id, split.index)\n+    val superGetOrCompute: (Partition, TaskContext) => Iterator[CachedBatch] = super.getOrCompute\n+    SparkEnv.get.blockManager.getSingle[InternalRow](metadataBlockId).map(_ =>\n+      superGetOrCompute(split, context)\n+    ).getOrElse {\n+      val batchIter = superGetOrCompute(split, context)\n+      if (containsPartitionMetadata && getStorageLevel != StorageLevel.NONE && batchIter.hasNext) {\n+        val cachedBatch = batchIter.next()\n+        SparkEnv.get.blockManager.putSingle(metadataBlockId, cachedBatch.stats,\n+          expectedStorageLevel)\n+        new InterruptibleIterator[CachedBatch](context, Iterator(cachedBatch))\n+      } else {\n+        batchIter\n+      }\n+    }\n+  }\n+}\n+\n+private[columnar] object CachedColumnarRDD {\n+\n+  private val rddIdToMetadata = new ConcurrentHashMap[Int, mutable.ArraySeq[Option[InternalRow]]]()",
    "line": 71
  }],
  "prId": 19810
}, {
  "comments": [{
    "author": {
      "login": "eyalfa"
    },
    "body": "can this be avoided by maintaining two (zipped) RDDs? one of `CachedBatch`s and the other holding only the stats?\r\ncan this approach avoid the need for a specialized block type for managing metadata? \r\ncorrect me if i'm wrong, but first time the stats are accessed, your approach performs a full scan to extract the stats (happens in the sql code), so having a second RDD which is something like : `batches.map(_.stats).persist` should give the same behavior, right?",
    "commit": "9d450addd095e2ac78770bdd03c3bf77817379e8",
    "createdAt": "2018-09-19T08:40:13Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.columnar\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.storage.{RDDPartitionMetadataBlockId, StorageLevel}\n+\n+private[columnar] class CachedColumnarRDD(\n+    @transient private var _sc: SparkContext,\n+    private var dataRDD: RDD[CachedBatch],\n+    private[columnar] val containsPartitionMetadata: Boolean,\n+    expectedStorageLevel: StorageLevel)\n+  extends RDD[CachedBatch](_sc, Seq(new OneToOneDependency(dataRDD))) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[CachedBatch] = {\n+    firstParent.iterator(split, context)\n+  }\n+\n+  override def unpersist(blocking: Boolean = true): this.type = {\n+    CachedColumnarRDD.allMetadataFetched.remove(id)\n+    CachedColumnarRDD.rddIdToMetadata.remove(id)\n+    super.unpersist(blocking)\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = dataRDD.partitions\n+\n+  override private[spark] def getOrCompute(split: Partition, context: TaskContext):\n+      Iterator[CachedBatch] = {",
    "line": 50
  }],
  "prId": 19810
}, {
  "comments": [{
    "author": {
      "login": "eyalfa"
    },
    "body": "assert post condition `!batchIter.hasNext`, you expect this partition to contain a single batch",
    "commit": "9d450addd095e2ac78770bdd03c3bf77817379e8",
    "createdAt": "2018-09-19T10:03:48Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.columnar\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.storage.{RDDPartitionMetadataBlockId, StorageLevel}\n+\n+private[columnar] class CachedColumnarRDD(\n+    @transient private var _sc: SparkContext,\n+    private var dataRDD: RDD[CachedBatch],\n+    private[columnar] val containsPartitionMetadata: Boolean,\n+    expectedStorageLevel: StorageLevel)\n+  extends RDD[CachedBatch](_sc, Seq(new OneToOneDependency(dataRDD))) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[CachedBatch] = {\n+    firstParent.iterator(split, context)\n+  }\n+\n+  override def unpersist(blocking: Boolean = true): this.type = {\n+    CachedColumnarRDD.allMetadataFetched.remove(id)\n+    CachedColumnarRDD.rddIdToMetadata.remove(id)\n+    super.unpersist(blocking)\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = dataRDD.partitions\n+\n+  override private[spark] def getOrCompute(split: Partition, context: TaskContext):\n+      Iterator[CachedBatch] = {\n+    val metadataBlockId = RDDPartitionMetadataBlockId(id, split.index)\n+    val superGetOrCompute: (Partition, TaskContext) => Iterator[CachedBatch] = super.getOrCompute\n+    SparkEnv.get.blockManager.getSingle[InternalRow](metadataBlockId).map(_ =>\n+      superGetOrCompute(split, context)\n+    ).getOrElse {\n+      val batchIter = superGetOrCompute(split, context)\n+      if (containsPartitionMetadata && getStorageLevel != StorageLevel.NONE && batchIter.hasNext) {\n+        val cachedBatch = batchIter.next()",
    "line": 58
  }],
  "prId": 19810
}]