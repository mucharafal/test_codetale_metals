[{
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "whoa what does `FileSystem => _` do?",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T00:26:52Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "whoa .. i dont know either... my intellij did some weird magic :/",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T01:11:42Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}"
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: `logDebug`",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T00:31:03Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def create(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.create(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")",
    "line": 139
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I was thinking of having it as logInfo, so that we can debug stuff if the renaming goes wrong in some way. ",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T17:03:54Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def create(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.create(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")",
    "line": 139
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: separate lines please",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T00:35:08Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def create(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.create(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        super.close()\n+        fm.rename(tempPath, finalPath, overwriteIfPossible)\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  fs.setVerifyChecksum(false)\n+  fs.setWriteChecksum(false)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def create(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {"
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "ditto on two lines",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T00:40:00Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def create(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.create(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        super.close()\n+        fm.rename(tempPath, finalPath, overwriteIfPossible)\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  fs.setVerifyChecksum(false)\n+  fs.setWriteChecksum(false)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def create(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    try {\n+      if (!fs.rename(srcPath, dstPath) && !overwriteIfPossible) {\n+        if (fs.exists(dstPath)) {\n+          // Some implementations of FileSystem may not throw FileAlreadyExistsException but\n+          // only return false if file already exists. Explicitly throw the error.\n+          // Note that this is definitely not atomic, so this is only a best-effort attempt\n+          // to throw the most appropriate exception when rename returned false.\n+          throw new FileAlreadyExistsException(s\"$dstPath already exists\")\n+        } else {\n+          val msg = s\"Failed to rename temp file $srcPath to $dstPath as rename returned false\"\n+          logWarning(msg)\n+          throw new IOException(msg)\n+        }\n+      }\n+    } catch {\n+      case fe: FileAlreadyExistsException =>\n+        // Some implementation of FileSystem can directly throw FileAlreadyExistsException if file\n+        // already exists. Ignore the error if overwriteIfPossible = true as it is expected to be\n+        // best effort.\n+        logWarning(s\"Failed to rename temp file $srcPath to $dstPath because file exists\", fe)\n+        if (!overwriteIfPossible) throw fe\n+    }\n+  }\n+\n+  override def delete(path: Path): Unit = {\n+    try {\n+      fs.delete(path, true)\n+    } catch {\n+      case e: FileNotFoundException =>\n+        logInfo(s\"Failed to delete $path as it does not exist\")\n+        // ignore if file has already been deleted\n+    }\n+  }\n+\n+  override def isLocal: Boolean = fs match {\n+    case _: LocalFileSystem | _: RawLocalFileSystem => true\n+    case _ => false\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileContext]] API. */\n+class FileContextBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  private val fc = if (path.toUri.getScheme == null) {\n+    FileContext.getFileContext(hadoopConf)\n+  } else {\n+    FileContext.getFileContext(path.toUri, hadoopConf)\n+  }\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fc.util.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fc.mkdir(path, FsPermission.getDirDefault, true)\n+  }\n+\n+  override def create(path: Path): FSDataOutputStream = {\n+    import CreateFlag._\n+    fc.create(path, EnumSet.of(CREATE, OVERWRITE))\n+  }\n+\n+  override def createAtomic(\n+      path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {"
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "we should ensure that these are file paths and not directories, in case someone else tries to use these APIs elsewhere",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T00:46:59Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def create(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.create(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        super.close()\n+        fm.rename(tempPath, finalPath, overwriteIfPossible)\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  fs.setVerifyChecksum(false)\n+  fs.setWriteChecksum(false)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def create(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    try {\n+      if (!fs.rename(srcPath, dstPath) && !overwriteIfPossible) {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Well, they should not. This is specifically designed for one purpose. \r\nAlso, generally in all implementations, rename fails if you are doing something non-sensical like rename a file to a path where a directory already exists.\r\n\r\nEssentially, I want to keep this code absolutely minimal such that its easy to reason about and has minimal latency. Each check of whether its file or not will add to the latency. Most implementations will do those checks anyway. That's why there were patches in the past that removed unnecessary checks (e.g. https://github.com/apache/spark/commit/e24f21b5f8365ed25346e986748b393e0b4be25c). ",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T01:10:59Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileSystem => _, _}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def create(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.create(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        super.close()\n+        fm.rename(tempPath, finalPath, overwriteIfPossible)\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  fs.setVerifyChecksum(false)\n+  fs.setWriteChecksum(false)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def create(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def rename(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    try {\n+      if (!fs.rename(srcPath, dstPath) && !overwriteIfPossible) {"
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Fix this comment.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T01:46:04Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {\n+      // If overwriteIfPossible = false, then we want to find out why the rename failed and\n+      // try to throw the right error."
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "there's a lot of ambiguity about \"what does it mean if FileSystem.rename() returns false. FileContext.rename() is stricter here, One troublespot in particular is \"what if the source file is not there?\". If normally gets downgraded to a \"return false\".\r\n\r\nProposed: add a `fs.getFileStatus(srcPath)` before the rename call, as it will raise an FNFE if the source file isn't visible. Low cost against HDFS; more against remote object stores, but S3 isn't going to use this one, is it?\r\n",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T11:07:40Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {",
    "line": 254
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Ideally, I would like the default implementation with rename to work correctly for all FileSystems, including S3AFileSystem (it may not be efficient, but it should be correct). \r\n\r\nAlso it important to note that this CheckpointFileManager is built not as a general-purpose common API across FileSystem and FileContext, but specifically for the purpose of checkpointing. So in this case, this absolutely not expected that the source path (i.e. the temp file just written by `RenameBasedFSDataOutputStream`) is not present. So I dont think its worth adding another RPC in the common path just to handle that unexpected case. What I can do is add another check below after the rename return false to try to throw FileNotFoundException instead of the fallback IOException.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T19:05:41Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {",
    "line": 254
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Makes sense. \r\n\r\nAs you are doing a one-file rename, the rename() will have that atomic-operation semantics you want everywhere, it's just O(data) on s3 and swift. The direct write on both of those is what's critical to avoid checksum writes to block while that rename takes place, but not for the checksum output commit to take.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T19:14:24Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {",
    "line": 254
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "Should *never* be raised by a filesystem. Have you actually seen this?",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T11:08:44Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {\n+      // If overwriteIfPossible = false, then we want to find out why the rename failed and\n+      // try to throw the right error.\n+      if (fs.exists(dstPath)) {\n+        // Some implementations of FileSystem may only return false instead of throwing\n+        // FileAlreadyExistsException. In that case, explicitly throw the error the error\n+        // if overwriteIfPossible = false. Note that this is definitely not atomic.\n+        // This is only a best-effort attempt to identify the situation when rename returned\n+        // false.\n+        if (!overwriteIfPossible) {\n+          throw new FileAlreadyExistsException(s\"$dstPath already exists\")\n+        }\n+      } else {\n+        val msg = s\"Failed to rename temp file $srcPath to $dstPath as rename returned false\"\n+        logWarning(msg)\n+        throw new IOException(msg)\n+      }\n+    }\n+  }\n+\n+  override def delete(path: Path): Unit = {\n+    try {\n+      fs.delete(path, true)\n+    } catch {\n+      case e: FileNotFoundException =>\n+        logInfo(s\"Failed to delete $path as it does not exist\")",
    "line": 276
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Not really. This is just precautionary, as I really havent seen all the code of all FileSystem implementations. Doesn't add any overhead by adding this.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T18:53:30Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {\n+      // If overwriteIfPossible = false, then we want to find out why the rename failed and\n+      // try to throw the right error.\n+      if (fs.exists(dstPath)) {\n+        // Some implementations of FileSystem may only return false instead of throwing\n+        // FileAlreadyExistsException. In that case, explicitly throw the error the error\n+        // if overwriteIfPossible = false. Note that this is definitely not atomic.\n+        // This is only a best-effort attempt to identify the situation when rename returned\n+        // false.\n+        if (!overwriteIfPossible) {\n+          throw new FileAlreadyExistsException(s\"$dstPath already exists\")\n+        }\n+      } else {\n+        val msg = s\"Failed to rename temp file $srcPath to $dstPath as rename returned false\"\n+        logWarning(msg)\n+        throw new IOException(msg)\n+      }\n+    }\n+  }\n+\n+  override def delete(path: Path): Unit = {\n+    try {\n+      fs.delete(path, true)\n+    } catch {\n+      case e: FileNotFoundException =>\n+        logInfo(s\"Failed to delete $path as it does not exist\")",
    "line": 276
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Basic FS compliance tests, which even GCS now runs, requires that [delete just returns false if there's no path](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractDeleteTest.java#L48). I wouldn't personally worry about it, as if it did ever arise, something has seriously broken.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T19:19:10Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {\n+      // If overwriteIfPossible = false, then we want to find out why the rename failed and\n+      // try to throw the right error.\n+      if (fs.exists(dstPath)) {\n+        // Some implementations of FileSystem may only return false instead of throwing\n+        // FileAlreadyExistsException. In that case, explicitly throw the error the error\n+        // if overwriteIfPossible = false. Note that this is definitely not atomic.\n+        // This is only a best-effort attempt to identify the situation when rename returned\n+        // false.\n+        if (!overwriteIfPossible) {\n+          throw new FileAlreadyExistsException(s\"$dstPath already exists\")\n+        }\n+      } else {\n+        val msg = s\"Failed to rename temp file $srcPath to $dstPath as rename returned false\"\n+        logWarning(msg)\n+        throw new IOException(msg)\n+      }\n+    }\n+  }\n+\n+  override def delete(path: Path): Unit = {\n+    try {\n+      fs.delete(path, true)\n+    } catch {\n+      case e: FileNotFoundException =>\n+        logInfo(s\"Failed to delete $path as it does not exist\")",
    "line": 276
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Well, some people have to deal with older versions of FileSystems, not just the latest \"good\" ones. :)\r\nAs I said, since there isnt any overhead, I am inclined to keep it in there.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T19:27:15Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)\n+  }\n+\n+  override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = {\n+    if (!overwriteIfPossible && fs.exists(dstPath)) {\n+      throw new FileAlreadyExistsException(\n+        s\"Failed to rename $srcPath to $dstPath as destination already exists\")\n+    }\n+\n+    if (!fs.rename(srcPath, dstPath)) {\n+      // If overwriteIfPossible = false, then we want to find out why the rename failed and\n+      // try to throw the right error.\n+      if (fs.exists(dstPath)) {\n+        // Some implementations of FileSystem may only return false instead of throwing\n+        // FileAlreadyExistsException. In that case, explicitly throw the error the error\n+        // if overwriteIfPossible = false. Note that this is definitely not atomic.\n+        // This is only a best-effort attempt to identify the situation when rename returned\n+        // false.\n+        if (!overwriteIfPossible) {\n+          throw new FileAlreadyExistsException(s\"$dstPath already exists\")\n+        }\n+      } else {\n+        val msg = s\"Failed to rename temp file $srcPath to $dstPath as rename returned false\"\n+        logWarning(msg)\n+        throw new IOException(msg)\n+      }\n+    }\n+  }\n+\n+  override def delete(path: Path): Unit = {\n+    try {\n+      fs.delete(path, true)\n+    } catch {\n+      case e: FileNotFoundException =>\n+        logInfo(s\"Failed to delete $path as it does not exist\")",
    "line": 276
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "This is tagged as deprecated in Hadoop 3 as too much code was doing some exists() check alongside some other reinvocation fo the same underlying code, e.g `if (fs.exists(path) && fs.isFile(path)) { .,.. }`. If you call `fs.getFileStatus()` direct you get an FNFE raised if it isn't and a `FileStatus` if is is there, which can often be used.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T11:14:33Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Then basically I have to just copy the FileSystem.exists() code right here :)",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T19:14:58Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "yeah, tough call. Having looked through much of the spark/hive/ etc code, they to often do that exists() before delete, getFilestatus, ... which is why tag it as deprecated: to make clear that often the workflow is inefficient on non-HDFS, non-Posix stores (Where it's fine)",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-16T09:28:00Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io.{FileNotFoundException, IOException, OutputStream}\n+import java.util.{EnumSet, UUID}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import org.apache.hadoop.fs.local.{LocalFs, RawLocalFs}\n+import org.apache.hadoop.fs.permission.FsPermission\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager.RenameHelperMethods\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An interface to abstract out all operation related to streaming checkpoints. Most importantly,\n+ * the key operation this interface provides is `createAtomic(path, overwrite)` which returns a\n+ * `CancellableFSDataOutputStream`. This method is used by [[HDFSMetadataLog]] and\n+ * [[org.apache.spark.sql.execution.streaming.state.StateStore StateStore]] implementations\n+ * to write a complete checkpoint file atomically (i.e. no partial file will be visible), with or\n+ * without overwrite.\n+ *\n+ * This higher-level interface above the Hadoop FileSystem is necessary because\n+ * different implementation of FileSystem/FileContext may have different combination of operations\n+ * to provide the desired atomic guarantees (e.g. write-to-temp-file-and-rename,\n+ * direct-write-and-cancel-on-failure) and this abstraction allow different implementations while\n+ * keeping the usage simple (`createAtomic` -> `close` or `cancel`).\n+ */\n+trait CheckpointFileManager {\n+\n+  import org.apache.spark.sql.execution.streaming.CheckpointFileManager._\n+\n+  /**\n+   * Create a file and make its contents available atomically after the output stream is closed.\n+   *\n+   * @param path                Path to create\n+   * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+   *                            overwrite the file if it already exists. It should not throw\n+   *                            any exception if the file exists. However, if false, then the\n+   *                            implementation must not overwrite if the file alraedy exists and\n+   *                            must throw `FileAlreadyExistsException` in that case.\n+   */\n+  def createAtomic(path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n+\n+  /** Open a file for reading, or throw exception if it does not exist. */\n+  def open(path: Path): FSDataInputStream\n+\n+  /** List the files in a path that match a filter. */\n+  def list(path: Path, filter: PathFilter): Array[FileStatus]\n+\n+  /** List all the files in a path. */\n+  def list(path: Path): Array[FileStatus] = {\n+    list(path, new PathFilter { override def accept(path: Path): Boolean = true })\n+  }\n+\n+  /** Make directory at the give path and all its parent directories as needed. */\n+  def mkdirs(path: Path): Unit\n+\n+  /** Whether path exists */\n+  def exists(path: Path): Boolean\n+\n+  /** Recursively delete a path if it exists. Should not throw exception if file doesn't exist. */\n+  def delete(path: Path): Unit\n+\n+  /** Is the default file system this implementation is operating on the local file system. */\n+  def isLocal: Boolean\n+}\n+\n+object CheckpointFileManager extends Logging {\n+\n+  /**\n+   * Additional methods in CheckpointFileManager implementations that allows\n+   * [[RenameBasedFSDataOutputStream]] get atomicity by write-to-temp-file-and-rename\n+   */\n+  sealed trait RenameHelperMethods { self => CheckpointFileManager\n+    /** Create a file with overwrite. */\n+    def createTempFile(path: Path): FSDataOutputStream\n+\n+    /**\n+     * Rename a file.\n+     *\n+     * @param srcPath             Source path to rename\n+     * @param dstPath             Destination path to rename to\n+     * @param overwriteIfPossible If true, then the implementations must do a best-effort attempt to\n+     *                            overwrite the file if it already exists. It should not throw\n+     *                            any exception if the file exists. However, if false, then the\n+     *                            implementation must not overwrite if the file alraedy exists and\n+     *                            must throw `FileAlreadyExistsException` in that case.\n+     */\n+    def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit\n+  }\n+\n+  /**\n+   * An interface to add the cancel() operation to [[FSDataOutputStream]]. This is used\n+   * mainly by `CheckpointFileManager.createAtomic` to write a file atomically.\n+   *\n+   * @see [[CheckpointFileManager]].\n+   */\n+  abstract class CancellableFSDataOutputStream(protected val underlyingStream: OutputStream)\n+    extends FSDataOutputStream(underlyingStream, null) {\n+    /** Cancel the `underlyingStream` and ensure that the output file is not generated. */\n+    def cancel(): Unit\n+  }\n+\n+  /**\n+   * An implementation of [[CancellableFSDataOutputStream]] that writes a file atomically by writing\n+   * to a temporary file and then renames.\n+   */\n+  sealed class RenameBasedFSDataOutputStream(\n+      fm: CheckpointFileManager with RenameHelperMethods,\n+      finalPath: Path,\n+      tempPath: Path,\n+      overwriteIfPossible: Boolean)\n+    extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) {\n+\n+    def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = {\n+      this(fm, path, generateTempPath(path), overwrite)\n+    }\n+\n+    logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\")\n+    @volatile private var terminated = false\n+\n+    override def close(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        try {\n+          fm.renameTempFile(tempPath, finalPath, overwriteIfPossible)\n+        } catch {\n+          case fe: FileAlreadyExistsException =>\n+            logWarning(\n+              s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe)\n+            if (!overwriteIfPossible) throw fe\n+        }\n+        logInfo(s\"Renamed temp file $tempPath to $finalPath\")\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+\n+    override def cancel(): Unit = synchronized {\n+      try {\n+        if (terminated) return\n+        underlyingStream.close()\n+        fm.delete(tempPath)\n+      } catch {\n+        case NonFatal(e) =>\n+          logWarning(s\"Error cancelling write to $finalPath\", e)\n+      } finally {\n+        terminated = true\n+      }\n+    }\n+  }\n+\n+\n+  /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */\n+  def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = {\n+    val fileManagerClass = hadoopConf.get(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)\n+    if (fileManagerClass != null) {\n+      return Utils.classForName(fileManagerClass)\n+        .getConstructor(classOf[Path], classOf[Configuration])\n+        .newInstance(path, hadoopConf)\n+        .asInstanceOf[CheckpointFileManager]\n+    }\n+    try {\n+      // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename()\n+      // gives atomic renames, which is what we rely on for the default implementation\n+      // `CheckpointFileManager.createAtomic`.\n+      new FileContextBasedCheckpointFileManager(path, hadoopConf)\n+    } catch {\n+      case e: UnsupportedFileSystemException =>\n+        logWarning(\n+          \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" +\n+            s\"$path. Using FileSystem API instead for managing log files. If the implementation \" +\n+            s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" +\n+            s\"your Structured Streaming is not guaranteed.\")\n+        new FileSystemBasedCheckpointFileManager(path, hadoopConf)\n+    }\n+  }\n+\n+  private def generateTempPath(path: Path): Path = {\n+    val tc = org.apache.spark.TaskContext.get\n+    val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\"\n+    new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\")\n+  }\n+}\n+\n+\n+/** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */\n+class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends CheckpointFileManager with RenameHelperMethods with Logging {\n+\n+  import CheckpointFileManager._\n+\n+  protected val fs = path.getFileSystem(hadoopConf)\n+\n+  override def list(path: Path, filter: PathFilter): Array[FileStatus] = {\n+    fs.listStatus(path, filter)\n+  }\n+\n+  override def mkdirs(path: Path): Unit = {\n+    fs.mkdirs(path, FsPermission.getDirDefault)\n+  }\n+\n+  override def createTempFile(path: Path): FSDataOutputStream = {\n+    fs.create(path, true)\n+  }\n+\n+  override def createAtomic(\n+      path: Path,\n+      overwriteIfPossible: Boolean): CancellableFSDataOutputStream = {\n+    new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible)\n+  }\n+\n+  override def open(path: Path): FSDataInputStream = {\n+    fs.open(path)\n+  }\n+\n+  override def exists(path: Path): Boolean = {\n+    fs.exists(path)"
  }],
  "prId": 21048
}]