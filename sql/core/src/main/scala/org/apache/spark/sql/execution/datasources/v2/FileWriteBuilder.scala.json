[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this method is too long, could be better if we can separate it into multiple methods",
    "commit": "7bd1c092e304de83acc902a320ed2151e29a92e3",
    "createdAt": "2019-01-31T06:26:07Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.{Optional, UUID}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.sql.{AnalysisException, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.{CaseInsensitiveMap, DateTimeUtils}\n+import org.apache.spark.sql.execution.datasources.{BasicWriteJobStatsTracker, DataSource, OutputWriterFactory, WriteJobDescription}\n+import org.apache.spark.sql.execution.metric.SQLMetric\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, SupportsBatchWrite}\n+import org.apache.spark.sql.sources.v2.writer.{BatchWrite, SupportsSaveMode, WriteBuilder}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.SerializableConfiguration\n+\n+abstract class FileWriteBuilder(options: DataSourceOptions)\n+  extends WriteBuilder with SupportsSaveMode {\n+  private var schema: StructType = _\n+  private var queryId: String = _\n+  private var mode: SaveMode = _\n+\n+  override def withInputDataSchema(schema: StructType): WriteBuilder = {\n+    this.schema = schema\n+    this\n+  }\n+\n+  override def withQueryId(queryId: String): WriteBuilder = {\n+    this.queryId = queryId\n+    this\n+  }\n+\n+  override def mode(mode: SaveMode): WriteBuilder = {\n+    this.mode = mode\n+    this\n+  }\n+\n+  override def buildForBatch(): BatchWrite = {",
    "line": 61
  }],
  "prId": 23601
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "What happened if the path does not exist? It is possible that the underlying committer's deleteWithJob might not handle this case. ",
    "commit": "7bd1c092e304de83acc902a320ed2151e29a92e3",
    "createdAt": "2019-02-23T08:07:48Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+\n+import org.apache.spark.internal.io.FileCommitProtocol\n+import org.apache.spark.sql.{AnalysisException, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.{CaseInsensitiveMap, DateTimeUtils}\n+import org.apache.spark.sql.execution.datasources.{BasicWriteJobStatsTracker, DataSource, OutputWriterFactory, WriteJobDescription}\n+import org.apache.spark.sql.execution.metric.SQLMetric\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.writer.{BatchWrite, SupportsSaveMode, WriteBuilder}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+abstract class FileWriteBuilder(options: DataSourceOptions)\n+  extends WriteBuilder with SupportsSaveMode {\n+  private var schema: StructType = _\n+  private var queryId: String = _\n+  private var mode: SaveMode = _\n+\n+  override def withInputDataSchema(schema: StructType): WriteBuilder = {\n+    this.schema = schema\n+    this\n+  }\n+\n+  override def withQueryId(queryId: String): WriteBuilder = {\n+    this.queryId = queryId\n+    this\n+  }\n+\n+  override def mode(mode: SaveMode): WriteBuilder = {\n+    this.mode = mode\n+    this\n+  }\n+\n+  override def buildForBatch(): BatchWrite = {\n+    validateInputs()\n+    val pathName = options.paths().head\n+    val path = new Path(pathName)\n+    val sparkSession = SparkSession.active\n+    val optionsAsScala = options.asMap().asScala.toMap\n+    val hadoopConf = sparkSession.sessionState.newHadoopConfWithOptions(optionsAsScala)\n+    val job = getJobInstance(hadoopConf, path)\n+    val committer = FileCommitProtocol.instantiate(\n+      sparkSession.sessionState.conf.fileCommitProtocolClass,\n+      jobId = java.util.UUID.randomUUID().toString,\n+      outputPath = pathName)\n+    lazy val description =\n+      createWriteJobDescription(sparkSession, hadoopConf, job, pathName, optionsAsScala)\n+\n+    val fs = path.getFileSystem(hadoopConf)\n+    mode match {\n+      case SaveMode.ErrorIfExists if fs.exists(path) =>\n+        val qualifiedOutputPath = path.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        throw new AnalysisException(s\"path $qualifiedOutputPath already exists.\")\n+\n+      case SaveMode.Ignore if fs.exists(path) =>\n+        null\n+\n+      case SaveMode.Overwrite =>\n+        committer.deleteWithJob(fs, path, true)",
    "line": 86
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "```Scala\r\n        if (fs.exists(path)) {\r\n          committer.deleteWithJob(fs, path, recursive = true)\r\n        }\r\n```",
    "commit": "7bd1c092e304de83acc902a320ed2151e29a92e3",
    "createdAt": "2019-02-23T08:40:30Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+\n+import org.apache.spark.internal.io.FileCommitProtocol\n+import org.apache.spark.sql.{AnalysisException, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.{CaseInsensitiveMap, DateTimeUtils}\n+import org.apache.spark.sql.execution.datasources.{BasicWriteJobStatsTracker, DataSource, OutputWriterFactory, WriteJobDescription}\n+import org.apache.spark.sql.execution.metric.SQLMetric\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.writer.{BatchWrite, SupportsSaveMode, WriteBuilder}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+abstract class FileWriteBuilder(options: DataSourceOptions)\n+  extends WriteBuilder with SupportsSaveMode {\n+  private var schema: StructType = _\n+  private var queryId: String = _\n+  private var mode: SaveMode = _\n+\n+  override def withInputDataSchema(schema: StructType): WriteBuilder = {\n+    this.schema = schema\n+    this\n+  }\n+\n+  override def withQueryId(queryId: String): WriteBuilder = {\n+    this.queryId = queryId\n+    this\n+  }\n+\n+  override def mode(mode: SaveMode): WriteBuilder = {\n+    this.mode = mode\n+    this\n+  }\n+\n+  override def buildForBatch(): BatchWrite = {\n+    validateInputs()\n+    val pathName = options.paths().head\n+    val path = new Path(pathName)\n+    val sparkSession = SparkSession.active\n+    val optionsAsScala = options.asMap().asScala.toMap\n+    val hadoopConf = sparkSession.sessionState.newHadoopConfWithOptions(optionsAsScala)\n+    val job = getJobInstance(hadoopConf, path)\n+    val committer = FileCommitProtocol.instantiate(\n+      sparkSession.sessionState.conf.fileCommitProtocolClass,\n+      jobId = java.util.UUID.randomUUID().toString,\n+      outputPath = pathName)\n+    lazy val description =\n+      createWriteJobDescription(sparkSession, hadoopConf, job, pathName, optionsAsScala)\n+\n+    val fs = path.getFileSystem(hadoopConf)\n+    mode match {\n+      case SaveMode.ErrorIfExists if fs.exists(path) =>\n+        val qualifiedOutputPath = path.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        throw new AnalysisException(s\"path $qualifiedOutputPath already exists.\")\n+\n+      case SaveMode.Ignore if fs.exists(path) =>\n+        null\n+\n+      case SaveMode.Overwrite =>\n+        committer.deleteWithJob(fs, path, true)",
    "line": 86
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "@gatorsmile I check the source code. Actually, all the implementations (that I can see in IDE) handle the case that the file path does not exist. But in `InsertIntoHadoopFsRelationCommand` the `deleteWithJob` is used as following:\r\n```\r\nif (fs.exists(path) && !committer.deleteWithJob(fs, path, true)) {\r\n        throw new IOException(s\"Unable to clear partition \" +\r\n          s\"directory $path prior to writing to it\")\r\n}\r\n```\r\n\r\nShould we follow it?",
    "commit": "7bd1c092e304de83acc902a320ed2151e29a92e3",
    "createdAt": "2019-02-25T07:32:06Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+\n+import org.apache.spark.internal.io.FileCommitProtocol\n+import org.apache.spark.sql.{AnalysisException, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.{CaseInsensitiveMap, DateTimeUtils}\n+import org.apache.spark.sql.execution.datasources.{BasicWriteJobStatsTracker, DataSource, OutputWriterFactory, WriteJobDescription}\n+import org.apache.spark.sql.execution.metric.SQLMetric\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.writer.{BatchWrite, SupportsSaveMode, WriteBuilder}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+abstract class FileWriteBuilder(options: DataSourceOptions)\n+  extends WriteBuilder with SupportsSaveMode {\n+  private var schema: StructType = _\n+  private var queryId: String = _\n+  private var mode: SaveMode = _\n+\n+  override def withInputDataSchema(schema: StructType): WriteBuilder = {\n+    this.schema = schema\n+    this\n+  }\n+\n+  override def withQueryId(queryId: String): WriteBuilder = {\n+    this.queryId = queryId\n+    this\n+  }\n+\n+  override def mode(mode: SaveMode): WriteBuilder = {\n+    this.mode = mode\n+    this\n+  }\n+\n+  override def buildForBatch(): BatchWrite = {\n+    validateInputs()\n+    val pathName = options.paths().head\n+    val path = new Path(pathName)\n+    val sparkSession = SparkSession.active\n+    val optionsAsScala = options.asMap().asScala.toMap\n+    val hadoopConf = sparkSession.sessionState.newHadoopConfWithOptions(optionsAsScala)\n+    val job = getJobInstance(hadoopConf, path)\n+    val committer = FileCommitProtocol.instantiate(\n+      sparkSession.sessionState.conf.fileCommitProtocolClass,\n+      jobId = java.util.UUID.randomUUID().toString,\n+      outputPath = pathName)\n+    lazy val description =\n+      createWriteJobDescription(sparkSession, hadoopConf, job, pathName, optionsAsScala)\n+\n+    val fs = path.getFileSystem(hadoopConf)\n+    mode match {\n+      case SaveMode.ErrorIfExists if fs.exists(path) =>\n+        val qualifiedOutputPath = path.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        throw new AnalysisException(s\"path $qualifiedOutputPath already exists.\")\n+\n+      case SaveMode.Ignore if fs.exists(path) =>\n+        null\n+\n+      case SaveMode.Overwrite =>\n+        committer.deleteWithJob(fs, path, true)",
    "line": 86
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "yea let's follow it.",
    "commit": "7bd1c092e304de83acc902a320ed2151e29a92e3",
    "createdAt": "2019-02-25T15:14:36Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+\n+import org.apache.spark.internal.io.FileCommitProtocol\n+import org.apache.spark.sql.{AnalysisException, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.{CaseInsensitiveMap, DateTimeUtils}\n+import org.apache.spark.sql.execution.datasources.{BasicWriteJobStatsTracker, DataSource, OutputWriterFactory, WriteJobDescription}\n+import org.apache.spark.sql.execution.metric.SQLMetric\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.writer.{BatchWrite, SupportsSaveMode, WriteBuilder}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+abstract class FileWriteBuilder(options: DataSourceOptions)\n+  extends WriteBuilder with SupportsSaveMode {\n+  private var schema: StructType = _\n+  private var queryId: String = _\n+  private var mode: SaveMode = _\n+\n+  override def withInputDataSchema(schema: StructType): WriteBuilder = {\n+    this.schema = schema\n+    this\n+  }\n+\n+  override def withQueryId(queryId: String): WriteBuilder = {\n+    this.queryId = queryId\n+    this\n+  }\n+\n+  override def mode(mode: SaveMode): WriteBuilder = {\n+    this.mode = mode\n+    this\n+  }\n+\n+  override def buildForBatch(): BatchWrite = {\n+    validateInputs()\n+    val pathName = options.paths().head\n+    val path = new Path(pathName)\n+    val sparkSession = SparkSession.active\n+    val optionsAsScala = options.asMap().asScala.toMap\n+    val hadoopConf = sparkSession.sessionState.newHadoopConfWithOptions(optionsAsScala)\n+    val job = getJobInstance(hadoopConf, path)\n+    val committer = FileCommitProtocol.instantiate(\n+      sparkSession.sessionState.conf.fileCommitProtocolClass,\n+      jobId = java.util.UUID.randomUUID().toString,\n+      outputPath = pathName)\n+    lazy val description =\n+      createWriteJobDescription(sparkSession, hadoopConf, job, pathName, optionsAsScala)\n+\n+    val fs = path.getFileSystem(hadoopConf)\n+    mode match {\n+      case SaveMode.ErrorIfExists if fs.exists(path) =>\n+        val qualifiedOutputPath = path.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        throw new AnalysisException(s\"path $qualifiedOutputPath already exists.\")\n+\n+      case SaveMode.Ignore if fs.exists(path) =>\n+        null\n+\n+      case SaveMode.Overwrite =>\n+        committer.deleteWithJob(fs, path, true)",
    "line": 86
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "OK, create https://github.com/apache/spark/pull/23889 for this.",
    "commit": "7bd1c092e304de83acc902a320ed2151e29a92e3",
    "createdAt": "2019-02-25T16:05:24Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+\n+import org.apache.spark.internal.io.FileCommitProtocol\n+import org.apache.spark.sql.{AnalysisException, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.{CaseInsensitiveMap, DateTimeUtils}\n+import org.apache.spark.sql.execution.datasources.{BasicWriteJobStatsTracker, DataSource, OutputWriterFactory, WriteJobDescription}\n+import org.apache.spark.sql.execution.metric.SQLMetric\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.writer.{BatchWrite, SupportsSaveMode, WriteBuilder}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+abstract class FileWriteBuilder(options: DataSourceOptions)\n+  extends WriteBuilder with SupportsSaveMode {\n+  private var schema: StructType = _\n+  private var queryId: String = _\n+  private var mode: SaveMode = _\n+\n+  override def withInputDataSchema(schema: StructType): WriteBuilder = {\n+    this.schema = schema\n+    this\n+  }\n+\n+  override def withQueryId(queryId: String): WriteBuilder = {\n+    this.queryId = queryId\n+    this\n+  }\n+\n+  override def mode(mode: SaveMode): WriteBuilder = {\n+    this.mode = mode\n+    this\n+  }\n+\n+  override def buildForBatch(): BatchWrite = {\n+    validateInputs()\n+    val pathName = options.paths().head\n+    val path = new Path(pathName)\n+    val sparkSession = SparkSession.active\n+    val optionsAsScala = options.asMap().asScala.toMap\n+    val hadoopConf = sparkSession.sessionState.newHadoopConfWithOptions(optionsAsScala)\n+    val job = getJobInstance(hadoopConf, path)\n+    val committer = FileCommitProtocol.instantiate(\n+      sparkSession.sessionState.conf.fileCommitProtocolClass,\n+      jobId = java.util.UUID.randomUUID().toString,\n+      outputPath = pathName)\n+    lazy val description =\n+      createWriteJobDescription(sparkSession, hadoopConf, job, pathName, optionsAsScala)\n+\n+    val fs = path.getFileSystem(hadoopConf)\n+    mode match {\n+      case SaveMode.ErrorIfExists if fs.exists(path) =>\n+        val qualifiedOutputPath = path.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        throw new AnalysisException(s\"path $qualifiedOutputPath already exists.\")\n+\n+      case SaveMode.Ignore if fs.exists(path) =>\n+        null\n+\n+      case SaveMode.Overwrite =>\n+        committer.deleteWithJob(fs, path, true)",
    "line": 86
  }],
  "prId": 23601
}]