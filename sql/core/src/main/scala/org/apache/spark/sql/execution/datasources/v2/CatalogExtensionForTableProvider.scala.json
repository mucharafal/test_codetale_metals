[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why is this written as a catalog extension? I think this should be the default behavior of the v2 wrapper for Spark's catalog. Why use the extension in interface for this?",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-25T20:47:13Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I did struggle for it many times. If we put this logic in `V2SessionCatalog`, then the catalog extension API becomes less useful. If a user creates a `CustomExtension` and tries to overwrite the `loadTable` method, then there is not much he can do as the table provider resolving is already done.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-26T13:50:46Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "This should be part of the built-in implementation, `V2SessionCatalog`, not an extension. The extension API has all of the same information -- the table identifier, table schema, partitioning, and properties. It can still return a different instance of `Table` if it needs to.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-27T17:14:05Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {"
  }],
  "prId": 25651
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "This block isn't correct. If the table exists, then this must throw `TableAlreadyExistsException`: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java#L99\r\n\r\nCreate table is not allowed to return an existing table and override partitioning.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-25T20:50:51Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "There are 2 \"table\" in this context:\r\n1. the table entry in Spark metastore\r\n2. the table returned by `TableProvider`\r\n\r\nFor example, `CREATE TABLE abc USING jdbc OPTIONS(table='xyz')`, `abc` is the table entry in Spark metastore, `xyz` is the table returned by `TableProvider`. The table entry in Spark metastore is simply a link to the table returned by `TableProvider`. So the CREATE TABLE here is not to create a table in JDBC, but to create a table in Spark metastore that links to the JDBC table.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-26T13:54:32Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Okay, so this create table uses the schema and partitioning from the external table because it wasn't specified in the DDL create statement. Sounds reasonable.\r\n\r\nI don't think this implementation is correct. There are two cases: the schema and/or partitioning were not specified and are empty, or they were specified and need to be validated. I think that this should load the external table and either override (if empty) or validate if non-empty.\r\n\r\nLooks like we didn't think of this case in the API, but we should add `IllegalArgumentException` for when the external table schema or partitioning doesn't match the incoming settings.\r\n\r\nWhen the `inferSchema` and `inferPartitioning` methods are added, we will need to update this to use those methods instead of loading the table and getting its schema and partitioning. After those methods are added, the table implementation must always report the schema and partitioning that were passed in to build the table.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-27T17:26:50Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()"
  }],
  "prId": 25651
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why would this happen?",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-25T20:51:37Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()\n+    } else {\n+      schema -> partitions\n+    }\n+    super.createTable(ident, actualSchema, actualPartitioning, properties)\n+    // call `loadTable` to make sure the schema/partitioning specified in `CREATE TABLE ... USING`\n+    // matches the actual data schema/partitioning. If error happens during table loading, drop\n+    // the table."
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "e.g. `CREATE TABLE t(i int) USING jdbc OPTIONS (table=t2)`. It's possible that the JDBC table `t2` has a different schema.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-26T05:39:38Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()\n+    } else {\n+      schema -> partitions\n+    }\n+    super.createTable(ident, actualSchema, actualPartitioning, properties)\n+    // call `loadTable` to make sure the schema/partitioning specified in `CREATE TABLE ... USING`\n+    // matches the actual data schema/partitioning. If error happens during table loading, drop\n+    // the table."
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "This should be validated above when the table is loaded, not by loading the table as second time.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-27T17:27:12Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()\n+    } else {\n+      schema -> partitions\n+    }\n+    super.createTable(ident, actualSchema, actualPartitioning, properties)\n+    // call `loadTable` to make sure the schema/partitioning specified in `CREATE TABLE ... USING`\n+    // matches the actual data schema/partitioning. If error happens during table loading, drop\n+    // the table."
  }],
  "prId": 25651
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why is file source handled here? File source should use `SupportsSpecifiedSchemaPartitioning.getTable`.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-25T20:52:55Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()\n+    } else {\n+      schema -> partitions\n+    }\n+    super.createTable(ident, actualSchema, actualPartitioning, properties)\n+    // call `loadTable` to make sure the schema/partitioning specified in `CREATE TABLE ... USING`\n+    // matches the actual data schema/partitioning. If error happens during table loading, drop\n+    // the table.\n+    try {\n+      loadTable(ident)\n+    } catch {\n+      case NonFatal(e) =>\n+        dropTable(ident)\n+        throw e\n+    }\n+  }\n+\n+  private def tryResolveTableProvider(table: Table): Table = {\n+    val providerName = table.properties().get(\"provider\")\n+    assert(providerName != null)\n+    DataSource.lookupDataSourceV2(providerName, conf).map {\n+      // TODO: support file source v2 in CREATE TABLE USING.\n+      case _: FileDataSourceV2 => table"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "As you already found out in https://github.com/apache/spark/pull/25651#discussion_r328336988\r\n\r\nFile source v2 can't take the partitioning from metastore because the `TableProvider` API was incompleted before. I don't want to fix file source v2 in this PR, so I simply ignore it here.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-26T05:44:44Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()\n+    } else {\n+      schema -> partitions\n+    }\n+    super.createTable(ident, actualSchema, actualPartitioning, properties)\n+    // call `loadTable` to make sure the schema/partitioning specified in `CREATE TABLE ... USING`\n+    // matches the actual data schema/partitioning. If error happens during table loading, drop\n+    // the table.\n+    try {\n+      loadTable(ident)\n+    } catch {\n+      case NonFatal(e) =>\n+        dropTable(ident)\n+        throw e\n+    }\n+  }\n+\n+  private def tryResolveTableProvider(table: Table): Table = {\n+    val providerName = table.properties().get(\"provider\")\n+    assert(providerName != null)\n+    DataSource.lookupDataSourceV2(providerName, conf).map {\n+      // TODO: support file source v2 in CREATE TABLE USING.\n+      case _: FileDataSourceV2 => table"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Okay, so this will be removed in the commit that updates file sources?",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-27T17:09:27Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()\n+    } else {\n+      schema -> partitions\n+    }\n+    super.createTable(ident, actualSchema, actualPartitioning, properties)\n+    // call `loadTable` to make sure the schema/partitioning specified in `CREATE TABLE ... USING`\n+    // matches the actual data schema/partitioning. If error happens during table loading, drop\n+    // the table.\n+    try {\n+      loadTable(ident)\n+    } catch {\n+      case NonFatal(e) =>\n+        dropTable(ident)\n+        throw e\n+    }\n+  }\n+\n+  private def tryResolveTableProvider(table: Table): Table = {\n+    val providerName = table.properties().get(\"provider\")\n+    assert(providerName != null)\n+    DataSource.lookupDataSourceV2(providerName, conf).map {\n+      // TODO: support file source v2 in CREATE TABLE USING.\n+      case _: FileDataSourceV2 => table"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "yup",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-28T05:44:59Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.{DelegatingCatalogExtension, Identifier, SupportsSpecifiedSchemaPartitioning, Table}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.datasources.DataSource\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CatalogExtensionForTableProvider extends DelegatingCatalogExtension {\n+\n+  private val conf = SQLConf.get\n+\n+  override def loadTable(ident: Identifier): Table = {\n+    val table = super.loadTable(ident)\n+    tryResolveTableProvider(table)\n+  }\n+\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    val provider = properties.getOrDefault(\"provider\", conf.defaultDataSourceName)\n+    val maybeProvider = DataSource.lookupDataSourceV2(provider, conf)\n+    val (actualSchema, actualPartitioning) = if (maybeProvider.isDefined && schema.isEmpty) {\n+      // A sanity check. The parser should guarantee it.\n+      assert(partitions.isEmpty)\n+      // If `CREATE TABLE ... USING` does not specify table metadata, get the table metadata from\n+      // data source first.\n+      val table = maybeProvider.get.getTable(new CaseInsensitiveStringMap(properties))\n+      table.schema() -> table.partitioning()\n+    } else {\n+      schema -> partitions\n+    }\n+    super.createTable(ident, actualSchema, actualPartitioning, properties)\n+    // call `loadTable` to make sure the schema/partitioning specified in `CREATE TABLE ... USING`\n+    // matches the actual data schema/partitioning. If error happens during table loading, drop\n+    // the table.\n+    try {\n+      loadTable(ident)\n+    } catch {\n+      case NonFatal(e) =>\n+        dropTable(ident)\n+        throw e\n+    }\n+  }\n+\n+  private def tryResolveTableProvider(table: Table): Table = {\n+    val providerName = table.properties().get(\"provider\")\n+    assert(providerName != null)\n+    DataSource.lookupDataSourceV2(providerName, conf).map {\n+      // TODO: support file source v2 in CREATE TABLE USING.\n+      case _: FileDataSourceV2 => table"
  }],
  "prId": 25651
}]