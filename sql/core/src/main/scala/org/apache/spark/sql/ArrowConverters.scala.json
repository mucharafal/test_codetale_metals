[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "setNull and setValue should be protected.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-02-15T07:13:50Z",
    "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.channels.Channels\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.ArrowWriter\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  private class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+    private val iter = batches.iterator\n+\n+    override def next(): ArrowRecordBatch = iter.next()\n+    override def hasNext: Boolean = iter.hasNext\n+  }\n+\n+  def internalRowsToPayload(rows: Array[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowsToArrowRecordBatch(rows, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Transfer an array of InternalRow to an ArrowRecordBatch.\n+   */\n+  private[sql] def internalRowsToArrowRecordBatch(\n+      rows: Array[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+    val fieldAndBuf = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      internalRowToArrowBuf(rows, ordinal, field, allocator)\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1.flatten\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val recordBatch = new ArrowRecordBatch(rows.length,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Write a Field from array of InternalRow to an ArrowBuf.\n+   */\n+  private def internalRowToArrowBuf(\n+      rows: Array[InternalRow],\n+      ordinal: Int,\n+      field: StructField,\n+      allocator: RootAllocator): (Array[ArrowFieldNode], Array[ArrowBuf]) = {\n+    val numOfRows = rows.length\n+    val columnWriter = ColumnWriter(allocator, field.dataType)\n+    columnWriter.init(numOfRows)\n+    var index = 0\n+\n+    while(index < numOfRows) {\n+      val row = rows(index)\n+      if (row.isNullAt(ordinal)) {\n+        columnWriter.writeNull()\n+      } else {\n+        columnWriter.write(row, ordinal)\n+      }\n+      index += 1\n+    }\n+\n+    val (arrowFieldNodes, arrowBufs) = columnWriter.finish()\n+    (arrowFieldNodes.toArray, arrowBufs.toArray)\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    try {\n+      payload.foreach(writer.writeRecordBatch)\n+    } catch {\n+      case e: Exception =>\n+        throw e\n+    } finally {\n+      writer.close()\n+      payload.foreach(_.close())\n+    }\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(initialSize: Int): Unit\n+  def writeNull(): Unit\n+  def write(row: InternalRow, ordinal: Int): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(protected val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  protected def valueVector: BaseDataValueVector\n+  protected def valueMutator: BaseMutator\n+\n+  protected def setNull(): Unit\n+  protected def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(initialSize: Int): Unit = {\n+    valueVector.allocateNew()\n+  }\n+\n+  override def writeNull(): Unit = {\n+    setNull()\n+    nullCount += 1\n+    count += 1\n+  }\n+\n+  override def write(row: InternalRow, ordinal: Int): Unit = {\n+    setValue(row, ordinal)\n+    count += 1\n+  }\n+\n+  override def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers: Seq[ArrowBuf] = valueVector.getBuffers(true)\n+    (List(fieldNode), valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override protected val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override protected val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Do we need to prevent a `ColumnWriter` has been written after `finish` is called?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-02-15T07:15:18Z",
    "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.channels.Channels\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.ArrowWriter\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  private class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+    private val iter = batches.iterator\n+\n+    override def next(): ArrowRecordBatch = iter.next()\n+    override def hasNext: Boolean = iter.hasNext\n+  }\n+\n+  def internalRowsToPayload(rows: Array[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowsToArrowRecordBatch(rows, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Transfer an array of InternalRow to an ArrowRecordBatch.\n+   */\n+  private[sql] def internalRowsToArrowRecordBatch(\n+      rows: Array[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+    val fieldAndBuf = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      internalRowToArrowBuf(rows, ordinal, field, allocator)\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1.flatten\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val recordBatch = new ArrowRecordBatch(rows.length,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Write a Field from array of InternalRow to an ArrowBuf.\n+   */\n+  private def internalRowToArrowBuf(\n+      rows: Array[InternalRow],\n+      ordinal: Int,\n+      field: StructField,\n+      allocator: RootAllocator): (Array[ArrowFieldNode], Array[ArrowBuf]) = {\n+    val numOfRows = rows.length\n+    val columnWriter = ColumnWriter(allocator, field.dataType)\n+    columnWriter.init(numOfRows)\n+    var index = 0\n+\n+    while(index < numOfRows) {\n+      val row = rows(index)\n+      if (row.isNullAt(ordinal)) {\n+        columnWriter.writeNull()\n+      } else {\n+        columnWriter.write(row, ordinal)\n+      }\n+      index += 1\n+    }\n+\n+    val (arrowFieldNodes, arrowBufs) = columnWriter.finish()\n+    (arrowFieldNodes.toArray, arrowBufs.toArray)\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    try {\n+      payload.foreach(writer.writeRecordBatch)\n+    } catch {\n+      case e: Exception =>\n+        throw e\n+    } finally {\n+      writer.close()\n+      payload.foreach(_.close())\n+    }\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(initialSize: Int): Unit\n+  def writeNull(): Unit\n+  def write(row: InternalRow, ordinal: Int): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(protected val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  protected def valueVector: BaseDataValueVector\n+  protected def valueMutator: BaseMutator\n+\n+  protected def setNull(): Unit\n+  protected def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(initialSize: Int): Unit = {\n+    valueVector.allocateNew()\n+  }\n+\n+  override def writeNull(): Unit = {\n+    setNull()\n+    nullCount += 1\n+    count += 1\n+  }\n+\n+  override def write(row: InternalRow, ordinal: Int): Unit = {\n+    setValue(row, ordinal)\n+    count += 1\n+  }\n+\n+  override def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf]) = {"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "isn't length an Int already?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T02:45:03Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "Yes, you're right.  What are your thoughts on making the Arrow version of this class public in a util package?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T18:31:41Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt"
  }, {
    "author": {
      "login": "julienledem"
    },
    "body": "Yes. Feel free to send a PR to Arrow for this.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T19:07:48Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "for instead of while?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T02:50:54Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "trying to avoid any wrapping of the code that might cause overhead.  I believe a for loop is translated to a foreach and wraps the code. It might not make a difference but just being extra cautious!",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T18:40:11Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1"
  }, {
    "author": {
      "login": "leifwalsh"
    },
    "body": "Yep, scala for loops are known to be quite slower than while loops, this is a pretty standard practice for performance sensitive scala projects. ",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T19:15:37Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "if we create an allocator we should have a way to close it in the end.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T02:51:49Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)"
  }, {
    "author": {
      "login": "julienledem"
    },
    "body": "the allocator should probably passed from where it is centrally initialized",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T03:20:14Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "I was trying to encapsulate the usage of the Allocator to this class for now.  I'll add a method to close it here.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T18:33:53Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "columnWriter.forEach( writer => writer.write(row )) ?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T02:56:19Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "how about\r\n```\r\nval (fieldNodes, buf) = columnWriters.map( _.finish()).unzip\r\nval buffers = buf.flatten\r\n```",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T02:58:25Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val fieldAndBuf = columnWriters.map { writer =>\n+      writer.finish()\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1\n+    val buffers = fieldAndBuf._2.flatten"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "try / finally for this one as well",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T03:00:43Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val fieldAndBuf = columnWriters.map { writer =>\n+      writer.finish()\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    payload.foreach { batch =>\n+      try {\n+        writer.writeRecordBatch(batch)\n+      } finally {\n+        batch.close()\n+      }\n+    }\n+    writer.close()"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "I think this part needs to be reworked a little.  If an exception is thrown, it should make sure all buffers are closed too.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T18:45:45Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val fieldAndBuf = columnWriters.map { writer =>\n+      writer.finish()\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    payload.foreach { batch =>\n+      try {\n+        writer.writeRecordBatch(batch)\n+      } finally {\n+        batch.close()\n+      }\n+    }\n+    writer.close()"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "possibly define a case class for this so that we name the two members.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T03:03:48Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val fieldAndBuf = columnWriters.map { writer =>\n+      writer.finish()\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    payload.foreach { batch =>\n+      try {\n+        writer.writeRecordBatch(batch)\n+      } finally {\n+        batch.close()\n+      }\n+    }\n+    writer.close()\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit\n+  def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(): this.type = {\n+    valueVector.allocateNew()\n+    this\n+  }\n+\n+  override def write(row: InternalRow): Unit = {\n+    if (row.isNullAt(ordinal)) {\n+      setNull()\n+      nullCount += 1\n+    } else {\n+      setValue(row, ordinal)\n+    }\n+    count += 1\n+  }\n+\n+  override def finish(): (ArrowFieldNode, Array[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers = valueVector.getBuffers(true)\n+    (fieldNode, valueBuffers)"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "when do we use ordinal and when do we use count?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T03:06:12Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val fieldAndBuf = columnWriters.map { writer =>\n+      writer.finish()\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    payload.foreach { batch =>\n+      try {\n+        writer.writeRecordBatch(batch)\n+      } finally {\n+        batch.close()\n+      }\n+    }\n+    writer.close()\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit\n+  def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(): this.type = {\n+    valueVector.allocateNew()\n+    this\n+  }\n+\n+  override def write(row: InternalRow): Unit = {\n+    if (row.isNullAt(ordinal)) {\n+      setNull()\n+      nullCount += 1\n+    } else {\n+      setValue(row, ordinal)\n+    }\n+    count += 1\n+  }\n+\n+  override def finish(): (ArrowFieldNode, Array[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers = valueVector.getBuffers(true)\n+    (fieldNode, valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "there's ordinal the field of this class and ordinal the parameter of this method.\r\nWhat is the meaning of each of them? \r\nPossibly rename them to be more specific.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T03:09:06Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val fieldAndBuf = columnWriters.map { writer =>\n+      writer.finish()\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    payload.foreach { batch =>\n+      try {\n+        writer.writeRecordBatch(batch)\n+      } finally {\n+        batch.close()\n+      }\n+    }\n+    writer.close()\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit\n+  def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(): this.type = {\n+    valueVector.allocateNew()\n+    this\n+  }\n+\n+  override def write(row: InternalRow): Unit = {\n+    if (row.isNullAt(ordinal)) {\n+      setNull()\n+      nullCount += 1\n+    } else {\n+      setValue(row, ordinal)\n+    }\n+    count += 1\n+  }\n+\n+  override def finish(): (ArrowFieldNode, Array[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers = valueVector.getBuffers(true)\n+    (fieldNode, valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, bool2int(row.getBoolean(ordinal)))"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "Actually, `setValue` does not need the ordinal passed in anymore since the `PrimitiveColumnWriter` is bound to an ordinal when created.  I'll clean this up.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T18:51:45Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length.toInt\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and write to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val fieldAndBuf = columnWriters.map { writer =>\n+      writer.finish()\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+    payload.foreach { batch =>\n+      try {\n+        writer.writeRecordBatch(batch)\n+      } finally {\n+        batch.close()\n+      }\n+    }\n+    writer.close()\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit\n+  def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(): this.type = {\n+    valueVector.allocateNew()\n+    this\n+  }\n+\n+  override def write(row: InternalRow): Unit = {\n+    if (row.isNullAt(ordinal)) {\n+      setNull()\n+      nullCount += 1\n+    } else {\n+      setValue(row, ordinal)\n+    }\n+    count += 1\n+  }\n+\n+  override def finish(): (ArrowFieldNode, Array[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers = valueVector.getBuffers(true)\n+    (fieldNode, valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, bool2int(row.getBoolean(ordinal)))"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "It would be good to have a test that make sure this is the exception thrown when trying to convert an unsupported Spark type.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-14T17:46:54Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.arrow.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "yes, there is a test for unsupported types here https://github.com/BryanCutler/spark/blob/wip-toPandas_with_arrow-SPARK-13534/sql/core/src/test/scala/org/apache/spark/sql/ArrowConvertersSuite.scala#L155",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-15T17:37:16Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.arrow.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "Please add scaladoc for this even though its internal.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-14T22:26:03Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.arrow.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "Calling the input \"byteArray\" is maybe a bit confusing given how many byte arrays are floating around - can we give it a better name?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-14T22:27:44Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.arrow.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "Sure, just fyi this class is now part of the arrow-vector jar so it can be removed next release.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-15T17:39:35Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * NOTE - this is taken from test org.apache.arrow.vector.file, see about moving to public util pkg\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "your indentation style doesn't match Spark's.\r\n\r\n```\r\nprivate[sql] abstract class PrimitiveColumnWriter(val ordinal: Int, val allocator: BaseAllocator)\r\n  extends ColumnWriter{\r\n\r\n}\r\n```\r\n    ",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T05:32:55Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "will there be other payload types that are not ArrowStaticPayload?\r\n",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T05:39:50Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "No, I think this should be changed a little.  `ArrowPayload` is meant to encapsulate Arrow classes from the rest of Spark and wrap Arrow data to extend `serializable` to allow an `RDD[ArrowPayload]`.  I'll push an update that will clean this up.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-27T01:44:07Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "why even have this function? just change the signature of ArrowConverters.internalRowIterToArrowBatch and call that directly. ",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T05:48:35Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this is only used once? use private.\r\n",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T05:55:05Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "My latest update needed to use this outside of the object, but I will change it to `private[arrow]`",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-27T01:45:34Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "move init to the previous line",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T06:23:40Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "in what conditions would this fail? explain as inline comment",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T06:24:11Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "isn't the implementation of setNull the same across all implementations?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T06:39:02Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "Yes, but unfortunately it needs to call `valueMutator.setNull()` that is only a member of the concrete Mutator class, so I can't abstract that out. I think on the arrow side it could be defined as an interface and then I could clean it up here.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-27T01:47:55Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "rename this to append?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T06:42:04Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "It is writing to a buffer that has been allocated, so I think maybe `write` makes more sense - to me at least.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-27T01:49:17Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "just inline this",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T06:42:58Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit\n+  def setValue(row: InternalRow): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(): this.type = {\n+    valueVector.allocateNew()\n+    this\n+  }\n+\n+  override def write(row: InternalRow): Unit = {\n+    if (row.isNullAt(ordinal)) {\n+      setNull()\n+      nullCount += 1\n+    } else {\n+      setValue(row)\n+    }\n+    count += 1\n+  }\n+\n+  override def finish(): (ArrowFieldNode, Array[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers = valueVector.getBuffers(true)\n+    (fieldNode, valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "just resolve this todo?\r\n",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T06:43:51Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit\n+  def setValue(row: InternalRow): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(): this.type = {\n+    valueVector.allocateNew()\n+    this\n+  }\n+\n+  override def write(row: InternalRow): Unit = {\n+    if (row.isNullAt(ordinal)) {\n+      setNull()\n+      nullCount += 1\n+    } else {\n+      setValue(row)\n+    }\n+    count += 1\n+  }\n+\n+  override def finish(): (ArrowFieldNode, Array[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers = valueVector.getBuffers(true)\n+    (fieldNode, valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, bool2int(row.getBoolean(ordinal)))\n+}\n+\n+private[sql] class ShortColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableSmallIntVector\n+    = new NullableSmallIntVector(\"ShortValue\", allocator)\n+  override val valueMutator: NullableSmallIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getShort(ordinal))\n+}\n+\n+private[sql] class IntegerColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableIntVector\n+    = new NullableIntVector(\"IntValue\", allocator)\n+  override val valueMutator: NullableIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getInt(ordinal))\n+}\n+\n+private[sql] class LongColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableBigIntVector\n+    = new NullableBigIntVector(\"LongValue\", allocator)\n+  override val valueMutator: NullableBigIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getLong(ordinal))\n+}\n+\n+private[sql] class FloatColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableFloat4Vector\n+    = new NullableFloat4Vector(\"FloatValue\", allocator)\n+  override val valueMutator: NullableFloat4Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getFloat(ordinal))\n+}\n+\n+private[sql] class DoubleColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableFloat8Vector\n+    = new NullableFloat8Vector(\"DoubleValue\", allocator)\n+  override val valueMutator: NullableFloat8Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getDouble(ordinal))\n+}\n+\n+private[sql] class ByteColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableUInt1Vector\n+    = new NullableUInt1Vector(\"ByteValue\", allocator)\n+  override val valueMutator: NullableUInt1Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getByte(ordinal))\n+}\n+\n+private[sql] class UTF8StringColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)\n+  override val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit = {\n+    val bytes = row.getUTF8String(ordinal).getBytes\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class BinaryColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"BinaryValue\", allocator)\n+  override val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit = {\n+    val bytes = row.getBinary(ordinal)\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class DateColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableDateVector\n+    = new NullableDateVector(\"DateValue\", allocator)\n+  override val valueMutator: NullableDateVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit = {\n+    // TODO: comment on diff btw value representations of date/timestamp"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "The Arrow classes are defined more clearly now, so I don't think an explanation will be needed in my next update.",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-27T01:50:00Z",
    "diffHunk": "@@ -0,0 +1,432 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import java.io.ByteArrayOutputStream\n+import java.nio.ByteBuffer\n+import java.nio.channels.{Channels, SeekableByteChannel}\n+\n+import scala.collection.JavaConverters._\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.file.{ArrowReader, ArrowWriter}\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ArrowReader requires a seekable byte channel.\n+ * TODO: This is available in arrow-vector now with ARROW-615, to be included in 0.2.1 release\n+ */\n+private[sql] class ByteArrayReadableSeekableByteChannel(var byteArray: Array[Byte])\n+  extends SeekableByteChannel {\n+  var _position: Long = 0L\n+\n+  override def isOpen: Boolean = {\n+    byteArray != null\n+  }\n+\n+  override def close(): Unit = {\n+    byteArray = null\n+  }\n+\n+  override def read(dst: ByteBuffer): Int = {\n+    val remainingBuf = byteArray.length - _position\n+    val length = Math.min(dst.remaining(), remainingBuf).toInt\n+    dst.put(byteArray, _position.toInt, length)\n+    _position += length\n+    length\n+  }\n+\n+  override def position(): Long = _position\n+\n+  override def position(newPosition: Long): SeekableByteChannel = {\n+    _position = newPosition.toLong\n+    this\n+  }\n+\n+  override def size: Long = {\n+    byteArray.length.toLong\n+  }\n+\n+  override def write(src: ByteBuffer): Int = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+\n+  override def truncate(size: Long): SeekableByteChannel = {\n+    throw new UnsupportedOperationException(\"Read Only\")\n+  }\n+}\n+\n+/**\n+ * Intermediate data structure returned from Arrow conversions\n+ */\n+private[sql] abstract class ArrowPayload extends Iterator[ArrowRecordBatch]\n+\n+/**\n+ * Build a payload from existing ArrowRecordBatches\n+ */\n+private[sql] class ArrowStaticPayload(batches: ArrowRecordBatch*) extends ArrowPayload {\n+  private val iter = batches.iterator\n+  override def next(): ArrowRecordBatch = iter.next()\n+  override def hasNext: Boolean = iter.hasNext\n+}\n+\n+/**\n+ * Class that wraps an Arrow RootAllocator used in conversion\n+ */\n+private[sql] class ArrowConverters {\n+  private val _allocator = new RootAllocator(Long.MaxValue)\n+\n+  private[sql] def allocator: RootAllocator = _allocator\n+\n+  /**\n+   * Iterate over the rows and convert to an ArrowPayload, using RootAllocator from this class\n+   */\n+  def interalRowIterToPayload(rowIter: Iterator[InternalRow], schema: StructType): ArrowPayload = {\n+    val batch = ArrowConverters.internalRowIterToArrowBatch(rowIter, schema, _allocator)\n+    new ArrowStaticPayload(batch)\n+  }\n+\n+  /**\n+   * Read an Array of Arrow Record batches as byte Arrays into an ArrowPayload, using\n+   * RootAllocator from this class\n+   */\n+  def readPayloadByteArrays(payloadByteArrays: Array[Array[Byte]]): ArrowPayload = {\n+    val batches = scala.collection.mutable.ArrayBuffer.empty[ArrowRecordBatch]\n+    var i = 0\n+    while (i < payloadByteArrays.length) {\n+      val payloadBytes = payloadByteArrays(i)\n+      val in = new ByteArrayReadableSeekableByteChannel(payloadBytes)\n+      val reader = new ArrowReader(in, _allocator)\n+      val footer = reader.readFooter()\n+      val batchBlocks = footer.getRecordBatches.asScala.toArray\n+      batchBlocks.foreach(block => batches += reader.readRecordBatch(block))\n+      i += 1\n+    }\n+    new ArrowStaticPayload(batches: _*)\n+  }\n+\n+  /**\n+   * Call when done using this converter, will close RootAllocator so any ArrowBuffers should be\n+   * closed first\n+   */\n+  def close(): Unit = {\n+    _allocator.close()\n+  }\n+}\n+\n+private[sql] object ArrowConverters {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      // TODO: Enable Date and Timestamp type with Arrow 0.3\n+      // case DateType => ArrowType.Date.INSTANCE\n+      // case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: $dataType\")\n+    }\n+  }\n+\n+  /**\n+   * Iterate over InternalRows and convert to an ArrowRecordBatch.\n+   */\n+  private def internalRowIterToArrowBatch(\n+      rowIter: Iterator[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+\n+    val columnWriters = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      ColumnWriter(ordinal, allocator, field.dataType)\n+        .init()\n+    }\n+\n+    val writerLength = columnWriters.length\n+    while (rowIter.hasNext) {\n+      val row = rowIter.next()\n+      var i = 0\n+      while (i < writerLength) {\n+        columnWriters(i).write(row)\n+        i += 1\n+      }\n+    }\n+\n+    val (fieldNodes, bufferArrays) = columnWriters.map(_.finish()).unzip\n+    val buffers = bufferArrays.flatten\n+\n+    val rowLength = if (fieldNodes.nonEmpty) fieldNodes.head.getLength else 0\n+    val recordBatch = new ArrowRecordBatch(rowLength,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+\n+  /**\n+   * Write an ArrowPayload to a byte array\n+   */\n+  private[sql] def payloadToByteArray(payload: ArrowPayload, schema: StructType): Array[Byte] = {\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(schema)\n+    val out = new ByteArrayOutputStream()\n+    val writer = new ArrowWriter(Channels.newChannel(out), arrowSchema)\n+\n+    // Iterate over payload batches to write each one, ensure all batches get closed\n+    var batch: ArrowRecordBatch = null\n+    Utils.tryWithSafeFinallyAndFailureCallbacks {\n+      while (payload.hasNext) {\n+        batch = payload.next()\n+        writer.writeRecordBatch(batch)\n+        batch.close()\n+      }\n+    }(catchBlock = {\n+      Option(batch).foreach(_.close())\n+      payload.foreach(_.close())\n+    }, finallyBlock = writer.close())\n+\n+    out.toByteArray\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(): this.type\n+  def write(row: InternalRow): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (ArrowFieldNode, Array[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(\n+  val ordinal: Int,\n+  val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  def valueVector: BaseDataValueVector\n+  def valueMutator: BaseMutator\n+\n+  def setNull(): Unit\n+  def setValue(row: InternalRow): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(): this.type = {\n+    valueVector.allocateNew()\n+    this\n+  }\n+\n+  override def write(row: InternalRow): Unit = {\n+    if (row.isNullAt(ordinal)) {\n+      setNull()\n+      nullCount += 1\n+    } else {\n+      setValue(row)\n+    }\n+    count += 1\n+  }\n+\n+  override def finish(): (ArrowFieldNode, Array[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers = valueVector.getBuffers(true)\n+    (fieldNode, valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, bool2int(row.getBoolean(ordinal)))\n+}\n+\n+private[sql] class ShortColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableSmallIntVector\n+    = new NullableSmallIntVector(\"ShortValue\", allocator)\n+  override val valueMutator: NullableSmallIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getShort(ordinal))\n+}\n+\n+private[sql] class IntegerColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableIntVector\n+    = new NullableIntVector(\"IntValue\", allocator)\n+  override val valueMutator: NullableIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getInt(ordinal))\n+}\n+\n+private[sql] class LongColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableBigIntVector\n+    = new NullableBigIntVector(\"LongValue\", allocator)\n+  override val valueMutator: NullableBigIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getLong(ordinal))\n+}\n+\n+private[sql] class FloatColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableFloat4Vector\n+    = new NullableFloat4Vector(\"FloatValue\", allocator)\n+  override val valueMutator: NullableFloat4Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getFloat(ordinal))\n+}\n+\n+private[sql] class DoubleColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableFloat8Vector\n+    = new NullableFloat8Vector(\"DoubleValue\", allocator)\n+  override val valueMutator: NullableFloat8Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getDouble(ordinal))\n+}\n+\n+private[sql] class ByteColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableUInt1Vector\n+    = new NullableUInt1Vector(\"ByteValue\", allocator)\n+  override val valueMutator: NullableUInt1Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit\n+    = valueMutator.setSafe(count, row.getByte(ordinal))\n+}\n+\n+private[sql] class UTF8StringColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)\n+  override val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit = {\n+    val bytes = row.getUTF8String(ordinal).getBytes\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class BinaryColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"BinaryValue\", allocator)\n+  override val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit = {\n+    val bytes = row.getBinary(ordinal)\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class DateColumnWriter(ordinal: Int, allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(ordinal, allocator) {\n+  override val valueVector: NullableDateVector\n+    = new NullableDateVector(\"DateValue\", allocator)\n+  override val valueMutator: NullableDateVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow): Unit = {\n+    // TODO: comment on diff btw value representations of date/timestamp"
  }],
  "prId": 15821
}]