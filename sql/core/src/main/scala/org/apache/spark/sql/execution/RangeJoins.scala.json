[{
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "Please remove extra new lines.\n",
    "commit": "381d5030e15da798732e1d9b12b0edaa4e62c248",
    "createdAt": "2014-10-27T08:26:31Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.mutable.{ArrayBuffer, BitSet}\n+\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.SQLContext\n+\n+\n+"
  }],
  "prId": 2939
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "Please remove extra new lines.\n",
    "commit": "381d5030e15da798732e1d9b12b0edaa4e62c248",
    "createdAt": "2014-10-27T08:26:49Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.mutable.{ArrayBuffer, BitSet}\n+\n+"
  }],
  "prId": 2939
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "extra new lines.\n",
    "commit": "381d5030e15da798732e1d9b12b0edaa4e62c248",
    "createdAt": "2014-10-27T08:30:17Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.mutable.{ArrayBuffer, BitSet}\n+\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.SQLContext\n+\n+\n+\n+@DeveloperApi\n+case class RangeJoin(left: SparkPlan,\n+                     right: SparkPlan,\n+                     condition: Seq[Expression],\n+                     context: SQLContext) extends BinaryNode with Serializable{\n+  def output = left.output ++ right.output\n+\n+  lazy val (buildPlan, streamedPlan) = (left, right)\n+\n+  lazy val (buildKeys, streamedKeys) = (List(condition(0),condition(1)),\n+    List(condition(2), condition(3)))\n+\n+  @transient lazy val buildKeyGenerator = new InterpretedProjection(buildKeys, buildPlan.output)\n+  @transient lazy val streamKeyGenerator = new InterpretedProjection(streamedKeys,\n+    streamedPlan.output)\n+\n+  def execute() = {\n+\n+    val v1 = left.execute()\n+    val v1kv = v1.map(x => {\n+      val v1Key = buildKeyGenerator(x)\n+      (new Interval[Long](v1Key.apply(0).asInstanceOf[Long], v1Key.apply(1).asInstanceOf[Long]),\n+        x.copy() )\n+    })\n+\n+    val v2 = right.execute()\n+    val v2kv = v2.map(x => {\n+      val v2Key = streamKeyGenerator(x)\n+      (new Interval[Long](v2Key.apply(0).asInstanceOf[Long], v2Key.apply(1).asInstanceOf[Long]),\n+        x.copy() )\n+    })\n+\n+    /*As we are going to collect v1 and build an interval tree on its intervals,\n+    make sure that its size is the smaller one.*/\n+   assert(v1.count <= v2.count)\n+\n+\n+    val v3 = RangeJoinImpl.overlapJoin(context.sparkContext, v1kv, v2kv)\n+      .flatMap(l => l._2.map(r => (l._1,r)))\n+\n+    val v4 = v3.map {\n+      case (l: Row, r: Row) => new JoinedRow(l, r).withLeft(l)\n+    }\n+    v4\n+  }\n+\n+"
  }],
  "prId": 2939
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "extra new lines.\n",
    "commit": "381d5030e15da798732e1d9b12b0edaa4e62c248",
    "createdAt": "2014-10-27T08:30:24Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.mutable.{ArrayBuffer, BitSet}\n+\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.SQLContext\n+\n+\n+\n+@DeveloperApi\n+case class RangeJoin(left: SparkPlan,\n+                     right: SparkPlan,\n+                     condition: Seq[Expression],\n+                     context: SQLContext) extends BinaryNode with Serializable{\n+  def output = left.output ++ right.output\n+\n+  lazy val (buildPlan, streamedPlan) = (left, right)\n+\n+  lazy val (buildKeys, streamedKeys) = (List(condition(0),condition(1)),\n+    List(condition(2), condition(3)))\n+\n+  @transient lazy val buildKeyGenerator = new InterpretedProjection(buildKeys, buildPlan.output)\n+  @transient lazy val streamKeyGenerator = new InterpretedProjection(streamedKeys,\n+    streamedPlan.output)\n+\n+  def execute() = {\n+\n+    val v1 = left.execute()\n+    val v1kv = v1.map(x => {\n+      val v1Key = buildKeyGenerator(x)\n+      (new Interval[Long](v1Key.apply(0).asInstanceOf[Long], v1Key.apply(1).asInstanceOf[Long]),\n+        x.copy() )\n+    })\n+\n+    val v2 = right.execute()\n+    val v2kv = v2.map(x => {\n+      val v2Key = streamKeyGenerator(x)\n+      (new Interval[Long](v2Key.apply(0).asInstanceOf[Long], v2Key.apply(1).asInstanceOf[Long]),\n+        x.copy() )\n+    })\n+\n+    /*As we are going to collect v1 and build an interval tree on its intervals,\n+    make sure that its size is the smaller one.*/\n+   assert(v1.count <= v2.count)\n+\n+"
  }],
  "prId": 2939
}, {
  "comments": [{
    "author": {
      "login": "sarutak"
    },
    "body": "Please add white space between \")\" and \"{\" here and other places.\n",
    "commit": "381d5030e15da798732e1d9b12b0edaa4e62c248",
    "createdAt": "2014-10-27T08:31:01Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.mutable.{ArrayBuffer, BitSet}\n+\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.SQLContext\n+\n+\n+\n+@DeveloperApi\n+case class RangeJoin(left: SparkPlan,\n+                     right: SparkPlan,\n+                     condition: Seq[Expression],\n+                     context: SQLContext) extends BinaryNode with Serializable{\n+  def output = left.output ++ right.output\n+\n+  lazy val (buildPlan, streamedPlan) = (left, right)\n+\n+  lazy val (buildKeys, streamedKeys) = (List(condition(0),condition(1)),\n+    List(condition(2), condition(3)))\n+\n+  @transient lazy val buildKeyGenerator = new InterpretedProjection(buildKeys, buildPlan.output)\n+  @transient lazy val streamKeyGenerator = new InterpretedProjection(streamedKeys,\n+    streamedPlan.output)\n+\n+  def execute() = {\n+\n+    val v1 = left.execute()\n+    val v1kv = v1.map(x => {\n+      val v1Key = buildKeyGenerator(x)\n+      (new Interval[Long](v1Key.apply(0).asInstanceOf[Long], v1Key.apply(1).asInstanceOf[Long]),\n+        x.copy() )\n+    })\n+\n+    val v2 = right.execute()\n+    val v2kv = v2.map(x => {\n+      val v2Key = streamKeyGenerator(x)\n+      (new Interval[Long](v2Key.apply(0).asInstanceOf[Long], v2Key.apply(1).asInstanceOf[Long]),\n+        x.copy() )\n+    })\n+\n+    /*As we are going to collect v1 and build an interval tree on its intervals,\n+    make sure that its size is the smaller one.*/\n+   assert(v1.count <= v2.count)\n+\n+\n+    val v3 = RangeJoinImpl.overlapJoin(context.sparkContext, v1kv, v2kv)\n+      .flatMap(l => l._2.map(r => (l._1,r)))\n+\n+    val v4 = v3.map {\n+      case (l: Row, r: Row) => new JoinedRow(l, r).withLeft(l)\n+    }\n+    v4\n+  }\n+\n+\n+}\n+\n+case class Interval[T <% Long](start: T, end: T){"
  }],
  "prId": 2939
}]