[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "two things...\n1. you should use intellij simple paste (right click and then \"simple paste\") to avoid intellij auto formatting the code.\n2. update your intellij setting so it doesn't use scaladoc format, but use javadoc. in \"preferences -> editor -> code style -> scala -> spaces tab -> Other -> uncheck \"Use formatting for ScalaDoc2 options\"\n",
    "commit": "b7b319ccd4232c738fcb16f49ddcf7bd2f2370d5",
    "createdAt": "2015-11-19T21:42:10Z",
    "diffHunk": "@@ -48,26 +49,26 @@ private[spark] class SqlNewHadoopPartition(\n }\n \n /**\n- * An RDD that provides core functionality for reading data stored in Hadoop (e.g., files in HDFS,\n- * sources in HBase, or S3), using the new MapReduce API (`org.apache.hadoop.mapreduce`).\n- * It is based on [[org.apache.spark.rdd.NewHadoopRDD]]. It has three additions.\n- * 1. A shared broadcast Hadoop Configuration.\n- * 2. An optional closure `initDriverSideJobFuncOpt` that set configurations at the driver side\n- *    to the shared Hadoop Configuration.\n- * 3. An optional closure `initLocalJobFuncOpt` that set configurations at both the driver side\n- *    and the executor side to the shared Hadoop Configuration.\n- *\n- * Note: This is RDD is basically a cloned version of [[org.apache.spark.rdd.NewHadoopRDD]] with\n- * changes based on [[org.apache.spark.rdd.HadoopRDD]].\n- */\n+  * An RDD that provides core functionality for reading data stored in Hadoop (e.g., files in HDFS,"
  }],
  "prId": 9845
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "indent args 4 spaces.\n",
    "commit": "b7b319ccd4232c738fcb16f49ddcf7bd2f2370d5",
    "createdAt": "2015-11-20T19:38:46Z",
    "diffHunk": "@@ -61,13 +62,13 @@ private[spark] class SqlNewHadoopPartition(\n  * changes based on [[org.apache.spark.rdd.HadoopRDD]].\n  */\n private[spark] class SqlNewHadoopRDD[V: ClassTag](\n-    sc : SparkContext,\n-    broadcastedConf: Broadcast[SerializableConfiguration],\n-    @transient private val initDriverSideJobFuncOpt: Option[Job => Unit],\n-    initLocalJobFuncOpt: Option[Job => Unit],\n-    inputFormatClass: Class[_ <: InputFormat[Void, V]],\n-    valueClass: Class[V])\n-  extends RDD[V](sc, Nil)\n+  sqlContext: SQLContext,\n+  broadcastedConf: Broadcast[SerializableConfiguration],\n+  @transient private val initDriverSideJobFuncOpt: Option[Job => Unit],\n+  initLocalJobFuncOpt: Option[Job => Unit],\n+  inputFormatClass: Class[_ <: InputFormat[Void, V]],\n+  valueClass: Class[V])"
  }],
  "prId": 9845
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "indent args 4 spaces.\n",
    "commit": "b7b319ccd4232c738fcb16f49ddcf7bd2f2370d5",
    "createdAt": "2015-11-20T19:38:53Z",
    "diffHunk": "@@ -28,18 +30,17 @@ import org.apache.spark.broadcast.Broadcast\n import org.apache.spark.deploy.SparkHadoopUtil\n import org.apache.spark.executor.DataReadMethod\n import org.apache.spark.mapreduce.SparkHadoopMapReduceUtil\n+import org.apache.spark.sql.{SQLConf, SQLContext}\n+import org.apache.spark.sql.execution.datasources.parquet.UnsafeRowParquetRecordReader\n import org.apache.spark.storage.StorageLevel\n-import org.apache.spark.unsafe.types.UTF8String\n-import org.apache.spark.util.{Utils, SerializableConfiguration, ShutdownHookManager}\n+import org.apache.spark.util.{SerializableConfiguration, ShutdownHookManager}\n import org.apache.spark.{Partition => SparkPartition, _}\n \n-import scala.reflect.ClassTag\n-\n \n private[spark] class SqlNewHadoopPartition(\n-    rddId: Int,\n-    val index: Int,\n-    rawSplit: InputSplit with Writable)\n+  rddId: Int,\n+  val index: Int,\n+  rawSplit: InputSplit with Writable)"
  }],
  "prId": 9845
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "indent args 4 spaces.\n",
    "commit": "b7b319ccd4232c738fcb16f49ddcf7bd2f2370d5",
    "createdAt": "2015-11-20T19:39:11Z",
    "diffHunk": "@@ -276,32 +275,15 @@ private[spark] class SqlNewHadoopRDD[V: ClassTag](\n     }\n     super.persist(storageLevel)\n   }\n-}\n-\n-private[spark] object SqlNewHadoopRDD {\n-\n-  /**\n-   * The thread variable for the name of the current file being read. This is used by\n-   * the InputFileName function in Spark SQL.\n-   */\n-  private[this] val inputFileName: ThreadLocal[UTF8String] = new ThreadLocal[UTF8String] {\n-    override protected def initialValue(): UTF8String = UTF8String.fromString(\"\")\n-  }\n-\n-  def getInputFileName(): UTF8String = inputFileName.get()\n-\n-  private[spark] def setInputFileName(file: String) = inputFileName.set(UTF8String.fromString(file))\n-\n-  private[spark] def unsetInputFileName(): Unit = inputFileName.remove()\n \n   /**\n    * Analogous to [[org.apache.spark.rdd.MapPartitionsRDD]], but passes in an InputSplit to\n    * the given function rather than the index of the partition.\n    */\n   private[spark] class NewHadoopMapPartitionsWithSplitRDD[U: ClassTag, T: ClassTag](\n-      prev: RDD[T],\n-      f: (InputSplit, Iterator[T]) => Iterator[U],\n-      preservesPartitioning: Boolean = false)\n+    prev: RDD[T],\n+    f: (InputSplit, Iterator[T]) => Iterator[U],\n+    preservesPartitioning: Boolean = false)"
  }],
  "prId": 9845
}]