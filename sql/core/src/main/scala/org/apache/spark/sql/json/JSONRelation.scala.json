[{
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Is that the same with the `refresh` invoking in `DataSourceStrategy`? As every time we build the rdd, we need to refresh the file status.\n\nActually, I've updated the #8023, by simply removing all of the `refresh` calls in the `DataSourceStrategy`, and the `InsertIntoHadoopFsRelation` will actually update the file status implicitly while writing the data.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-07T16:59:10Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "The refresh added in `DataSourceStrategy` also affects Parquet and ORC.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-07T17:36:30Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "I haven't finished reviewing #8023, let's have an offline discussion about it tomorrow :)\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-07T17:55:43Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Why calling refresh at here will work? Is `inputPaths` containing all file paths or dir paths?\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-07T22:14:57Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "`inputPaths` contains the \"base\" input paths, while the `inputPaths` of the other `buildScan` method in `JSONRelation` contains `FileStatus`es of leaf files under base input paths, which is retrieved from the `FileStatusCache` in `HadoopFsRelation`.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-08T05:48:13Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Sorry for the late reply, but I am wondering if the ORC  / Parquet has the similar bug. \nhttps://github.com/apache/spark/pull/8023/files#diff-7c81d57959cd291f5297e4585fd7cdc3R273\n\nHopefully #8023 can fix this.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-11T03:48:27Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "What issue? Cached metadata can be out-of-date when users add data directly? Because refreshing ORC and Parquet tables' metadata is expensive, I think that is a expected behavior (users need to call refresh explicitly).\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-11T03:57:14Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Ok, but seems only `ParquetRelation` will refresh the meta data, for Json and ORC only will refresh the file status.\n\nThe problem here, once the table cached, even if people call the refresh() implicitly, we will not get the refreshed data either.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-11T04:22:10Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "For json, we do not have other metadata to cache other than file status, right?\n\n`once the table cached` =>  you mean after we call `cache table`? I do see we drop the cached logical plan from our cache manager. Can you try it and see if we drop the cached table data?\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-11T04:29:06Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "I don't mean we need to refresh the schema / metadata, but fetching the latest file under the table path.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-11T04:36:33Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "As the files under the table path probably changed by other applications, I think we have to refresh the file status before the file scanning.\n\nProbably we can move out the parquet meta data refreshing from the `HadoopFsRelation.refresh`.\nUpdate: I mean from the `ParquetRelation.refresh()`\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-11T04:38:53Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {\n+    refresh()",
    "line": 36
  }],
  "prId": 8035
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "When we create rdds for partitioned json table, are we using this broadcastedConf?\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-07T22:13:46Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {",
    "line": 35
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "No, 'cause we're not using `SqlNewHadoopRDD` here. This method was originally marked as `final` in `HadoopFsRelation` and wasn't supposed to be overriden in concrete data source implementations.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-08T05:45:11Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {",
    "line": 35
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Then, if we read a table with lots of partitioned, the time spent on planning the query will be pretty long, right?\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-08T18:52:47Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {",
    "line": 35
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Let's file a followup jira to re-use this `broadcastedConf`. So, we will not broadcast hadoop conf for every RDD created for every partition.\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-10T16:02:33Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {",
    "line": 35
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "btw, do we have a jira for investigating using `SerializableConfiguration` instead of this shared broad conf for 1.6?\n",
    "commit": "ec1957ded822c458a9d4ed732b955eadc6a2568f",
    "createdAt": "2015-08-10T16:05:42Z",
    "diffHunk": "@@ -105,6 +107,15 @@ private[sql] class JSONRelation(\n     jsonSchema\n   }\n \n+  override private[sql] def buildScan(\n+      requiredColumns: Array[String],\n+      filters: Array[Filter],\n+      inputPaths: Array[String],\n+      broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row] = {",
    "line": 35
  }],
  "prId": 8035
}]