[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why was this change included here and not in a separate PR?",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-03T23:21:02Z",
    "diffHunk": "@@ -20,17 +20,20 @@ package org.apache.spark.sql.execution.datasources.v2\n import org.apache.spark.sql.AnalysisException\n import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n-import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+import org.apache.spark.sql.sources.v2.TableCapability.{BATCH_READ, CONTINUOUS_READ, MICRO_BATCH_READ}\n \n /**\n  * This rules adds some basic table capability check for streaming scan, without knowing the actual\n  * streaming execution mode.\n  */\n-object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+object V2ScanSupportCheck extends (LogicalPlan => Unit) {\n   import DataSourceV2Implicits._\n \n   override def apply(plan: LogicalPlan): Unit = {\n     plan.foreach {\n+      case r: DataSourceV2Relation if !r.table.supports(BATCH_READ) =>\n+        throw new AnalysisException(\n+          s\"Table ${r.table.name()} does not support batch scan.\")"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I've created a separate PR to fix it: https://github.com/apache/spark/pull/25679",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-04T13:49:31Z",
    "diffHunk": "@@ -20,17 +20,20 @@ package org.apache.spark.sql.execution.datasources.v2\n import org.apache.spark.sql.AnalysisException\n import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n-import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+import org.apache.spark.sql.sources.v2.TableCapability.{BATCH_READ, CONTINUOUS_READ, MICRO_BATCH_READ}\n \n /**\n  * This rules adds some basic table capability check for streaming scan, without knowing the actual\n  * streaming execution mode.\n  */\n-object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+object V2ScanSupportCheck extends (LogicalPlan => Unit) {\n   import DataSourceV2Implicits._\n \n   override def apply(plan: LogicalPlan): Unit = {\n     plan.foreach {\n+      case r: DataSourceV2Relation if !r.table.supports(BATCH_READ) =>\n+        throw new AnalysisException(\n+          s\"Table ${r.table.name()} does not support batch scan.\")"
  }],
  "prId": 25651
}]