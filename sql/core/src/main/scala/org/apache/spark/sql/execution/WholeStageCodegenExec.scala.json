[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Instead of adding these logics into `object WholeStageCodegenExec`, could we just create a new object?",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-13T20:52:25Z",
    "diffHunk": "@@ -312,6 +313,24 @@ case class InputAdapter(child: SparkPlan) extends UnaryExecNode with CodegenSupp\n object WholeStageCodegenExec {\n   val PIPELINE_DURATION_METRIC = \"duration\"\n \n+  private val codegenStageCounter = ThreadLocal.withInitial(new Supplier[Integer] {"
  }, {
    "author": {
      "login": "rednaxelafx"
    },
    "body": "But IMHO this is the place to put it, since it's closely tied to the initialization of `WholeStageCodegenExec` object instances.\r\nAre you suggesting something like a `object WholeStageCodegenId` and move the newly added methods there instead? That would work too.",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-17T08:13:31Z",
    "diffHunk": "@@ -312,6 +313,24 @@ case class InputAdapter(child: SparkPlan) extends UnaryExecNode with CodegenSupp\n object WholeStageCodegenExec {\n   val PIPELINE_DURATION_METRIC = \"duration\"\n \n+  private val codegenStageCounter = ThreadLocal.withInitial(new Supplier[Integer] {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Yeah. Creating a new `object WholeStageCodegenId` is better. ",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-17T08:27:54Z",
    "diffHunk": "@@ -312,6 +313,24 @@ case class InputAdapter(child: SparkPlan) extends UnaryExecNode with CodegenSupp\n object WholeStageCodegenExec {\n   val PIPELINE_DURATION_METRIC = \"duration\"\n \n+  private val codegenStageCounter = ThreadLocal.withInitial(new Supplier[Integer] {"
  }],
  "prId": 20224
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "We need to reset the counter? Is it a bad idea to use just a simple incremental counter like `exprId`?",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-14T04:10:24Z",
    "diffHunk": "@@ -575,7 +609,10 @@ case class CollapseCodegenStages(conf: SQLConf) extends Rule[SparkPlan] {\n \n   def apply(plan: SparkPlan): SparkPlan = {\n     if (conf.wholeStageEnabled) {\n-      insertWholeStageCodegen(plan)\n+      WholeStageCodegenExec.initializeCodegenStageCounterPerQuery()\n+      val newPlan = insertWholeStageCodegen(plan)\n+      WholeStageCodegenExec.resetCodegenStageCounter()"
  }, {
    "author": {
      "login": "rednaxelafx"
    },
    "body": "It's exactly the concern that @kiszk brought up: the codegen cache uses the generated source code as the key, so any differences in the source code text would break the cache hit.\r\nImagine the counter were a globally (or Spark session-local globally) atomically incrementing, then if the same query were run twice, the codegen stages in those two runs will actually get different sets of IDs, resulting in different source code text (everything's the same except for the ID), and then it'll render the codegen cache useless -- basically nothing will never hit the cache since the ID will always be different.\r\n\r\nIn fact, you'll find that the IDs in the explain output through `df.explain()` are going to be different from the ones you see in Spark UI's SQL tab's treeString, because explain is actually \"one query with the `ExplainCommand` as the root\". I had hit this exact problem in my early prototype and soon realized this isn't going to be user-friendly.\r\n\r\nBy making the ID only increment within a query, we can make sure the codegen cache works for multiple runs of the same (or identically structured) query, and still be able to differentiate the codegen stages within a query.",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-17T08:20:36Z",
    "diffHunk": "@@ -575,7 +609,10 @@ case class CollapseCodegenStages(conf: SQLConf) extends Rule[SparkPlan] {\n \n   def apply(plan: SparkPlan): SparkPlan = {\n     if (conf.wholeStageEnabled) {\n-      insertWholeStageCodegen(plan)\n+      WholeStageCodegenExec.initializeCodegenStageCounterPerQuery()\n+      val newPlan = insertWholeStageCodegen(plan)\n+      WholeStageCodegenExec.resetCodegenStageCounter()"
  }],
  "prId": 20224
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "my major concern is, what if we transform the physical plan tree after adding `WholeStageCodegenExec`? e.g. during transformation we may copy a plan code, then a copied `WholeStageCodegenExec` will have a different stage id.",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-24T04:17:57Z",
    "diffHunk": "@@ -365,6 +388,26 @@ case class WholeStageCodegenExec(child: SparkPlan) extends UnaryExecNode with Co\n     \"pipelineTime\" -> SQLMetrics.createTimingMetric(sparkContext,\n       WholeStageCodegenExec.PIPELINE_DURATION_METRIC))\n \n+  /**\n+   * ID for codegen stages within a query plan.\n+   * It does not affect equality, nor does it participate in destructuring pattern matching.\n+   *\n+   * Within a query, a codegen stage in a plan starts counting from 1, in \"insertion order\".\n+   * WholeStageCodegenExec operators are inserted into a plan in depth-first post-order.\n+   * See CollapseCodegenStages.insertWholeStageCodegen for the definition of insertion order.\n+   *\n+   * 0 is reserved as a special ID value to indicate a temporary WholeStageCodegenExec object\n+   * is created, e.g. for special fallback handling when an existing WholeStageCodegenExec\n+   * failed to generate/compile code.\n+   */\n+  val codegenStageId = WholeStageCodegenId.getNextStageId()"
  }, {
    "author": {
      "login": "rednaxelafx"
    },
    "body": "Thanks for your comment, @cloud-fan ! That's a nice catch that I hadn't really thought about.\r\n\r\nAll the examples that I've run with are ones that wouldn't trigger changes to the plan after `CollapseCodegenStages`, which means for those examples `ReuseExchange` / `ReuseSubqueries` wouldn't have triggered.\r\nYes, these two rules could potentially change the physical plan, which means when we `transformUp` in those rules (and any future rule after `CollapseCodegenStages`) it'd create new `WholeStageCodegenExec` objects outside of `CollapseCodegenStages`, and with my current implementation that'll result in the WSC copies having a codegen stage ID of 0.\r\n\r\nOne way to workaround this is to move `CollapseCodegenStages` to always be the last rule in `org.apache.spark.sql.execution.QueryExecution#preparations`, so that we're sure there's no other transformation on the physical plan that could change the structure of the plan, except for fallback handling that could happen in a couple of  `doExecute()`s -- these exception cases are expected and to me they are acceptable.\r\n\r\n~~If we go down that route, I'll probably have to tweak `CollapseCodegenStages` a little bit so that it can cope with the physical query plan potentially becoming a DAG instead of a tree, as the `ReuseExchange` / `ReuseSubqueries` rules may do that kind of transformation. This tweak is easy to implement and low risk: simply bailing out of the transforming a subtree when it sees a `WholeStageCodegenExec` already inserted into the plan would suffice.~~\r\n^^ scratch that. I'll need something a bit more involved to deal with DAGs in this case.\r\n\r\nLet me actually update the PR with this tweak and see what happens in tests.",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-24T06:17:29Z",
    "diffHunk": "@@ -365,6 +388,26 @@ case class WholeStageCodegenExec(child: SparkPlan) extends UnaryExecNode with Co\n     \"pipelineTime\" -> SQLMetrics.createTimingMetric(sparkContext,\n       WholeStageCodegenExec.PIPELINE_DURATION_METRIC))\n \n+  /**\n+   * ID for codegen stages within a query plan.\n+   * It does not affect equality, nor does it participate in destructuring pattern matching.\n+   *\n+   * Within a query, a codegen stage in a plan starts counting from 1, in \"insertion order\".\n+   * WholeStageCodegenExec operators are inserted into a plan in depth-first post-order.\n+   * See CollapseCodegenStages.insertWholeStageCodegen for the definition of insertion order.\n+   *\n+   * 0 is reserved as a special ID value to indicate a temporary WholeStageCodegenExec object\n+   * is created, e.g. for special fallback handling when an existing WholeStageCodegenExec\n+   * failed to generate/compile code.\n+   */\n+  val codegenStageId = WholeStageCodegenId.getNextStageId()"
  }],
  "prId": 20224
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we just use 1 as initial value and add a comment to say that 0 is preserved for temporary WholeStageCodegenExec objects? Then we only need a `initialize` method.",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-25T04:33:44Z",
    "diffHunk": "@@ -325,6 +326,28 @@ object WholeStageCodegenExec {\n   }\n }\n \n+object WholeStageCodegenId {\n+  private val codegenStageCounter = ThreadLocal.withInitial(new Supplier[Integer] {\n+    override def get() = 0  // TODO: change to Scala lambda syntax when upgraded to Scala 2.12+"
  }, {
    "author": {
      "login": "rednaxelafx"
    },
    "body": "With the updated PR that uses the secondary constructor in `WholeStageCodegenExec`, yes you're making a good point. All the places that create temporary `WholeStageCodegenExec` objects are explicitly passing in `0` as the codegen stage ID now, so we can indeed simplify the counter logic here.\r\n\r\nWill address in the next update.",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-25T06:17:06Z",
    "diffHunk": "@@ -325,6 +326,28 @@ object WholeStageCodegenExec {\n   }\n }\n \n+object WholeStageCodegenId {\n+  private val codegenStageCounter = ThreadLocal.withInitial(new Supplier[Integer] {\n+    override def get() = 0  // TODO: change to Scala lambda syntax when upgraded to Scala 2.12+"
  }],
  "prId": 20224
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "I think we should describe about the usage of such codegen stage id, e.g., the codegen stage id would show up in explain string and generated class name.",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-25T08:01:49Z",
    "diffHunk": "@@ -325,6 +326,32 @@ object WholeStageCodegenExec {\n   }\n }\n \n+object WholeStageCodegenId {\n+  // codegenStageId: ID for codegen stages within a query plan.\n+  // It does not affect equality, nor does it participate in destructuring pattern matching.\n+  //\n+  // Within a query, a codegen stage in a plan starts counting from 1, in \"insertion order\".\n+  // WholeStageCodegenExec operators are inserted into a plan in depth-first post-order.\n+  // See CollapseCodegenStages.insertWholeStageCodegen for the definition of insertion order.\n+  //\n+  // 0 is reserved as a special ID value to indicate a temporary WholeStageCodegenExec object\n+  // is created, e.g. for special fallback handling when an existing WholeStageCodegenExec\n+  // failed to generate/compile code.\n+",
    "line": 49
  }, {
    "author": {
      "login": "rednaxelafx"
    },
    "body": "Sure thing. Will address it in the next update. Thanks!",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-25T09:12:12Z",
    "diffHunk": "@@ -325,6 +326,32 @@ object WholeStageCodegenExec {\n   }\n }\n \n+object WholeStageCodegenId {\n+  // codegenStageId: ID for codegen stages within a query plan.\n+  // It does not affect equality, nor does it participate in destructuring pattern matching.\n+  //\n+  // Within a query, a codegen stage in a plan starts counting from 1, in \"insertion order\".\n+  // WholeStageCodegenExec operators are inserted into a plan in depth-first post-order.\n+  // See CollapseCodegenStages.insertWholeStageCodegen for the definition of insertion order.\n+  //\n+  // 0 is reserved as a special ID value to indicate a temporary WholeStageCodegenExec object\n+  // is created, e.g. for special fallback handling when an existing WholeStageCodegenExec\n+  // failed to generate/compile code.\n+",
    "line": 49
  }],
  "prId": 20224
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "Can we always add `codegenStageId` as a comment by using ctx.registerComment()?",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-25T19:31:11Z",
    "diffHunk": "@@ -471,19 +531,21 @@ case class WholeStageCodegenExec(child: SparkPlan) extends UnaryExecNode with Co\n         }\n        \"\"\", inlineToOuterClass = true)\n \n+    val className = generatedClassName()\n+\n     val source = s\"\"\"\n       public Object generate(Object[] references) {\n-        return new GeneratedIterator(references);\n+        return new $className(references);\n       }\n \n       ${ctx.registerComment(s\"\"\"Codegend pipeline for\\n${child.treeString.trim}\"\"\")}\n-      final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n+      final class $className extends ${classOf[BufferedRowIterator].getName} {"
  }, {
    "author": {
      "login": "rednaxelafx"
    },
    "body": "Yes, that's a great idea that I missed! Thanks for your comments, @kiszk !\r\n\r\nWe can get that effect in two ways:\r\n1. Change the current line `${ctx.registerComment(s\"\"\"Codegend pipeline for\\n${child.treeString.trim}\"\"\")}` from `child.treeString` to `this.treeString`, which will include the codegen stage ID through the `treeString`, just like the explain output.\r\n2. Simply add `$codegenStageId` into the `Codegend pipeline for` line.\r\n3. Do both above. (Did I say two...?)\r\n\r\nWhich do you prefer?\r\n```diff\r\n$ git diff\r\ndiff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala\r\nindex b0090af77e..0e525b1e22 100644\r\n--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala\r\n+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala\r\n@@ -538,7 +538,9 @@ case class WholeStageCodegenExec(child: SparkPlan)(val codegenStageId: Int)\r\n         return new $className(references);\r\n       }\r\n \r\n-      ${ctx.registerComment(s\"\"\"Codegend pipeline for\\n${child.treeString.trim}\"\"\")}\r\n+      ${ctx.registerComment(\r\n+        s\"\"\"Codegend pipeline for stage (id=$codegenStageId)\r\n+           |${this.treeString.trim}\"\"\".stripMargin)}\r\n       final class $className extends ${classOf[BufferedRowIterator].getName} {\r\n \r\n         private Object[] references;\r\n```\r\nAn example generated code with comments enabled is:\r\n```java\r\n/* 001 */ public Object generate(Object[] references) {\r\n/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\r\n/* 003 */ }\r\n/* 004 */\r\n/* 005 */ /**\r\n * Codegend pipeline for stage (id=2)\r\n * *(2) Project [(id#0L + 1) AS x#4L]\r\n * +- *(2) Sort [id#0L ASC NULLS FIRST], true, 0\r\n *    +- Exchange rangepartitioning(id#0L ASC NULLS FIRST, 200)\r\n *       +- *(1) Range (0, 1, step=1, splits=8)\r\n */\r\n/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\r\n/* 007 */   private Object[] references;\r\n...\r\n```",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-25T19:42:43Z",
    "diffHunk": "@@ -471,19 +531,21 @@ case class WholeStageCodegenExec(child: SparkPlan) extends UnaryExecNode with Co\n         }\n        \"\"\", inlineToOuterClass = true)\n \n+    val className = generatedClassName()\n+\n     val source = s\"\"\"\n       public Object generate(Object[] references) {\n-        return new GeneratedIterator(references);\n+        return new $className(references);\n       }\n \n       ${ctx.registerComment(s\"\"\"Codegend pipeline for\\n${child.treeString.trim}\"\"\")}\n-      final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n+      final class $className extends ${classOf[BufferedRowIterator].getName} {"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "Thank for your changes. They look very good.\r\n\r\nI missed one fact (I was sleepy :)). `ctx.registerComment()` is enabled only when `spark.sql.codegen.comments` is `true`. It would be good to add the id in the comment regardless of `spark.sql.codegen.comments` since this comment is very small.\r\nI could create a follow-up PR this afternoon.\r\n",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-26T01:43:55Z",
    "diffHunk": "@@ -471,19 +531,21 @@ case class WholeStageCodegenExec(child: SparkPlan) extends UnaryExecNode with Co\n         }\n        \"\"\", inlineToOuterClass = true)\n \n+    val className = generatedClassName()\n+\n     val source = s\"\"\"\n       public Object generate(Object[] references) {\n-        return new GeneratedIterator(references);\n+        return new $className(references);\n       }\n \n       ${ctx.registerComment(s\"\"\"Codegend pipeline for\\n${child.treeString.trim}\"\"\")}\n-      final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n+      final class $className extends ${classOf[BufferedRowIterator].getName} {"
  }, {
    "author": {
      "login": "rednaxelafx"
    },
    "body": "Yes, I like that idea too. Please ping me on the follow-up PR as well. Thanks!",
    "commit": "fd8983edaee5fe9a7968de45a174d6781128296e",
    "createdAt": "2018-01-26T01:58:11Z",
    "diffHunk": "@@ -471,19 +531,21 @@ case class WholeStageCodegenExec(child: SparkPlan) extends UnaryExecNode with Co\n         }\n        \"\"\", inlineToOuterClass = true)\n \n+    val className = generatedClassName()\n+\n     val source = s\"\"\"\n       public Object generate(Object[] references) {\n-        return new GeneratedIterator(references);\n+        return new $className(references);\n       }\n \n       ${ctx.registerComment(s\"\"\"Codegend pipeline for\\n${child.treeString.trim}\"\"\")}\n-      final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n+      final class $className extends ${classOf[BufferedRowIterator].getName} {"
  }],
  "prId": 20224
}]