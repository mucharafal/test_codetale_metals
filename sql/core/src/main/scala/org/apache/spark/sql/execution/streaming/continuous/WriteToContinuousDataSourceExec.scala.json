[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Could you please explain the needs of additional handling? Since ContinuousWriteRDD is still handling the error case.",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-01T10:47:23Z",
    "diffHunk": "@@ -46,28 +46,34 @@ case class WriteToContinuousDataSourceExec(writer: StreamWriter, query: SparkPla\n       case _ => new InternalRowDataWriterFactory(writer.createWriterFactory(), query.schema)\n     }\n \n-    val rdd = query.execute()\n+    val rdd = new ContinuousWriteRDD(query.execute(), writerFactory)\n+    val messages = new Array[WriterCommitMessage](rdd.partitions.length)\n \n     logInfo(s\"Start processing data source writer: $writer. \" +\n-      s\"The input RDD has ${rdd.getNumPartitions} partitions.\")\n-    // Let the epoch coordinator know how many partitions the write RDD has.\n+      s\"The input RDD has ${messages.length} partitions.\")\n     EpochCoordinatorRef.get(\n-        sparkContext.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n-        sparkContext.env)\n+      sparkContext.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n+      sparkContext.env)\n       .askSync[Unit](SetWriterPartitions(rdd.getNumPartitions))\n \n     try {\n       // Force the RDD to run so continuous processing starts; no data is actually being collected\n       // to the driver, as ContinuousWriteRDD outputs nothing.\n-      sparkContext.runJob(\n-        rdd,\n-        (context: TaskContext, iter: Iterator[InternalRow]) =>\n-          WriteToContinuousDataSourceExec.run(writerFactory, context, iter),\n-        rdd.partitions.indices)\n+      rdd.collect()\n     } catch {\n       case _: InterruptedException =>\n-        // Interruption is how continuous queries are ended, so accept and ignore the exception.\n+      // Interruption is how continuous queries are ended, so accept and ignore the exception.\n       case cause: Throwable =>\n+        logError(s\"Data source writer $writer is aborting.\")"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Sorry, I rebased wrong. This change wasn't meant to be here.",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-01T16:43:13Z",
    "diffHunk": "@@ -46,28 +46,34 @@ case class WriteToContinuousDataSourceExec(writer: StreamWriter, query: SparkPla\n       case _ => new InternalRowDataWriterFactory(writer.createWriterFactory(), query.schema)\n     }\n \n-    val rdd = query.execute()\n+    val rdd = new ContinuousWriteRDD(query.execute(), writerFactory)\n+    val messages = new Array[WriterCommitMessage](rdd.partitions.length)\n \n     logInfo(s\"Start processing data source writer: $writer. \" +\n-      s\"The input RDD has ${rdd.getNumPartitions} partitions.\")\n-    // Let the epoch coordinator know how many partitions the write RDD has.\n+      s\"The input RDD has ${messages.length} partitions.\")\n     EpochCoordinatorRef.get(\n-        sparkContext.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n-        sparkContext.env)\n+      sparkContext.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n+      sparkContext.env)\n       .askSync[Unit](SetWriterPartitions(rdd.getNumPartitions))\n \n     try {\n       // Force the RDD to run so continuous processing starts; no data is actually being collected\n       // to the driver, as ContinuousWriteRDD outputs nothing.\n-      sparkContext.runJob(\n-        rdd,\n-        (context: TaskContext, iter: Iterator[InternalRow]) =>\n-          WriteToContinuousDataSourceExec.run(writerFactory, context, iter),\n-        rdd.partitions.indices)\n+      rdd.collect()\n     } catch {\n       case _: InterruptedException =>\n-        // Interruption is how continuous queries are ended, so accept and ignore the exception.\n+      // Interruption is how continuous queries are ended, so accept and ignore the exception.\n       case cause: Throwable =>\n+        logError(s\"Data source writer $writer is aborting.\")"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "(It was in an older version of WriteToContinuousDataSourceExec, and has since been removed.)",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-01T16:44:13Z",
    "diffHunk": "@@ -46,28 +46,34 @@ case class WriteToContinuousDataSourceExec(writer: StreamWriter, query: SparkPla\n       case _ => new InternalRowDataWriterFactory(writer.createWriterFactory(), query.schema)\n     }\n \n-    val rdd = query.execute()\n+    val rdd = new ContinuousWriteRDD(query.execute(), writerFactory)\n+    val messages = new Array[WriterCommitMessage](rdd.partitions.length)\n \n     logInfo(s\"Start processing data source writer: $writer. \" +\n-      s\"The input RDD has ${rdd.getNumPartitions} partitions.\")\n-    // Let the epoch coordinator know how many partitions the write RDD has.\n+      s\"The input RDD has ${messages.length} partitions.\")\n     EpochCoordinatorRef.get(\n-        sparkContext.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n-        sparkContext.env)\n+      sparkContext.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n+      sparkContext.env)\n       .askSync[Unit](SetWriterPartitions(rdd.getNumPartitions))\n \n     try {\n       // Force the RDD to run so continuous processing starts; no data is actually being collected\n       // to the driver, as ContinuousWriteRDD outputs nothing.\n-      sparkContext.runJob(\n-        rdd,\n-        (context: TaskContext, iter: Iterator[InternalRow]) =>\n-          WriteToContinuousDataSourceExec.run(writerFactory, context, iter),\n-        rdd.partitions.indices)\n+      rdd.collect()\n     } catch {\n       case _: InterruptedException =>\n-        // Interruption is how continuous queries are ended, so accept and ignore the exception.\n+      // Interruption is how continuous queries are ended, so accept and ignore the exception.\n       case cause: Throwable =>\n+        logError(s\"Data source writer $writer is aborting.\")"
  }],
  "prId": 21200
}]