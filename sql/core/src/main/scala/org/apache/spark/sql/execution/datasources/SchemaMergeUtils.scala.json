[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Please fix this line. Also run `dev/scalastyle`.\r\n```\r\n[error] /home/jenkins/workspace/SparkPullRequestBuilder/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaMergeUtils.scala:72: File line length exceeds 100 characters\r\n```",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-09T16:04:40Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,\n+    files: Seq[FileStatus],\n+    parallelSchemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+    : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet requires `FileStatus`es to read footers.  Here we try to send cached `FileStatus`es\n+    // to executor side to avoid fetching them again.  However, `FileStatus` is not `Serializable`\n+    // but only `Writable`.  What makes it worse, for some reason, `FileStatus` doesn't play well\n+    // with `SerializableWritable[T]` and always causes a weird `IllegalStateException`.  These\n+    // facts virtually prevents us to serialize `FileStatus`es.\n+    //\n+    // Since Parquet only relies on path and length information of those `FileStatus`es to read\n+    // footers, here we just extract them (which can be easily serialized), send them to executor\n+    // side, and resemble fake `FileStatus`es there.\n+    val partialFileStatusInfo = files.map(f => (f.getPath.toString, f.getLen))\n+\n+    // Set the number of partitions to prevent following schema reads from generating many tasks\n+    // in case of a small number of orc files.\n+    val numParallelism = Math.min(Math.max(partialFileStatusInfo.size, 1),\n+      sparkSession.sparkContext.defaultParallelism)\n+\n+    val ignoreCorruptFiles = sparkSession.sessionState.conf.ignoreCorruptFiles\n+\n+    // Issues a Spark job to read Parquet/ORC schema in parallel.\n+    val partiallyMergedSchemas =\n+      sparkSession\n+        .sparkContext\n+        .parallelize(partialFileStatusInfo, numParallelism)\n+        .mapPartitions { iterator =>\n+          // Resembles fake `FileStatus`es with serialized path and length information.\n+          val fakeFileStatuses = iterator.map { case (path, length) =>\n+            new FileStatus(length, false, 0, 0, 0, 0, null, null, null, new Path(path))\n+          }.toSeq\n+\n+          // Reads schemas in multi-threaded manner within each task\n+          val schemas = parallelSchemaReader(fakeFileStatuses, serializedConf.value, ignoreCorruptFiles)"
  }, {
    "author": {
      "login": "WangGuangxin"
    },
    "body": "Done. Sorry for the mistake",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-10T03:49:31Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,\n+    files: Seq[FileStatus],\n+    parallelSchemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+    : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet requires `FileStatus`es to read footers.  Here we try to send cached `FileStatus`es\n+    // to executor side to avoid fetching them again.  However, `FileStatus` is not `Serializable`\n+    // but only `Writable`.  What makes it worse, for some reason, `FileStatus` doesn't play well\n+    // with `SerializableWritable[T]` and always causes a weird `IllegalStateException`.  These\n+    // facts virtually prevents us to serialize `FileStatus`es.\n+    //\n+    // Since Parquet only relies on path and length information of those `FileStatus`es to read\n+    // footers, here we just extract them (which can be easily serialized), send them to executor\n+    // side, and resemble fake `FileStatus`es there.\n+    val partialFileStatusInfo = files.map(f => (f.getPath.toString, f.getLen))\n+\n+    // Set the number of partitions to prevent following schema reads from generating many tasks\n+    // in case of a small number of orc files.\n+    val numParallelism = Math.min(Math.max(partialFileStatusInfo.size, 1),\n+      sparkSession.sparkContext.defaultParallelism)\n+\n+    val ignoreCorruptFiles = sparkSession.sessionState.conf.ignoreCorruptFiles\n+\n+    // Issues a Spark job to read Parquet/ORC schema in parallel.\n+    val partiallyMergedSchemas =\n+      sparkSession\n+        .sparkContext\n+        .parallelize(partialFileStatusInfo, numParallelism)\n+        .mapPartitions { iterator =>\n+          // Resembles fake `FileStatus`es with serialized path and length information.\n+          val fakeFileStatuses = iterator.map { case (path, length) =>\n+            new FileStatus(length, false, 0, 0, 0, 0, null, null, null, new Path(path))\n+          }.toSeq\n+\n+          // Reads schemas in multi-threaded manner within each task\n+          val schemas = parallelSchemaReader(fakeFileStatuses, serializedConf.value, ignoreCorruptFiles)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "No problem.",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-10T05:26:22Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,\n+    files: Seq[FileStatus],\n+    parallelSchemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+    : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet requires `FileStatus`es to read footers.  Here we try to send cached `FileStatus`es\n+    // to executor side to avoid fetching them again.  However, `FileStatus` is not `Serializable`\n+    // but only `Writable`.  What makes it worse, for some reason, `FileStatus` doesn't play well\n+    // with `SerializableWritable[T]` and always causes a weird `IllegalStateException`.  These\n+    // facts virtually prevents us to serialize `FileStatus`es.\n+    //\n+    // Since Parquet only relies on path and length information of those `FileStatus`es to read\n+    // footers, here we just extract them (which can be easily serialized), send them to executor\n+    // side, and resemble fake `FileStatus`es there.\n+    val partialFileStatusInfo = files.map(f => (f.getPath.toString, f.getLen))\n+\n+    // Set the number of partitions to prevent following schema reads from generating many tasks\n+    // in case of a small number of orc files.\n+    val numParallelism = Math.min(Math.max(partialFileStatusInfo.size, 1),\n+      sparkSession.sparkContext.defaultParallelism)\n+\n+    val ignoreCorruptFiles = sparkSession.sessionState.conf.ignoreCorruptFiles\n+\n+    // Issues a Spark job to read Parquet/ORC schema in parallel.\n+    val partiallyMergedSchemas =\n+      sparkSession\n+        .sparkContext\n+        .parallelize(partialFileStatusInfo, numParallelism)\n+        .mapPartitions { iterator =>\n+          // Resembles fake `FileStatus`es with serialized path and length information.\n+          val fakeFileStatuses = iterator.map { case (path, length) =>\n+            new FileStatus(length, false, 0, 0, 0, 0, null, null, null, new Path(path))\n+          }.toSeq\n+\n+          // Reads schemas in multi-threaded manner within each task\n+          val schemas = parallelSchemaReader(fakeFileStatuses, serializedConf.value, ignoreCorruptFiles)"
  }],
  "prId": 24043
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "two more spaces indent",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-11T14:01:34Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,"
  }, {
    "author": {
      "login": "WangGuangxin"
    },
    "body": "done",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-12T16:34:23Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,"
  }],
  "prId": 24043
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "The util is also for ORC. The comment should be updated.",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-11T14:02:26Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,\n+    files: Seq[FileStatus],\n+    parallelSchemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+    : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet requires `FileStatus`es to read footers.  Here we try to send cached `FileStatus`es"
  }, {
    "author": {
      "login": "WangGuangxin"
    },
    "body": "updated",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-12T16:34:15Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,\n+    files: Seq[FileStatus],\n+    parallelSchemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+    : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet requires `FileStatus`es to read footers.  Here we try to send cached `FileStatus`es"
  }],
  "prId": 24043
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "A minor comment: reading the schemas within a single task in parallel or not is an implementation detail of the `parallelSchemaReader` argument. It's not part of the contract of the `mergeSchemasInParallel` method. I'd propose removing this comment and renaming `parallelSchemaReader` into `schemaReader`.",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-11T20:58:28Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,\n+    files: Seq[FileStatus],\n+    parallelSchemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+    : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet requires `FileStatus`es to read footers.  Here we try to send cached `FileStatus`es\n+    // to executor side to avoid fetching them again.  However, `FileStatus` is not `Serializable`\n+    // but only `Writable`.  What makes it worse, for some reason, `FileStatus` doesn't play well\n+    // with `SerializableWritable[T]` and always causes a weird `IllegalStateException`.  These\n+    // facts virtually prevents us to serialize `FileStatus`es.\n+    //\n+    // Since Parquet only relies on path and length information of those `FileStatus`es to read\n+    // footers, here we just extract them (which can be easily serialized), send them to executor\n+    // side, and resemble fake `FileStatus`es there.\n+    val partialFileStatusInfo = files.map(f => (f.getPath.toString, f.getLen))\n+\n+    // Set the number of partitions to prevent following schema reads from generating many tasks\n+    // in case of a small number of orc files.\n+    val numParallelism = Math.min(Math.max(partialFileStatusInfo.size, 1),\n+      sparkSession.sparkContext.defaultParallelism)\n+\n+    val ignoreCorruptFiles = sparkSession.sessionState.conf.ignoreCorruptFiles\n+\n+    // Issues a Spark job to read Parquet/ORC schema in parallel.\n+    val partiallyMergedSchemas =\n+      sparkSession\n+        .sparkContext\n+        .parallelize(partialFileStatusInfo, numParallelism)\n+        .mapPartitions { iterator =>\n+          // Resembles fake `FileStatus`es with serialized path and length information.\n+          val fakeFileStatuses = iterator.map { case (path, length) =>\n+            new FileStatus(length, false, 0, 0, 0, 0, null, null, null, new Path(path))\n+          }.toSeq\n+\n+          // Reads schemas in multi-threaded manner within each task"
  }, {
    "author": {
      "login": "WangGuangxin"
    },
    "body": "Thanks for your comments. Good points. I'v updated there.",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-12T16:32:17Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+    sparkSession: SparkSession,\n+    files: Seq[FileStatus],\n+    parallelSchemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+    : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet requires `FileStatus`es to read footers.  Here we try to send cached `FileStatus`es\n+    // to executor side to avoid fetching them again.  However, `FileStatus` is not `Serializable`\n+    // but only `Writable`.  What makes it worse, for some reason, `FileStatus` doesn't play well\n+    // with `SerializableWritable[T]` and always causes a weird `IllegalStateException`.  These\n+    // facts virtually prevents us to serialize `FileStatus`es.\n+    //\n+    // Since Parquet only relies on path and length information of those `FileStatus`es to read\n+    // footers, here we just extract them (which can be easily serialized), send them to executor\n+    // side, and resemble fake `FileStatus`es there.\n+    val partialFileStatusInfo = files.map(f => (f.getPath.toString, f.getLen))\n+\n+    // Set the number of partitions to prevent following schema reads from generating many tasks\n+    // in case of a small number of orc files.\n+    val numParallelism = Math.min(Math.max(partialFileStatusInfo.size, 1),\n+      sparkSession.sparkContext.defaultParallelism)\n+\n+    val ignoreCorruptFiles = sparkSession.sessionState.conf.ignoreCorruptFiles\n+\n+    // Issues a Spark job to read Parquet/ORC schema in parallel.\n+    val partiallyMergedSchemas =\n+      sparkSession\n+        .sparkContext\n+        .parallelize(partialFileStatusInfo, numParallelism)\n+        .mapPartitions { iterator =>\n+          // Resembles fake `FileStatus`es with serialized path and length information.\n+          val fakeFileStatuses = iterator.map { case (path, length) =>\n+            new FileStatus(length, false, 0, 0, 0, 0, null, null, null, new Path(path))\n+          }.toSeq\n+\n+          // Reads schemas in multi-threaded manner within each task"
  }],
  "prId": 24043
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "Sorry, I check the code again and I think only Parquet requires `FileStatus`, while Orc requires only `Path`. So the comments here are not accurate. \r\nWe should explain that the hack is for Parquet, but it can be used by ORC as well.",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-17T06:43:58Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+      sparkSession: SparkSession,\n+      files: Seq[FileStatus],\n+      schemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+      : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet/ORC requires `FileStatus`es to read footers."
  }, {
    "author": {
      "login": "WangGuangxin"
    },
    "body": "I've added one line to address this, please check if it is suitable. ",
    "commit": "a6fc2d0d3b542c402e426ff125ff42822ddb4b7c",
    "createdAt": "2019-06-17T08:30:16Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+object SchemaMergeUtils extends Logging {\n+  /**\n+   * Figures out a merged Parquet/ORC schema with a distributed Spark job.\n+   */\n+  def mergeSchemasInParallel(\n+      sparkSession: SparkSession,\n+      files: Seq[FileStatus],\n+      schemaReader: (Seq[FileStatus], Configuration, Boolean) => Seq[StructType])\n+      : Option[StructType] = {\n+    val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())\n+\n+    // !! HACK ALERT !!\n+    //\n+    // Parquet/ORC requires `FileStatus`es to read footers."
  }],
  "prId": 24043
}]