[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This should return `CreateTableWriter`. It doesn't make sense to specify table properties when inserting to an existing table.",
    "commit": "ab7c3e89888d5233cefe64a8a91789e8e7a5504f",
    "createdAt": "2019-09-23T14:10:07Z",
    "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException, TableAlreadyExistsException}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Bucket, Days, Hours, Literal, Months, Years}\n+import org.apache.spark.sql.catalyst.plans.logical.{AppendData, CreateTableAsSelect, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic, ReplaceTableAsSelect}\n+import org.apache.spark.sql.connector.expressions.{LogicalExpressions, Transform}\n+import org.apache.spark.sql.execution.SQLExecution\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.types.IntegerType\n+\n+/**\n+ * Interface used to write a [[org.apache.spark.sql.Dataset]] to external storage using the v2 API.\n+ *\n+ * @since 3.0.0\n+ */\n+@Experimental\n+final class DataFrameWriterV2[T] private[sql](table: String, ds: Dataset[T])\n+    extends CreateTableWriter[T] {\n+\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+  import df.sparkSession.sessionState.analyzer.CatalogObjectIdentifier\n+\n+  private val df: DataFrame = ds.toDF()\n+\n+  private val sparkSession = ds.sparkSession\n+\n+  private val catalogManager = sparkSession.sessionState.analyzer.catalogManager\n+\n+  private val tableName = sparkSession.sessionState.sqlParser.parseMultipartIdentifier(table)\n+\n+  private val (catalog, identifier) = {\n+    val CatalogObjectIdentifier(maybeCatalog, identifier) = tableName\n+    val catalog = maybeCatalog.orElse(catalogManager.currentCatalog.map(catalogManager.catalog))\n+        .getOrElse(throw new AnalysisException(\n+          s\"No catalog specified for table ${identifier.quoted} and no default v2 catalog is set\"))\n+        .asTableCatalog\n+\n+    (catalog, identifier)\n+  }\n+\n+  private val logicalPlan = df.queryExecution.logical\n+\n+  private var provider: Option[String] = None\n+\n+  private val options = new mutable.HashMap[String, String]()\n+\n+  private val properties = new mutable.HashMap[String, String]()\n+\n+  private var partitioning: Option[Seq[Transform]] = None\n+\n+  override def using(provider: String): CreateTableWriter[T] = {\n+    this.provider = Some(provider)\n+    this\n+  }\n+\n+  override def option(key: String, value: String): DataFrameWriterV2[T] = {\n+    this.options.put(key, value)\n+    this\n+  }\n+\n+  override def options(options: scala.collection.Map[String, String]): DataFrameWriterV2[T] = {\n+    options.foreach {\n+      case (key, value) =>\n+        this.options.put(key, value)\n+    }\n+    this\n+  }\n+\n+  override def options(options: java.util.Map[String, String]): DataFrameWriterV2[T] = {\n+    this.options(options.asScala)\n+    this\n+  }\n+\n+  override def tableProperty(property: String, value: String): DataFrameWriterV2[T] = {",
    "line": 96
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I agree.",
    "commit": "ab7c3e89888d5233cefe64a8a91789e8e7a5504f",
    "createdAt": "2019-09-23T17:18:06Z",
    "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException, TableAlreadyExistsException}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Bucket, Days, Hours, Literal, Months, Years}\n+import org.apache.spark.sql.catalyst.plans.logical.{AppendData, CreateTableAsSelect, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic, ReplaceTableAsSelect}\n+import org.apache.spark.sql.connector.expressions.{LogicalExpressions, Transform}\n+import org.apache.spark.sql.execution.SQLExecution\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.types.IntegerType\n+\n+/**\n+ * Interface used to write a [[org.apache.spark.sql.Dataset]] to external storage using the v2 API.\n+ *\n+ * @since 3.0.0\n+ */\n+@Experimental\n+final class DataFrameWriterV2[T] private[sql](table: String, ds: Dataset[T])\n+    extends CreateTableWriter[T] {\n+\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+  import df.sparkSession.sessionState.analyzer.CatalogObjectIdentifier\n+\n+  private val df: DataFrame = ds.toDF()\n+\n+  private val sparkSession = ds.sparkSession\n+\n+  private val catalogManager = sparkSession.sessionState.analyzer.catalogManager\n+\n+  private val tableName = sparkSession.sessionState.sqlParser.parseMultipartIdentifier(table)\n+\n+  private val (catalog, identifier) = {\n+    val CatalogObjectIdentifier(maybeCatalog, identifier) = tableName\n+    val catalog = maybeCatalog.orElse(catalogManager.currentCatalog.map(catalogManager.catalog))\n+        .getOrElse(throw new AnalysisException(\n+          s\"No catalog specified for table ${identifier.quoted} and no default v2 catalog is set\"))\n+        .asTableCatalog\n+\n+    (catalog, identifier)\n+  }\n+\n+  private val logicalPlan = df.queryExecution.logical\n+\n+  private var provider: Option[String] = None\n+\n+  private val options = new mutable.HashMap[String, String]()\n+\n+  private val properties = new mutable.HashMap[String, String]()\n+\n+  private var partitioning: Option[Seq[Transform]] = None\n+\n+  override def using(provider: String): CreateTableWriter[T] = {\n+    this.provider = Some(provider)\n+    this\n+  }\n+\n+  override def option(key: String, value: String): DataFrameWriterV2[T] = {\n+    this.options.put(key, value)\n+    this\n+  }\n+\n+  override def options(options: scala.collection.Map[String, String]): DataFrameWriterV2[T] = {\n+    options.foreach {\n+      case (key, value) =>\n+        this.options.put(key, value)\n+    }\n+    this\n+  }\n+\n+  override def options(options: java.util.Map[String, String]): DataFrameWriterV2[T] = {\n+    this.options(options.asScala)\n+    this\n+  }\n+\n+  override def tableProperty(property: String, value: String): DataFrameWriterV2[T] = {",
    "line": 96
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Opened SPARK-29249 for this. Should have a PR posted soon.",
    "commit": "ab7c3e89888d5233cefe64a8a91789e8e7a5504f",
    "createdAt": "2019-09-25T22:42:19Z",
    "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException, TableAlreadyExistsException}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Bucket, Days, Hours, Literal, Months, Years}\n+import org.apache.spark.sql.catalyst.plans.logical.{AppendData, CreateTableAsSelect, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic, ReplaceTableAsSelect}\n+import org.apache.spark.sql.connector.expressions.{LogicalExpressions, Transform}\n+import org.apache.spark.sql.execution.SQLExecution\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.types.IntegerType\n+\n+/**\n+ * Interface used to write a [[org.apache.spark.sql.Dataset]] to external storage using the v2 API.\n+ *\n+ * @since 3.0.0\n+ */\n+@Experimental\n+final class DataFrameWriterV2[T] private[sql](table: String, ds: Dataset[T])\n+    extends CreateTableWriter[T] {\n+\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Util._\n+  import df.sparkSession.sessionState.analyzer.CatalogObjectIdentifier\n+\n+  private val df: DataFrame = ds.toDF()\n+\n+  private val sparkSession = ds.sparkSession\n+\n+  private val catalogManager = sparkSession.sessionState.analyzer.catalogManager\n+\n+  private val tableName = sparkSession.sessionState.sqlParser.parseMultipartIdentifier(table)\n+\n+  private val (catalog, identifier) = {\n+    val CatalogObjectIdentifier(maybeCatalog, identifier) = tableName\n+    val catalog = maybeCatalog.orElse(catalogManager.currentCatalog.map(catalogManager.catalog))\n+        .getOrElse(throw new AnalysisException(\n+          s\"No catalog specified for table ${identifier.quoted} and no default v2 catalog is set\"))\n+        .asTableCatalog\n+\n+    (catalog, identifier)\n+  }\n+\n+  private val logicalPlan = df.queryExecution.logical\n+\n+  private var provider: Option[String] = None\n+\n+  private val options = new mutable.HashMap[String, String]()\n+\n+  private val properties = new mutable.HashMap[String, String]()\n+\n+  private var partitioning: Option[Seq[Transform]] = None\n+\n+  override def using(provider: String): CreateTableWriter[T] = {\n+    this.provider = Some(provider)\n+    this\n+  }\n+\n+  override def option(key: String, value: String): DataFrameWriterV2[T] = {\n+    this.options.put(key, value)\n+    this\n+  }\n+\n+  override def options(options: scala.collection.Map[String, String]): DataFrameWriterV2[T] = {\n+    options.foreach {\n+      case (key, value) =>\n+        this.options.put(key, value)\n+    }\n+    this\n+  }\n+\n+  override def options(options: java.util.Map[String, String]): DataFrameWriterV2[T] = {\n+    this.options(options.asScala)\n+    this\n+  }\n+\n+  override def tableProperty(property: String, value: String): DataFrameWriterV2[T] = {",
    "line": 96
  }],
  "prId": 25681
}]