[{
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Thus it's very rare case, but we cannot assume that user will not call the `hasNext` multiple times before call the `next()`.\n",
    "commit": "8204eaed1b9399f17415afc6ce178c845f29746f",
    "createdAt": "2015-07-17T04:49:22Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.sql.catalyst.util.TypeUtils\n+import org.apache.spark.util.ThreadUtils\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * Performs an inner range join on two tables. A range join typically has the following form:\n+ *\n+ * SELECT A.*\n+ *        ,B.*\n+ * FROM   tableA A\n+ *        JOIN tableB B\n+ *         ON A.start <= B.end\n+ *          AND A.end > B.start\n+ *\n+ * The implementation builds a range index from the smaller build side, broadcasts this index\n+ * to all executors. The streaming side is then matched against the index. This reduces the number\n+ * of comparisons made by log(n) (n is the number of records in the build table) over the\n+ * typical solution (Nested Loop Join).\n+ *\n+ * TODO NaN values\n+ * TODO NULL values\n+ * TODO Outer joins? StreamSide is quite easy/BuildSide requires bookkeeping and\n+ * TODO This join will maintain sort order. The build side rows will also be added in a lower\n+ *      bound sorted fashion.\n+ */\n+@DeveloperApi\n+case class BroadcastRangeJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    equality: Seq[Boolean],\n+    buildSide: BuildSide,\n+    left: SparkPlan,\n+    right: SparkPlan)\n+  extends BinaryNode {\n+\n+  private[this] lazy val (buildPlan, streamedPlan) = buildSide match {\n+    case BuildLeft => (left, right)\n+    case BuildRight => (right, left)\n+  }\n+\n+  private[this] lazy val (buildKeys, streamedKeys) = buildSide match {\n+    case BuildLeft => (leftKeys, rightKeys)\n+    case BuildRight => (rightKeys, leftKeys)\n+  }\n+\n+  override def output: Seq[Attribute] = left.output ++ right.output\n+\n+  @transient\n+  private[this] lazy val buildSideKeyGenerator: Projection =\n+    newProjection(buildKeys, buildPlan.output)\n+\n+  @transient\n+  private[this] lazy val streamSideKeyGenerator: () => MutableProjection =\n+    newMutableProjection(streamedKeys, streamedPlan.output)\n+\n+  private[this] val timeout: Duration = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  // Construct the range index.\n+  @transient\n+  private[this] val indexBroadcastFuture = future {\n+    // Deal with equality.\n+    val Seq(allowLowEqual: Boolean, allowHighEqual: Boolean) = buildSide match {\n+      case BuildLeft => equality.reverse\n+      case BuildRight => equality\n+    }\n+\n+    // Get the ordering for the datatype.\n+    val ordering = TypeUtils.getOrdering(buildKeys.head.dataType)\n+\n+    // Note that we use .execute().collect() because we don't want to convert data to Scala types\n+    // TODO find out if the result of a sort and a collect is still sorted.\n+    val eventifier = RangeIndex.toRangeEvent(buildSideKeyGenerator, ordering)\n+    val events = buildPlan.execute().map(_.copy()).collect().flatMap(eventifier)\n+\n+    // Create the index.\n+    val index = RangeIndex.build(ordering, events, allowLowEqual, allowHighEqual)\n+\n+    // Broadcast the index.\n+    sparkContext.broadcast(index)\n+  }(BroadcastRangeJoin.broadcastRangeJoinExecutionContext)\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    // Construct the range index.\n+    val indexBC = Await.result(indexBroadcastFuture, timeout)\n+\n+    // Iterate over the streaming relation.\n+    streamedPlan.execute().mapPartitions { stream =>\n+      new Iterator[InternalRow] {\n+        private[this] val index = indexBC.value\n+        private[this] val streamSideKeys = streamSideKeyGenerator()\n+        private[this] val join = new JoinedRow2 // TODO create our own join row...\n+        private[this] var row: InternalRow = EmptyRow\n+        private[this] var iterator: Iterator[InternalRow] = Iterator.empty\n+\n+        override final def hasNext: Boolean = {\n+          var result = iterator.hasNext",
    "line": 133
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Multiple calls to hasNext shouldn't be a problemen. Granted the first call can have a side effect (updating the state of the iterator), but the subsequent ones won't.\n\nA problem will occur when the next is called without calling hasNext first. I was inspired by the HashedRelation class in the same package when writing this.\n",
    "commit": "8204eaed1b9399f17415afc6ce178c845f29746f",
    "createdAt": "2015-07-17T05:05:08Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.sql.catalyst.util.TypeUtils\n+import org.apache.spark.util.ThreadUtils\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * Performs an inner range join on two tables. A range join typically has the following form:\n+ *\n+ * SELECT A.*\n+ *        ,B.*\n+ * FROM   tableA A\n+ *        JOIN tableB B\n+ *         ON A.start <= B.end\n+ *          AND A.end > B.start\n+ *\n+ * The implementation builds a range index from the smaller build side, broadcasts this index\n+ * to all executors. The streaming side is then matched against the index. This reduces the number\n+ * of comparisons made by log(n) (n is the number of records in the build table) over the\n+ * typical solution (Nested Loop Join).\n+ *\n+ * TODO NaN values\n+ * TODO NULL values\n+ * TODO Outer joins? StreamSide is quite easy/BuildSide requires bookkeeping and\n+ * TODO This join will maintain sort order. The build side rows will also be added in a lower\n+ *      bound sorted fashion.\n+ */\n+@DeveloperApi\n+case class BroadcastRangeJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    equality: Seq[Boolean],\n+    buildSide: BuildSide,\n+    left: SparkPlan,\n+    right: SparkPlan)\n+  extends BinaryNode {\n+\n+  private[this] lazy val (buildPlan, streamedPlan) = buildSide match {\n+    case BuildLeft => (left, right)\n+    case BuildRight => (right, left)\n+  }\n+\n+  private[this] lazy val (buildKeys, streamedKeys) = buildSide match {\n+    case BuildLeft => (leftKeys, rightKeys)\n+    case BuildRight => (rightKeys, leftKeys)\n+  }\n+\n+  override def output: Seq[Attribute] = left.output ++ right.output\n+\n+  @transient\n+  private[this] lazy val buildSideKeyGenerator: Projection =\n+    newProjection(buildKeys, buildPlan.output)\n+\n+  @transient\n+  private[this] lazy val streamSideKeyGenerator: () => MutableProjection =\n+    newMutableProjection(streamedKeys, streamedPlan.output)\n+\n+  private[this] val timeout: Duration = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  // Construct the range index.\n+  @transient\n+  private[this] val indexBroadcastFuture = future {\n+    // Deal with equality.\n+    val Seq(allowLowEqual: Boolean, allowHighEqual: Boolean) = buildSide match {\n+      case BuildLeft => equality.reverse\n+      case BuildRight => equality\n+    }\n+\n+    // Get the ordering for the datatype.\n+    val ordering = TypeUtils.getOrdering(buildKeys.head.dataType)\n+\n+    // Note that we use .execute().collect() because we don't want to convert data to Scala types\n+    // TODO find out if the result of a sort and a collect is still sorted.\n+    val eventifier = RangeIndex.toRangeEvent(buildSideKeyGenerator, ordering)\n+    val events = buildPlan.execute().map(_.copy()).collect().flatMap(eventifier)\n+\n+    // Create the index.\n+    val index = RangeIndex.build(ordering, events, allowLowEqual, allowHighEqual)\n+\n+    // Broadcast the index.\n+    sparkContext.broadcast(index)\n+  }(BroadcastRangeJoin.broadcastRangeJoinExecutionContext)\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    // Construct the range index.\n+    val indexBC = Await.result(indexBroadcastFuture, timeout)\n+\n+    // Iterate over the streaming relation.\n+    streamedPlan.execute().mapPartitions { stream =>\n+      new Iterator[InternalRow] {\n+        private[this] val index = indexBC.value\n+        private[this] val streamSideKeys = streamSideKeyGenerator()\n+        private[this] val join = new JoinedRow2 // TODO create our own join row...\n+        private[this] var row: InternalRow = EmptyRow\n+        private[this] var iterator: Iterator[InternalRow] = Iterator.empty\n+\n+        override final def hasNext: Boolean = {\n+          var result = iterator.hasNext",
    "line": 133
  }],
  "prId": 7379
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Sorry, actually I mean here, we probably skip some rows if we call `hasNext` multiple times.\n",
    "commit": "8204eaed1b9399f17415afc6ce178c845f29746f",
    "createdAt": "2015-07-17T05:45:39Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.sql.catalyst.util.TypeUtils\n+import org.apache.spark.util.ThreadUtils\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * Performs an inner range join on two tables. A range join typically has the following form:\n+ *\n+ * SELECT A.*\n+ *        ,B.*\n+ * FROM   tableA A\n+ *        JOIN tableB B\n+ *         ON A.start <= B.end\n+ *          AND A.end > B.start\n+ *\n+ * The implementation builds a range index from the smaller build side, broadcasts this index\n+ * to all executors. The streaming side is then matched against the index. This reduces the number\n+ * of comparisons made by log(n) (n is the number of records in the build table) over the\n+ * typical solution (Nested Loop Join).\n+ *\n+ * TODO NaN values\n+ * TODO NULL values\n+ * TODO Outer joins? StreamSide is quite easy/BuildSide requires bookkeeping and\n+ * TODO This join will maintain sort order. The build side rows will also be added in a lower\n+ *      bound sorted fashion.\n+ */\n+@DeveloperApi\n+case class BroadcastRangeJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    equality: Seq[Boolean],\n+    buildSide: BuildSide,\n+    left: SparkPlan,\n+    right: SparkPlan)\n+  extends BinaryNode {\n+\n+  private[this] lazy val (buildPlan, streamedPlan) = buildSide match {\n+    case BuildLeft => (left, right)\n+    case BuildRight => (right, left)\n+  }\n+\n+  private[this] lazy val (buildKeys, streamedKeys) = buildSide match {\n+    case BuildLeft => (leftKeys, rightKeys)\n+    case BuildRight => (rightKeys, leftKeys)\n+  }\n+\n+  override def output: Seq[Attribute] = left.output ++ right.output\n+\n+  @transient\n+  private[this] lazy val buildSideKeyGenerator: Projection =\n+    newProjection(buildKeys, buildPlan.output)\n+\n+  @transient\n+  private[this] lazy val streamSideKeyGenerator: () => MutableProjection =\n+    newMutableProjection(streamedKeys, streamedPlan.output)\n+\n+  private[this] val timeout: Duration = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  // Construct the range index.\n+  @transient\n+  private[this] val indexBroadcastFuture = future {\n+    // Deal with equality.\n+    val Seq(allowLowEqual: Boolean, allowHighEqual: Boolean) = buildSide match {\n+      case BuildLeft => equality.reverse\n+      case BuildRight => equality\n+    }\n+\n+    // Get the ordering for the datatype.\n+    val ordering = TypeUtils.getOrdering(buildKeys.head.dataType)\n+\n+    // Note that we use .execute().collect() because we don't want to convert data to Scala types\n+    // TODO find out if the result of a sort and a collect is still sorted.\n+    val eventifier = RangeIndex.toRangeEvent(buildSideKeyGenerator, ordering)\n+    val events = buildPlan.execute().map(_.copy()).collect().flatMap(eventifier)\n+\n+    // Create the index.\n+    val index = RangeIndex.build(ordering, events, allowLowEqual, allowHighEqual)\n+\n+    // Broadcast the index.\n+    sparkContext.broadcast(index)\n+  }(BroadcastRangeJoin.broadcastRangeJoinExecutionContext)\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    // Construct the range index.\n+    val indexBC = Await.result(indexBroadcastFuture, timeout)\n+\n+    // Iterate over the streaming relation.\n+    streamedPlan.execute().mapPartitions { stream =>\n+      new Iterator[InternalRow] {\n+        private[this] val index = indexBC.value\n+        private[this] val streamSideKeys = streamSideKeyGenerator()\n+        private[this] val join = new JoinedRow2 // TODO create our own join row...\n+        private[this] var row: InternalRow = EmptyRow\n+        private[this] var iterator: Iterator[InternalRow] = Iterator.empty\n+\n+        override final def hasNext: Boolean = {\n+          var result = iterator.hasNext\n+          while (!result && stream.hasNext) {\n+            row = stream.next()",
    "line": 135
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Ah I see. The current (WIP) implementation only allows for inner joins, and we drop rows if they don't have a match in the index. Outer Joins are possible, however BuildSide Outerjoins will require a bit of bookkeeping.\n",
    "commit": "8204eaed1b9399f17415afc6ce178c845f29746f",
    "createdAt": "2015-07-17T13:58:42Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.sql.catalyst.util.TypeUtils\n+import org.apache.spark.util.ThreadUtils\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * Performs an inner range join on two tables. A range join typically has the following form:\n+ *\n+ * SELECT A.*\n+ *        ,B.*\n+ * FROM   tableA A\n+ *        JOIN tableB B\n+ *         ON A.start <= B.end\n+ *          AND A.end > B.start\n+ *\n+ * The implementation builds a range index from the smaller build side, broadcasts this index\n+ * to all executors. The streaming side is then matched against the index. This reduces the number\n+ * of comparisons made by log(n) (n is the number of records in the build table) over the\n+ * typical solution (Nested Loop Join).\n+ *\n+ * TODO NaN values\n+ * TODO NULL values\n+ * TODO Outer joins? StreamSide is quite easy/BuildSide requires bookkeeping and\n+ * TODO This join will maintain sort order. The build side rows will also be added in a lower\n+ *      bound sorted fashion.\n+ */\n+@DeveloperApi\n+case class BroadcastRangeJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    equality: Seq[Boolean],\n+    buildSide: BuildSide,\n+    left: SparkPlan,\n+    right: SparkPlan)\n+  extends BinaryNode {\n+\n+  private[this] lazy val (buildPlan, streamedPlan) = buildSide match {\n+    case BuildLeft => (left, right)\n+    case BuildRight => (right, left)\n+  }\n+\n+  private[this] lazy val (buildKeys, streamedKeys) = buildSide match {\n+    case BuildLeft => (leftKeys, rightKeys)\n+    case BuildRight => (rightKeys, leftKeys)\n+  }\n+\n+  override def output: Seq[Attribute] = left.output ++ right.output\n+\n+  @transient\n+  private[this] lazy val buildSideKeyGenerator: Projection =\n+    newProjection(buildKeys, buildPlan.output)\n+\n+  @transient\n+  private[this] lazy val streamSideKeyGenerator: () => MutableProjection =\n+    newMutableProjection(streamedKeys, streamedPlan.output)\n+\n+  private[this] val timeout: Duration = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  // Construct the range index.\n+  @transient\n+  private[this] val indexBroadcastFuture = future {\n+    // Deal with equality.\n+    val Seq(allowLowEqual: Boolean, allowHighEqual: Boolean) = buildSide match {\n+      case BuildLeft => equality.reverse\n+      case BuildRight => equality\n+    }\n+\n+    // Get the ordering for the datatype.\n+    val ordering = TypeUtils.getOrdering(buildKeys.head.dataType)\n+\n+    // Note that we use .execute().collect() because we don't want to convert data to Scala types\n+    // TODO find out if the result of a sort and a collect is still sorted.\n+    val eventifier = RangeIndex.toRangeEvent(buildSideKeyGenerator, ordering)\n+    val events = buildPlan.execute().map(_.copy()).collect().flatMap(eventifier)\n+\n+    // Create the index.\n+    val index = RangeIndex.build(ordering, events, allowLowEqual, allowHighEqual)\n+\n+    // Broadcast the index.\n+    sparkContext.broadcast(index)\n+  }(BroadcastRangeJoin.broadcastRangeJoinExecutionContext)\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    // Construct the range index.\n+    val indexBC = Await.result(indexBroadcastFuture, timeout)\n+\n+    // Iterate over the streaming relation.\n+    streamedPlan.execute().mapPartitions { stream =>\n+      new Iterator[InternalRow] {\n+        private[this] val index = indexBC.value\n+        private[this] val streamSideKeys = streamSideKeyGenerator()\n+        private[this] val join = new JoinedRow2 // TODO create our own join row...\n+        private[this] var row: InternalRow = EmptyRow\n+        private[this] var iterator: Iterator[InternalRow] = Iterator.empty\n+\n+        override final def hasNext: Boolean = {\n+          var result = iterator.hasNext\n+          while (!result && stream.hasNext) {\n+            row = stream.next()",
    "line": 135
  }],
  "prId": 7379
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "it should be\n",
    "commit": "8204eaed1b9399f17415afc6ce178c845f29746f",
    "createdAt": "2015-07-17T23:59:57Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.sql.catalyst.util.TypeUtils\n+import org.apache.spark.util.ThreadUtils\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * Performs an inner range join on two tables. A range join typically has the following form:\n+ *\n+ * SELECT A.*\n+ *        ,B.*\n+ * FROM   tableA A\n+ *        JOIN tableB B\n+ *         ON A.start <= B.end\n+ *          AND A.end > B.start\n+ *\n+ * The implementation builds a range index from the smaller build side, broadcasts this index\n+ * to all executors. The streaming side is then matched against the index. This reduces the number\n+ * of comparisons made by log(n) (n is the number of records in the build table) over the\n+ * typical solution (Nested Loop Join).\n+ *\n+ * TODO NaN values\n+ * TODO NULL values\n+ * TODO Outer joins? StreamSide is quite easy/BuildSide requires bookkeeping and\n+ * TODO This join will maintain sort order. The build side rows will also be added in a lower\n+ *      bound sorted fashion.\n+ */\n+@DeveloperApi\n+case class BroadcastRangeJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    equality: Seq[Boolean],\n+    buildSide: BuildSide,\n+    left: SparkPlan,\n+    right: SparkPlan)\n+  extends BinaryNode {\n+\n+  private[this] lazy val (buildPlan, streamedPlan) = buildSide match {\n+    case BuildLeft => (left, right)\n+    case BuildRight => (right, left)\n+  }\n+\n+  private[this] lazy val (buildKeys, streamedKeys) = buildSide match {\n+    case BuildLeft => (leftKeys, rightKeys)\n+    case BuildRight => (rightKeys, leftKeys)\n+  }\n+\n+  override def output: Seq[Attribute] = left.output ++ right.output\n+\n+  @transient\n+  private[this] lazy val buildSideKeyGenerator: Projection =\n+    newProjection(buildKeys, buildPlan.output)\n+\n+  @transient\n+  private[this] lazy val streamSideKeyGenerator: () => MutableProjection =\n+    newMutableProjection(streamedKeys, streamedPlan.output)\n+\n+  private[this] val timeout: Duration = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  // Construct the range index.\n+  @transient\n+  private[this] val indexBroadcastFuture = future {\n+    // Deal with equality.\n+    val Seq(allowLowEqual: Boolean, allowHighEqual: Boolean) = buildSide match {\n+      case BuildLeft => equality.reverse\n+      case BuildRight => equality\n+    }\n+\n+    // Get the ordering for the datatype.\n+    val ordering = TypeUtils.getOrdering(buildKeys.head.dataType)\n+\n+    // Note that we use .execute().collect() because we don't want to convert data to Scala types\n+    // TODO find out if the result of a sort and a collect is still sorted.",
    "line": 108
  }],
  "prId": 7379
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "maybe we should just put this logic in `SQLConf` so we can use it in both places.\n",
    "commit": "8204eaed1b9399f17415afc6ce178c845f29746f",
    "createdAt": "2015-07-18T00:06:02Z",
    "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.sql.catalyst.util.TypeUtils\n+import org.apache.spark.util.ThreadUtils\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * Performs an inner range join on two tables. A range join typically has the following form:\n+ *\n+ * SELECT A.*\n+ *        ,B.*\n+ * FROM   tableA A\n+ *        JOIN tableB B\n+ *         ON A.start <= B.end\n+ *          AND A.end > B.start\n+ *\n+ * The implementation builds a range index from the smaller build side, broadcasts this index\n+ * to all executors. The streaming side is then matched against the index. This reduces the number\n+ * of comparisons made by log(n) (n is the number of records in the build table) over the\n+ * typical solution (Nested Loop Join).\n+ *\n+ * TODO NaN values\n+ * TODO NULL values\n+ * TODO Outer joins? StreamSide is quite easy/BuildSide requires bookkeeping and\n+ * TODO This join will maintain sort order. The build side rows will also be added in a lower\n+ *      bound sorted fashion.\n+ */\n+@DeveloperApi\n+case class BroadcastRangeJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    equality: Seq[Boolean],\n+    buildSide: BuildSide,\n+    left: SparkPlan,\n+    right: SparkPlan)\n+  extends BinaryNode {\n+\n+  private[this] lazy val (buildPlan, streamedPlan) = buildSide match {\n+    case BuildLeft => (left, right)\n+    case BuildRight => (right, left)\n+  }\n+\n+  private[this] lazy val (buildKeys, streamedKeys) = buildSide match {\n+    case BuildLeft => (leftKeys, rightKeys)\n+    case BuildRight => (rightKeys, leftKeys)\n+  }\n+\n+  override def output: Seq[Attribute] = left.output ++ right.output\n+\n+  @transient\n+  private[this] lazy val buildSideKeyGenerator: Projection =\n+    newProjection(buildKeys, buildPlan.output)\n+\n+  @transient\n+  private[this] lazy val streamSideKeyGenerator: () => MutableProjection =\n+    newMutableProjection(streamedKeys, streamedPlan.output)\n+\n+  private[this] val timeout: Duration = {",
    "line": 86
  }],
  "prId": 7379
}]