[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Remove blank line\n",
    "commit": "a4a43c99b49156ef90fa3b9493b008823dbc01d3",
    "createdAt": "2014-12-01T23:45:03Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Row}\n+import org.apache.spark.sql.catalyst.plans.physical.ClusteredDistribution\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Build the right table's join keys into a HashSet, and iteratively go through the left\n+ * table, to find the if join keys are in the Hash set.\n+ */\n+@DeveloperApi\n+case class BroadcastLeftSemiJoinHash(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with HashJoin {\n+\n+  override val buildSide = BuildRight\n+\n+  override def output = left.output\n+\n+  override def execute() = {\n+\n+    val buildIter= buildPlan.execute().map(_.copy()).collect().toIterator"
  }],
  "prId": 3442
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Remove blank line.\n",
    "commit": "a4a43c99b49156ef90fa3b9493b008823dbc01d3",
    "createdAt": "2014-12-01T23:45:12Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Row}\n+import org.apache.spark.sql.catalyst.plans.physical.ClusteredDistribution\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Build the right table's join keys into a HashSet, and iteratively go through the left\n+ * table, to find the if join keys are in the Hash set.\n+ */\n+@DeveloperApi\n+case class BroadcastLeftSemiJoinHash(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with HashJoin {\n+\n+  override val buildSide = BuildRight\n+\n+  override def output = left.output\n+\n+  override def execute() = {\n+\n+    val buildIter= buildPlan.execute().map(_.copy()).collect().toIterator\n+    val hashSet = new java.util.HashSet[Row]()\n+    var currentRow: Row = null\n+\n+    // Create a Hash set of buildKeys\n+    while (buildIter.hasNext) {\n+      currentRow = buildIter.next()\n+      val rowKey = buildSideKeyGenerator(currentRow)\n+      if (!rowKey.anyNull) {\n+        val keyExists = hashSet.contains(rowKey)\n+        if (!keyExists) {\n+          hashSet.add(rowKey)\n+        }\n+      }\n+    }\n+\n+    val broadcastedRelation = sparkContext.broadcast(hashSet)\n+\n+    streamedPlan.execute().mapPartitions { streamIter =>\n+\n+      val joinKeys = streamSideKeyGenerator()"
  }],
  "prId": 3442
}]