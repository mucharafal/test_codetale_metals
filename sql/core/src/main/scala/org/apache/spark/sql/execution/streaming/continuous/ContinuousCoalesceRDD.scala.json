[{
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "why the `reader` and `prev` both is var here?",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-14T13:10:30Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(var reader: ContinuousShuffleReadRDD, var prev: RDD[InternalRow])"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "They are to make it this RDD checkpointable by make this clearable. This raises a good point, I dont think we should make this checkpointable. Rather I suggest this, make these simple vals (well, just remove modifier), and in clearDependencies, just throw an error saying \"Checkpoint this RDD is not supported\".\r\n\r\n\r\nWe should do this for all the continuous shuffle RDDs.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-19T21:46:55Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(var reader: ContinuousShuffleReadRDD, var prev: RDD[InternalRow])"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "maybe use a thread pool (using `org...spark.util.ThreadUtils`) with a name to track threads. Then the cached threads in threadpool can be reused across epochs.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-19T21:34:08Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(var reader: ContinuousShuffleReadRDD, var prev: RDD[InternalRow])\n+  extends RDD[InternalRow](reader.context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    reader.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = reader.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val threads = prev.partitions.map { prevSplit =>\n+        new Thread() {"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "clean up using threadpool",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-19T21:36:28Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(var reader: ContinuousShuffleReadRDD, var prev: RDD[InternalRow])\n+  extends RDD[InternalRow](reader.context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    reader.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = reader.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val threads = prev.partitions.map { prevSplit =>\n+        new Thread() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threads.foreach(_.interrupt())"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Add docs on what this means. ",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-19T21:48:09Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "As commented above, this should actually throw exception so that this is never checkpointed.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-19T23:08:31Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(var reader: ContinuousShuffleReadRDD, var prev: RDD[InternalRow])\n+  extends RDD[InternalRow](reader.context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    reader.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = reader.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val threads = prev.partitions.map { prevSplit =>\n+        new Thread() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threads.foreach(_.interrupt())\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+      threads.foreach(_.start())\n+    }\n+\n+    reader.compute(reader.partitions(split.index), context)\n+  }\n+\n+  override def getDependencies: Seq[Dependency[_]] = {\n+    Seq(new NarrowDependency(prev) {\n+      def getParents(id: Int): Seq[Int] = Seq(0)\n+    })\n+  }\n+\n+  override def clearDependencies() {\n+    super.clearDependencies()"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Should 1 partition of this class depend on all parant RDD partitions, and not just the 0. \r\n",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-19T23:28:47Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(var reader: ContinuousShuffleReadRDD, var prev: RDD[InternalRow])\n+  extends RDD[InternalRow](reader.context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    reader.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = reader.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val threads = prev.partitions.map { prevSplit =>\n+        new Thread() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threads.foreach(_.interrupt())\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+      threads.foreach(_.start())\n+    }\n+\n+    reader.compute(reader.partitions(split.index), context)\n+  }\n+\n+  override def getDependencies: Seq[Dependency[_]] = {\n+    Seq(new NarrowDependency(prev) {\n+      def getParents(id: Int): Seq[Int] = Seq(0)"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Yeah, I confused myself when looking at the normal coalesce RDD. The default dependency handling is correct here.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-20T20:06:40Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(var reader: ContinuousShuffleReadRDD, var prev: RDD[InternalRow])\n+  extends RDD[InternalRow](reader.context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    reader.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = reader.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val threads = prev.partitions.map { prevSplit =>\n+        new Thread() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threads.foreach(_.interrupt())\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+      threads.foreach(_.start())\n+    }\n+\n+    reader.compute(reader.partitions(split.index), context)\n+  }\n+\n+  override def getDependencies: Seq[Dependency[_]] = {\n+    Seq(new NarrowDependency(prev) {\n+      def getParents(id: Int): Seq[Int] = Seq(0)"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "We are addressing only the specific case that number of partitions is 1, but we could have some assertion for that and try to write complete code so that we don't modify it again.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-21T03:11:48Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Agree. And since theres an `assert (numpartitions == 1)` in `ContinuousCoalesceExec`, we can probably create any array of `numPartitions` here.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-22T18:10:22Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I've made some changes to try to restrict the assumption that the number of partitions is 1 to two places:\r\n\r\n* ContinuousCoalesceExec\r\n* The output partitioner in ContinuousCoalesceRDD, since it's not obvious to me what the right strategy to get this would be in the general case. If you have ideas I'm open to removing this too.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-25T20:35:47Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Maybe I am missing. Is this more like a re-partition (just shuffles) than coalesce?",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-22T20:30:34Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Repartition would normally imply distributed execution, which isn't happening here.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-25T20:25:32Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "The `writer.write` and `readerRDD.compute()` is going to execute as a separate tasks (but concurrently since there are no stage boundaries) correct? ",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-22T20:42:06Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = readerRDD.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val runnables = prev.partitions.map { prevSplit =>\n+        new Runnable() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threadPool.shutdownNow()\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+\n+      runnables.foreach(threadPool.execute)\n+    }\n+\n+    readerRDD.compute(readerRDD.partitions(split.index), context)"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "No, they'll be in the same task. Just different threads. ",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-25T20:24:30Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = readerRDD.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val runnables = prev.partitions.map { prevSplit =>\n+        new Runnable() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threadPool.shutdownNow()\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+\n+      runnables.foreach(threadPool.execute)\n+    }\n+\n+    readerRDD.compute(readerRDD.partitions(split.index), context)"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "If its the same task, then do we need the RPC mechanism to pass the rows around ?",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-26T00:57:37Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = readerRDD.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val runnables = prev.partitions.map { prevSplit =>\n+        new Runnable() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threadPool.shutdownNow()\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+\n+      runnables.foreach(threadPool.execute)\n+    }\n+\n+    readerRDD.compute(readerRDD.partitions(split.index), context)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "There is a queue inside the `ContinuousShuffleReadRDD` that is buffering all the records that are being sent out by the `RPCContinuousShuffleWriter`. And the compute function is returning data from that queue.\r\n\r\nAs I commented above, we dont really need the ContinuousShuffleReadRDD, just the ContinuousShuffleReader",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-26T08:29:11Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = readerRDD.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val runnables = prev.partitions.map { prevSplit =>\n+        new Runnable() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threadPool.shutdownNow()\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+\n+      runnables.foreach(threadPool.execute)\n+    }\n+\n+    readerRDD.compute(readerRDD.partitions(split.index), context)"
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Agree. Also the 2*numShuffle partition threads here looks like an overhead. Maybe ok for now but the CoalesceRDD iterator could just iterate over the parent RDD partitions tracking the epochs, returning the rows and terminating when the epoch is received from all its parents.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-26T16:59:37Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = readerRDD.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val runnables = prev.partitions.map { prevSplit =>\n+        new Runnable() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threadPool.shutdownNow()\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+\n+      runnables.foreach(threadPool.execute)\n+    }\n+\n+    readerRDD.compute(readerRDD.partitions(split.index), context)"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Yeah, it could be made more efficient. Part of the goal here is to ensure that the shuffling does indeed work end-to-end, so we can work on both the shuffle framework and distributed repartitioning in parallel.",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-27T17:09:40Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    readerEndpointName: String,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] = Array(ContinuousCoalesceRDDPartition(0))\n+\n+ val readerRDD = new ContinuousShuffleReadRDD(\n+    sparkContext,\n+    numPartitions,\n+    readerQueueSize,\n+    prev.getNumPartitions,\n+    epochIntervalMs,\n+    Seq(readerEndpointName))\n+\n+  private lazy val threadPool = ThreadUtils.newDaemonFixedThreadPool(\n+    prev.getNumPartitions,\n+    this.name)\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    assert(split.index == 0)\n+    // lazy initialize endpoint so writer can send to it\n+    readerRDD.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+\n+    if (!split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized) {\n+      val rpcEnv = SparkEnv.get.rpcEnv\n+      val outputPartitioner = new HashPartitioner(1)\n+      val endpointRefs = readerRDD.endpointNames.map { endpointName =>\n+          rpcEnv.setupEndpointRef(rpcEnv.address, endpointName)\n+      }\n+\n+      val runnables = prev.partitions.map { prevSplit =>\n+        new Runnable() {\n+          override def run(): Unit = {\n+            TaskContext.setTaskContext(context)\n+\n+            val writer: ContinuousShuffleWriter = new RPCContinuousShuffleWriter(\n+              prevSplit.index, outputPartitioner, endpointRefs.toArray)\n+\n+            EpochTracker.initializeCurrentEpoch(\n+              context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong)\n+            while (!context.isInterrupted() && !context.isCompleted()) {\n+              writer.write(prev.compute(prevSplit, context).asInstanceOf[Iterator[UnsafeRow]])\n+              // Note that current epoch is a non-inheritable thread local, so each writer thread\n+              // can properly increment its own epoch without affecting the main task thread.\n+              EpochTracker.incrementCurrentEpoch()\n+            }\n+          }\n+        }\n+      }\n+\n+      context.addTaskCompletionListener { ctx =>\n+        threadPool.shutdownNow()\n+      }\n+\n+      split.asInstanceOf[ContinuousCoalesceRDDPartition].writersInitialized = true\n+\n+      runnables.foreach(threadPool.execute)\n+    }\n+\n+    readerRDD.compute(readerRDD.partitions(split.index), context)"
  }],
  "prId": 21560
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "private",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-26T08:22:32Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.UUID\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] =\n+    (0 until numPartitions).map(ContinuousCoalesceRDDPartition).toArray\n+\n+  // When we support more than 1 target partition, we'll need to figure out how to pass in the\n+  // required partitioner.\n+  private val outputPartitioner = new HashPartitioner(1)\n+\n+  private val readerEndpointNames = (0 until numPartitions).map { i =>\n+    s\"ContinuousCoalesceRDD-part$i-${UUID.randomUUID()}\"\n+  }\n+\n+  val readerRDD = new ContinuousShuffleReadRDD("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Also, honestly, you dont need the RDD here. You only need the shuffle reading code, which is the `ContinuousShuffleReader` and endpoint. So you can just instantiate that in the compute function. Its very confusing to an RDD inside another RDD which is not hooked to the dependency chain. ",
    "commit": "f77b12ba92a868274ecdfea331786addb2d9ca83",
    "createdAt": "2018-06-26T08:28:24Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import java.util.UUID\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{CoalescedRDDPartition, RDD}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle._\n+import org.apache.spark.util.ThreadUtils\n+\n+case class ContinuousCoalesceRDDPartition(index: Int) extends Partition {\n+  // This flag will be flipped on the executors to indicate that the threads processing\n+  // partitions of the write-side RDD have been started. These will run indefinitely\n+  // asynchronously as epochs of the coalesce RDD complete on the read side.\n+  private[continuous] var writersInitialized: Boolean = false\n+}\n+\n+/**\n+ * RDD for continuous coalescing. Asynchronously writes all partitions of `prev` into a local\n+ * continuous shuffle, and then reads them in the task thread using `reader`.\n+ */\n+class ContinuousCoalesceRDD(\n+    context: SparkContext,\n+    numPartitions: Int,\n+    readerQueueSize: Int,\n+    epochIntervalMs: Long,\n+    prev: RDD[InternalRow])\n+  extends RDD[InternalRow](context, Nil) {\n+\n+  override def getPartitions: Array[Partition] =\n+    (0 until numPartitions).map(ContinuousCoalesceRDDPartition).toArray\n+\n+  // When we support more than 1 target partition, we'll need to figure out how to pass in the\n+  // required partitioner.\n+  private val outputPartitioner = new HashPartitioner(1)\n+\n+  private val readerEndpointNames = (0 until numPartitions).map { i =>\n+    s\"ContinuousCoalesceRDD-part$i-${UUID.randomUUID()}\"\n+  }\n+\n+  val readerRDD = new ContinuousShuffleReadRDD("
  }],
  "prId": 21560
}]