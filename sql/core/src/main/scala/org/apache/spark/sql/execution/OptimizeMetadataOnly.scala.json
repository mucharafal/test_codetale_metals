[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "does col2 need to be a partition column?\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:10:13Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2."
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Yea, it's indicated by the first sentence of the class doc: `When scanning only partition columns`\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:12:04Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2."
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "OK I would say something like the following to make it very explicit.\n\n```\nThis rule optimizes the execution of queries that can be answered by looking only at partition-level metadata.\nThis applies when all the columns scanned are partition columns, and the query has an aggregate operator\nthat satisfies the following conditions:\n1.\n2.\n3.\n```\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:30:29Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2."
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "OptimizeMetadataOnlyQuery\n\nif you don't add \"query\" it is not clear to me this actually optimizing a query or optimizing some metadata operation.\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:11:24Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly("
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "we need comment explaining what this function does\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:32:22Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "actually we should probably just rename this function to make it more self evident, e.g. replaceTableScanWithPartitionMetadata\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:47:10Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "use named argument, e.g. \n\n```\nval partitionData = fsRelation.location.listFiles(filters = Nil)\n```\n\nfor a while I was wondering what that argument does.\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:34:13Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this cannot be hit unless there is a bug in Spark right? if that's the case, add a comment saying that.\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:36:44Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))\n+\n+          case relation: CatalogRelation =>\n+            val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+            val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = catalog.listPartitions(relation.catalogTable.identifier).map { p =>\n+              InternalRow.fromSeq(partAttrs.map { attr =>\n+                Cast(Literal(p.spec(attr.name)), attr.dataType).eval()\n+              })\n+            }\n+            LocalRelation(partAttrs, partitionData)\n+\n+          case _ => throw new IllegalStateException()"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i think `aggFunctions.isEmpty` is not necessary, since forall returns true if aggFunctions is empty?\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:39:57Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "does this work for\n\n```\nselect col1, col2 from table group by col1\n```\n\n?\n\nIt'd be good to handle that too.\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:46:34Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "it can't pass analysis... `col2` is not grouping column and must be put inside agg functions.\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:51:18Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "```\nselect col1, max(col2) from table group by col1\n```\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T05:12:53Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "yea, as long as `col1` and `col2` are both partition columns\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T06:46:02Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "we need comment explaining what this does\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:47:35Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))\n+\n+          case relation: CatalogRelation =>\n+            val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+            val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = catalog.listPartitions(relation.catalogTable.identifier).map { p =>\n+              InternalRow.fromSeq(partAttrs.map { attr =>\n+                Cast(Literal(p.spec(attr.name)), attr.dataType).eval()\n+              })\n+            }\n+            LocalRelation(partAttrs, partitionData)\n+\n+          case _ => throw new IllegalStateException()\n+        }\n+    }\n+  }\n+\n+  object PartitionedRelation {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "also need to document what the returned tuple means\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:47:52Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))\n+\n+          case relation: CatalogRelation =>\n+            val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+            val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = catalog.listPartitions(relation.catalogTable.identifier).map { p =>\n+              InternalRow.fromSeq(partAttrs.map { attr =>\n+                Cast(Literal(p.spec(attr.name)), attr.dataType).eval()\n+              })\n+            }\n+            LocalRelation(partAttrs, partitionData)\n+\n+          case _ => throw new IllegalStateException()\n+        }\n+    }\n+  }\n+\n+  object PartitionedRelation {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "we should also explain what patterns are acceptable, e.g. filter and project with deterministic expressions.\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:50:09Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))\n+\n+          case relation: CatalogRelation =>\n+            val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+            val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = catalog.listPartitions(relation.catalogTable.identifier).map { p =>\n+              InternalRow.fromSeq(partAttrs.map { attr =>\n+                Cast(Literal(p.spec(attr.name)), attr.dataType).eval()\n+              })\n+            }\n+            LocalRelation(partAttrs, partitionData)\n+\n+          case _ => throw new IllegalStateException()\n+        }\n+    }\n+  }\n+\n+  object PartitionedRelation {"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "a nit: we can save some whitespace and one line \n\n```\nunapply(child).flatMap { case (partAttrs, relation) =>\n  if (p.references.subsetOf(partAttrs)) Some(p.outputSet, relation) else None\n}\n```\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T04:59:47Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))\n+\n+          case relation: CatalogRelation =>\n+            val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+            val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = catalog.listPartitions(relation.catalogTable.identifier).map { p =>\n+              InternalRow.fromSeq(partAttrs.map { attr =>\n+                Cast(Literal(p.spec(attr.name)), attr.dataType).eval()\n+              })\n+            }\n+            LocalRelation(partAttrs, partitionData)\n+\n+          case _ => throw new IllegalStateException()\n+        }\n+    }\n+  }\n+\n+  object PartitionedRelation {\n+    def unapply(plan: LogicalPlan): Option[(AttributeSet, LogicalPlan)] = plan match {\n+      case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _)\n+        if fsRelation.partitionSchema.nonEmpty =>\n+        val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+        val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+        Some(AttributeSet(partAttrs), l)\n+\n+      case relation: CatalogRelation if relation.catalogTable.partitionColumnNames.nonEmpty =>\n+        val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+        val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+        Some(AttributeSet(partAttrs), relation)\n+\n+      case p @ Project(projectList, child) if projectList.forall(_.deterministic) =>\n+        unapply(child).flatMap {"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "same thing here\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-06T05:00:02Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col1) FROM tbl GROUP BY col2.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class OptimizeMetadataOnly(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))\n+\n+          case relation: CatalogRelation =>\n+            val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+            val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = catalog.listPartitions(relation.catalogTable.identifier).map { p =>\n+              InternalRow.fromSeq(partAttrs.map { attr =>\n+                Cast(Literal(p.spec(attr.name)), attr.dataType).eval()\n+              })\n+            }\n+            LocalRelation(partAttrs, partitionData)\n+\n+          case _ => throw new IllegalStateException()\n+        }\n+    }\n+  }\n+\n+  object PartitionedRelation {\n+    def unapply(plan: LogicalPlan): Option[(AttributeSet, LogicalPlan)] = plan match {\n+      case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _)\n+        if fsRelation.partitionSchema.nonEmpty =>\n+        val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+        val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+        Some(AttributeSet(partAttrs), l)\n+\n+      case relation: CatalogRelation if relation.catalogTable.partitionColumnNames.nonEmpty =>\n+        val partColumns = relation.catalogTable.partitionColumnNames.map(_.toLowerCase).toSet\n+        val partAttrs = relation.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+        Some(AttributeSet(partAttrs), relation)\n+\n+      case p @ Project(projectList, child) if projectList.forall(_.deterministic) =>\n+        unapply(child).flatMap {\n+          case (partAttrs, relation) =>\n+            if (p.references.subsetOf(partAttrs)) {\n+              Some(p.outputSet, relation)\n+            } else {\n+              None\n+            }\n+        }\n+\n+      case f @ Filter(condition, child) if condition.deterministic =>\n+        unapply(child).flatMap {\n+          case (partAttrs, relation) =>"
  }],
  "prId": 13494
}]