[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "@zsxwing am I doing this right?\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-10-31T22:24:05Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+class MaxLong(protected var currentValue: Long = 0)\n+  extends AccumulatorV2[Long, Long]\n+  with Serializable {\n+\n+  override def isZero: Boolean = value == 0\n+  override def value: Long = currentValue\n+  override def copy(): AccumulatorV2[Long, Long] = new MaxLong(currentValue)\n+\n+  override def reset(): Unit = {\n+    currentValue = 0\n+  }\n+\n+  override def add(v: Long): Unit = {\n+    if (value < v) { currentValue = v }\n+  }\n+\n+  override def merge(other: AccumulatorV2[Long, Long]): Unit = {\n+    if (currentValue < other.value) {\n+      currentValue = other.value\n+    }\n+  }\n+}\n+\n+/**\n+ * Used to mark a column as the containing the event time for a given record. In addition to\n+ * adding appropriate metadata to this column, this operator also tracks the maximum observed event\n+ * time. Based on the maximum observed time and a user specified delay, we can calculate the\n+ * `watermark` after which we assume we will no longer see late records for a particular time\n+ * period.\n+ */\n+case class EventTimeWatermarkExec(\n+    eventTime: Attribute,\n+    delay: CalendarInterval,\n+    child: SparkPlan) extends SparkPlan {\n+\n+  // TODO: Use Spark SQL Metrics?\n+  val maxEventTime = new MaxLong",
    "line": 65
  }],
  "prId": 15702
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: `protected` -> `private`\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-11-02T20:05:20Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+class MaxLong(protected var currentValue: Long = 0)"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: Could you document that this one only support positive longs?\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-11-02T20:19:08Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+class MaxLong(protected var currentValue: Long = 0)"
  }],
  "prId": 15702
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: not needed. `AccumulatorV2` is already `Serializable`.\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-11-02T20:05:48Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+class MaxLong(protected var currentValue: Long = 0)\n+  extends AccumulatorV2[Long, Long]\n+  with Serializable {"
  }],
  "prId": 15702
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Just a small question: which place will check the `eventTime` type? I guess `getLong` just throws an exception if the format is wrong. Can we fail it before starting the spark job?\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-11-02T20:26:11Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+class MaxLong(protected var currentValue: Long = 0)\n+  extends AccumulatorV2[Long, Long]\n+  with Serializable {\n+\n+  override def isZero: Boolean = value == 0\n+  override def value: Long = currentValue\n+  override def copy(): AccumulatorV2[Long, Long] = new MaxLong(currentValue)\n+\n+  override def reset(): Unit = {\n+    currentValue = 0\n+  }\n+\n+  override def add(v: Long): Unit = {\n+    if (value < v) { currentValue = v }\n+  }\n+\n+  override def merge(other: AccumulatorV2[Long, Long]): Unit = {\n+    if (currentValue < other.value) {\n+      currentValue = other.value\n+    }\n+  }\n+}\n+\n+/**\n+ * Used to mark a column as the containing the event time for a given record. In addition to\n+ * adding appropriate metadata to this column, this operator also tracks the maximum observed event\n+ * time. Based on the maximum observed time and a user specified delay, we can calculate the\n+ * `watermark` after which we assume we will no longer see late records for a particular time\n+ * period.\n+ */\n+case class EventTimeWatermarkExec(\n+    eventTime: Attribute,\n+    delay: CalendarInterval,\n+    child: SparkPlan) extends SparkPlan {\n+\n+  // TODO: Use Spark SQL Metrics?\n+  val maxEventTime = new MaxLong\n+  sparkContext.register(maxEventTime)\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions { iter =>\n+      val getEventTime = UnsafeProjection.create(eventTime :: Nil, child.output)\n+      iter.map { row =>\n+        maxEventTime.add(getEventTime(row).getLong(0))",
    "line": 72
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Added to checkAnalysis.\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-11-02T22:43:52Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+class MaxLong(protected var currentValue: Long = 0)\n+  extends AccumulatorV2[Long, Long]\n+  with Serializable {\n+\n+  override def isZero: Boolean = value == 0\n+  override def value: Long = currentValue\n+  override def copy(): AccumulatorV2[Long, Long] = new MaxLong(currentValue)\n+\n+  override def reset(): Unit = {\n+    currentValue = 0\n+  }\n+\n+  override def add(v: Long): Unit = {\n+    if (value < v) { currentValue = v }\n+  }\n+\n+  override def merge(other: AccumulatorV2[Long, Long]): Unit = {\n+    if (currentValue < other.value) {\n+      currentValue = other.value\n+    }\n+  }\n+}\n+\n+/**\n+ * Used to mark a column as the containing the event time for a given record. In addition to\n+ * adding appropriate metadata to this column, this operator also tracks the maximum observed event\n+ * time. Based on the maximum observed time and a user specified delay, we can calculate the\n+ * `watermark` after which we assume we will no longer see late records for a particular time\n+ * period.\n+ */\n+case class EventTimeWatermarkExec(\n+    eventTime: Attribute,\n+    delay: CalendarInterval,\n+    child: SparkPlan) extends SparkPlan {\n+\n+  // TODO: Use Spark SQL Metrics?\n+  val maxEventTime = new MaxLong\n+  sparkContext.register(maxEventTime)\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions { iter =>\n+      val getEventTime = UnsafeProjection.create(eventTime :: Nil, child.output)\n+      iter.map { row =>\n+        maxEventTime.add(getEventTime(row).getLong(0))",
    "line": 72
  }],
  "prId": 15702
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: less confusing to read  if `if (currentValue < v) { currentValue = v }`. \nIn fact why not used math.max?\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-11-03T00:09:31Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+/** Tracks the maximum positive long seen. */\n+class MaxLong(protected var currentValue: Long = 0)\n+  extends AccumulatorV2[Long, Long] {\n+\n+  override def isZero: Boolean = value == 0\n+  override def value: Long = currentValue\n+  override def copy(): AccumulatorV2[Long, Long] = new MaxLong(currentValue)\n+\n+  override def reset(): Unit = {\n+    currentValue = 0\n+  }\n+\n+  override def add(v: Long): Unit = {\n+    if (value < v) { currentValue = v }"
  }],
  "prId": 15702
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: same as above, why not use math.max\n",
    "commit": "87d8618234a86d666a711a97080e2b014214b84a",
    "createdAt": "2016-11-03T00:09:50Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.types.MetadataBuilder\n+import org.apache.spark.unsafe.types.CalendarInterval\n+import org.apache.spark.util.AccumulatorV2\n+\n+/** Tracks the maximum positive long seen. */\n+class MaxLong(protected var currentValue: Long = 0)\n+  extends AccumulatorV2[Long, Long] {\n+\n+  override def isZero: Boolean = value == 0\n+  override def value: Long = currentValue\n+  override def copy(): AccumulatorV2[Long, Long] = new MaxLong(currentValue)\n+\n+  override def reset(): Unit = {\n+    currentValue = 0\n+  }\n+\n+  override def add(v: Long): Unit = {\n+    if (value < v) { currentValue = v }\n+  }\n+\n+  override def merge(other: AccumulatorV2[Long, Long]): Unit = {\n+    if (currentValue < other.value) {"
  }],
  "prId": 15702
}]