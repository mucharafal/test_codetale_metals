[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "can you comment that rowCount is the cumulative count? or maybe name it `numCumulativeOutputRows`?",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-07-09T18:12:18Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned. This operator is meant for streams in Append mode only.\n+ */\n+case class StreamingGlobalLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None,\n+    outputMode: Option[OutputMode] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    assert(outputMode.isDefined && outputMode.get == InternalOutputModes.Append,\n+      \"StreamingGlobalLimitExec is only valid for streams in Append output mode\")\n+\n+    child.execute().mapPartitionsWithStateStore(\n+        getStateInfo,\n+        keySchema,\n+        valueSchema,\n+        indexOrdinal = None,\n+        sqlContext.sessionState,\n+        Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+      val key = UnsafeProjection.create(keySchema)(new GenericInternalRow(Array[Any](null)))\n+      val numOutputRows = longMetric(\"numOutputRows\")\n+      val numUpdatedStateRows = longMetric(\"numUpdatedStateRows\")\n+      val allUpdatesTimeMs = longMetric(\"allUpdatesTimeMs\")\n+      val commitTimeMs = longMetric(\"commitTimeMs\")\n+      val updatesStartTimeNs = System.nanoTime\n+\n+      val startCount: Long = Option(store.get(key)).map(_.getLong(0)).getOrElse(0L)\n+      var rowCount = startCount"
  }],
  "prId": 21662
}]