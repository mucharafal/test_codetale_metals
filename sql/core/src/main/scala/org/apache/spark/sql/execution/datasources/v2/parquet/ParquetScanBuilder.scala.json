[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Sorry, but this looks like a violation of the contact of `SupportsPushDownFilters ` interface. I understand the reasoning, but personally, I'm -1 for this kind of abuse because this Parquet code will be used as a reference model in the community for all the 3rd party data sources. Do we really need to allow this in Apache Spark code, @gengliangwang , @cloud-fan , @rxin , @rdblue , @dbtsai ?",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-07T21:40:30Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  private var filters: Array[Filter] = _\n+\n+  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+    this.filters = filters\n+    this.filters\n+  }\n+\n+  // The actual filter push down happens in [[ParquetPartitionReaderFactory]].\n+  // It requires the Parquet physical schema to determine whether a filter is convertible.\n+  // So here we simply mark that all the filters are pushed down.\n+  override def pushedFilters(): Array[Filter] = this.filters"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "As per the discussion in https://github.com/apache/spark/pull/21696#pullrequestreview-133842990, we need to push down filters according to the actual physical schema in read tasks.\r\n\r\nWith the current Data Source V2 API, I think this is the simplest way we can make it happen. I understand this is ugly. But I can't think of other better solution here for now.\r\n\r\nAlso, in V1 (`FileSourceScanExec`), all the data filters without subqueries are considered as \"pushed down\" as well.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-07T22:28:40Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  private var filters: Array[Filter] = _\n+\n+  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+    this.filters = filters\n+    this.filters\n+  }\n+\n+  // The actual filter push down happens in [[ParquetPartitionReaderFactory]].\n+  // It requires the Parquet physical schema to determine whether a filter is convertible.\n+  // So here we simply mark that all the filters are pushed down.\n+  override def pushedFilters(): Array[Filter] = this.filters"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I agree with @dongjoon-hyun, this should not violate the method contract.\r\n\r\nWhy is the file schema needed to push the filters? Iceberg pushes Parquet filters without knowing the physical schema of all the files. Can't this just return the filters that can be represented as Parquet filters?",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-07T22:50:55Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  private var filters: Array[Filter] = _\n+\n+  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+    this.filters = filters\n+    this.filters\n+  }\n+\n+  // The actual filter push down happens in [[ParquetPartitionReaderFactory]].\n+  // It requires the Parquet physical schema to determine whether a filter is convertible.\n+  // So here we simply mark that all the filters are pushed down.\n+  override def pushedFilters(): Array[Filter] = this.filters"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "@dongjoon-hyun @rdblue I see. I will correct this. Thanks.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-07T23:16:42Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  private var filters: Array[Filter] = _\n+\n+  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+    this.filters = filters\n+    this.filters\n+  }\n+\n+  // The actual filter push down happens in [[ParquetPartitionReaderFactory]].\n+  // It requires the Parquet physical schema to determine whether a filter is convertible.\n+  // So here we simply mark that all the filters are pushed down.\n+  override def pushedFilters(): Array[Filter] = this.filters"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thank you, @gengliangwang !",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-08T01:45:21Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  private var filters: Array[Filter] = _\n+\n+  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+    this.filters = filters\n+    this.filters\n+  }\n+\n+  // The actual filter push down happens in [[ParquetPartitionReaderFactory]].\n+  // It requires the Parquet physical schema to determine whether a filter is convertible.\n+  // So here we simply mark that all the filters are pushed down.\n+  override def pushedFilters(): Array[Filter] = this.filters"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "created https://github.com/apache/spark/pull/24597 for this.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-14T15:18:31Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  private var filters: Array[Filter] = _\n+\n+  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+    this.filters = filters\n+    this.filters\n+  }\n+\n+  // The actual filter push down happens in [[ParquetPartitionReaderFactory]].\n+  // It requires the Parquet physical schema to determine whether a filter is convertible.\n+  // So here we simply mark that all the filters are pushed down.\n+  override def pushedFilters(): Array[Filter] = this.filters"
  }],
  "prId": 24327
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "This comment isn't correct. All filters that can be converted to Parquet are pushed down.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-06T19:41:07Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFilters, SparkToParquetSchemaConverter}\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  lazy val _pushedFilters = {\n+    val sqlConf = sparkSession.sessionState.conf\n+    val pushDownDate = sqlConf.parquetFilterPushDownDate\n+    val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+    val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+    val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+    val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+    val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+    val parquetSchema =\n+      new SparkToParquetSchemaConverter(sparkSession.sessionState.conf).convert(schema)\n+    val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+      pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+    parquetFilters.convertibleFilters(this.filters).toArray\n+  }\n+\n+  private var filters: Array[Filter] = Array.empty\n+\n+  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+    this.filters = filters\n+    this.filters\n+  }\n+\n+  // The actual filter push down happens in [[ParquetPartitionReaderFactory]].\n+  // It requires the Parquet physical schema to determine whether a filter is convertible.\n+  // So here we simply mark that all the filters are pushed down."
  }],
  "prId": 24327
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "not a big deal here too but I'd name it to `pushedParquetFilters`",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-14T01:06:23Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFilters, SparkToParquetSchemaConverter}\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  lazy val _pushedFilters = {"
  }],
  "prId": 24327
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Sorry if I missed some context. What's diff between `ParquetFilters.convertibleFilters` and `ParquetFilters.createFilters`? Seems like logic is duplicated.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-14T07:55:08Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFilters, SparkToParquetSchemaConverter}\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  lazy val _pushedFilters = {\n+    val sqlConf = sparkSession.sessionState.conf\n+    val pushDownDate = sqlConf.parquetFilterPushDownDate\n+    val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+    val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+    val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+    val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+    val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+    val parquetSchema =\n+      new SparkToParquetSchemaConverter(sparkSession.sessionState.conf).convert(schema)\n+    val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+      pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+    parquetFilters.convertibleFilters(this.filters).toArray",
    "line": 56
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "`ParquetFilters.convertibleFilters` returns `Seq[org.apache.spark.sql.sources.Filter] `\r\n`ParquetFilters.createFilters` returns `org.apache.parquet.filter2.predicate.FilterPredicate`\r\n\r\nThe overlap of the two methods is only on the `And`/`Or`/`Not` operator.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-14T16:21:32Z",
    "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFilters, SparkToParquetSchemaConverter}\n+import org.apache.spark.sql.execution.datasources.v2.FileScanBuilder\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{Scan, SupportsPushDownFilters}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+case class ParquetScanBuilder(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    schema: StructType,\n+    dataSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends FileScanBuilder(sparkSession, fileIndex, dataSchema) with SupportsPushDownFilters {\n+  lazy val hadoopConf = {\n+    val caseSensitiveMap = options.asCaseSensitiveMap.asScala.toMap\n+    // Hadoop Configurations are case sensitive.\n+    sparkSession.sessionState.newHadoopConfWithOptions(caseSensitiveMap)\n+  }\n+\n+  lazy val _pushedFilters = {\n+    val sqlConf = sparkSession.sessionState.conf\n+    val pushDownDate = sqlConf.parquetFilterPushDownDate\n+    val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+    val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+    val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+    val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+    val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+    val parquetSchema =\n+      new SparkToParquetSchemaConverter(sparkSession.sessionState.conf).convert(schema)\n+    val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+      pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+    parquetFilters.convertibleFilters(this.filters).toArray",
    "line": 56
  }],
  "prId": 24327
}]