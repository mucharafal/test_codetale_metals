[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "2?\n",
    "commit": "0da1e8cd2b8279aa0da7a7c146d2cb3a39e3f9ca",
    "createdAt": "2014-08-27T20:00:05Z",
    "diffHunk": "@@ -18,8 +18,13 @@\n package org.apache.spark.sql.test\n \n import org.apache.spark.{SparkConf, SparkContext}\n-import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.{SQLConf, SQLContext}\n \n /** A SQLContext that can be used for local testing. */\n object TestSQLContext\n-  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf()))\n+  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf())) {\n+\n+  /** Fewer partitions to speed up testing. */\n+  override private[spark] def numShufflePartitions: Int =\n+    getConf(SQLConf.SHUFFLE_PARTITIONS, \"5\").toInt"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "I don't know... I was thinking a little more parallelism might be more likely to find bugs without incurring too much overhead.  I could be convinced otherwise...\n",
    "commit": "0da1e8cd2b8279aa0da7a7c146d2cb3a39e3f9ca",
    "createdAt": "2014-08-27T20:16:51Z",
    "diffHunk": "@@ -18,8 +18,13 @@\n package org.apache.spark.sql.test\n \n import org.apache.spark.{SparkConf, SparkContext}\n-import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.{SQLConf, SQLContext}\n \n /** A SQLContext that can be used for local testing. */\n object TestSQLContext\n-  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf()))\n+  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf())) {\n+\n+  /** Fewer partitions to speed up testing. */\n+  override private[spark] def numShufflePartitions: Int =\n+    getConf(SQLConf.SHUFFLE_PARTITIONS, \"5\").toInt"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "there isn't any parallelism really here since we run with \"local\", which is single threaded. increasing this from 2 to 5 simply breaks each dataset into more chunks, to be processed sequentially.\n",
    "commit": "0da1e8cd2b8279aa0da7a7c146d2cb3a39e3f9ca",
    "createdAt": "2014-08-27T20:28:32Z",
    "diffHunk": "@@ -18,8 +18,13 @@\n package org.apache.spark.sql.test\n \n import org.apache.spark.{SparkConf, SparkContext}\n-import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.{SQLConf, SQLContext}\n \n /** A SQLContext that can be used for local testing. */\n object TestSQLContext\n-  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf()))\n+  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf())) {\n+\n+  /** Fewer partitions to speed up testing. */\n+  override private[spark] def numShufflePartitions: Int =\n+    getConf(SQLConf.SHUFFLE_PARTITIONS, \"5\").toInt"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "I guess parallelism isn't really what I meant, I'm thinking more about bugs that could be related to expecting data to be copartitioned when it actually isn't.\n\nThat said, perhaps the test should also run in `local[2]` or higher.  We have found a couple of bugs after deploying that are the result of concurrency issues (scala reflection... i'm looking at you :P)\n",
    "commit": "0da1e8cd2b8279aa0da7a7c146d2cb3a39e3f9ca",
    "createdAt": "2014-08-27T20:31:09Z",
    "diffHunk": "@@ -18,8 +18,13 @@\n package org.apache.spark.sql.test\n \n import org.apache.spark.{SparkConf, SparkContext}\n-import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.{SQLConf, SQLContext}\n \n /** A SQLContext that can be used for local testing. */\n object TestSQLContext\n-  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf()))\n+  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf())) {\n+\n+  /** Fewer partitions to speed up testing. */\n+  override private[spark] def numShufflePartitions: Int =\n+    getConf(SQLConf.SHUFFLE_PARTITIONS, \"5\").toInt"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "+1 to run this using local[2]\n",
    "commit": "0da1e8cd2b8279aa0da7a7c146d2cb3a39e3f9ca",
    "createdAt": "2014-08-27T20:38:39Z",
    "diffHunk": "@@ -18,8 +18,13 @@\n package org.apache.spark.sql.test\n \n import org.apache.spark.{SparkConf, SparkContext}\n-import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.{SQLConf, SQLContext}\n \n /** A SQLContext that can be used for local testing. */\n object TestSQLContext\n-  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf()))\n+  extends SQLContext(new SparkContext(\"local\", \"TestSQLContext\", new SparkConf())) {\n+\n+  /** Fewer partitions to speed up testing. */\n+  override private[spark] def numShufflePartitions: Int =\n+    getConf(SQLConf.SHUFFLE_PARTITIONS, \"5\").toInt"
  }],
  "prId": 2164
}]