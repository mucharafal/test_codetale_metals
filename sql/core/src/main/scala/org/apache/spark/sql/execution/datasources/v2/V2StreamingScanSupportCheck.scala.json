[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I think this should be `isEmpty` because v1 sources don't support continuous mode. If there is a v1 source and `nonEmpty` is true, then not all sources support continuous.\r\n\r\nThis would have been caught by a test. Can you add a few test cases for this like the [`V2WriteSupportCheckSuite`](https://github.com/apache/spark/pull/24012/files#diff-7f34a3615b9aee9e50cc62e15ea09b6bR33)?",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T16:09:05Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    val streamingSources = plan.collect {\n+      case r: StreamingRelationV2 => r.table\n+    }\n+    val v1StreamingRelations = plan.collect {\n+      case r: StreamingRelation => r\n+    }\n+\n+    if ((streamingSources ++ v1StreamingRelations).length > 1) {\n+      val allSupportsMicroBatch = streamingSources.forall(_.supports(MICRO_BATCH_READ))\n+      // v1 streaming data source only supports micro-batch.\n+      val allSupportsContinuous = streamingSources.forall(_.supports(CONTINUOUS_READ)) &&\n+        v1StreamingRelations.nonEmpty"
  }],
  "prId": 24129
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "This error message isn't very specific. Is it possible to show the table names that support each mode so that the user has enough feedback to know what went wrong? That would be more helpful.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T16:15:41Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    val streamingSources = plan.collect {\n+      case r: StreamingRelationV2 => r.table\n+    }\n+    val v1StreamingRelations = plan.collect {\n+      case r: StreamingRelation => r\n+    }\n+\n+    if ((streamingSources ++ v1StreamingRelations).length > 1) {\n+      val allSupportsMicroBatch = streamingSources.forall(_.supports(MICRO_BATCH_READ))\n+      // v1 streaming data source only supports micro-batch.\n+      val allSupportsContinuous = streamingSources.forall(_.supports(CONTINUOUS_READ)) &&\n+        v1StreamingRelations.nonEmpty\n+      if (!allSupportsMicroBatch && !allSupportsContinuous) {\n+        throw new AnalysisException(\n+          \"The streaming sources in a query do not have a common supported execution mode.\")"
  }],
  "prId": 24129
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I think these checks should also include a validation for individual tables. If a table doesn't support streaming at all, a more helpful error message is that a specific table doesn't support streaming, not just that there isn't a streaming mode that can handle all of the sources.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T16:17:08Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "If we add this check here, it will never be hit because we already checked it earlier in `DataStreamReader`. Like I explained, it's non-trivial to move the check from `DataStreamReader`  to here, because it's coupled with the v2 -> v1 fallback logic.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T16:51:25Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think eventually we will have an analyzer rule that checks streaming scan capability and fallback to v1 if necessary. This checker rule is not suitable because it just traverses the plan, not returning a new plan. So we can't implement the fallback logic in this rule.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T16:55:28Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Even if it will never be hit, it should still be in the analyzer so that plans that are created through other paths in the future are still checked. The idea is to avoid relying on a particular way of creating a logical plan to validate logical plans.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T17:02:23Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "that's why I opened https://issues.apache.org/jira/browse/SPARK-27483\r\n\r\nplans that are created through other APIs(not `DataStreamReader`) still need the fallback logic, which can not be done within this checker rule.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T23:03:15Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Why would this check depend on fallback logic? These checks run after resolution rules have reached a fixed point. If there is a streaming DSv2 relation in the plan, fallback should already be done. Fallback logic is separate and this check can be done here.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T23:07:30Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "> Why would this check depend on fallback logic?\r\n\r\nIt's the opposite. The fallback logic depends on the check. That said, the future analyzer rule would do 3 things:\r\n1. find the v2 streaming relation in the plan, check scan capability\r\n2. if check failed, fallback to v1 relation, and use the v1 relation to replace the v2 streaming relation.\r\n3. if fallback is not applicable, fail\r\n\r\nYou can see that, it's not a simple check anymore, which can not be done within this simpler checker rule (`LogicalPlan => Unit`).",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T23:20:57Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@cloud-fan, let's get the validation in now.\r\n\r\nI don't think that the fallback rule should be implemented as you describe. I think it should be done in 2 parts: the rule to fallback and update the plan, and this validation that all sources support streaming.\r\n\r\nSpark should not combine transform rules and validations. There are a couple of reasons for this principle:\r\n\r\n1. Validations are used to ensure that the query is valid *and* to ensure that rules are run correctly. If the transform rule is added to the analyzer in a single-run batch, we want validation to catch that. These checks catch errors in Spark, too.\r\n2. Rules should be as small as possible and focused on a single task. The fallback rule should not fail analysis if it doesn't know what to do because some other rule may be added later that does. For example, what if we build an adapter from continuous execution to micro-batch execution for a source?\r\n\r\nSo we will need a validation rule either way. When the fallback rule runs and can't fix the problem, this check should be what fails the plan.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-19T00:22:45Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "added.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-19T00:30:54Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {",
    "line": 32
  }],
  "prId": 24129
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Minor: Is `++` a cheap operation? There's no need to create a new seq here when this could check the length of both individually.",
    "commit": "30ca36ca50e46cfaa9eaa1d7e56f9af1c8256542",
    "createdAt": "2019-04-18T16:19:53Z",
    "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.streaming.{StreamingRelation, StreamingRelationV2}\n+import org.apache.spark.sql.sources.v2.TableCapability.{CONTINUOUS_READ, MICRO_BATCH_READ}\n+\n+/**\n+ * This rules adds some basic table capability check for streaming scan, without knowing the actual\n+ * streaming execution mode.\n+ */\n+object V2StreamingScanSupportCheck extends (LogicalPlan => Unit) {\n+  import DataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    val streamingSources = plan.collect {\n+      case r: StreamingRelationV2 => r.table\n+    }\n+    val v1StreamingRelations = plan.collect {\n+      case r: StreamingRelation => r\n+    }\n+\n+    if ((streamingSources ++ v1StreamingRelations).length > 1) {"
  }],
  "prId": 24129
}]