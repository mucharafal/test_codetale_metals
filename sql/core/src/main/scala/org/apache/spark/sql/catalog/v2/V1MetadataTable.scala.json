[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "@cloud-fan, I updated this PR that adds the `TableCatalog` API to include an implementation that uses the existing `SessionCatalog`. This `Table` class demonstrates how `Table` would implement `ReadSupport` and `WriteSupport`.\r\n\r\nThe catalog returns these tables, which have `ReadSupport` and `WriteSupport` mixed in depending on whether the underlying `DataSourceV2` also supports them. In your updated API, it would use the `ReadSupportProvider` instead of the `DataSourceV2` directly, but the difference isn't very large.\r\n\r\nThe follow-up PR for CTAS and RTAS, #21877, demonstrates how this would be used in the new logical plans.",
    "commit": "6b45a119df8e6382fa2503f854b4a85aed3e3785",
    "createdAt": "2018-08-18T00:19:35Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SaveMode\n+import org.apache.spark.sql.catalog.v2.PartitionTransforms.{bucket, identity}\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport, WriteSupport}\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader\n+import org.apache.spark.sql.sources.v2.writer.DataSourceWriter\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * An implementation of catalog v2 [[Table]] to expose v1 table metadata.\n+ */\n+private[sql] class V1MetadataTable(",
    "line": 35
  }],
  "prId": 21306
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Consider `.map(...).getOrElse`, but we haven't been consistent on using or not using `match` on `Option` types throughout Spark in general anyways so it's fine to leave as-is.",
    "commit": "6b45a119df8e6382fa2503f854b4a85aed3e3785",
    "createdAt": "2018-11-30T01:46:01Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SaveMode\n+import org.apache.spark.sql.catalog.v2.PartitionTransforms.{bucket, identity}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, CatalogUtils}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport, WriteSupport}\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader\n+import org.apache.spark.sql.sources.v2.writer.DataSourceWriter\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * An implementation of catalog v2 [[Table]] to expose v1 table metadata.\n+ */\n+private[sql] class V1MetadataTable(\n+    v1Table: CatalogTable,\n+    v2Source: Option[DataSourceV2]) extends Table {\n+\n+  def readDelegate: ReadSupport = v2Source match {\n+    case r: ReadSupport => r\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support reads: $v2Source\")\n+  }\n+\n+  def writeDelegate: WriteSupport = v2Source match {\n+    case w: WriteSupport => w\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support writes: $v2Source\")\n+  }\n+\n+  lazy val options: Map[String, String] = {\n+    v1Table.storage.locationUri match {",
    "line": 50
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Also any particular reason this and the following variables have to be `lazy`?",
    "commit": "6b45a119df8e6382fa2503f854b4a85aed3e3785",
    "createdAt": "2018-11-30T01:46:40Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SaveMode\n+import org.apache.spark.sql.catalog.v2.PartitionTransforms.{bucket, identity}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, CatalogUtils}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport, WriteSupport}\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader\n+import org.apache.spark.sql.sources.v2.writer.DataSourceWriter\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * An implementation of catalog v2 [[Table]] to expose v1 table metadata.\n+ */\n+private[sql] class V1MetadataTable(\n+    v1Table: CatalogTable,\n+    v2Source: Option[DataSourceV2]) extends Table {\n+\n+  def readDelegate: ReadSupport = v2Source match {\n+    case r: ReadSupport => r\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support reads: $v2Source\")\n+  }\n+\n+  def writeDelegate: WriteSupport = v2Source match {\n+    case w: WriteSupport => w\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support writes: $v2Source\")\n+  }\n+\n+  lazy val options: Map[String, String] = {\n+    v1Table.storage.locationUri match {",
    "line": 50
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "How would the `getOrElse` pattern work here? If the URI is undefined, what tuple should be added to the table properties?",
    "commit": "6b45a119df8e6382fa2503f854b4a85aed3e3785",
    "createdAt": "2018-11-30T19:18:34Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SaveMode\n+import org.apache.spark.sql.catalog.v2.PartitionTransforms.{bucket, identity}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, CatalogUtils}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport, WriteSupport}\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader\n+import org.apache.spark.sql.sources.v2.writer.DataSourceWriter\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * An implementation of catalog v2 [[Table]] to expose v1 table metadata.\n+ */\n+private[sql] class V1MetadataTable(\n+    v1Table: CatalogTable,\n+    v2Source: Option[DataSourceV2]) extends Table {\n+\n+  def readDelegate: ReadSupport = v2Source match {\n+    case r: ReadSupport => r\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support reads: $v2Source\")\n+  }\n+\n+  def writeDelegate: WriteSupport = v2Source match {\n+    case w: WriteSupport => w\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support writes: $v2Source\")\n+  }\n+\n+  lazy val options: Map[String, String] = {\n+    v1Table.storage.locationUri match {",
    "line": 50
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "A second read over this, don't think we necessarily have to use the `Option` lambdas here, and in fact may be less legible, varying from developer to developer.\r\n\r\nBut if one were to do so, it'd be something like this...\r\n\r\n```\r\nv1Table.storage.properties + v1Table.storage.locationUri.map(uri -> Map(\"path\" -> CatalogUtils.URITOString(uri)).getOrElse(Map.empty)\r\n```\r\n\r\n",
    "commit": "6b45a119df8e6382fa2503f854b4a85aed3e3785",
    "createdAt": "2018-11-30T19:41:21Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SaveMode\n+import org.apache.spark.sql.catalog.v2.PartitionTransforms.{bucket, identity}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, CatalogUtils}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport, WriteSupport}\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader\n+import org.apache.spark.sql.sources.v2.writer.DataSourceWriter\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * An implementation of catalog v2 [[Table]] to expose v1 table metadata.\n+ */\n+private[sql] class V1MetadataTable(\n+    v1Table: CatalogTable,\n+    v2Source: Option[DataSourceV2]) extends Table {\n+\n+  def readDelegate: ReadSupport = v2Source match {\n+    case r: ReadSupport => r\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support reads: $v2Source\")\n+  }\n+\n+  def writeDelegate: WriteSupport = v2Source match {\n+    case w: WriteSupport => w\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support writes: $v2Source\")\n+  }\n+\n+  lazy val options: Map[String, String] = {\n+    v1Table.storage.locationUri match {",
    "line": 50
  }],
  "prId": 21306
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I use lazy for a couple reasons. First, to avoid building maps or other data values that are never used. Second, to avoid a required ordering for fields. If fields depend on one another, then they have to be reordered when those dependencies change. Lazy values never require reordering.",
    "commit": "6b45a119df8e6382fa2503f854b4a85aed3e3785",
    "createdAt": "2018-11-30T19:21:01Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalog.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.sql.SaveMode\n+import org.apache.spark.sql.catalog.v2.PartitionTransforms.{bucket, identity}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, CatalogUtils}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport, WriteSupport}\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader\n+import org.apache.spark.sql.sources.v2.writer.DataSourceWriter\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * An implementation of catalog v2 [[Table]] to expose v1 table metadata.\n+ */\n+private[sql] class V1MetadataTable(\n+    v1Table: CatalogTable,\n+    v2Source: Option[DataSourceV2]) extends Table {\n+\n+  def readDelegate: ReadSupport = v2Source match {\n+    case r: ReadSupport => r\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support reads: $v2Source\")\n+  }\n+\n+  def writeDelegate: WriteSupport = v2Source match {\n+    case w: WriteSupport => w\n+    case _ => throw new UnsupportedOperationException(s\"Source does not support writes: $v2Source\")\n+  }\n+\n+  lazy val options: Map[String, String] = {\n+    v1Table.storage.locationUri match {",
    "line": 50
  }],
  "prId": 21306
}]