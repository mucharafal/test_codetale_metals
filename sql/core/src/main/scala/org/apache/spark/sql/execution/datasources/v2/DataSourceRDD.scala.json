[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why is this necessary? I think the TODO should be handled in this commit and that Spark shouldn't cast RDD[ColumnarBatch] to RDD[InternalRow].\r\n\r\nWhat about having the RDD iterate over the rows in the batch to actually implement the interface? It can provide the underlying batches through a different API.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:49:10Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "The problem is that, we don't really have a batch API in Spark SQL. We rely on type erasure and codegen hack to implement columnar scan. It's hardcoded in the engine: `SparkPlan#execute` returns `RDD[InternalRow]`.\r\n\r\nif we have a RDD iterate over the rows in the batch, then whole stage codegen will break, as it iterates the input RDD and cast the record to `ColumnarBatch`.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T02:27:14Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Where is the issue to fix this hack?\r\n\r\nThis seems like something that should never have happened. We can simply have an additional trait on a `RDD[InternalRow]` to fetch the underlying iterator of `ColumnarBatch`. The codegen path should check for that and use it.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T15:52:57Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "SGTM. Can we do it in followup? The type hack is there for years and the fix may be non-trivial. We also need to fix the file source side.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T17:07:09Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "> We can simply have an additional trait on a RDD[InternalRow] to fetch the underlying iterator of ColumnarBatch\r\n\r\nBTW this still need to make a `RDD[ColumnarBatch]` to pretend itself as `RDD[InternalRow]`?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T18:35:31Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "We can fix it in a follow-up. Please open an issue for it and link to it from this PR.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T18:55:27Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "> BTW this still need to make a RDD[ColumnarBatch] to pretend itself as RDD[InternalRow]?\r\n\r\nWhat I'm proposing is to make the RDD actually implement RDD[InternalRow] and allow accessing the batches directly through a different iterator.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T18:56:20Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])"
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why doesn't this use `match` to get a DataSourceRDDPartition and default to no locality for any other splits?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:49:58Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])\n   }\n \n   override def getPreferredLocations(split: Partition): Seq[String] = {\n-    split.asInstanceOf[DataSourceRDDPartition[T]].inputPartition.preferredLocations()\n+    split.asInstanceOf[DataSourceRDDPartition].inputPartition.preferredLocations()"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "It's a common pattern in RDD that we cast the `split` to the concrete `Partition` class defined by this RDD.\r\n\r\nThe partitions are created in `RDD#getPartitions`, so if we see other splits here, it's a bug.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T02:29:25Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])\n   }\n \n   override def getPreferredLocations(split: Partition): Seq[String] = {\n-    split.asInstanceOf[DataSourceRDDPartition[T]].inputPartition.preferredLocations()\n+    split.asInstanceOf[DataSourceRDDPartition].inputPartition.preferredLocations()"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Makes sense. If it's a bug, then the error message should indicate that it's a bug to users instead of throwing a `ClassCastException`. Even if someone goes to the code here, it there's no comment to indicate that it's a Spark bug.\r\n\r\nI'd prefer something like this:\r\n```scala\r\noverride def getPreferredLocations(split: Partition): Seq[String] = {\r\n  split match {\r\n    case dsp: DataSourceRDDPartition => dsp.inputPartition.preferredLocations\r\n    case _ => throw new SparkException(s\"[BUG] Not a DataSourceRDDPartition: $split\")\r\n  }\r\n}\r\n```",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T15:55:41Z",
    "diffHunk": "@@ -51,18 +58,19 @@ class DataSourceRDD[T: ClassTag](\n         valuePrepared\n       }\n \n-      override def next(): T = {\n+      override def next(): Any = {\n         if (!hasNext) {\n           throw new java.util.NoSuchElementException(\"End of stream\")\n         }\n         valuePrepared = false\n         reader.get()\n       }\n     }\n-    new InterruptibleIterator(context, iter)\n+    // TODO: get rid of this type hack.\n+    new InterruptibleIterator(context, iter.asInstanceOf[Iterator[InternalRow]])\n   }\n \n   override def getPreferredLocations(split: Partition): Seq[String] = {\n-    split.asInstanceOf[DataSourceRDDPartition[T]].inputPartition.preferredLocations()\n+    split.asInstanceOf[DataSourceRDDPartition].inputPartition.preferredLocations()"
  }],
  "prId": 22009
}]