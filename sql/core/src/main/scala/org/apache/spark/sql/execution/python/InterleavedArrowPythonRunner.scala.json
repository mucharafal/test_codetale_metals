[{
  "comments": [{
    "author": {
      "login": "d80tb7"
    },
    "body": "I need this so that the python side knows whether it should expect another group.  I'm using SpecialLengths.START_ARROW_STREAM to signal that there will be another group coming and SpecialLengths.END_OF_DATA_SECTION to indicate that we've finished sending all the arrow data.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-06-27T10:30:20Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.dictionary.DictionaryProvider\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+                                          env: SparkEnv,\n+                                          worker: Socket,\n+                                          inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+                                          partitionIndex: Int,\n+                                          context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "These `SpecialLengths` have slightly different meanings elsewhere and I think it gets confusing trying to sort out how they are used. I think it would be clearer if we just say we are sending an integer before the data which will indicate how many groups to read, with a value of 0 to represent the end-of-stream. So we would send 2 before writing the left and right groups, and if we end up sending more groups in the future, then it wouldn't change.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-10T23:02:08Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.dictionary.DictionaryProvider\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+                                          env: SparkEnv,\n+                                          worker: Socket,\n+                                          inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+                                          partitionIndex: Int,\n+                                          context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)"
  }, {
    "author": {
      "login": "d80tb7"
    },
    "body": "yes that makes sense.  Changed",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-11T20:59:01Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.dictionary.DictionaryProvider\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+                                          env: SparkEnv,\n+                                          worker: Socket,\n+                                          inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+                                          partitionIndex: Int,\n+                                          context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)"
  }],
  "prId": 24981
}, {
  "comments": [{
    "author": {
      "login": "icexelloss"
    },
    "body": "I see. In this implementation we are writing out the complete arrow stream for each group. I am ok with this one but is a little concerned about performance. I think I'd like to understand the performance diffs between the two POCs. Is it possible to do a microbenchmark of maybe 100M of data with very small to very large groups?",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-02T22:01:00Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+        env: SparkEnv,\n+        worker: Socket,\n+        inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+        partitionIndex: Int,\n+        context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut)"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "e.g. comparing the performance with:\r\n* Each group has 1 row\r\n* Each group has 10 row\r\n* Each group has 100 row\r\n...\r\n* The dataframe contains a single group.\r\n\r\n@BryanCutler @HyukjinKwon WDYT?",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-02T22:02:13Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+        env: SparkEnv,\n+        worker: Socket,\n+        inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+        partitionIndex: Int,\n+        context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yea, we should at least know the rough estimate about the performance.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-08T07:08:59Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+        env: SparkEnv,\n+        worker: Socket,\n+        inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+        partitionIndex: Int,\n+        context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut)"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "If it's easy to run some numbers between the two POCs then that would be nice to see, but I think there would have to be a significant difference to break from the Arrow stream protocol. I would rather stick with this PR for now and leave performance improvements for followups.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-10T23:23:42Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+        env: SparkEnv,\n+        worker: Socket,\n+        inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+        partitionIndex: Int,\n+        context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut)"
  }, {
    "author": {
      "login": "d80tb7"
    },
    "body": "I don't think it should be too bad to test- I'll try and get one in the next few days.  That said, I'll try and tidy up this code first as Bryan suggests.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-11T21:00:55Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+        env: SparkEnv,\n+        worker: Socket,\n+        inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+        partitionIndex: Int,\n+        context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut)"
  }],
  "prId": 24981
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "should name the allocator different for the left and right groups, it can help if memory errors are thrown",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-10T23:05:18Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+class InterleavedArrowPythonRunner(\n+                         funcs: Seq[ChainedPythonFunctions],\n+                         evalType: Int,\n+                         argOffsets: Array[Array[Int]],\n+                         leftSchema: StructType,\n+                         rightSchema: StructType,\n+                         timeZoneId: String,\n+                         conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+        env: SparkEnv,\n+        worker: Socket,\n+        inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+        partitionIndex: Int,\n+        context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(SpecialLengths.START_ARROW_STREAM)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut)\n+          writeGroup(nextRight, rightSchema, dataOut)\n+        }\n+        dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION)\n+      }\n+\n+      def writeGroup(group: Iterator[InternalRow], schema: StructType, dataOut: DataOutputStream\n+                    ) = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema, timeZoneId)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)"
  }],
  "prId": 24981
}]