[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`Orc` -> `CSV`.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-08T18:12:22Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.csv.{CSVHeaderChecker, CSVOptions, UnivocityParser}\n+import org.apache.spark.sql.catalyst.expressions.ExprUtils\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Orc readers."
  }],
  "prId": 24005
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`orc` -> `csv`.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-08T18:13:53Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.csv.{CSVHeaderChecker, CSVOptions, UnivocityParser}\n+import org.apache.spark.sql.catalyst.expressions.ExprUtils\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Orc readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of orc files."
  }],
  "prId": 24005
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`Broadcast` -> `Broadcasted`.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-20T17:22:10Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.csv.{CSVHeaderChecker, CSVOptions, UnivocityParser}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create CSV readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration."
  }],
  "prId": 24005
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "nit. let's remove empty line.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-22T05:53:10Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.csv.{CSVHeaderChecker, CSVOptions, UnivocityParser}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create CSV readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcasted serializable Hadoop Configuration.\n+ * @param dataSchema Schema of CSV files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param readSchema Required schema in the batch scan.\n+ */\n+case class CSVPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    partitionSchema: StructType,\n+    readSchema: StructType,\n+    parsedOptions: CSVOptions) extends FilePartitionReaderFactory {\n+  private val columnPruning = sqlConf.csvColumnPruning\n+  private val readDataSchema =\n+    getReadDataSchema(readSchema, partitionSchema, sqlConf.caseSensitiveAnalysis)\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val parser = new UnivocityParser(\n+      StructType(dataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      StructType(readDataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      parsedOptions)\n+    val schema = if (columnPruning) readDataSchema else dataSchema\n+    val isStartOfFile = file.start == 0\n+    val headerChecker = new CSVHeaderChecker(\n+      schema, parsedOptions, source = s\"CSV file: ${file.filePath}\", isStartOfFile)\n+    val iter = CSVDataSource(parsedOptions).readFile(\n+      conf,\n+      file,\n+      parser,\n+      headerChecker,\n+      readSchema)\n+    val fileReader = new PartitionReaderFromIterator[InternalRow](iter)\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+"
  }],
  "prId": 24005
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "This should be `readDataSchema`. This seems to be hidden by upper layer. If we use `readSchema`, that will redundantly read a column whose name is equal to the partition column.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-22T06:41:32Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.csv.{CSVHeaderChecker, CSVOptions, UnivocityParser}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create CSV readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcasted serializable Hadoop Configuration.\n+ * @param dataSchema Schema of CSV files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param readSchema Required schema in the batch scan.\n+ */\n+case class CSVPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    partitionSchema: StructType,\n+    readSchema: StructType,\n+    parsedOptions: CSVOptions) extends FilePartitionReaderFactory {\n+  private val columnPruning = sqlConf.csvColumnPruning\n+  private val readDataSchema =\n+    getReadDataSchema(readSchema, partitionSchema, sqlConf.caseSensitiveAnalysis)\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val parser = new UnivocityParser(\n+      StructType(dataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      StructType(readDataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      parsedOptions)\n+    val schema = if (columnPruning) readDataSchema else dataSchema\n+    val isStartOfFile = file.start == 0\n+    val headerChecker = new CSVHeaderChecker(\n+      schema, parsedOptions, source = s\"CSV file: ${file.filePath}\", isStartOfFile)\n+    val iter = CSVDataSource(parsedOptions).readFile(\n+      conf,\n+      file,\n+      parser,\n+      headerChecker,\n+      readSchema)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "To make it sure, could you confirm this, @HyukjinKwon ?",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-22T06:45:10Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.csv.{CSVHeaderChecker, CSVOptions, UnivocityParser}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create CSV readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcasted serializable Hadoop Configuration.\n+ * @param dataSchema Schema of CSV files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param readSchema Required schema in the batch scan.\n+ */\n+case class CSVPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    partitionSchema: StructType,\n+    readSchema: StructType,\n+    parsedOptions: CSVOptions) extends FilePartitionReaderFactory {\n+  private val columnPruning = sqlConf.csvColumnPruning\n+  private val readDataSchema =\n+    getReadDataSchema(readSchema, partitionSchema, sqlConf.caseSensitiveAnalysis)\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val parser = new UnivocityParser(\n+      StructType(dataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      StructType(readDataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      parsedOptions)\n+    val schema = if (columnPruning) readDataSchema else dataSchema\n+    val isStartOfFile = file.start == 0\n+    val headerChecker = new CSVHeaderChecker(\n+      schema, parsedOptions, source = s\"CSV file: ${file.filePath}\", isStartOfFile)\n+    val iter = CSVDataSource(parsedOptions).readFile(\n+      conf,\n+      file,\n+      parser,\n+      headerChecker,\n+      readSchema)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yes, nice catch. It looks `readSchema` can include partitioning columns but CSV parsing should be orthogonal with partitioning columns.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-22T07:01:18Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.csv.{CSVHeaderChecker, CSVOptions, UnivocityParser}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create CSV readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcasted serializable Hadoop Configuration.\n+ * @param dataSchema Schema of CSV files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param readSchema Required schema in the batch scan.\n+ */\n+case class CSVPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    partitionSchema: StructType,\n+    readSchema: StructType,\n+    parsedOptions: CSVOptions) extends FilePartitionReaderFactory {\n+  private val columnPruning = sqlConf.csvColumnPruning\n+  private val readDataSchema =\n+    getReadDataSchema(readSchema, partitionSchema, sqlConf.caseSensitiveAnalysis)\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val parser = new UnivocityParser(\n+      StructType(dataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      StructType(readDataSchema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord)),\n+      parsedOptions)\n+    val schema = if (columnPruning) readDataSchema else dataSchema\n+    val isStartOfFile = file.start == 0\n+    val headerChecker = new CSVHeaderChecker(\n+      schema, parsedOptions, source = s\"CSV file: ${file.filePath}\", isStartOfFile)\n+    val iter = CSVDataSource(parsedOptions).readFile(\n+      conf,\n+      file,\n+      parser,\n+      headerChecker,\n+      readSchema)"
  }],
  "prId": 24005
}]