[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Does v2 also use Parquet `_metadata` files?",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-06T19:45:42Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.{Job, OutputCommitter, TaskAttemptContext}\n+import org.apache.parquet.hadoop.{ParquetOutputCommitter, ParquetOutputFormat}\n+import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel\n+import org.apache.parquet.hadoop.codec.CodecConfig\n+import org.apache.parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2.FileWriteBuilder\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class ParquetWriteBuilder(\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n+    formatName: String,\n+    supportsDataType: DataType => Boolean)\n+  extends FileWriteBuilder(options, paths, formatName, supportsDataType) with Logging {\n+\n+  override def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parquetOptions = new ParquetOptions(options, sqlConf)\n+\n+    val conf = ContextUtil.getConfiguration(job)\n+\n+    val committerClass =\n+      conf.getClass(\n+        SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key,",
    "line": 53
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I think it is disabled by default",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-07T16:26:14Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.{Job, OutputCommitter, TaskAttemptContext}\n+import org.apache.parquet.hadoop.{ParquetOutputCommitter, ParquetOutputFormat}\n+import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel\n+import org.apache.parquet.hadoop.codec.CodecConfig\n+import org.apache.parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2.FileWriteBuilder\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class ParquetWriteBuilder(\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n+    formatName: String,\n+    supportsDataType: DataType => Boolean)\n+  extends FileWriteBuilder(options, paths, formatName, supportsDataType) with Logging {\n+\n+  override def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parquetOptions = new ParquetOptions(options, sqlConf)\n+\n+    val conf = ContextUtil.getConfiguration(job)\n+\n+    val committerClass =\n+      conf.getClass(\n+        SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key,",
    "line": 53
  }],
  "prId": 24327
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "If they are disabled by default in v1, why allow writing them in v2?",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-06T19:48:00Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.{Job, OutputCommitter, TaskAttemptContext}\n+import org.apache.parquet.hadoop.{ParquetOutputCommitter, ParquetOutputFormat}\n+import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel\n+import org.apache.parquet.hadoop.codec.CodecConfig\n+import org.apache.parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2.FileWriteBuilder\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class ParquetWriteBuilder(\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n+    formatName: String,\n+    supportsDataType: DataType => Boolean)\n+  extends FileWriteBuilder(options, paths, formatName, supportsDataType) with Logging {\n+\n+  override def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parquetOptions = new ParquetOptions(options, sqlConf)\n+\n+    val conf = ContextUtil.getConfiguration(job)\n+\n+    val committerClass =\n+      conf.getClass(\n+        SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key,\n+        classOf[ParquetOutputCommitter],\n+        classOf[OutputCommitter])\n+\n+    if (conf.get(SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key) == null) {\n+      logInfo(\"Using default output committer for Parquet: \" +\n+        classOf[ParquetOutputCommitter].getCanonicalName)\n+    } else {\n+      logInfo(\"Using user defined output committer for Parquet: \" + committerClass.getCanonicalName)\n+    }\n+\n+    conf.setClass(\n+      SQLConf.OUTPUT_COMMITTER_CLASS.key,\n+      committerClass,\n+      classOf[OutputCommitter])\n+\n+    // We're not really using `ParquetOutputFormat[Row]` for writing data here, because we override\n+    // it in `ParquetOutputWriter` to support appending and dynamic partitioning.  The reason why\n+    // we set it here is to setup the output committer class to `ParquetOutputCommitter`, which is\n+    // bundled with `ParquetOutputFormat[Row]`.\n+    job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])\n+\n+    ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n+\n+    // This metadata is useful for keeping UDTs like Vector/Matrix.\n+    ParquetWriteSupport.setSchema(dataSchema, conf)\n+\n+    // Sets flags for `ParquetWriteSupport`, which converts Catalyst schema to Parquet\n+    // schema and writes actual rows to Parquet files.\n+    conf.set(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key, sqlConf.writeLegacyParquetFormat.toString)\n+\n+    conf.set(SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key, sqlConf.parquetOutputTimestampType.toString)\n+\n+    // Sets compression scheme\n+    conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)\n+\n+    // SPARK-15719: Disables writing Parquet summary files by default.",
    "line": 89
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I think the behavior in V1 and V2 are the same: by default set \"parquet.summary.metadata.level\" as  \"NONE\" and don't write the summary file. If the conf \"parquet.summary.metadata.level\" is set by user and `spark.sql.parquet.output.committer.class` is set correctly, then it will write the summary file.\r\nSee: https://issues.apache.org/jira/browse/SPARK-15719",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-07T16:31:15Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.{Job, OutputCommitter, TaskAttemptContext}\n+import org.apache.parquet.hadoop.{ParquetOutputCommitter, ParquetOutputFormat}\n+import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel\n+import org.apache.parquet.hadoop.codec.CodecConfig\n+import org.apache.parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2.FileWriteBuilder\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class ParquetWriteBuilder(\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n+    formatName: String,\n+    supportsDataType: DataType => Boolean)\n+  extends FileWriteBuilder(options, paths, formatName, supportsDataType) with Logging {\n+\n+  override def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parquetOptions = new ParquetOptions(options, sqlConf)\n+\n+    val conf = ContextUtil.getConfiguration(job)\n+\n+    val committerClass =\n+      conf.getClass(\n+        SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key,\n+        classOf[ParquetOutputCommitter],\n+        classOf[OutputCommitter])\n+\n+    if (conf.get(SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key) == null) {\n+      logInfo(\"Using default output committer for Parquet: \" +\n+        classOf[ParquetOutputCommitter].getCanonicalName)\n+    } else {\n+      logInfo(\"Using user defined output committer for Parquet: \" + committerClass.getCanonicalName)\n+    }\n+\n+    conf.setClass(\n+      SQLConf.OUTPUT_COMMITTER_CLASS.key,\n+      committerClass,\n+      classOf[OutputCommitter])\n+\n+    // We're not really using `ParquetOutputFormat[Row]` for writing data here, because we override\n+    // it in `ParquetOutputWriter` to support appending and dynamic partitioning.  The reason why\n+    // we set it here is to setup the output committer class to `ParquetOutputCommitter`, which is\n+    // bundled with `ParquetOutputFormat[Row]`.\n+    job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])\n+\n+    ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n+\n+    // This metadata is useful for keeping UDTs like Vector/Matrix.\n+    ParquetWriteSupport.setSchema(dataSchema, conf)\n+\n+    // Sets flags for `ParquetWriteSupport`, which converts Catalyst schema to Parquet\n+    // schema and writes actual rows to Parquet files.\n+    conf.set(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key, sqlConf.writeLegacyParquetFormat.toString)\n+\n+    conf.set(SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key, sqlConf.parquetOutputTimestampType.toString)\n+\n+    // Sets compression scheme\n+    conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)\n+\n+    // SPARK-15719: Disables writing Parquet summary files by default.",
    "line": 89
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Why should v2 support deprecated metadata files?",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-12T00:06:07Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.{Job, OutputCommitter, TaskAttemptContext}\n+import org.apache.parquet.hadoop.{ParquetOutputCommitter, ParquetOutputFormat}\n+import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel\n+import org.apache.parquet.hadoop.codec.CodecConfig\n+import org.apache.parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2.FileWriteBuilder\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class ParquetWriteBuilder(\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n+    formatName: String,\n+    supportsDataType: DataType => Boolean)\n+  extends FileWriteBuilder(options, paths, formatName, supportsDataType) with Logging {\n+\n+  override def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parquetOptions = new ParquetOptions(options, sqlConf)\n+\n+    val conf = ContextUtil.getConfiguration(job)\n+\n+    val committerClass =\n+      conf.getClass(\n+        SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key,\n+        classOf[ParquetOutputCommitter],\n+        classOf[OutputCommitter])\n+\n+    if (conf.get(SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key) == null) {\n+      logInfo(\"Using default output committer for Parquet: \" +\n+        classOf[ParquetOutputCommitter].getCanonicalName)\n+    } else {\n+      logInfo(\"Using user defined output committer for Parquet: \" + committerClass.getCanonicalName)\n+    }\n+\n+    conf.setClass(\n+      SQLConf.OUTPUT_COMMITTER_CLASS.key,\n+      committerClass,\n+      classOf[OutputCommitter])\n+\n+    // We're not really using `ParquetOutputFormat[Row]` for writing data here, because we override\n+    // it in `ParquetOutputWriter` to support appending and dynamic partitioning.  The reason why\n+    // we set it here is to setup the output committer class to `ParquetOutputCommitter`, which is\n+    // bundled with `ParquetOutputFormat[Row]`.\n+    job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])\n+\n+    ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n+\n+    // This metadata is useful for keeping UDTs like Vector/Matrix.\n+    ParquetWriteSupport.setSchema(dataSchema, conf)\n+\n+    // Sets flags for `ParquetWriteSupport`, which converts Catalyst schema to Parquet\n+    // schema and writes actual rows to Parquet files.\n+    conf.set(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key, sqlConf.writeLegacyParquetFormat.toString)\n+\n+    conf.set(SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key, sqlConf.parquetOutputTimestampType.toString)\n+\n+    // Sets compression scheme\n+    conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)\n+\n+    // SPARK-15719: Disables writing Parquet summary files by default.",
    "line": 89
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I think it is consistent with V1 here.\r\nThe value of `parquet.summary.metadata.level` is `ALL` by default. As per SPARK-15719, we should set it as `NONE` by default in Spark.\r\nIf users set the conf `parquet.summary.metadata.level` as `ALL` or `COMMON_ONLY` explicitly, Spark should write metadata files.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-12T00:32:21Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.{Job, OutputCommitter, TaskAttemptContext}\n+import org.apache.parquet.hadoop.{ParquetOutputCommitter, ParquetOutputFormat}\n+import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel\n+import org.apache.parquet.hadoop.codec.CodecConfig\n+import org.apache.parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2.FileWriteBuilder\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class ParquetWriteBuilder(\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n+    formatName: String,\n+    supportsDataType: DataType => Boolean)\n+  extends FileWriteBuilder(options, paths, formatName, supportsDataType) with Logging {\n+\n+  override def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parquetOptions = new ParquetOptions(options, sqlConf)\n+\n+    val conf = ContextUtil.getConfiguration(job)\n+\n+    val committerClass =\n+      conf.getClass(\n+        SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key,\n+        classOf[ParquetOutputCommitter],\n+        classOf[OutputCommitter])\n+\n+    if (conf.get(SQLConf.PARQUET_OUTPUT_COMMITTER_CLASS.key) == null) {\n+      logInfo(\"Using default output committer for Parquet: \" +\n+        classOf[ParquetOutputCommitter].getCanonicalName)\n+    } else {\n+      logInfo(\"Using user defined output committer for Parquet: \" + committerClass.getCanonicalName)\n+    }\n+\n+    conf.setClass(\n+      SQLConf.OUTPUT_COMMITTER_CLASS.key,\n+      committerClass,\n+      classOf[OutputCommitter])\n+\n+    // We're not really using `ParquetOutputFormat[Row]` for writing data here, because we override\n+    // it in `ParquetOutputWriter` to support appending and dynamic partitioning.  The reason why\n+    // we set it here is to setup the output committer class to `ParquetOutputCommitter`, which is\n+    // bundled with `ParquetOutputFormat[Row]`.\n+    job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])\n+\n+    ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n+\n+    // This metadata is useful for keeping UDTs like Vector/Matrix.\n+    ParquetWriteSupport.setSchema(dataSchema, conf)\n+\n+    // Sets flags for `ParquetWriteSupport`, which converts Catalyst schema to Parquet\n+    // schema and writes actual rows to Parquet files.\n+    conf.set(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key, sqlConf.writeLegacyParquetFormat.toString)\n+\n+    conf.set(SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key, sqlConf.parquetOutputTimestampType.toString)\n+\n+    // Sets compression scheme\n+    conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)\n+\n+    // SPARK-15719: Disables writing Parquet summary files by default.",
    "line": 89
  }],
  "prId": 24327
}]