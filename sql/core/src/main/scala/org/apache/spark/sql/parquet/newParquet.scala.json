[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can you explain what is going on with the flatMap and reduceOption here? Is this the simplest way to write this?\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T17:51:53Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "This is a pretty common construct throughout the query planner, but we can certainly add comments.\n\nIts essentially take any filters that we can convert a parquet filter (skipping those that parquet does not support), and `And` them together into a single predicate.  Set that in the job conf, or do nothing if no filters can be converted.\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:01:11Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Though I will agree the explicit if is much clearer than `.filter(_ => sqlContext.conf.parquetFilterPushDown)` from the previous implementation.\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:11:03Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "This is simply\n\n``` scala\nval pushdown = predicates\n  .filter { pred =>\n    val partitionColNames = partitionColumns.map(_.name).toSet\n    val referencedColNames = pred.references.map(_.name).toSet\n    referencedColNames.intersect(partitionColNames).isEmpty\n  }\n  .flatMap(ParquetFilters.createFilter)\n\nif (pushdown.nonEmpty) {\n  ParquetInputFormat.setFilterPredicate(jobConf, pushdown.reduce(FilterApi.and))\n}\n```\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:29:17Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Yeah reynold's one seems more clear to me.\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:31:13Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "I think you guys are optimizing for the wrong thing here.  `reduceOption(<and>)` is an incredibly common pattern when you are munging predicates.  It occurs 15+ times in the code base.\n\nIn my experience, `reduce` is always a red flag and it means you aren't considering what to do in the case that the list is empty.  There have been several bugs in catalyst where this was the problem.\n\nYou are correct, that you _can_ add an explicit if check.  However, now as I scan the code, I have to connect these two disparate lines and reason about them independently.  When a developer uses `reduceOption` I know that the compiler is going to yell at them if they aren't handling all cases correctly, so I can focus my review on the other logic.\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:36:04Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Reasons why I think our style guide should prefer `reduceOption` to `reduce`/`reduceLeft`:\n- SPARK-877\n  java.lang.UnsupportedOperationException: empty.reduceLeft in UI\n- SPARK-744\n  BlockManagerUI with no RDD: java.lang.UnsupportedOperationException: empty.reduceLeft\n- SPARK-4968\n  [SparkSQL] java.lang.UnsupportedOperationException when hive partition doesn't exist and order by and limit are used\n- SPARK-4318\n  Fix empty sum distinct.\n- SPARK-5852\n  Fail to convert a newly created empty metastore parquet table to a data source parquet table.\n- SPARK-6245\n  jsonRDD() of empty RDD results in exception\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:57:28Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Ok I think you are convincing me that we should ban reduce in the codebase.\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T21:16:02Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }, {
    "author": {
      "login": "aarondav"
    },
    "body": "I have to agree that if we're going to use reduce here, then reduceOption does seem clearer from a safety perspective. However, a transformation from Seq[T] -> Option[U] is a bit confusing, since we're doing two things at once; it's pretty hard to accurately follow the typing information, and the fact that foreach() can apply to either Seq or Option makes it somewhat harder to figure out what it's applying to and when.\n\nThis is one of those \"trust the programmer\" ordeals, where I just have to assume the programmer wrote it correctly and it does what it seems, but verifying that (i.e., reviewing the code) is made significantly harder.\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T21:32:22Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet\n+          val referencedColNames = pred.references.map(_.name).toSet\n+          referencedColNames.intersect(partitionColNames).isEmpty\n+        }\n+        .flatMap(ParquetFilters.createFilter)",
    "line": 17
  }],
  "prId": 5210
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "btw we don't need to compute partitionColumns for every predicate, do we?\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:31:53Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet",
    "line": 13
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Thats a good point, but likely inconsequential from a performance standpoint.\n",
    "commit": "4f7ec03dd159c9b5add34533768acb45b6bc8a33",
    "createdAt": "2015-03-26T20:54:11Z",
    "diffHunk": "@@ -435,11 +435,18 @@ private[sql] case class ParquetRelation2(\n     // Push down filters when possible. Notice that not all filters can be converted to Parquet\n     // filter predicate. Here we try to convert each individual predicate and only collect those\n     // convertible ones.\n-    predicates\n-      .flatMap(ParquetFilters.createFilter)\n-      .reduceOption(FilterApi.and)\n-      .filter(_ => sqlContext.conf.parquetFilterPushDown)\n-      .foreach(ParquetInputFormat.setFilterPredicate(jobConf, _))\n+    if (sqlContext.conf.parquetFilterPushDown) {\n+      predicates\n+        // Don't push down predicates which reference partition columns\n+        .filter { pred =>\n+          val partitionColNames = partitionColumns.map(_.name).toSet",
    "line": 13
  }],
  "prId": 5210
}]