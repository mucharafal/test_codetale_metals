[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "`SQLExecutionUIData` is OK\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T05:46:25Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{AccumulatorParam, JobExecutionStatus}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SparkSQLExecution\n+\n+private[sql] class SQLSparkListener(sqlContext: SQLContext) extends SparkListener {\n+\n+  private val retainedExecutions =\n+    sqlContext.sparkContext.conf.getInt(\"spark.sql.ui.retainedExecutions\", 1000)\n+\n+  private val activeExecutions = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  // Old data in the following fields must be removed in \"trimExecutionsIfNecessary\".\n+  // If adding new fields, make sure \"trimExecutionsIfNecessary\" can clean up old data\n+  private val executionIdToData = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  /**\n+   * Maintain the relation between job id and execution id so that we can get the execution id in\n+   * the \"onJobEnd\" method.\n+   */\n+  private val jobIdToExecutionId = mutable.HashMap[Long, Long]()\n+\n+  private val stageIdToStageMetrics = mutable.HashMap[Long, SQLStageMetrics]()\n+\n+  private val failedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private val completedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private def trimExecutionsIfNecessary(\n+      executions: mutable.ListBuffer[SparkSQLExecutionUIData]): Unit = {\n+    if (executions.size > retainedExecutions) {\n+      val toRemove = math.max(retainedExecutions / 10, 1)\n+      executions.take(toRemove).foreach { execution =>\n+        for (executionUIData <- executionIdToData.remove(execution.executionId)) {\n+          for (jobId <- executionUIData.jobs.keys) {\n+            jobIdToExecutionId.remove(jobId)\n+          }\n+          for (stageId <- executionUIData.stages) {\n+            stageIdToStageMetrics.remove(stageId)\n+          }\n+        }\n+      }\n+      executions.trimStart(toRemove)\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val executionId = jobStart.properties.getProperty(SparkSQLExecution.EXECUTION_ID_KEY)\n+    if (executionId == null) {\n+      // This is not a job created by SQL\n+      return\n+    }\n+    val jobId = jobStart.jobId\n+    val stageIds = jobStart.stageIds\n+\n+    synchronized {\n+      activeExecutions.get(executionId.toLong).foreach { executionUIData =>\n+        executionUIData.jobs(jobId) = JobExecutionStatus.RUNNING\n+        executionUIData.stages ++= stageIds\n+        // attemptId must be 0. Right?\n+        stageIds.foreach(stageId =>\n+          stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId = 0))\n+        jobIdToExecutionId(jobId) = executionUIData.executionId\n+      }\n+    }\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = synchronized {\n+    val jobId = jobEnd.jobId\n+    for (executionId <- jobIdToExecutionId.get(jobId);\n+         executionUIData <- executionIdToData.get(executionId)) {\n+      jobEnd.jobResult match {\n+        case JobSucceeded => executionUIData.jobs(jobId) = JobExecutionStatus.SUCCEEDED\n+        case JobFailed(_) => executionUIData.jobs(jobId) = JobExecutionStatus.FAILED\n+      }\n+    }\n+  }\n+\n+  override def onExecutorMetricsUpdate(\n+      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = synchronized {\n+    for ((taskId, stageId, stageAttemptID, metrics) <- executorMetricsUpdate.taskMetrics) {\n+      updateTaskMetrics(taskId, stageId, stageAttemptID, metrics, false)\n+    }\n+  }\n+\n+  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = synchronized {\n+    val stageId = stageSubmitted.stageInfo.stageId\n+    val stageAttemptId = stageSubmitted.stageInfo.attemptId\n+    // Always override metrics for old stage attempt\n+    stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId)\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    updateTaskMetrics(\n+      taskEnd.taskInfo.taskId, taskEnd.stageId, taskEnd.stageAttemptId, taskEnd.taskMetrics, true)\n+  }\n+\n+  private def updateTaskMetrics(taskId: Long, stageId: Int, stageAttemptID: Int,\n+      metrics: TaskMetrics, finishTask: Boolean): Unit = {\n+    stageIdToStageMetrics.get(stageId) match {\n+      case Some(stageMetrics) =>\n+        if (stageAttemptID < stageMetrics.stageAttemptId) {\n+          // A task of an old stage attempt. Because a new stage is submitted, we can ignore it.\n+        } else if (stageAttemptID > stageMetrics.stageAttemptId) {\n+          // TODO A running task with a higher stageAttemptID??\n+        } else {\n+          // TODO We don't know the attemptId. Currently, what we can do is overriding the\n+          // accumulator updates. However, if there are two same task are running, such as\n+          // speculation, the accumulator updates will be overriding by different task attempts,\n+          // the results will be weird.\n+          stageMetrics.taskIdToMetricUpdates.get(taskId) match {\n+            case Some(taskMetrics) =>\n+              if (finishTask) {\n+                taskMetrics.finished = true\n+                taskMetrics.accumulatorUpdates = metrics.accumulatorUpdates()\n+              } else if (!taskMetrics.finished){\n+                // If a task is finished, we should not override with accumulator updates from\n+                // heartbeat reports\n+                taskMetrics.accumulatorUpdates = metrics.accumulatorUpdates()\n+              }\n+            case None =>\n+              // TODO Now just set attemptId to 0. Should fix here when we can get the attempt\n+              // id from SparkListenerExecutorMetricsUpdate\n+              stageMetrics.taskIdToMetricUpdates(taskId) =\n+                SQLTaskMetrics(attemptId = 0, finished = finishTask, metrics.accumulatorUpdates())\n+          }\n+        }\n+      case None =>\n+      // This execution and its stage have been dropped\n+    }\n+  }\n+\n+  def onExecutionStart(\n+      executionId: Long, description: String, details: String, df: DataFrame, time: Long): Unit = {\n+    val physicalPlanDescription = df.queryExecution.toString\n+    val physicalPlanGraph = SparkPlanGraph(df.queryExecution.executedPlan)\n+    val metrics = physicalPlanGraph.nodes.flatMap { node =>\n+      node.metrics.map(metric => metric.accumulatorId -> metric)\n+    }\n+\n+    val executionUIData = SparkSQLExecutionUIData(executionId, description, details,\n+      physicalPlanDescription, physicalPlanGraph, metrics.toMap, time)\n+\n+    synchronized {\n+      activeExecutions(executionId) = executionUIData\n+      executionIdToData(executionId) = executionUIData\n+    }\n+  }\n+\n+  def onExecutionEnd(executionId: Long, time: Long): Unit = synchronized {\n+    activeExecutions.remove(executionId).foreach { executionUIData =>\n+      executionUIData.completionTime = Some(time)\n+      if (executionUIData.isFailed) {\n+        failedExecutions += executionUIData\n+        trimExecutionsIfNecessary(failedExecutions)\n+      } else {\n+        completedExecutions += executionUIData\n+        trimExecutionsIfNecessary(completedExecutions)\n+      }\n+    }\n+  }\n+\n+  def getRunningExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    activeExecutions.values.toSeq\n+  }\n+\n+  def getFailedExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    failedExecutions\n+  }\n+\n+  def getCompletedExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    completedExecutions\n+  }\n+\n+  def getExecution(executionId: Long): Option[SparkSQLExecutionUIData] = synchronized {\n+    executionIdToData.get(executionId)\n+  }\n+\n+  def getExecutionMetrics(executionId: Long): Map[Long, Any] = synchronized {\n+    executionIdToData.get(executionId) match {\n+      case Some(executionUIData) =>\n+        // Get all accumulator updates from all tasks which belong to this execution and merge them\n+        val accumulatorUpdates = {\n+          for (stageId <- executionUIData.stages;\n+               stageMetrics <- stageIdToStageMetrics.get(stageId).toIterable;\n+               taskMetrics <- stageMetrics.taskIdToMetricUpdates.values;\n+               accumulatorUpdate <- taskMetrics.accumulatorUpdates.toSeq)\n+            yield accumulatorUpdate\n+        }\n+        mergeAccumulatorUpdates(accumulatorUpdates, accumulatorId =>\n+          executionUIData.accumulatorMetrics(accumulatorId).accumulatorParam)\n+      case None =>\n+        // This execution has been dropped\n+        Map.empty\n+    }\n+  }\n+\n+  private def mergeAccumulatorUpdates(\n+     accumulatorUpdates: Seq[(Long, Any)],\n+     paramFunc: Long => AccumulatorParam[Any]): Map[Long, Any] = {\n+    accumulatorUpdates.groupBy(_._1).map { case (accumulatorId, values) =>\n+      val param = paramFunc(accumulatorId)\n+      (accumulatorId, values.map(_._2).reduceLeft(param.addInPlace))\n+    }\n+  }\n+\n+}\n+\n+/**\n+ * Represent all necessary data for an execution that will be used in Web UI.\n+ */\n+private[ui] case class SparkSQLExecutionUIData("
  }],
  "prId": 7774
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "`SQLListener` is probably ok\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T05:46:50Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{AccumulatorParam, JobExecutionStatus}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SparkSQLExecution\n+\n+private[sql] class SQLSparkListener(sqlContext: SQLContext) extends SparkListener {"
  }],
  "prId": 7774
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "style:\n\n```\nprivate def updateTaskMetrics(\n    taskId: Long,\n    ...\n    finishTask: Boolean): Unit = {\n  stageIdToStageMetrics...\n```\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T06:00:33Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{AccumulatorParam, JobExecutionStatus}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SparkSQLExecution\n+\n+private[sql] class SQLSparkListener(sqlContext: SQLContext) extends SparkListener {\n+\n+  private val retainedExecutions =\n+    sqlContext.sparkContext.conf.getInt(\"spark.sql.ui.retainedExecutions\", 1000)\n+\n+  private val activeExecutions = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  // Old data in the following fields must be removed in \"trimExecutionsIfNecessary\".\n+  // If adding new fields, make sure \"trimExecutionsIfNecessary\" can clean up old data\n+  private val executionIdToData = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  /**\n+   * Maintain the relation between job id and execution id so that we can get the execution id in\n+   * the \"onJobEnd\" method.\n+   */\n+  private val jobIdToExecutionId = mutable.HashMap[Long, Long]()\n+\n+  private val stageIdToStageMetrics = mutable.HashMap[Long, SQLStageMetrics]()\n+\n+  private val failedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private val completedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private def trimExecutionsIfNecessary(\n+      executions: mutable.ListBuffer[SparkSQLExecutionUIData]): Unit = {\n+    if (executions.size > retainedExecutions) {\n+      val toRemove = math.max(retainedExecutions / 10, 1)\n+      executions.take(toRemove).foreach { execution =>\n+        for (executionUIData <- executionIdToData.remove(execution.executionId)) {\n+          for (jobId <- executionUIData.jobs.keys) {\n+            jobIdToExecutionId.remove(jobId)\n+          }\n+          for (stageId <- executionUIData.stages) {\n+            stageIdToStageMetrics.remove(stageId)\n+          }\n+        }\n+      }\n+      executions.trimStart(toRemove)\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val executionId = jobStart.properties.getProperty(SparkSQLExecution.EXECUTION_ID_KEY)\n+    if (executionId == null) {\n+      // This is not a job created by SQL\n+      return\n+    }\n+    val jobId = jobStart.jobId\n+    val stageIds = jobStart.stageIds\n+\n+    synchronized {\n+      activeExecutions.get(executionId.toLong).foreach { executionUIData =>\n+        executionUIData.jobs(jobId) = JobExecutionStatus.RUNNING\n+        executionUIData.stages ++= stageIds\n+        // attemptId must be 0. Right?\n+        stageIds.foreach(stageId =>\n+          stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId = 0))\n+        jobIdToExecutionId(jobId) = executionUIData.executionId\n+      }\n+    }\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = synchronized {\n+    val jobId = jobEnd.jobId\n+    for (executionId <- jobIdToExecutionId.get(jobId);\n+         executionUIData <- executionIdToData.get(executionId)) {\n+      jobEnd.jobResult match {\n+        case JobSucceeded => executionUIData.jobs(jobId) = JobExecutionStatus.SUCCEEDED\n+        case JobFailed(_) => executionUIData.jobs(jobId) = JobExecutionStatus.FAILED\n+      }\n+    }\n+  }\n+\n+  override def onExecutorMetricsUpdate(\n+      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = synchronized {\n+    for ((taskId, stageId, stageAttemptID, metrics) <- executorMetricsUpdate.taskMetrics) {\n+      updateTaskMetrics(taskId, stageId, stageAttemptID, metrics, false)\n+    }\n+  }\n+\n+  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = synchronized {\n+    val stageId = stageSubmitted.stageInfo.stageId\n+    val stageAttemptId = stageSubmitted.stageInfo.attemptId\n+    // Always override metrics for old stage attempt\n+    stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId)\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    updateTaskMetrics(\n+      taskEnd.taskInfo.taskId, taskEnd.stageId, taskEnd.stageAttemptId, taskEnd.taskMetrics, true)\n+  }\n+\n+  private def updateTaskMetrics(taskId: Long, stageId: Int, stageAttemptID: Int,\n+      metrics: TaskMetrics, finishTask: Boolean): Unit = {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "(also other methods in this class)\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T06:01:12Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{AccumulatorParam, JobExecutionStatus}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SparkSQLExecution\n+\n+private[sql] class SQLSparkListener(sqlContext: SQLContext) extends SparkListener {\n+\n+  private val retainedExecutions =\n+    sqlContext.sparkContext.conf.getInt(\"spark.sql.ui.retainedExecutions\", 1000)\n+\n+  private val activeExecutions = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  // Old data in the following fields must be removed in \"trimExecutionsIfNecessary\".\n+  // If adding new fields, make sure \"trimExecutionsIfNecessary\" can clean up old data\n+  private val executionIdToData = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  /**\n+   * Maintain the relation between job id and execution id so that we can get the execution id in\n+   * the \"onJobEnd\" method.\n+   */\n+  private val jobIdToExecutionId = mutable.HashMap[Long, Long]()\n+\n+  private val stageIdToStageMetrics = mutable.HashMap[Long, SQLStageMetrics]()\n+\n+  private val failedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private val completedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private def trimExecutionsIfNecessary(\n+      executions: mutable.ListBuffer[SparkSQLExecutionUIData]): Unit = {\n+    if (executions.size > retainedExecutions) {\n+      val toRemove = math.max(retainedExecutions / 10, 1)\n+      executions.take(toRemove).foreach { execution =>\n+        for (executionUIData <- executionIdToData.remove(execution.executionId)) {\n+          for (jobId <- executionUIData.jobs.keys) {\n+            jobIdToExecutionId.remove(jobId)\n+          }\n+          for (stageId <- executionUIData.stages) {\n+            stageIdToStageMetrics.remove(stageId)\n+          }\n+        }\n+      }\n+      executions.trimStart(toRemove)\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val executionId = jobStart.properties.getProperty(SparkSQLExecution.EXECUTION_ID_KEY)\n+    if (executionId == null) {\n+      // This is not a job created by SQL\n+      return\n+    }\n+    val jobId = jobStart.jobId\n+    val stageIds = jobStart.stageIds\n+\n+    synchronized {\n+      activeExecutions.get(executionId.toLong).foreach { executionUIData =>\n+        executionUIData.jobs(jobId) = JobExecutionStatus.RUNNING\n+        executionUIData.stages ++= stageIds\n+        // attemptId must be 0. Right?\n+        stageIds.foreach(stageId =>\n+          stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId = 0))\n+        jobIdToExecutionId(jobId) = executionUIData.executionId\n+      }\n+    }\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = synchronized {\n+    val jobId = jobEnd.jobId\n+    for (executionId <- jobIdToExecutionId.get(jobId);\n+         executionUIData <- executionIdToData.get(executionId)) {\n+      jobEnd.jobResult match {\n+        case JobSucceeded => executionUIData.jobs(jobId) = JobExecutionStatus.SUCCEEDED\n+        case JobFailed(_) => executionUIData.jobs(jobId) = JobExecutionStatus.FAILED\n+      }\n+    }\n+  }\n+\n+  override def onExecutorMetricsUpdate(\n+      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = synchronized {\n+    for ((taskId, stageId, stageAttemptID, metrics) <- executorMetricsUpdate.taskMetrics) {\n+      updateTaskMetrics(taskId, stageId, stageAttemptID, metrics, false)\n+    }\n+  }\n+\n+  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = synchronized {\n+    val stageId = stageSubmitted.stageInfo.stageId\n+    val stageAttemptId = stageSubmitted.stageInfo.attemptId\n+    // Always override metrics for old stage attempt\n+    stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId)\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    updateTaskMetrics(\n+      taskEnd.taskInfo.taskId, taskEnd.stageId, taskEnd.stageAttemptId, taskEnd.taskMetrics, true)\n+  }\n+\n+  private def updateTaskMetrics(taskId: Long, stageId: Int, stageAttemptID: Int,\n+      metrics: TaskMetrics, finishTask: Boolean): Unit = {"
  }],
  "prId": 7774
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "`jobs.filter { case (_, status) => status == ... }.keys.toSeq`\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T06:04:19Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{AccumulatorParam, JobExecutionStatus}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SparkSQLExecution\n+\n+private[sql] class SQLSparkListener(sqlContext: SQLContext) extends SparkListener {\n+\n+  private val retainedExecutions =\n+    sqlContext.sparkContext.conf.getInt(\"spark.sql.ui.retainedExecutions\", 1000)\n+\n+  private val activeExecutions = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  // Old data in the following fields must be removed in \"trimExecutionsIfNecessary\".\n+  // If adding new fields, make sure \"trimExecutionsIfNecessary\" can clean up old data\n+  private val executionIdToData = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  /**\n+   * Maintain the relation between job id and execution id so that we can get the execution id in\n+   * the \"onJobEnd\" method.\n+   */\n+  private val jobIdToExecutionId = mutable.HashMap[Long, Long]()\n+\n+  private val stageIdToStageMetrics = mutable.HashMap[Long, SQLStageMetrics]()\n+\n+  private val failedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private val completedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private def trimExecutionsIfNecessary(\n+      executions: mutable.ListBuffer[SparkSQLExecutionUIData]): Unit = {\n+    if (executions.size > retainedExecutions) {\n+      val toRemove = math.max(retainedExecutions / 10, 1)\n+      executions.take(toRemove).foreach { execution =>\n+        for (executionUIData <- executionIdToData.remove(execution.executionId)) {\n+          for (jobId <- executionUIData.jobs.keys) {\n+            jobIdToExecutionId.remove(jobId)\n+          }\n+          for (stageId <- executionUIData.stages) {\n+            stageIdToStageMetrics.remove(stageId)\n+          }\n+        }\n+      }\n+      executions.trimStart(toRemove)\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val executionId = jobStart.properties.getProperty(SparkSQLExecution.EXECUTION_ID_KEY)\n+    if (executionId == null) {\n+      // This is not a job created by SQL\n+      return\n+    }\n+    val jobId = jobStart.jobId\n+    val stageIds = jobStart.stageIds\n+\n+    synchronized {\n+      activeExecutions.get(executionId.toLong).foreach { executionUIData =>\n+        executionUIData.jobs(jobId) = JobExecutionStatus.RUNNING\n+        executionUIData.stages ++= stageIds\n+        // attemptId must be 0. Right?\n+        stageIds.foreach(stageId =>\n+          stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId = 0))\n+        jobIdToExecutionId(jobId) = executionUIData.executionId\n+      }\n+    }\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = synchronized {\n+    val jobId = jobEnd.jobId\n+    for (executionId <- jobIdToExecutionId.get(jobId);\n+         executionUIData <- executionIdToData.get(executionId)) {\n+      jobEnd.jobResult match {\n+        case JobSucceeded => executionUIData.jobs(jobId) = JobExecutionStatus.SUCCEEDED\n+        case JobFailed(_) => executionUIData.jobs(jobId) = JobExecutionStatus.FAILED\n+      }\n+    }\n+  }\n+\n+  override def onExecutorMetricsUpdate(\n+      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = synchronized {\n+    for ((taskId, stageId, stageAttemptID, metrics) <- executorMetricsUpdate.taskMetrics) {\n+      updateTaskMetrics(taskId, stageId, stageAttemptID, metrics, false)\n+    }\n+  }\n+\n+  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = synchronized {\n+    val stageId = stageSubmitted.stageInfo.stageId\n+    val stageAttemptId = stageSubmitted.stageInfo.attemptId\n+    // Always override metrics for old stage attempt\n+    stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId)\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    updateTaskMetrics(\n+      taskEnd.taskInfo.taskId, taskEnd.stageId, taskEnd.stageAttemptId, taskEnd.taskMetrics, true)\n+  }\n+\n+  private def updateTaskMetrics(taskId: Long, stageId: Int, stageAttemptID: Int,\n+      metrics: TaskMetrics, finishTask: Boolean): Unit = {\n+    stageIdToStageMetrics.get(stageId) match {\n+      case Some(stageMetrics) =>\n+        if (stageAttemptID < stageMetrics.stageAttemptId) {\n+          // A task of an old stage attempt. Because a new stage is submitted, we can ignore it.\n+        } else if (stageAttemptID > stageMetrics.stageAttemptId) {\n+          // TODO A running task with a higher stageAttemptID??\n+        } else {\n+          // TODO We don't know the attemptId. Currently, what we can do is overriding the\n+          // accumulator updates. However, if there are two same task are running, such as\n+          // speculation, the accumulator updates will be overriding by different task attempts,\n+          // the results will be weird.\n+          stageMetrics.taskIdToMetricUpdates.get(taskId) match {\n+            case Some(taskMetrics) =>\n+              if (finishTask) {\n+                taskMetrics.finished = true\n+                taskMetrics.accumulatorUpdates = metrics.accumulatorUpdates()\n+              } else if (!taskMetrics.finished){\n+                // If a task is finished, we should not override with accumulator updates from\n+                // heartbeat reports\n+                taskMetrics.accumulatorUpdates = metrics.accumulatorUpdates()\n+              }\n+            case None =>\n+              // TODO Now just set attemptId to 0. Should fix here when we can get the attempt\n+              // id from SparkListenerExecutorMetricsUpdate\n+              stageMetrics.taskIdToMetricUpdates(taskId) =\n+                SQLTaskMetrics(attemptId = 0, finished = finishTask, metrics.accumulatorUpdates())\n+          }\n+        }\n+      case None =>\n+      // This execution and its stage have been dropped\n+    }\n+  }\n+\n+  def onExecutionStart(\n+      executionId: Long, description: String, details: String, df: DataFrame, time: Long): Unit = {\n+    val physicalPlanDescription = df.queryExecution.toString\n+    val physicalPlanGraph = SparkPlanGraph(df.queryExecution.executedPlan)\n+    val metrics = physicalPlanGraph.nodes.flatMap { node =>\n+      node.metrics.map(metric => metric.accumulatorId -> metric)\n+    }\n+\n+    val executionUIData = SparkSQLExecutionUIData(executionId, description, details,\n+      physicalPlanDescription, physicalPlanGraph, metrics.toMap, time)\n+\n+    synchronized {\n+      activeExecutions(executionId) = executionUIData\n+      executionIdToData(executionId) = executionUIData\n+    }\n+  }\n+\n+  def onExecutionEnd(executionId: Long, time: Long): Unit = synchronized {\n+    activeExecutions.remove(executionId).foreach { executionUIData =>\n+      executionUIData.completionTime = Some(time)\n+      if (executionUIData.isFailed) {\n+        failedExecutions += executionUIData\n+        trimExecutionsIfNecessary(failedExecutions)\n+      } else {\n+        completedExecutions += executionUIData\n+        trimExecutionsIfNecessary(completedExecutions)\n+      }\n+    }\n+  }\n+\n+  def getRunningExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    activeExecutions.values.toSeq\n+  }\n+\n+  def getFailedExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    failedExecutions\n+  }\n+\n+  def getCompletedExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    completedExecutions\n+  }\n+\n+  def getExecution(executionId: Long): Option[SparkSQLExecutionUIData] = synchronized {\n+    executionIdToData.get(executionId)\n+  }\n+\n+  def getExecutionMetrics(executionId: Long): Map[Long, Any] = synchronized {\n+    executionIdToData.get(executionId) match {\n+      case Some(executionUIData) =>\n+        // Get all accumulator updates from all tasks which belong to this execution and merge them\n+        val accumulatorUpdates = {\n+          for (stageId <- executionUIData.stages;\n+               stageMetrics <- stageIdToStageMetrics.get(stageId).toIterable;\n+               taskMetrics <- stageMetrics.taskIdToMetricUpdates.values;\n+               accumulatorUpdate <- taskMetrics.accumulatorUpdates.toSeq)\n+            yield accumulatorUpdate\n+        }\n+        mergeAccumulatorUpdates(accumulatorUpdates, accumulatorId =>\n+          executionUIData.accumulatorMetrics(accumulatorId).accumulatorParam)\n+      case None =>\n+        // This execution has been dropped\n+        Map.empty\n+    }\n+  }\n+\n+  private def mergeAccumulatorUpdates(\n+     accumulatorUpdates: Seq[(Long, Any)],\n+     paramFunc: Long => AccumulatorParam[Any]): Map[Long, Any] = {\n+    accumulatorUpdates.groupBy(_._1).map { case (accumulatorId, values) =>\n+      val param = paramFunc(accumulatorId)\n+      (accumulatorId, values.map(_._2).reduceLeft(param.addInPlace))\n+    }\n+  }\n+\n+}\n+\n+/**\n+ * Represent all necessary data for an execution that will be used in Web UI.\n+ */\n+private[ui] case class SparkSQLExecutionUIData(\n+    executionId: Long,\n+    description: String,\n+    details: String,\n+    physicalPlanDescription: String,\n+    physicalPlanGraph: SparkPlanGraph,\n+    accumulatorMetrics: Map[Long, SQLPlanMetric],\n+    submissionTime: Long,\n+    var completionTime: Option[Long] = None,\n+    jobs: mutable.HashMap[Long, JobExecutionStatus] = mutable.HashMap.empty,\n+    stages: mutable.ArrayBuffer[Int] = mutable.ArrayBuffer()) {\n+\n+  def isFailed: Boolean = jobs.exists(_._2 == JobExecutionStatus.FAILED)\n+\n+  def runningJobs: Seq[Long] = jobs.filter(_._2 == JobExecutionStatus.RUNNING).map(_._1).toSeq\n+\n+  def succeededJobs: Seq[Long] = jobs.filter(_._2 == JobExecutionStatus.SUCCEEDED).map(_._1).toSeq\n+\n+  def failedJobs: Seq[Long] = jobs.filter(_._2 == JobExecutionStatus.FAILED).map(_._1).toSeq"
  }],
  "prId": 7774
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "`jobs.values.exists(_ == FAILED)`\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T06:04:39Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{AccumulatorParam, JobExecutionStatus}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SparkSQLExecution\n+\n+private[sql] class SQLSparkListener(sqlContext: SQLContext) extends SparkListener {\n+\n+  private val retainedExecutions =\n+    sqlContext.sparkContext.conf.getInt(\"spark.sql.ui.retainedExecutions\", 1000)\n+\n+  private val activeExecutions = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  // Old data in the following fields must be removed in \"trimExecutionsIfNecessary\".\n+  // If adding new fields, make sure \"trimExecutionsIfNecessary\" can clean up old data\n+  private val executionIdToData = mutable.HashMap[Long, SparkSQLExecutionUIData]()\n+\n+  /**\n+   * Maintain the relation between job id and execution id so that we can get the execution id in\n+   * the \"onJobEnd\" method.\n+   */\n+  private val jobIdToExecutionId = mutable.HashMap[Long, Long]()\n+\n+  private val stageIdToStageMetrics = mutable.HashMap[Long, SQLStageMetrics]()\n+\n+  private val failedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private val completedExecutions = mutable.ListBuffer[SparkSQLExecutionUIData]()\n+\n+  private def trimExecutionsIfNecessary(\n+      executions: mutable.ListBuffer[SparkSQLExecutionUIData]): Unit = {\n+    if (executions.size > retainedExecutions) {\n+      val toRemove = math.max(retainedExecutions / 10, 1)\n+      executions.take(toRemove).foreach { execution =>\n+        for (executionUIData <- executionIdToData.remove(execution.executionId)) {\n+          for (jobId <- executionUIData.jobs.keys) {\n+            jobIdToExecutionId.remove(jobId)\n+          }\n+          for (stageId <- executionUIData.stages) {\n+            stageIdToStageMetrics.remove(stageId)\n+          }\n+        }\n+      }\n+      executions.trimStart(toRemove)\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val executionId = jobStart.properties.getProperty(SparkSQLExecution.EXECUTION_ID_KEY)\n+    if (executionId == null) {\n+      // This is not a job created by SQL\n+      return\n+    }\n+    val jobId = jobStart.jobId\n+    val stageIds = jobStart.stageIds\n+\n+    synchronized {\n+      activeExecutions.get(executionId.toLong).foreach { executionUIData =>\n+        executionUIData.jobs(jobId) = JobExecutionStatus.RUNNING\n+        executionUIData.stages ++= stageIds\n+        // attemptId must be 0. Right?\n+        stageIds.foreach(stageId =>\n+          stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId = 0))\n+        jobIdToExecutionId(jobId) = executionUIData.executionId\n+      }\n+    }\n+  }\n+\n+  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = synchronized {\n+    val jobId = jobEnd.jobId\n+    for (executionId <- jobIdToExecutionId.get(jobId);\n+         executionUIData <- executionIdToData.get(executionId)) {\n+      jobEnd.jobResult match {\n+        case JobSucceeded => executionUIData.jobs(jobId) = JobExecutionStatus.SUCCEEDED\n+        case JobFailed(_) => executionUIData.jobs(jobId) = JobExecutionStatus.FAILED\n+      }\n+    }\n+  }\n+\n+  override def onExecutorMetricsUpdate(\n+      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = synchronized {\n+    for ((taskId, stageId, stageAttemptID, metrics) <- executorMetricsUpdate.taskMetrics) {\n+      updateTaskMetrics(taskId, stageId, stageAttemptID, metrics, false)\n+    }\n+  }\n+\n+  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = synchronized {\n+    val stageId = stageSubmitted.stageInfo.stageId\n+    val stageAttemptId = stageSubmitted.stageInfo.attemptId\n+    // Always override metrics for old stage attempt\n+    stageIdToStageMetrics(stageId) = SQLStageMetrics(stageAttemptId)\n+  }\n+\n+  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized {\n+    updateTaskMetrics(\n+      taskEnd.taskInfo.taskId, taskEnd.stageId, taskEnd.stageAttemptId, taskEnd.taskMetrics, true)\n+  }\n+\n+  private def updateTaskMetrics(taskId: Long, stageId: Int, stageAttemptID: Int,\n+      metrics: TaskMetrics, finishTask: Boolean): Unit = {\n+    stageIdToStageMetrics.get(stageId) match {\n+      case Some(stageMetrics) =>\n+        if (stageAttemptID < stageMetrics.stageAttemptId) {\n+          // A task of an old stage attempt. Because a new stage is submitted, we can ignore it.\n+        } else if (stageAttemptID > stageMetrics.stageAttemptId) {\n+          // TODO A running task with a higher stageAttemptID??\n+        } else {\n+          // TODO We don't know the attemptId. Currently, what we can do is overriding the\n+          // accumulator updates. However, if there are two same task are running, such as\n+          // speculation, the accumulator updates will be overriding by different task attempts,\n+          // the results will be weird.\n+          stageMetrics.taskIdToMetricUpdates.get(taskId) match {\n+            case Some(taskMetrics) =>\n+              if (finishTask) {\n+                taskMetrics.finished = true\n+                taskMetrics.accumulatorUpdates = metrics.accumulatorUpdates()\n+              } else if (!taskMetrics.finished){\n+                // If a task is finished, we should not override with accumulator updates from\n+                // heartbeat reports\n+                taskMetrics.accumulatorUpdates = metrics.accumulatorUpdates()\n+              }\n+            case None =>\n+              // TODO Now just set attemptId to 0. Should fix here when we can get the attempt\n+              // id from SparkListenerExecutorMetricsUpdate\n+              stageMetrics.taskIdToMetricUpdates(taskId) =\n+                SQLTaskMetrics(attemptId = 0, finished = finishTask, metrics.accumulatorUpdates())\n+          }\n+        }\n+      case None =>\n+      // This execution and its stage have been dropped\n+    }\n+  }\n+\n+  def onExecutionStart(\n+      executionId: Long, description: String, details: String, df: DataFrame, time: Long): Unit = {\n+    val physicalPlanDescription = df.queryExecution.toString\n+    val physicalPlanGraph = SparkPlanGraph(df.queryExecution.executedPlan)\n+    val metrics = physicalPlanGraph.nodes.flatMap { node =>\n+      node.metrics.map(metric => metric.accumulatorId -> metric)\n+    }\n+\n+    val executionUIData = SparkSQLExecutionUIData(executionId, description, details,\n+      physicalPlanDescription, physicalPlanGraph, metrics.toMap, time)\n+\n+    synchronized {\n+      activeExecutions(executionId) = executionUIData\n+      executionIdToData(executionId) = executionUIData\n+    }\n+  }\n+\n+  def onExecutionEnd(executionId: Long, time: Long): Unit = synchronized {\n+    activeExecutions.remove(executionId).foreach { executionUIData =>\n+      executionUIData.completionTime = Some(time)\n+      if (executionUIData.isFailed) {\n+        failedExecutions += executionUIData\n+        trimExecutionsIfNecessary(failedExecutions)\n+      } else {\n+        completedExecutions += executionUIData\n+        trimExecutionsIfNecessary(completedExecutions)\n+      }\n+    }\n+  }\n+\n+  def getRunningExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    activeExecutions.values.toSeq\n+  }\n+\n+  def getFailedExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    failedExecutions\n+  }\n+\n+  def getCompletedExecutions: Seq[SparkSQLExecutionUIData] = synchronized {\n+    completedExecutions\n+  }\n+\n+  def getExecution(executionId: Long): Option[SparkSQLExecutionUIData] = synchronized {\n+    executionIdToData.get(executionId)\n+  }\n+\n+  def getExecutionMetrics(executionId: Long): Map[Long, Any] = synchronized {\n+    executionIdToData.get(executionId) match {\n+      case Some(executionUIData) =>\n+        // Get all accumulator updates from all tasks which belong to this execution and merge them\n+        val accumulatorUpdates = {\n+          for (stageId <- executionUIData.stages;\n+               stageMetrics <- stageIdToStageMetrics.get(stageId).toIterable;\n+               taskMetrics <- stageMetrics.taskIdToMetricUpdates.values;\n+               accumulatorUpdate <- taskMetrics.accumulatorUpdates.toSeq)\n+            yield accumulatorUpdate\n+        }\n+        mergeAccumulatorUpdates(accumulatorUpdates, accumulatorId =>\n+          executionUIData.accumulatorMetrics(accumulatorId).accumulatorParam)\n+      case None =>\n+        // This execution has been dropped\n+        Map.empty\n+    }\n+  }\n+\n+  private def mergeAccumulatorUpdates(\n+     accumulatorUpdates: Seq[(Long, Any)],\n+     paramFunc: Long => AccumulatorParam[Any]): Map[Long, Any] = {\n+    accumulatorUpdates.groupBy(_._1).map { case (accumulatorId, values) =>\n+      val param = paramFunc(accumulatorId)\n+      (accumulatorId, values.map(_._2).reduceLeft(param.addInPlace))\n+    }\n+  }\n+\n+}\n+\n+/**\n+ * Represent all necessary data for an execution that will be used in Web UI.\n+ */\n+private[ui] case class SparkSQLExecutionUIData(\n+    executionId: Long,\n+    description: String,\n+    details: String,\n+    physicalPlanDescription: String,\n+    physicalPlanGraph: SparkPlanGraph,\n+    accumulatorMetrics: Map[Long, SQLPlanMetric],\n+    submissionTime: Long,\n+    var completionTime: Option[Long] = None,\n+    jobs: mutable.HashMap[Long, JobExecutionStatus] = mutable.HashMap.empty,\n+    stages: mutable.ArrayBuffer[Int] = mutable.ArrayBuffer()) {\n+\n+  def isFailed: Boolean = jobs.exists(_._2 == JobExecutionStatus.FAILED)"
  }],
  "prId": 7774
}]