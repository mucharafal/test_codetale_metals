[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: tutorials -> testing (i know it was like that, but lets fix it since we are changing it anyway)",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-09T02:32:44Z",
    "diffHunk": "@@ -15,40 +15,48 @@\n  * limitations under the License.\n  */\n \n-package org.apache.spark.sql.execution.streaming\n+package org.apache.spark.sql.execution.streaming.sources\n \n import java.io.{BufferedReader, InputStreamReader, IOException}\n import java.net.Socket\n import java.sql.Timestamp\n import java.text.SimpleDateFormat\n-import java.util.{Calendar, Locale}\n+import java.util.{Calendar, List => JList, Locale, Optional}\n import javax.annotation.concurrent.GuardedBy\n \n+import scala.collection.JavaConverters._\n import scala.collection.mutable.ListBuffer\n import scala.util.{Failure, Success, Try}\n \n import org.apache.spark.internal.Logging\n import org.apache.spark.sql._\n-import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.sources.{DataSourceRegister, StreamSourceProvider}\n+import org.apache.spark.sql.execution.streaming.LongOffset\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, DataReaderFactory, MicroBatchReadSupport}\n+import org.apache.spark.sql.sources.v2.reader.streaming.{MicroBatchReader, Offset}\n import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n-import org.apache.spark.unsafe.types.UTF8String\n \n-\n-object TextSocketSource {\n+object TextSocketMicroBatchReader {\n   val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n   val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n     StructField(\"timestamp\", TimestampType) :: Nil)\n   val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n }\n \n /**\n- * A source that reads text lines through a TCP socket, designed only for tutorials and debugging.\n- * This source will *not* work in production applications due to multiple reasons, including no\n- * support for fault recovery and keeping all of the text read in memory forever.\n+ * A MicroBatchReader that reads text lines through a TCP socket, designed only for tutorials and",
    "line": 44
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Tutorials is correct here; see e.g. StructuredSessionization.scala",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-12T22:54:37Z",
    "diffHunk": "@@ -15,40 +15,48 @@\n  * limitations under the License.\n  */\n \n-package org.apache.spark.sql.execution.streaming\n+package org.apache.spark.sql.execution.streaming.sources\n \n import java.io.{BufferedReader, InputStreamReader, IOException}\n import java.net.Socket\n import java.sql.Timestamp\n import java.text.SimpleDateFormat\n-import java.util.{Calendar, Locale}\n+import java.util.{Calendar, List => JList, Locale, Optional}\n import javax.annotation.concurrent.GuardedBy\n \n+import scala.collection.JavaConverters._\n import scala.collection.mutable.ListBuffer\n import scala.util.{Failure, Success, Try}\n \n import org.apache.spark.internal.Logging\n import org.apache.spark.sql._\n-import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.sources.{DataSourceRegister, StreamSourceProvider}\n+import org.apache.spark.sql.execution.streaming.LongOffset\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, DataReaderFactory, MicroBatchReadSupport}\n+import org.apache.spark.sql.sources.v2.reader.streaming.{MicroBatchReader, Offset}\n import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n-import org.apache.spark.unsafe.types.UTF8String\n \n-\n-object TextSocketSource {\n+object TextSocketMicroBatchReader {\n   val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n   val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n     StructField(\"timestamp\", TimestampType) :: Nil)\n   val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n }\n \n /**\n- * A source that reads text lines through a TCP socket, designed only for tutorials and debugging.\n- * This source will *not* work in production applications due to multiple reasons, including no\n- * support for fault recovery and keeping all of the text read in memory forever.\n+ * A MicroBatchReader that reads text lines through a TCP socket, designed only for tutorials and",
    "line": 44
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "this does not keep it forever. so remove this reason, just keep \"no support for fault recover\".",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-09T02:33:56Z",
    "diffHunk": "@@ -15,40 +15,48 @@\n  * limitations under the License.\n  */\n \n-package org.apache.spark.sql.execution.streaming\n+package org.apache.spark.sql.execution.streaming.sources\n \n import java.io.{BufferedReader, InputStreamReader, IOException}\n import java.net.Socket\n import java.sql.Timestamp\n import java.text.SimpleDateFormat\n-import java.util.{Calendar, Locale}\n+import java.util.{Calendar, List => JList, Locale, Optional}\n import javax.annotation.concurrent.GuardedBy\n \n+import scala.collection.JavaConverters._\n import scala.collection.mutable.ListBuffer\n import scala.util.{Failure, Success, Try}\n \n import org.apache.spark.internal.Logging\n import org.apache.spark.sql._\n-import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.sources.{DataSourceRegister, StreamSourceProvider}\n+import org.apache.spark.sql.execution.streaming.LongOffset\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, DataReaderFactory, MicroBatchReadSupport}\n+import org.apache.spark.sql.sources.v2.reader.streaming.{MicroBatchReader, Offset}\n import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n-import org.apache.spark.unsafe.types.UTF8String\n \n-\n-object TextSocketSource {\n+object TextSocketMicroBatchReader {\n   val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n   val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n     StructField(\"timestamp\", TimestampType) :: Nil)\n   val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n }\n \n /**\n- * A source that reads text lines through a TCP socket, designed only for tutorials and debugging.\n- * This source will *not* work in production applications due to multiple reasons, including no\n- * support for fault recovery and keeping all of the text read in memory forever.\n+ * A MicroBatchReader that reads text lines through a TCP socket, designed only for tutorials and\n+ * debugging. This MicroBatchReader will *not* work in production applications due to multiple\n+ * reasons, including no support for fault recovery and keeping all of the text read in memory\n+ * forever."
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "supernit: is there need for a variable here?",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-09T02:37:21Z",
    "diffHunk": "@@ -103,23 +111,40 @@ class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlCo\n     readThread.start()\n   }\n \n-  /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n+  override def setOffsetRange(\n+      start: Optional[Offset],\n+      end: Optional[Offset]): Unit = synchronized {\n+    startOffset = start.orElse(LongOffset(-1L))\n+    endOffset = end.orElse(currentOffset)\n+  }\n \n-  override def getOffset: Option[Offset] = synchronized {\n-    if (currentOffset.offset == -1) {\n-      None\n+  override def getStartOffset(): Offset = {\n+    Option(startOffset).getOrElse(throw new IllegalStateException(\"start offset not set\"))\n+  }\n+\n+  override def getEndOffset(): Offset = {\n+    Option(endOffset).getOrElse(throw new IllegalStateException(\"end offset not set\"))\n+  }\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    LongOffset(json.toLong)\n+  }\n+\n+  override def readSchema(): StructType = {\n+    val includeTimestamp = options.getBoolean(\"includeTimestamp\", false)"
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: wont this fit on a single line?",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-09T02:37:48Z",
    "diffHunk": "@@ -103,23 +111,40 @@ class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlCo\n     readThread.start()\n   }\n \n-  /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n+  override def setOffsetRange(\n+      start: Optional[Offset],"
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This shows up in the StreamingQueryProgressEvent as description, so it may be better to have it as \"TextSocket[...\"",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-09T02:46:17Z",
    "diffHunk": "@@ -164,54 +213,43 @@ class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlCo\n     }\n   }\n \n-  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+  override def toString: String = s\"TextSocketMicroBatchReader[host: $host, port: $port]\""
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "why not check it as DataSourceOptions (which is known to be case-insensitive) rather than a map which raises questions about case sensitivity?",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-09T02:50:26Z",
    "diffHunk": "@@ -164,54 +213,43 @@ class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlCo\n     }\n   }\n \n-  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+  override def toString: String = s\"TextSocketMicroBatchReader[host: $host, port: $port]\"\n }\n \n-class TextSocketSourceProvider extends StreamSourceProvider with DataSourceRegister with Logging {\n-  private def parseIncludeTimestamp(params: Map[String, String]): Boolean = {\n-    Try(params.getOrElse(\"includeTimestamp\", \"false\").toBoolean) match {\n-      case Success(bool) => bool\n-      case Failure(_) =>\n-        throw new AnalysisException(\"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\")\n-    }\n-  }\n+class TextSocketSourceProvider extends DataSourceV2\n+  with MicroBatchReadSupport with DataSourceRegister with Logging {\n \n-  /** Returns the name and schema of the source that can be used to continually read data. */\n-  override def sourceSchema(\n-      sqlContext: SQLContext,\n-      schema: Option[StructType],\n-      providerName: String,\n-      parameters: Map[String, String]): (String, StructType) = {\n+  private def checkParameters(params: Map[String, String]): Unit = {\n     logWarning(\"The socket source should not be used for production applications! \" +\n       \"It does not support recovery.\")\n-    if (!parameters.contains(\"host\")) {\n+    if (!params.contains(\"host\")) {\n       throw new AnalysisException(\"Set a host to read from with option(\\\"host\\\", ...).\")\n     }\n-    if (!parameters.contains(\"port\")) {\n+    if (!params.contains(\"port\")) {\n       throw new AnalysisException(\"Set a port to read from with option(\\\"port\\\", ...).\")\n     }\n-    if (schema.nonEmpty) {\n-      throw new AnalysisException(\"The socket source does not support a user-specified schema.\")\n+    Try {\n+      params.get(\"includeTimestamp\")\n+        .orElse(params.get(\"includetimestamp\"))\n+        .getOrElse(\"false\")\n+        .toBoolean\n+    } match {\n+      case Success(_) =>\n+      case Failure(_) =>\n+        throw new AnalysisException(\"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\")\n     }\n-\n-    val sourceSchema =\n-      if (parseIncludeTimestamp(parameters)) {\n-        TextSocketSource.SCHEMA_TIMESTAMP\n-      } else {\n-        TextSocketSource.SCHEMA_REGULAR\n-      }\n-    (\"textSocket\", sourceSchema)\n   }\n \n-  override def createSource(\n-      sqlContext: SQLContext,\n-      metadataPath: String,\n-      schema: Option[StructType],\n-      providerName: String,\n-      parameters: Map[String, String]): Source = {\n-    val host = parameters(\"host\")\n-    val port = parameters(\"port\").toInt\n-    new TextSocketSource(host, port, parseIncludeTimestamp(parameters), sqlContext)\n+  override def createMicroBatchReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceOptions): MicroBatchReader = {\n+    checkParameters(options.asMap().asScala.toMap)"
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "this does not make sense. you are directly accessing something that should be accessed while synchronized on this. ",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-02-28T12:17:25Z",
    "diffHunk": "@@ -61,13 +68,13 @@ class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlCo\n    * Stored in a ListBuffer to facilitate removing committed batches.\n    */\n   @GuardedBy(\"this\")\n-  protected val batches = new ListBuffer[(String, Timestamp)]\n+  private val batches = new ListBuffer[(String, Timestamp)]\n \n   @GuardedBy(\"this\")\n-  protected var currentOffset: LongOffset = new LongOffset(-1)\n+  private[sources] var currentOffset: LongOffset = LongOffset(-1L)"
  }],
  "prId": 20382
}]