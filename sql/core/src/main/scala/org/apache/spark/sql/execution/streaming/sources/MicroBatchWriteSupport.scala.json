[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "This fixed a typo in the class name.",
    "commit": "847300f76391c5e171e3f54b21bf6f2efc177f0e",
    "createdAt": "2018-08-22T21:48:09Z",
    "diffHunk": "@@ -18,27 +18,38 @@\n package org.apache.spark.sql.execution.streaming.sources\n \n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.sources.v2.writer.{BatchWriteSupport, DataWriter, DataWriterFactory, WriterCommitMessage}\n-import org.apache.spark.sql.sources.v2.writer.streaming.{StreamingDataWriterFactory, StreamingWriteSupport}\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.writer.{BatchWriteSupport, DataWriter, DataWriterFactory, WriteConfig, WriterCommitMessage}\n+import org.apache.spark.sql.sources.v2.writer.streaming.{StreamingDataWriterFactory, StreamingWriteConfig, StreamingWriteSupport}\n+import org.apache.spark.sql.types.StructType\n \n /**\n  * A [[BatchWriteSupport]] used to hook V2 stream writers into a microbatch plan. It implements\n  * the non-streaming interface, forwarding the epoch ID determined at construction to a wrapped\n  * streaming write support.\n  */\n-class MicroBatchWritSupport(eppchId: Long, val writeSupport: StreamingWriteSupport)\n+class MicroBatchWriteSupport(eppchId: Long, val writeSupport: StreamingWriteSupport)",
    "line": 17
  }],
  "prId": 22190
}]