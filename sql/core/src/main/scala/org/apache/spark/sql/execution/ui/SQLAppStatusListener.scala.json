[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: it was copied & pasted but better to fix here. `covert` -> `convert`",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-23T07:49:26Z",
    "diffHunk": "@@ -425,12 +432,72 @@ private class LiveExecutionData(val executionId: Long) extends LiveEntity {\n }\n \n private class LiveStageMetrics(\n-    val stageId: Int,\n-    var attemptId: Int,\n-    val accumulatorIds: Set[Long],\n-    val taskMetrics: ConcurrentHashMap[Long, LiveTaskMetrics])\n-\n-private class LiveTaskMetrics(\n-    val ids: Array[Long],\n-    val values: Array[Long],\n-    val succeeded: Boolean)\n+    val attemptId: Int,\n+    val numTasks: Int,\n+    val accumulatorIds: Set[Long]) {\n+\n+  /**\n+   * Mapping of task IDs to the partition index they're computing. Note this may contain more\n+   * elements than the stage's number of tasks, if speculative execution is on.\n+   */\n+  private val taskIndices = new OpenHashMap[Long, Int]()\n+\n+  /** Bit set tracking which partition indices have been successfully computed. */\n+  private val completedParts = new mutable.BitSet()\n+\n+  /**\n+   * Task metrics values for the stage. Maps the metric ID to the metric values for each\n+   * partition. For each metric ID, there will be the same number of values as the number\n+   * of partitions. This relies on `SQLMetrics.stringValue` treating 0 as a neutral value,\n+   * independent of the actual metric type.\n+   */\n+  private val taskMetrics = new ConcurrentHashMap[Long, Array[Long]]()\n+\n+  def registerTask(taskId: Long, partIdx: Int): Unit = {\n+    taskIndices.update(taskId, partIdx)\n+  }\n+\n+  def updateTaskMetrics(\n+      taskId: Long,\n+      eventPartIdx: Int,\n+      finished: Boolean,\n+      accumUpdates: Seq[AccumulableInfo]): Unit = {\n+    val partIdx = if (eventPartIdx == -1) {\n+      if (!taskIndices.contains(taskId)) {\n+        // We probably missed the start event for the task, just ignore it.\n+        return\n+      }\n+      taskIndices(taskId)\n+    } else {\n+      // Here we can recover from a missing task start event. Just register the task again.\n+      registerTask(taskId, eventPartIdx)\n+      eventPartIdx\n+    }\n+\n+    if (completedParts.contains(partIdx)) {\n+      return\n+    }\n+\n+    accumUpdates\n+      .filter { acc => acc.update.isDefined && accumulatorIds.contains(acc.id) }\n+      .foreach { acc =>\n+        // In a live application, accumulators have Long values, but when reading from event\n+        // logs, they have String values. For now, assume all accumulators are Long and covert"
  }],
  "prId": 26218
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: might be better to have constant for `-1` to denote unknown partition index.",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-23T08:21:34Z",
    "diffHunk": "@@ -425,12 +432,72 @@ private class LiveExecutionData(val executionId: Long) extends LiveEntity {\n }\n \n private class LiveStageMetrics(\n-    val stageId: Int,\n-    var attemptId: Int,\n-    val accumulatorIds: Set[Long],\n-    val taskMetrics: ConcurrentHashMap[Long, LiveTaskMetrics])\n-\n-private class LiveTaskMetrics(\n-    val ids: Array[Long],\n-    val values: Array[Long],\n-    val succeeded: Boolean)\n+    val attemptId: Int,\n+    val numTasks: Int,\n+    val accumulatorIds: Set[Long]) {\n+\n+  /**\n+   * Mapping of task IDs to the partition index they're computing. Note this may contain more\n+   * elements than the stage's number of tasks, if speculative execution is on.\n+   */\n+  private val taskIndices = new OpenHashMap[Long, Int]()\n+\n+  /** Bit set tracking which partition indices have been successfully computed. */\n+  private val completedParts = new mutable.BitSet()\n+\n+  /**\n+   * Task metrics values for the stage. Maps the metric ID to the metric values for each\n+   * partition. For each metric ID, there will be the same number of values as the number\n+   * of partitions. This relies on `SQLMetrics.stringValue` treating 0 as a neutral value,\n+   * independent of the actual metric type.\n+   */\n+  private val taskMetrics = new ConcurrentHashMap[Long, Array[Long]]()\n+\n+  def registerTask(taskId: Long, partIdx: Int): Unit = {\n+    taskIndices.update(taskId, partIdx)\n+  }\n+\n+  def updateTaskMetrics(\n+      taskId: Long,\n+      eventPartIdx: Int,\n+      finished: Boolean,\n+      accumUpdates: Seq[AccumulableInfo]): Unit = {\n+    val partIdx = if (eventPartIdx == -1) {"
  }],
  "prId": 26218
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "~In `{ stage =>`, can we use `metrics` or `stageMetrics` or `liveStageMetrics` instead of `stage`?~\r\nNever mind.",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-24T23:03:50Z",
    "diffHunk": "@@ -118,9 +122,11 @@ class SQLAppStatusListener(\n     }\n \n     // Reset the metrics tracking object for the new attempt.\n-    Option(stageMetrics.get(event.stageInfo.stageId)).foreach { metrics =>\n-      metrics.taskMetrics.clear()\n-      metrics.attemptId = event.stageInfo.attemptNumber\n+    Option(stageMetrics.get(event.stageInfo.stageId)).foreach { stage =>",
    "line": 41
  }],
  "prId": 26218
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Actually, it seems that we need a little comment for this, @vanzin .\r\nThis PR defines the following.\r\n```scala\r\n  /**\r\n   * Mapping of task IDs to the partition index they're computing. Note this may contain more\r\n   * elements than the stage's number of tasks, if speculative execution is on.\r\n   */\r\n  private val taskIndices = new OpenHashMap[Long, Int]()\r\n```\r\n\r\nHowever, we are putting `TaskInfo.Index` which is defined like the following.\r\n```scala\r\n@DeveloperApi\r\nclass TaskInfo(\r\n    val taskId: Long,\r\n    /**\r\n     * The index of this task within its task set. Not necessarily the same as the ID of the RDD\r\n     * partition that the task is computing.\r\n     */\r\n    val index: Int,\r\n```\r\n\r\nI guess we can update the definition of `taskIndices` by using `task index` instead of `partition index`. How do you think about that?",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-24T23:22:37Z",
    "diffHunk": "@@ -140,7 +146,16 @@ class SQLAppStatusListener(\n \n   override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = {\n     event.accumUpdates.foreach { case (taskId, stageId, attemptId, accumUpdates) =>\n-      updateStageMetrics(stageId, attemptId, taskId, accumUpdates, false)\n+      updateStageMetrics(stageId, attemptId, taskId, SQLAppStatusListener.UNKNOWN_PARTITION,\n+        accumUpdates, false)\n+    }\n+  }\n+\n+  override def onTaskStart(event: SparkListenerTaskStart): Unit = {\n+    Option(stageMetrics.get(event.stageId)).foreach { stage =>\n+      if (stage.attemptId == event.stageAttemptId) {\n+        stage.registerTask(event.taskInfo.taskId, event.taskInfo.index)",
    "line": 62
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "See below. ",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-24T23:52:30Z",
    "diffHunk": "@@ -140,7 +146,16 @@ class SQLAppStatusListener(\n \n   override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = {\n     event.accumUpdates.foreach { case (taskId, stageId, attemptId, accumUpdates) =>\n-      updateStageMetrics(stageId, attemptId, taskId, accumUpdates, false)\n+      updateStageMetrics(stageId, attemptId, taskId, SQLAppStatusListener.UNKNOWN_PARTITION,\n+        accumUpdates, false)\n+    }\n+  }\n+\n+  override def onTaskStart(event: SparkListenerTaskStart): Unit = {\n+    Option(stageMetrics.get(event.stageId)).foreach { stage =>\n+      if (stage.attemptId == event.stageAttemptId) {\n+        stage.registerTask(event.taskInfo.taskId, event.taskInfo.index)",
    "line": 62
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thank you for the explanation. However, simply put, when `taskIndices` maintain a copy of `TaskInfo.taskId` and `TaskInfo.index`, can we call them, `taskId` and `taskIndex`, literally? The variable name is already _task**Indices**_.",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-25T00:10:53Z",
    "diffHunk": "@@ -140,7 +146,16 @@ class SQLAppStatusListener(\n \n   override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = {\n     event.accumUpdates.foreach { case (taskId, stageId, attemptId, accumUpdates) =>\n-      updateStageMetrics(stageId, attemptId, taskId, accumUpdates, false)\n+      updateStageMetrics(stageId, attemptId, taskId, SQLAppStatusListener.UNKNOWN_PARTITION,\n+        accumUpdates, false)\n+    }\n+  }\n+\n+  override def onTaskStart(event: SparkListenerTaskStart): Unit = {\n+    Option(stageMetrics.get(event.stageId)).foreach { stage =>\n+      if (stage.attemptId == event.stageAttemptId) {\n+        stage.registerTask(event.taskInfo.taskId, event.taskInfo.index)",
    "line": 62
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Man, if that's going to make you drop this discussion, I'll call them \"pink unicorns\" if that's what you want. The variable name won't help you understand the relationship between partitions and indices and why it's important to track this.",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-25T00:53:13Z",
    "diffHunk": "@@ -140,7 +146,16 @@ class SQLAppStatusListener(\n \n   override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = {\n     event.accumUpdates.foreach { case (taskId, stageId, attemptId, accumUpdates) =>\n-      updateStageMetrics(stageId, attemptId, taskId, accumUpdates, false)\n+      updateStageMetrics(stageId, attemptId, taskId, SQLAppStatusListener.UNKNOWN_PARTITION,\n+        accumUpdates, false)\n+    }\n+  }\n+\n+  override def onTaskStart(event: SparkListenerTaskStart): Unit = {\n+    Option(stageMetrics.get(event.stageId)).foreach { stage =>\n+      if (stage.attemptId == event.stageAttemptId) {\n+        stage.registerTask(event.taskInfo.taskId, event.taskInfo.index)",
    "line": 62
  }],
  "prId": 26218
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`partIdx` -> `taskIdx`.",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-24T23:30:02Z",
    "diffHunk": "@@ -208,43 +246,13 @@ class SQLAppStatusListener(\n       stageId: Int,\n       attemptId: Int,\n       taskId: Long,\n+      partIdx: Int,"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "No. This is the index of the *partition* being computed.\r\n\r\nEDIT: actually let me look at your previous comment. I thought `task.index` was the partition index.",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-24T23:40:32Z",
    "diffHunk": "@@ -208,43 +246,13 @@ class SQLAppStatusListener(\n       stageId: Int,\n       attemptId: Int,\n       taskId: Long,\n+      partIdx: Int,"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "So this is what happens.\r\n\r\nWhen you don't have speculative tasks, then that comment in `TaskInfo` is true; in that it's the task's index in the task set.\r\n\r\nWhen you add speculative tasks, that becomes false; the new task will have the same index as some other task in the same task set. Speculation might have been added after `TaskInfo` and the comment never updated.\r\n\r\nSo the gist is that this `index` is basically a proxy for the partition index. Two tasks with the same index are computing the same partition. So I think it's more important to track the intent (that this field refers, even if indirectly, to what partition is being calculated) than trying to be exact about its meaning.",
    "commit": "6377c701620d9c6b344993a8bf4c2c1acd267c00",
    "createdAt": "2019-10-24T23:51:50Z",
    "diffHunk": "@@ -208,43 +246,13 @@ class SQLAppStatusListener(\n       stageId: Int,\n       attemptId: Int,\n       taskId: Long,\n+      partIdx: Int,"
  }],
  "prId": 26218
}]