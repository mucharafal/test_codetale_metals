[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Does this really help? once filter pushdown is applied, it's hard to make this rule idempotent. ",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T06:43:47Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join",
    "line": 201
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "This batch itself does not push down... but you have a point. Let's just keep it though.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T13:56:00Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join",
    "line": 201
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "what about `EqualNullSafe`?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T06:46:01Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)",
    "line": 223
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "Created a follow-up JIRA as https://issues.apache.org/jira/browse/SPARK-28959.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-03T14:40:18Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)",
    "line": 223
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Just think about it. Is EqualNullSafe different to EqualTo here? Can partition column be a null value? If it can not be, when building key is null, EqualTo's value is null, EqualNullSafe's value is false. For a Filter predicate, null is considered a false, isn't?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-10-01T00:34:41Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)",
    "line": 223
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "partition column can be null, we will use a special string to represent null in the file path. ",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-10-02T03:10:48Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)",
    "line": 223
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "oh, I see. `__HIVE_DEFAULT_PARTITION__`",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-10-02T03:29:32Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)",
    "line": 223
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "> ... and a filter on the dimension table\r\n\r\nwhat if the dimension table itself is very small?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T06:53:13Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)\n+              if fromDifferentSides(a, b) =>\n+            val (l, r) = if (a.references.subsetOf(left.outputSet) &&\n+              b.references.subsetOf(right.outputSet)) {\n+              a -> b\n+            } else {\n+              b -> a\n+            }\n+\n+            // there should be a partitioned table and a filter on the dimension table,",
    "line": 232
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "If the dimension table is smaller than the table(s) on build side, the `pruningHasBenefit` will return false, which means:\r\n1) the filter can still be planned as a broadcast pruning filter if it's eventually a BHJ with the expected build plan here as the build side, and that broadcast will be a reuse so hopefully not much overhead anyway;\r\n2) otherwise, since `pruningHasBenefit` is false, this filter will be turned into a bypass filter (constant `true`), so nothing will happen.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-29T20:32:47Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)\n+              if fromDifferentSides(a, b) =>\n+            val (l, r) = if (a.references.subsetOf(left.outputSet) &&\n+              b.references.subsetOf(right.outputSet)) {\n+              a -> b\n+            } else {\n+              b -> a\n+            }\n+\n+            // there should be a partitioned table and a filter on the dimension table,",
    "line": 232
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "> the filter can still be planned as a broadcast pruning filter if it's eventually a BHJ\r\n\r\nBut from the code, we only add the filter if `hasPartitionPruningFilter(r, right)` is true, which requires a seletive predicate on the build side.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-04T07:45:00Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)\n+              if fromDifferentSides(a, b) =>\n+            val (l, r) = if (a.references.subsetOf(left.outputSet) &&\n+              b.references.subsetOf(right.outputSet)) {\n+              a -> b\n+            } else {\n+              b -> a\n+            }\n+\n+            // there should be a partitioned table and a filter on the dimension table,",
    "line": 232
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "We can address it in a followup. The current check is more conservative.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-04T14:24:23Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan\n+   */\n+  private def hasPartitionPruningFilter(key: Expression, plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan) &&\n+      key.references.subsetOf(plan.outputSet)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)\n+              if fromDifferentSides(a, b) =>\n+            val (l, r) = if (a.references.subsetOf(left.outputSet) &&\n+              b.references.subsetOf(right.outputSet)) {\n+              a -> b\n+            } else {\n+              b -> a\n+            }\n+\n+            // there should be a partitioned table and a filter on the dimension table,",
    "line": 232
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "is it possible to violate this?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T06:55:09Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.joinFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningWithStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   *   (3) the join key is in the attribute set of the filtering plan"
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`we use a default value of 0.5` I think this is configurable.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T06:58:42Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5."
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "I'll update this comment",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-29T20:08:10Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit,\n+          broadcastHint),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use a default value of 0.5."
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: might be better to write `class PartitionPruning(conf: SQLConf)` instead of writing a lot of `SQLConf.get` in this rule.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T07:04:10Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {",
    "line": 49
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "I'd agree with you on that... but it gets confusing that a lot of optimization rules use SQLConf.get while others use conf as a param.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-29T20:06:15Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {",
    "line": 49
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`on the left side of the join` -> `on one side of the join`?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T07:06:57Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter"
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "hmm, when would users disable `dynamicPruningReuseBroadcast`?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-28T07:08:30Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {",
    "line": 91
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "It's just a fallback option and it's an internal conf.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-08-29T20:07:49Z",
    "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on the left side of the join using the filter\n+   * on the right side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run irrespective the type of join, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean,\n+      broadcastHint: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {",
    "line": 91
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "style: ident.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-04T18:42:26Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on one side of the join using the filter on the\n+   * other side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run regardless of the join strategy, or is too expensive and it should be run only if",
    "line": 79
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Are we always be certain that the references from partExpr and otherExpr can match one by one?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-04T18:45:05Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on one side of the join using the filter on the\n+   * other side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run regardless of the join strategy, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPartitionPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use the config value of\n+   * `spark.sql.optimizer.joinFilterRatio`.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.dynamicPartitionPruningFallbackFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {",
    "line": 129
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "No, we can't. So we fall back to the configured ratio if they don't match or have multiple attributes.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-04T19:27:17Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on one side of the join using the filter on the\n+   * other side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run regardless of the join strategy, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPartitionPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use the config value of\n+   * `spark.sql.optimizer.joinFilterRatio`.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.dynamicPartitionPruningFallbackFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {",
    "line": 129
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Don't we need to make sure the corresponding joining key r  is on the selective filter? seems hasPartitionPruningFilter only tells that right plan has a selective Filter, can't the Filter is selective on other join keys other than r?",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-04T18:59:32Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on one side of the join using the filter on the\n+   * other side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run regardless of the join strategy, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPartitionPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use the config value of\n+   * `spark.sql.optimizer.joinFilterRatio`.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.dynamicPartitionPruningFallbackFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningUseStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   */\n+  private def hasPartitionPruningFilter(plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)\n+              if fromDifferentSides(a, b) =>\n+            val (l, r) = if (a.references.subsetOf(left.outputSet) &&\n+              b.references.subsetOf(right.outputSet)) {\n+              a -> b\n+            } else {\n+              b -> a\n+            }\n+\n+            // there should be a partitioned table and a filter on the dimension table,\n+            // otherwise the pruning will not trigger\n+            var partScan = getPartitionTableScan(l, left)\n+            if (partScan.isDefined && canPruneLeft(joinType) &&\n+                hasPartitionPruningFilter(right)) {",
    "line": 236
  }, {
    "author": {
      "login": "maryannxue"
    },
    "body": "I don't think it matters. The reason why we need to check if we have a selective filter on the build plan is to make sure we have a rather small dataset to filter the partition columns with.",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-09-04T19:23:07Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and\n+ * selectivity of the join operation. During query optimization, we insert a\n+ * predicate on the partitioned table using the filter from the other side of\n+ * the join and a custom wrapper called DynamicPruning.\n+ *\n+ * The basic mechanism for DPP inserts a duplicated subquery with the filter from the other side,\n+ * when the following conditions are met:\n+ *    (1) the table to prune is partitioned by the JOIN key\n+ *    (2) the join operation is one of the following types: INNER, LEFT SEMI (partitioned on left),\n+ *    LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)\n+ *\n+ * In order to enable partition pruning directly in broadcasts, we use a custom DynamicPruning\n+ * clause that incorporates the In clause with the subquery and the benefit estimation.\n+ * During query planning, when the join type is known, we use the following mechanism:\n+ *    (1) if the join is a broadcast hash join, we replace the duplicated subquery with the reused\n+ *    results of the broadcast,\n+ *    (2) else if the estimated benefit of partition pruning outweighs the overhead of running the\n+ *    subquery query twice, we keep the duplicated subquery\n+ *    (3) otherwise, we drop the subquery.\n+ */\n+object PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n+\n+  /**\n+   * Search the partitioned table scan for a given partition column in a logical plan\n+   */\n+  def getPartitionTableScan(a: Expression, plan: LogicalPlan): Option[LogicalRelation] = {\n+    val srcInfo: Option[(Expression, LogicalPlan)] = findExpressionAndTrackLineageDown(a, plan)\n+    srcInfo.flatMap {\n+      case (resExp, l: LogicalRelation) =>\n+        l.relation match {\n+          case fs: HadoopFsRelation =>\n+            val partitionColumns = AttributeSet(\n+              l.resolve(fs.partitionSchema, fs.sparkSession.sessionState.analyzer.resolver))\n+            if (resExp.references.subsetOf(partitionColumns)) {\n+              return Some(l)\n+            } else {\n+              None\n+            }\n+          case _ => None\n+        }\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Insert a dynamic partition pruning predicate on one side of the join using the filter on the\n+   * other side of the join.\n+   *  - to be able to identify this filter during query planning, we use a custom\n+   *    DynamicPruning expression that wraps a regular In expression\n+   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n+   *  should run regardless of the join strategy, or is too expensive and it should be run only if\n+   *  we can reuse the results of a broadcast\n+   */\n+  private def insertPredicate(\n+      pruningKey: Expression,\n+      pruningPlan: LogicalPlan,\n+      filteringKey: Expression,\n+      filteringPlan: LogicalPlan,\n+      joinKeys: Seq[Expression],\n+      hasBenefit: Boolean): LogicalPlan = {\n+    val reuseEnabled = SQLConf.get.dynamicPartitionPruningReuseBroadcast\n+    val index = joinKeys.indexOf(filteringKey)\n+    if (hasBenefit || reuseEnabled) {\n+      // insert a DynamicPruning wrapper to identify the subquery during query planning\n+      Filter(\n+        DynamicPruningSubquery(\n+          pruningKey,\n+          filteringPlan,\n+          joinKeys,\n+          index,\n+          !hasBenefit),\n+        pruningPlan)\n+    } else {\n+      // abort dynamic partition pruning\n+      pruningPlan\n+    }\n+  }\n+\n+  /**\n+   * Given an estimated filtering ratio we assume the partition pruning has benefit if\n+   * the size in bytes of the partitioned plan after filtering is greater than the size\n+   * in bytes of the plan on the other side of the join. We estimate the filtering ratio\n+   * using column statistics if they are available, otherwise we use the config value of\n+   * `spark.sql.optimizer.joinFilterRatio`.\n+   */\n+  private def pruningHasBenefit(\n+      partExpr: Expression,\n+      partPlan: LogicalPlan,\n+      otherExpr: Expression,\n+      otherPlan: LogicalPlan): Boolean = {\n+\n+    // get the distinct counts of an attribute for a given table\n+    def distinctCounts(attr: Attribute, plan: LogicalPlan): Option[BigInt] = {\n+      plan.stats.attributeStats.get(attr).flatMap(_.distinctCount)\n+    }\n+\n+    // the default filtering ratio when CBO stats are missing, but there is a\n+    // predicate that is likely to be selective\n+    val fallbackRatio = SQLConf.get.dynamicPartitionPruningFallbackFilterRatio\n+    // the filtering ratio based on the type of the join condition and on the column statistics\n+    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n+      // filter out expressions with more than one attribute on any side of the operator\n+      case (leftAttr :: Nil, rightAttr :: Nil)\n+        if SQLConf.get.dynamicPartitionPruningUseStats =>\n+          // get the CBO stats for each attribute in the join condition\n+          val partDistinctCount = distinctCounts(leftAttr, partPlan)\n+          val otherDistinctCount = distinctCounts(rightAttr, otherPlan)\n+          val availableStats = partDistinctCount.isDefined && partDistinctCount.get > 0 &&\n+            otherDistinctCount.isDefined\n+          if (!availableStats) {\n+            fallbackRatio\n+          } else if (partDistinctCount.get.toDouble <= otherDistinctCount.get.toDouble) {\n+            // there is likely an estimation error, so we fallback\n+            fallbackRatio\n+          } else {\n+            1 - otherDistinctCount.get.toDouble / partDistinctCount.get.toDouble\n+          }\n+      case _ => fallbackRatio\n+    }\n+\n+    // the pruning overhead is the total size in bytes of all scan relations\n+    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat\n+\n+    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat\n+  }\n+\n+  /**\n+   * Returns whether an expression is likely to be selective\n+   */\n+  private def isLikelySelective(e: Expression): Boolean = e match {\n+    case Not(expr) => isLikelySelective(expr)\n+    case And(l, r) => isLikelySelective(l) || isLikelySelective(r)\n+    case Or(l, r) => isLikelySelective(l) && isLikelySelective(r)\n+    case Like(_, _) => true\n+    case _: BinaryComparison => true\n+    case _: In | _: InSet => true\n+    case _: StringPredicate => true\n+    case _ => false\n+  }\n+\n+  /**\n+   * Search a filtering predicate in a given logical plan\n+   */\n+  private def hasSelectivePredicate(plan: LogicalPlan): Boolean = {\n+    plan.find {\n+      case f: Filter => isLikelySelective(f.condition)\n+      case _ => false\n+    }.isDefined\n+  }\n+\n+  /**\n+   * To be able to prune partitions on a join key, the filtering side needs to\n+   * meet the following requirements:\n+   *   (1) it can not be a stream\n+   *   (2) it needs to contain a selective predicate used for filtering\n+   */\n+  private def hasPartitionPruningFilter(plan: LogicalPlan): Boolean = {\n+    !plan.isStreaming && hasSelectivePredicate(plan)\n+  }\n+\n+  private def canPruneLeft(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftSemi | RightOuter => true\n+    case _ => false\n+  }\n+\n+  private def canPruneRight(joinType: JoinType): Boolean = joinType match {\n+    case Inner | LeftOuter => true\n+    case _ => false\n+  }\n+\n+  private def prune(plan: LogicalPlan): LogicalPlan = {\n+    plan transformUp {\n+      // skip this rule if there's already a DPP subquery on the LHS of a join\n+      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n+      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j\n+      case j @ Join(left, right, joinType, Some(condition), hint) =>\n+        var newLeft = left\n+        var newRight = right\n+\n+        // extract the left and right keys of the join condition\n+        val (leftKeys, rightKeys) = j match {\n+          case ExtractEquiJoinKeys(_, lkeys, rkeys, _, _, _, _) => (lkeys, rkeys)\n+          case _ => (Nil, Nil)\n+        }\n+\n+        // checks if two expressions are on opposite sides of the join\n+        def fromDifferentSides(x: Expression, y: Expression): Boolean = {\n+          def fromLeftRight(x: Expression, y: Expression) =\n+            !x.references.isEmpty && x.references.subsetOf(left.outputSet) &&\n+              !y.references.isEmpty && y.references.subsetOf(right.outputSet)\n+          fromLeftRight(x, y) || fromLeftRight(y, x)\n+        }\n+\n+        splitConjunctivePredicates(condition).foreach {\n+          case EqualTo(a: Expression, b: Expression)\n+              if fromDifferentSides(a, b) =>\n+            val (l, r) = if (a.references.subsetOf(left.outputSet) &&\n+              b.references.subsetOf(right.outputSet)) {\n+              a -> b\n+            } else {\n+              b -> a\n+            }\n+\n+            // there should be a partitioned table and a filter on the dimension table,\n+            // otherwise the pruning will not trigger\n+            var partScan = getPartitionTableScan(l, left)\n+            if (partScan.isDefined && canPruneLeft(joinType) &&\n+                hasPartitionPruningFilter(right)) {",
    "line": 236
  }],
  "prId": 25600
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "it's best if you can give an example here. otherwise it's pretty dry and difficult to understand",
    "commit": "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "createdAt": "2019-10-02T20:57:29Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.dynamicpruning\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Dynamic partition pruning optimization is performed based on the type and",
    "line": 29
  }],
  "prId": 25600
}]