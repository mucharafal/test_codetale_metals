[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i think u need to add a new line here to not fail scalastyle\n",
    "commit": "22a01ae0fb9ee3c1b974acf3cbdca3abaf5e227e",
    "createdAt": "2015-07-20T18:08:49Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.TakeOrderedAndProject\n+import org.apache.spark.sql.sources.DataSourceStrategy\n+\n+@Experimental\n+class SparkPlanner(val sqlContext: SQLContext) extends org.apache.spark.sql.execution.SparkStrategies {\n+  val sparkContext: SparkContext = sqlContext.sparkContext\n+\n+  def codegenEnabled: Boolean = sqlContext.conf.codegenEnabled\n+\n+  def unsafeEnabled: Boolean = sqlContext.conf.unsafeEnabled\n+\n+  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+\n+  def strategies: Seq[Strategy] =\n+    sqlContext.experimental.extraStrategies ++ (\n+      DataSourceStrategy ::\n+        DDLStrategy ::\n+        TakeOrderedAndProject ::\n+        HashAggregation ::\n+        LeftSemiJoin ::\n+        HashJoin ::\n+        InMemoryScans ::\n+        ParquetOperations ::\n+        BasicOperators ::\n+        CartesianProduct ::\n+        BroadcastNestedLoopJoin :: Nil)\n+\n+  /**\n+   * Used to build table scan operators where complex projection and filtering are done using\n+   * separate physical operators.  This function returns the given scan operator with Project and\n+   * Filter nodes added only when needed.  For example, a Project operator is only used when the\n+   * final desired output requires complex expressions to be evaluated or when columns can be\n+   * further eliminated out after filtering has been done.\n+   *\n+   * The `prunePushedDownFilters` parameter is used to remove those filters that can be optimized\n+   * away by the filter pushdown optimization.\n+   *\n+   * The required attributes for both filtering and expression evaluation are passed to the\n+   * provided `scanBuilder` function so that it can avoid unnecessary column materialization.\n+   */\n+  def pruneFilterProject(\n+                          projectList: Seq[NamedExpression],\n+                          filterPredicates: Seq[Expression],\n+                          prunePushedDownFilters: Seq[Expression] => Seq[Expression],\n+                          scanBuilder: Seq[Attribute] => SparkPlan): SparkPlan = {\n+\n+    val projectSet = AttributeSet(projectList.flatMap(_.references))\n+    val filterSet = AttributeSet(filterPredicates.flatMap(_.references))\n+    val filterCondition =\n+      prunePushedDownFilters(filterPredicates).reduceLeftOption(catalyst.expressions.And)\n+\n+    // Right now we still use a projection even if the only evaluation is applying an alias\n+    // to a column.  Since this is a no-op, it could be avoided. However, using this\n+    // optimization with the current implementation would change the output schema.\n+    // TODO: Decouple final output schema from expression evaluation so this copy can be\n+    // avoided safely.\n+\n+    if (AttributeSet(projectList.map(_.toAttribute)) == projectSet &&\n+      filterSet.subsetOf(projectSet)) {\n+      // When it is possible to just use column pruning to get the right projection and\n+      // when the columns of this projection are enough to evaluate all filter conditions,\n+      // just do a scan followed by a filter, with no extra project.\n+      val scan = scanBuilder(projectList.asInstanceOf[Seq[Attribute]])\n+      filterCondition.map(Filter(_, scan)).getOrElse(scan)\n+    } else {\n+      val scan = scanBuilder((projectSet ++ filterSet).toSeq)\n+      Project(projectList, filterCondition.map(Filter(_, scan)).getOrElse(scan))\n+    }\n+  }\n+}",
    "line": 92
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "there might be more so u should check the output from jenkins\n",
    "commit": "22a01ae0fb9ee3c1b974acf3cbdca3abaf5e227e",
    "createdAt": "2015-07-20T18:09:02Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.TakeOrderedAndProject\n+import org.apache.spark.sql.sources.DataSourceStrategy\n+\n+@Experimental\n+class SparkPlanner(val sqlContext: SQLContext) extends org.apache.spark.sql.execution.SparkStrategies {\n+  val sparkContext: SparkContext = sqlContext.sparkContext\n+\n+  def codegenEnabled: Boolean = sqlContext.conf.codegenEnabled\n+\n+  def unsafeEnabled: Boolean = sqlContext.conf.unsafeEnabled\n+\n+  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+\n+  def strategies: Seq[Strategy] =\n+    sqlContext.experimental.extraStrategies ++ (\n+      DataSourceStrategy ::\n+        DDLStrategy ::\n+        TakeOrderedAndProject ::\n+        HashAggregation ::\n+        LeftSemiJoin ::\n+        HashJoin ::\n+        InMemoryScans ::\n+        ParquetOperations ::\n+        BasicOperators ::\n+        CartesianProduct ::\n+        BroadcastNestedLoopJoin :: Nil)\n+\n+  /**\n+   * Used to build table scan operators where complex projection and filtering are done using\n+   * separate physical operators.  This function returns the given scan operator with Project and\n+   * Filter nodes added only when needed.  For example, a Project operator is only used when the\n+   * final desired output requires complex expressions to be evaluated or when columns can be\n+   * further eliminated out after filtering has been done.\n+   *\n+   * The `prunePushedDownFilters` parameter is used to remove those filters that can be optimized\n+   * away by the filter pushdown optimization.\n+   *\n+   * The required attributes for both filtering and expression evaluation are passed to the\n+   * provided `scanBuilder` function so that it can avoid unnecessary column materialization.\n+   */\n+  def pruneFilterProject(\n+                          projectList: Seq[NamedExpression],\n+                          filterPredicates: Seq[Expression],\n+                          prunePushedDownFilters: Seq[Expression] => Seq[Expression],\n+                          scanBuilder: Seq[Attribute] => SparkPlan): SparkPlan = {\n+\n+    val projectSet = AttributeSet(projectList.flatMap(_.references))\n+    val filterSet = AttributeSet(filterPredicates.flatMap(_.references))\n+    val filterCondition =\n+      prunePushedDownFilters(filterPredicates).reduceLeftOption(catalyst.expressions.And)\n+\n+    // Right now we still use a projection even if the only evaluation is applying an alias\n+    // to a column.  Since this is a no-op, it could be avoided. However, using this\n+    // optimization with the current implementation would change the output schema.\n+    // TODO: Decouple final output schema from expression evaluation so this copy can be\n+    // avoided safely.\n+\n+    if (AttributeSet(projectList.map(_.toAttribute)) == projectSet &&\n+      filterSet.subsetOf(projectSet)) {\n+      // When it is possible to just use column pruning to get the right projection and\n+      // when the columns of this projection are enough to evaluate all filter conditions,\n+      // just do a scan followed by a filter, with no extra project.\n+      val scan = scanBuilder(projectList.asInstanceOf[Seq[Attribute]])\n+      filterCondition.map(Filter(_, scan)).getOrElse(scan)\n+    } else {\n+      val scan = scanBuilder((projectSet ++ filterSet).toSeq)\n+      Project(projectList, filterCondition.map(Filter(_, scan)).getOrElse(scan))\n+    }\n+  }\n+}",
    "line": 92
  }],
  "prId": 6356
}]