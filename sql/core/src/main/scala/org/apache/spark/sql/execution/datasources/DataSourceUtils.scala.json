[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Wait .. why we do the recursive thing here? What if the top level type is supported but nested is not? For example, Arrow integration in Spark doesn't currently support nested timestamp conversion for localization issue.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-02T14:28:09Z",
    "diffHunk": "@@ -42,63 +38,27 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.\n    */\n   private def verifySchema(format: FileFormat, schema: StructType, isReadPath: Boolean): Unit = {\n-    def throwUnsupportedException(dataType: DataType): Unit = {\n-      throw new UnsupportedOperationException(\n-        s\"$format data source does not support ${dataType.simpleString} data type.\")\n-    }\n-\n-    def verifyType(dataType: DataType): Unit = dataType match {\n-      case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\n-           StringType | BinaryType | DateType | TimestampType | _: DecimalType =>\n-\n-      // All the unsupported types for CSV\n-      case _: NullType | _: CalendarIntervalType | _: StructType | _: ArrayType | _: MapType\n-          if format.isInstanceOf[CSVFileFormat] =>\n-        throwUnsupportedException(dataType)\n-\n-      case st: StructType => st.foreach { f => verifyType(f.dataType) }\n-\n-      case ArrayType(elementType, _) => verifyType(elementType)\n-\n-      case MapType(keyType, valueType, _) =>\n-        verifyType(keyType)\n-        verifyType(valueType)\n-\n-      case udt: UserDefinedType[_] => verifyType(udt.sqlType)\n-\n-      // Interval type not supported in all the write path\n-      case _: CalendarIntervalType if !isReadPath =>\n-        throwUnsupportedException(dataType)\n-\n-      // JSON and ORC don't support an Interval type, but we pass it in read pass\n-      // for back-compatibility.\n-      case _: CalendarIntervalType if format.isInstanceOf[JsonFileFormat] ||\n-        format.isInstanceOf[OrcFileFormat] =>\n+    def verifyType(dataType: DataType): Unit = {\n+      if (!format.supportDataType(dataType, isReadPath)) {\n+        throw new UnsupportedOperationException(\n+          s\"$format data source does not support ${dataType.simpleString} data type.\")\n+      }\n+      dataType match {"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "This is for general purpose, so that developer can skip matching arrays/maps/structs.\r\nI don't know about nested timestamp, but we can override supportDataType to make sure the case is unsupported, right?",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-02T14:54:40Z",
    "diffHunk": "@@ -42,63 +38,27 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.\n    */\n   private def verifySchema(format: FileFormat, schema: StructType, isReadPath: Boolean): Unit = {\n-    def throwUnsupportedException(dataType: DataType): Unit = {\n-      throw new UnsupportedOperationException(\n-        s\"$format data source does not support ${dataType.simpleString} data type.\")\n-    }\n-\n-    def verifyType(dataType: DataType): Unit = dataType match {\n-      case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\n-           StringType | BinaryType | DateType | TimestampType | _: DecimalType =>\n-\n-      // All the unsupported types for CSV\n-      case _: NullType | _: CalendarIntervalType | _: StructType | _: ArrayType | _: MapType\n-          if format.isInstanceOf[CSVFileFormat] =>\n-        throwUnsupportedException(dataType)\n-\n-      case st: StructType => st.foreach { f => verifyType(f.dataType) }\n-\n-      case ArrayType(elementType, _) => verifyType(elementType)\n-\n-      case MapType(keyType, valueType, _) =>\n-        verifyType(keyType)\n-        verifyType(valueType)\n-\n-      case udt: UserDefinedType[_] => verifyType(udt.sqlType)\n-\n-      // Interval type not supported in all the write path\n-      case _: CalendarIntervalType if !isReadPath =>\n-        throwUnsupportedException(dataType)\n-\n-      // JSON and ORC don't support an Interval type, but we pass it in read pass\n-      // for back-compatibility.\n-      case _: CalendarIntervalType if format.isInstanceOf[JsonFileFormat] ||\n-        format.isInstanceOf[OrcFileFormat] =>\n+    def verifyType(dataType: DataType): Unit = {\n+      if (!format.supportDataType(dataType, isReadPath)) {\n+        throw new UnsupportedOperationException(\n+          s\"$format data source does not support ${dataType.simpleString} data type.\")\n+      }\n+      dataType match {"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yea.. then this code bit is not really general purpose anymore ... developers should check the codes inside and see if the nested types are automatically checked or not ..",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-02T14:56:34Z",
    "diffHunk": "@@ -42,63 +38,27 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.\n    */\n   private def verifySchema(format: FileFormat, schema: StructType, isReadPath: Boolean): Unit = {\n-    def throwUnsupportedException(dataType: DataType): Unit = {\n-      throw new UnsupportedOperationException(\n-        s\"$format data source does not support ${dataType.simpleString} data type.\")\n-    }\n-\n-    def verifyType(dataType: DataType): Unit = dataType match {\n-      case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\n-           StringType | BinaryType | DateType | TimestampType | _: DecimalType =>\n-\n-      // All the unsupported types for CSV\n-      case _: NullType | _: CalendarIntervalType | _: StructType | _: ArrayType | _: MapType\n-          if format.isInstanceOf[CSVFileFormat] =>\n-        throwUnsupportedException(dataType)\n-\n-      case st: StructType => st.foreach { f => verifyType(f.dataType) }\n-\n-      case ArrayType(elementType, _) => verifyType(elementType)\n-\n-      case MapType(keyType, valueType, _) =>\n-        verifyType(keyType)\n-        verifyType(valueType)\n-\n-      case udt: UserDefinedType[_] => verifyType(udt.sqlType)\n-\n-      // Interval type not supported in all the write path\n-      case _: CalendarIntervalType if !isReadPath =>\n-        throwUnsupportedException(dataType)\n-\n-      // JSON and ORC don't support an Interval type, but we pass it in read pass\n-      // for back-compatibility.\n-      case _: CalendarIntervalType if format.isInstanceOf[JsonFileFormat] ||\n-        format.isInstanceOf[OrcFileFormat] =>\n+    def verifyType(dataType: DataType): Unit = {\n+      if (!format.supportDataType(dataType, isReadPath)) {\n+        throw new UnsupportedOperationException(\n+          s\"$format data source does not support ${dataType.simpleString} data type.\")\n+      }\n+      dataType match {"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I know..But if developers didn't read inside and process the case of  arrays/maps/structs, the code should still work.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-02T15:09:15Z",
    "diffHunk": "@@ -42,63 +38,27 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.\n    */\n   private def verifySchema(format: FileFormat, schema: StructType, isReadPath: Boolean): Unit = {\n-    def throwUnsupportedException(dataType: DataType): Unit = {\n-      throw new UnsupportedOperationException(\n-        s\"$format data source does not support ${dataType.simpleString} data type.\")\n-    }\n-\n-    def verifyType(dataType: DataType): Unit = dataType match {\n-      case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\n-           StringType | BinaryType | DateType | TimestampType | _: DecimalType =>\n-\n-      // All the unsupported types for CSV\n-      case _: NullType | _: CalendarIntervalType | _: StructType | _: ArrayType | _: MapType\n-          if format.isInstanceOf[CSVFileFormat] =>\n-        throwUnsupportedException(dataType)\n-\n-      case st: StructType => st.foreach { f => verifyType(f.dataType) }\n-\n-      case ArrayType(elementType, _) => verifyType(elementType)\n-\n-      case MapType(keyType, valueType, _) =>\n-        verifyType(keyType)\n-        verifyType(valueType)\n-\n-      case udt: UserDefinedType[_] => verifyType(udt.sqlType)\n-\n-      // Interval type not supported in all the write path\n-      case _: CalendarIntervalType if !isReadPath =>\n-        throwUnsupportedException(dataType)\n-\n-      // JSON and ORC don't support an Interval type, but we pass it in read pass\n-      // for back-compatibility.\n-      case _: CalendarIntervalType if format.isInstanceOf[JsonFileFormat] ||\n-        format.isInstanceOf[OrcFileFormat] =>\n+    def verifyType(dataType: DataType): Unit = {\n+      if (!format.supportDataType(dataType, isReadPath)) {\n+        throw new UnsupportedOperationException(\n+          s\"$format data source does not support ${dataType.simpleString} data type.\")\n+      }\n+      dataType match {"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "In that case, this code bit becomes rather obsolete .. To me Spark's dev API is too difficult for me to understand :-) .. Personally, I don't like to be too clever when it comes to API thing.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-02T15:16:09Z",
    "diffHunk": "@@ -42,63 +38,27 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.\n    */\n   private def verifySchema(format: FileFormat, schema: StructType, isReadPath: Boolean): Unit = {\n-    def throwUnsupportedException(dataType: DataType): Unit = {\n-      throw new UnsupportedOperationException(\n-        s\"$format data source does not support ${dataType.simpleString} data type.\")\n-    }\n-\n-    def verifyType(dataType: DataType): Unit = dataType match {\n-      case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\n-           StringType | BinaryType | DateType | TimestampType | _: DecimalType =>\n-\n-      // All the unsupported types for CSV\n-      case _: NullType | _: CalendarIntervalType | _: StructType | _: ArrayType | _: MapType\n-          if format.isInstanceOf[CSVFileFormat] =>\n-        throwUnsupportedException(dataType)\n-\n-      case st: StructType => st.foreach { f => verifyType(f.dataType) }\n-\n-      case ArrayType(elementType, _) => verifyType(elementType)\n-\n-      case MapType(keyType, valueType, _) =>\n-        verifyType(keyType)\n-        verifyType(valueType)\n-\n-      case udt: UserDefinedType[_] => verifyType(udt.sqlType)\n-\n-      // Interval type not supported in all the write path\n-      case _: CalendarIntervalType if !isReadPath =>\n-        throwUnsupportedException(dataType)\n-\n-      // JSON and ORC don't support an Interval type, but we pass it in read pass\n-      // for back-compatibility.\n-      case _: CalendarIntervalType if format.isInstanceOf[JsonFileFormat] ||\n-        format.isInstanceOf[OrcFileFormat] =>\n+    def verifyType(dataType: DataType): Unit = {\n+      if (!format.supportDataType(dataType, isReadPath)) {\n+        throw new UnsupportedOperationException(\n+          s\"$format data source does not support ${dataType.simpleString} data type.\")\n+      }\n+      dataType match {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "It's tricky to rely on 2 places to correctly determine the unsupported type. `format.supportDataType` should handle complex types themselves, to make the code clearer and easier to maintain.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-03T06:25:03Z",
    "diffHunk": "@@ -42,63 +38,27 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.\n    */\n   private def verifySchema(format: FileFormat, schema: StructType, isReadPath: Boolean): Unit = {\n-    def throwUnsupportedException(dataType: DataType): Unit = {\n-      throw new UnsupportedOperationException(\n-        s\"$format data source does not support ${dataType.simpleString} data type.\")\n-    }\n-\n-    def verifyType(dataType: DataType): Unit = dataType match {\n-      case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\n-           StringType | BinaryType | DateType | TimestampType | _: DecimalType =>\n-\n-      // All the unsupported types for CSV\n-      case _: NullType | _: CalendarIntervalType | _: StructType | _: ArrayType | _: MapType\n-          if format.isInstanceOf[CSVFileFormat] =>\n-        throwUnsupportedException(dataType)\n-\n-      case st: StructType => st.foreach { f => verifyType(f.dataType) }\n-\n-      case ArrayType(elementType, _) => verifyType(elementType)\n-\n-      case MapType(keyType, valueType, _) =>\n-        verifyType(keyType)\n-        verifyType(valueType)\n-\n-      case udt: UserDefinedType[_] => verifyType(udt.sqlType)\n-\n-      // Interval type not supported in all the write path\n-      case _: CalendarIntervalType if !isReadPath =>\n-        throwUnsupportedException(dataType)\n-\n-      // JSON and ORC don't support an Interval type, but we pass it in read pass\n-      // for back-compatibility.\n-      case _: CalendarIntervalType if format.isInstanceOf[JsonFileFormat] ||\n-        format.isInstanceOf[OrcFileFormat] =>\n+    def verifyType(dataType: DataType): Unit = {\n+      if (!format.supportDataType(dataType, isReadPath)) {\n+        throw new UnsupportedOperationException(\n+          s\"$format data source does not support ${dataType.simpleString} data type.\")\n+      }\n+      dataType match {"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I see. I will update it.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-03T07:18:39Z",
    "diffHunk": "@@ -42,63 +38,27 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.\n    */\n   private def verifySchema(format: FileFormat, schema: StructType, isReadPath: Boolean): Unit = {\n-    def throwUnsupportedException(dataType: DataType): Unit = {\n-      throw new UnsupportedOperationException(\n-        s\"$format data source does not support ${dataType.simpleString} data type.\")\n-    }\n-\n-    def verifyType(dataType: DataType): Unit = dataType match {\n-      case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\n-           StringType | BinaryType | DateType | TimestampType | _: DecimalType =>\n-\n-      // All the unsupported types for CSV\n-      case _: NullType | _: CalendarIntervalType | _: StructType | _: ArrayType | _: MapType\n-          if format.isInstanceOf[CSVFileFormat] =>\n-        throwUnsupportedException(dataType)\n-\n-      case st: StructType => st.foreach { f => verifyType(f.dataType) }\n-\n-      case ArrayType(elementType, _) => verifyType(elementType)\n-\n-      case MapType(keyType, valueType, _) =>\n-        verifyType(keyType)\n-        verifyType(valueType)\n-\n-      case udt: UserDefinedType[_] => verifyType(udt.sqlType)\n-\n-      // Interval type not supported in all the write path\n-      case _: CalendarIntervalType if !isReadPath =>\n-        throwUnsupportedException(dataType)\n-\n-      // JSON and ORC don't support an Interval type, but we pass it in read pass\n-      // for back-compatibility.\n-      case _: CalendarIntervalType if format.isInstanceOf[JsonFileFormat] ||\n-        format.isInstanceOf[OrcFileFormat] =>\n+    def verifyType(dataType: DataType): Unit = {\n+      if (!format.supportDataType(dataType, isReadPath)) {\n+        throw new UnsupportedOperationException(\n+          s\"$format data source does not support ${dataType.simpleString} data type.\")\n+      }\n+      dataType match {"
  }],
  "prId": 21667
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`FileFormat` is internal so it's nothing about public API, but just about design choice.\r\n\r\nGenerally it's ok to have a central place to put some business logic for different cases. However, here we can't access all `FileFormat` implementations, Hive ORC is in Hive module. Now the only choice is: dispatch the business logic into implementations.\r\n\r\nSo +1 on the approach taken by this PR.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-03T15:54:07Z",
    "diffHunk": "@@ -42,65 +38,9 @@ object DataSourceUtils {\n \n   /**\n    * Verify if the schema is supported in datasource. This verification should be done\n-   * in a driver side, e.g., `prepareWrite`, `buildReader`, and `buildReaderWithPartitionValues`\n-   * in `FileFormat`.\n-   *\n-   * Unsupported data types of csv, json, orc, and parquet are as follows;\n-   *  csv -> R/W: Interval, Null, Array, Map, Struct\n-   *  json -> W: Interval\n-   *  orc -> W: Interval, Null\n-   *  parquet -> R/W: Interval, Null\n+   * in a driver side.",
    "line": 24
  }],
  "prId": 21667
}]