[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Minor: I'd rather not create the encoder each time a row is created. Can you move this and the method to a companion object?",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-06T19:34:15Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  private val EMPTY_ROW = toCatalystRow(\"\", \"\", \"\")\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")\n+    }\n+    sparkContext.parallelize(rows)\n+  }\n+\n+  private def addSchema(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows ++= table.schema.map{ column =>\n+      toCatalystRow(\n+        column.name, column.dataType.simpleString, column.getComment().getOrElse(\"\"))\n+    }\n+  }\n+\n+  private def addPartitioning(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Partitioning\", \"\", \"\")\n+    rows += toCatalystRow(\"--------------\", \"\", \"\")\n+    if (table.partitioning.isEmpty) {\n+      rows += toCatalystRow(\"Not partitioned\", \"\", \"\")\n+    } else {\n+      rows ++= table.partitioning.zipWithIndex.map {\n+        case (transform, index) => toCatalystRow(s\"Part $index\", transform.describe(), \"\")\n+      }\n+    }\n+  }\n+\n+  private def addProperties(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Table Property\", \" Value\", \"\")\n+    rows += toCatalystRow(\"----------------\", \"-------\", \"\")\n+    rows ++= table.properties.asScala.toList.sortBy(_._1).map {\n+      case (key, value) => toCatalystRow(key, value, \"\")\n+    }\n+  }\n+\n+  private def toCatalystRow(strs: String*): InternalRow = {\n+    val encoder = RowEncoder(DescribeTableSchemas.DESCRIBE_TABLE_SCHEMA).resolveAndBind()"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Sort of - couple of questions:\r\n\r\n- Is `RowEncoder` thread-safe?\r\n- I noticed if I create `RowEncoder` but immediately `resolveAndBind` it, and reuse the resolved encoder, the tests break as the describe returns incorrect rows. Presumably there's some kind of reused memory leak here. I didn't look into it that thoroughly - think we can just reuse the unresolved encoder and `resolveAndBind` before creating each row.",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-10T20:40:26Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  private val EMPTY_ROW = toCatalystRow(\"\", \"\", \"\")\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")\n+    }\n+    sparkContext.parallelize(rows)\n+  }\n+\n+  private def addSchema(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows ++= table.schema.map{ column =>\n+      toCatalystRow(\n+        column.name, column.dataType.simpleString, column.getComment().getOrElse(\"\"))\n+    }\n+  }\n+\n+  private def addPartitioning(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Partitioning\", \"\", \"\")\n+    rows += toCatalystRow(\"--------------\", \"\", \"\")\n+    if (table.partitioning.isEmpty) {\n+      rows += toCatalystRow(\"Not partitioned\", \"\", \"\")\n+    } else {\n+      rows ++= table.partitioning.zipWithIndex.map {\n+        case (transform, index) => toCatalystRow(s\"Part $index\", transform.describe(), \"\")\n+      }\n+    }\n+  }\n+\n+  private def addProperties(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Table Property\", \" Value\", \"\")\n+    rows += toCatalystRow(\"----------------\", \"-------\", \"\")\n+    rows ++= table.properties.asScala.toList.sortBy(_._1).map {\n+      case (key, value) => toCatalystRow(key, value, \"\")\n+    }\n+  }\n+\n+  private def toCatalystRow(strs: String*): InternalRow = {\n+    val encoder = RowEncoder(DescribeTableSchemas.DESCRIBE_TABLE_SCHEMA).resolveAndBind()"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "@cloud-fan, can you help answer these questions?",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-11T19:32:17Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  private val EMPTY_ROW = toCatalystRow(\"\", \"\", \"\")\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")\n+    }\n+    sparkContext.parallelize(rows)\n+  }\n+\n+  private def addSchema(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows ++= table.schema.map{ column =>\n+      toCatalystRow(\n+        column.name, column.dataType.simpleString, column.getComment().getOrElse(\"\"))\n+    }\n+  }\n+\n+  private def addPartitioning(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Partitioning\", \"\", \"\")\n+    rows += toCatalystRow(\"--------------\", \"\", \"\")\n+    if (table.partitioning.isEmpty) {\n+      rows += toCatalystRow(\"Not partitioned\", \"\", \"\")\n+    } else {\n+      rows ++= table.partitioning.zipWithIndex.map {\n+        case (transform, index) => toCatalystRow(s\"Part $index\", transform.describe(), \"\")\n+      }\n+    }\n+  }\n+\n+  private def addProperties(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Table Property\", \" Value\", \"\")\n+    rows += toCatalystRow(\"----------------\", \"-------\", \"\")\n+    rows ++= table.properties.asScala.toList.sortBy(_._1).map {\n+      case (key, value) => toCatalystRow(key, value, \"\")\n+    }\n+  }\n+\n+  private def toCatalystRow(strs: String*): InternalRow = {\n+    val encoder = RowEncoder(DescribeTableSchemas.DESCRIBE_TABLE_SCHEMA).resolveAndBind()"
  }],
  "prId": 25040
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shouldn't we throw exception when table not found?",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-12T09:57:53Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  import DescribeTableExec._\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think we can follow the https://github.com/apache/spark/pull/24937: The `DescribeTable` should contain an `UnresolvedRelation`, so that analyzer can check table existence for us.",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-12T10:00:10Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  import DescribeTableExec._\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Followed the `AlterTable` approach in the latest commit.",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-23T22:39:39Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  import DescribeTableExec._\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Ah but I didn't remove this - though I guess technically we should never hit this code path. We can throw an exception here instead.",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-23T22:43:30Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  import DescribeTableExec._\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")"
  }],
  "prId": 25040
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "the encoder only need to call `resolveAndBind` once",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-12T10:07:50Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  import DescribeTableExec._\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")\n+    }\n+    sparkContext.parallelize(rows)\n+  }\n+\n+  private def addSchema(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows ++= table.schema.map{ column =>\n+      toCatalystRow(\n+        column.name, column.dataType.simpleString, column.getComment().getOrElse(\"\"))\n+    }\n+  }\n+\n+  private def addPartitioning(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Partitioning\", \"\", \"\")\n+    rows += toCatalystRow(\"--------------\", \"\", \"\")\n+    if (table.partitioning.isEmpty) {\n+      rows += toCatalystRow(\"Not partitioned\", \"\", \"\")\n+    } else {\n+      rows ++= table.partitioning.zipWithIndex.map {\n+        case (transform, index) => toCatalystRow(s\"Part $index\", transform.describe(), \"\")\n+      }\n+    }\n+  }\n+\n+  private def addProperties(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Table Property\", \" Value\", \"\")\n+    rows += toCatalystRow(\"----------------\", \"-------\", \"\")\n+    rows ++= table.properties.asScala.toList.sortBy(_._1).map {\n+      case (key, value) => toCatalystRow(key, value, \"\")\n+    }\n+  }\n+}\n+\n+private object DescribeTableExec {\n+  private val ENCODER = RowEncoder(DescribeTableSchemas.DESCRIBE_TABLE_SCHEMA)\n+  private val EMPTY_ROW = toCatalystRow(\"\", \"\", \"\")\n+\n+  private def toCatalystRow(strs: String*): InternalRow = {\n+    ENCODER.resolveAndBind().toRow("
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I don't necessarily think so, but it could also be how this class is built. I think the encoder's state needs to be reset. When I don't resolveAndBind every time, the tests yield wrong results entirely.",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-23T22:03:10Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  import DescribeTableExec._\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")\n+    }\n+    sparkContext.parallelize(rows)\n+  }\n+\n+  private def addSchema(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows ++= table.schema.map{ column =>\n+      toCatalystRow(\n+        column.name, column.dataType.simpleString, column.getComment().getOrElse(\"\"))\n+    }\n+  }\n+\n+  private def addPartitioning(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Partitioning\", \"\", \"\")\n+    rows += toCatalystRow(\"--------------\", \"\", \"\")\n+    if (table.partitioning.isEmpty) {\n+      rows += toCatalystRow(\"Not partitioned\", \"\", \"\")\n+    } else {\n+      rows ++= table.partitioning.zipWithIndex.map {\n+        case (transform, index) => toCatalystRow(s\"Part $index\", transform.describe(), \"\")\n+      }\n+    }\n+  }\n+\n+  private def addProperties(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Table Property\", \" Value\", \"\")\n+    rows += toCatalystRow(\"----------------\", \"-------\", \"\")\n+    rows ++= table.properties.asScala.toList.sortBy(_._1).map {\n+      case (key, value) => toCatalystRow(key, value, \"\")\n+    }\n+  }\n+}\n+\n+private object DescribeTableExec {\n+  private val ENCODER = RowEncoder(DescribeTableSchemas.DESCRIBE_TABLE_SCHEMA)\n+  private val EMPTY_ROW = toCatalystRow(\"\", \"\", \"\")\n+\n+  private def toCatalystRow(strs: String*): InternalRow = {\n+    ENCODER.resolveAndBind().toRow("
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Fixed it - we have to copy the rows generated by the encoder since the encoder re-uses the same memory space.",
    "commit": "cff78a16e691917e812b4cd63bf7544a54af4742",
    "createdAt": "2019-07-24T00:04:03Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalog.v2.{Identifier, TableCatalog}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GenericRowWithSchema}\n+import org.apache.spark.sql.catalyst.plans.DescribeTableSchemas\n+import org.apache.spark.sql.execution.LeafExecNode\n+import org.apache.spark.sql.sources.v2.Table\n+\n+case class DescribeTableExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    isExtended: Boolean) extends LeafExecNode {\n+\n+  import DescribeTableExec._\n+\n+  override def output: Seq[AttributeReference] = DescribeTableSchemas.DESCRIBE_TABLE_ATTRIBUTES\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val rows = new ArrayBuffer[InternalRow]()\n+    if (catalog.tableExists(ident)) {\n+      val table = catalog.loadTable(ident)\n+      addSchema(rows, table)\n+\n+      if (isExtended) {\n+        addPartitioning(rows, table)\n+        addProperties(rows, table)\n+      }\n+\n+    } else {\n+      rows += toCatalystRow(s\"Table $ident does not exist.\", \"\", \"\")\n+    }\n+    sparkContext.parallelize(rows)\n+  }\n+\n+  private def addSchema(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows ++= table.schema.map{ column =>\n+      toCatalystRow(\n+        column.name, column.dataType.simpleString, column.getComment().getOrElse(\"\"))\n+    }\n+  }\n+\n+  private def addPartitioning(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Partitioning\", \"\", \"\")\n+    rows += toCatalystRow(\"--------------\", \"\", \"\")\n+    if (table.partitioning.isEmpty) {\n+      rows += toCatalystRow(\"Not partitioned\", \"\", \"\")\n+    } else {\n+      rows ++= table.partitioning.zipWithIndex.map {\n+        case (transform, index) => toCatalystRow(s\"Part $index\", transform.describe(), \"\")\n+      }\n+    }\n+  }\n+\n+  private def addProperties(rows: ArrayBuffer[InternalRow], table: Table): Unit = {\n+    rows += EMPTY_ROW\n+    rows += toCatalystRow(\" Table Property\", \" Value\", \"\")\n+    rows += toCatalystRow(\"----------------\", \"-------\", \"\")\n+    rows ++= table.properties.asScala.toList.sortBy(_._1).map {\n+      case (key, value) => toCatalystRow(key, value, \"\")\n+    }\n+  }\n+}\n+\n+private object DescribeTableExec {\n+  private val ENCODER = RowEncoder(DescribeTableSchemas.DESCRIBE_TABLE_SCHEMA)\n+  private val EMPTY_ROW = toCatalystRow(\"\", \"\", \"\")\n+\n+  private def toCatalystRow(strs: String*): InternalRow = {\n+    ENCODER.resolveAndBind().toRow("
  }],
  "prId": 25040
}]