[{
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "You should document that a new instance of this is created for each create table statement. Unless you didn't intend that, but in that case you'd need to keep these objects in a map somewhere, which is annoying.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:25:02Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "Additionally this should probably also be `@DeveloperApi` like the other stuff.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:26:39Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {"
  }],
  "prId": 2475
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Any thought on making subclasses of BaseRelation that are also abstract classes instead of using traits with self types? It seems like it would be more binary-compatibility-friendly and less likely to change across Scala releases.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:28:28Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "The self type annotation is unnecessary I guess.  I'm not sure how that plays with binary compatibility, but I can remove it if you are concerned.\n\nThe optimizer doesn't take advantage of this yet, but I could see cases where you might want to implement multiple of these (i.e. you cache the RDD when its a full table scan, but for column pruned cases you don't).  At least that is why I made them traits.  I guess this might be unnecessary.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:36:03Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "I see, that seems like it's not that useful, since you can always detect whether all attributes are selected. Abstract classes seemed easier to understand. What do other people think? @rxin @yhuai \n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:55:46Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "@rxin agrees with you, and after thinking about the possible ways to extend the filter API down the road by adding methods, I'm convinced to use abstract classes instead.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T02:33:10Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "Yeah, it's a bit annoying, but I think keeping compatibility with the people who build against the simpler APIs is quite useful. They can just link to a new Spark version. And if they're looking to use new features, they can anyway do the minimal refactoring to switch to a new version of the API (e.g. instead of extending PrunedScan they can extend PrunedScan2 or whatever).\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T03:16:45Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>"
  }],
  "prId": 2475
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Should this take AttributeReferences or field names instead? I'm not sure what the semantics here really is. Or can this also be used for nested ones (e.g. `select user.address.city`)?\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:31:42Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "I guess it could be limited to `AttributeReference` at this point in query execution.  `Attribute` is just the generic version that can also be unresolved.  From a usage standpoint they have essentially the same interfaces so all internal APIs are typed as `Attribute`\n\nI considered using field names instead of `Attribute`, but that seems a little weird with `Expression` being exposed below.  This does not support nested data access as that is represented by `GetField` expressions. \n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:45:29Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "I think it's better to resolve them here if we can (and we do have the schema for these to do it). I'm just worried about keeping this narrow in the case where we add new subclasses of Attribute in the future -- we don't want implementors of this to suddenly have to deal with them.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:57:56Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "To be clear they will always be resolved here (or the analyzer would have thrown an exception).  I just usually don't use AttributeReference in type signatures so as to hide their implementation details.\n\nYou raise some good points though, and are making me reconsider an earlier design where we don't expose `Attribute` or `Expression`.  Instead we have an interface like this:\n\nAttributes are `String`. This can just be `attributeName`.  We can possibly also support `attributeName.fieldName` (or  maybe even `attributeName[0]`?).  The planner will default to just pushing down full attribute projections, but the developer can override methods to turn on the more advanced projections.\n\nFor filters we define a simple set initially that we can add to as we go:\n\n``` scala\ncase class EqualTo(attribute: String, value: Any)\n```\n\nPros: Easier to make binary compatibility promises.  Users don't have to handle `a = 1` and `1 = a` any more.  Does not expose catalyst.\nCons: Users have to deal with string parsing for complex projections.  Expressions pushed down are now are limited to those we decide to write duplicate code for.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T03:53:42Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]"
  }],
  "prId": 2475
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "We should add an option for FilteredScans to guarantee that they've applied all the filters and avoid this checking. Maybe there can be a method in the class like isStrict. Otherwise we can't change the semantics here in the future (I guess we could create another class).\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:32:54Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns and filter using selected\n+   * predicates before producing an RDD containing all matching tuples as Row objects.\n+   *\n+   * The pushed down filters are currently purely an optimization as they will all be evaluated\n+   * again.  This means it is safe to use them with methods that produce false positives such\n+   * as filtering partitions based on a bloom filter."
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "This used to be strict and let you select the filters you could handle, but then Reynold and I decided to simplify it.  It's pretty hard to design an interface that allows you to handle all three cases cleanly:\n- Supported filters\n- Unsupported filters\n- Advisory filters (used to optimize but not guaranteed to be evaluated)\n\nSince filter evaluation is pretty cheap (we double evaluate filters pushed down to parquet now) we decided to do this.\n\nWhat were you thinking the semantics of isStrict would be?  Do we still pass them all down?\n\n``` scala\n/** Returns true if all filters can be handled by the base source */\ndef isStrict(filters: Seq[Expression]): Boolean\n```\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T00:57:20Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns and filter using selected\n+   * predicates before producing an RDD containing all matching tuples as Row objects.\n+   *\n+   * The pushed down filters are currently purely an optimization as they will all be evaluated\n+   * again.  This means it is safe to use them with methods that produce false positives such\n+   * as filtering partitions based on a bloom filter."
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "We could also add the following (now or down the road), which preserves the current simple semantics and allows for optimization.\n\n``` scala\nabstract class FilterType\ncase object SupportedFilter extends FilterType\ncase object AdvisoryFilter extends FilterType\ncase object UnsupportedFilter extends FilterType\n\ndef checkFilter(filter: Expression): FilterType = AdvisoryFilter\n```\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T02:22:41Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns and filter using selected\n+   * predicates before producing an RDD containing all matching tuples as Row objects.\n+   *\n+   * The pushed down filters are currently purely an optimization as they will all be evaluated\n+   * again.  This means it is safe to use them with methods that produce false positives such\n+   * as filtering partitions based on a bloom filter."
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "Ah, good point about unsupported expressions. I like this proposal with the filter types.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T03:20:40Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns before producing an RDD\n+   * containing all of its tuples as Row objects.\n+   */\n+  @DeveloperApi\n+  trait PrunedScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(requiredColumns: Seq[Attribute]): RDD[Row]\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can eliminate unneeded columns and filter using selected\n+   * predicates before producing an RDD containing all matching tuples as Row objects.\n+   *\n+   * The pushed down filters are currently purely an optimization as they will all be evaluated\n+   * again.  This means it is safe to use them with methods that produce false positives such\n+   * as filtering partitions based on a bloom filter."
  }],
  "prId": 2475
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "would be helfpul to link to an implementation of this, e.g. the json api.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T01:09:53Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL."
  }],
  "prId": 2475
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i think we need more documentation on the semantics of the returned RDD. I don't know what we should say yet, but one thing to add is whether Row can be reused.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T02:31:15Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "It would be safe to reuse the `Row`, but it seems a little vacuous to state that when there is no public API to mutate a row.  Should we expose that too? I'm a little hesitant given that it been changing quite a bit recently.\n\nIt is always safe to state this down the road.\n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T02:37:23Z",
    "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {\n+\n+  /**\n+   * Implemented by objects that produce relations for a specific kind of data source.  When\n+   * Spark SQL is given a DDL operation with a USING clause specified, this interface is used to\n+   * pass in the parameters specified by a user.\n+   *\n+   * Users may specify the fully qualified class name of a given data source.  When that class is\n+   * not found Spark SQL will append the class name `DefaultSource` to the path, allowing for\n+   * less verbose invocation.  For example, 'org.apache.spark.sql.json' would resolve to the\n+   * data source 'org.apache.spark.sql.json.DefaultSource'\n+   */\n+  trait RelationProvider {\n+    /** Returns a new base relation with the given parameters. */\n+    def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation\n+  }\n+\n+  /**\n+   * Represents a collection of tuples with a known schema.  Classes that extend BaseRelation must\n+   * be able to produce the schema of their data in the form of a [[StructType]]  In order to be\n+   * executed, a BaseRelation must also mix in at least one of the Scan traits.\n+   *\n+   * BaseRelations must also define a equality function that only returns true when the two\n+   * instances will return the same data.  This equality function is used when determining when\n+   * it is safe to substitute cached results for a given relation.\n+   */\n+  @DeveloperApi\n+  abstract class BaseRelation {\n+    def sqlContext: SQLContext\n+    def schema: StructType\n+  }\n+\n+  /**\n+   * Mixed into a BaseRelation that can produce all of its tuples as an RDD of Row objects.\n+   */\n+  @DeveloperApi\n+  trait TableScan {\n+    self: BaseRelation =>\n+\n+    def buildScan(): RDD[Row]"
  }],
  "prId": 2475
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "I did a test on the generated class name for classes embedded in a package object. The classname is actually something like \"package$RelationProvider\". \n\nWe should really just make these top level classes rather than putting them in the package object. \n",
    "commit": "1ed60106b05bd39d5a29fa0e1ac8f98c19d2412a",
    "createdAt": "2014-10-31T05:14:07Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Attribute}\n+\n+/**\n+ * A set of APIs for adding data sources to Spark SQL.\n+ */\n+package object sources {"
  }],
  "prId": 2475
}]