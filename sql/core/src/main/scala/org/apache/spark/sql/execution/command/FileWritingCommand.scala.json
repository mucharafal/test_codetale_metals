[{
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "This doesn't make sense if `_externalMetrics` are provided, as there's no guarantee that the user-provided map is exactly as you expect. It just happens to work when it's an empty map, but it's very fragile, not to mention the fact that it relies on the alphabetical ordering of the keys.. Please rework this.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T10:01:15Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them.\n+    val writingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+\n+    val metricsNames = metrics.keys.toSeq.sorted\n+    val metricsValues = Seq(writingTime, numFiles, totalNumBytes, totalNumOutput, numPartitions)\n+    metricsNames.zip(metricsValues).foreach(x => metrics(x._1).add(x._2))"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "I am not sure if I correctly understand your point.\r\n\r\nThe external metrics are always coming from the same kind of trait. It's not coming from user, how it's user-provided map?\r\n\r\nThe order of keys returned by `map.keys` is not determined as I tried. Without sorting, how do we make sure we match the metrics values with correct metrics keys?",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T13:04:10Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them.\n+    val writingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+\n+    val metricsNames = metrics.keys.toSeq.sorted\n+    val metricsValues = Seq(writingTime, numFiles, totalNumBytes, totalNumOutput, numPartitions)\n+    metricsNames.zip(metricsValues).foreach(x => metrics(x._1).add(x._2))"
  }, {
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "I meant user as in caller of this function. This function only works when the input satisfies some requirements and silently fails otherwise. I get it that it's a private method, but it's very error prone.\r\n\r\nImagine I want to extend this by adding another metric called `avgNumFilesPerPart`. If I add it to the end of the `metricsValues` Seq, then all metrics will get messed up (because of the ordering).\r\n\r\nAt the very least, leave a comment saying that the `metricValues` need to be sorted alphabetically (and rename `writingTime` to `avgWritingTime`). But you should rather consider using a `WriteMetrics` class instead of a Map.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T14:24:33Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them.\n+    val writingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+\n+    val metricsNames = metrics.keys.toSeq.sorted\n+    val metricsValues = Seq(writingTime, numFiles, totalNumBytes, totalNumOutput, numPartitions)\n+    metricsNames.zip(metricsValues).foreach(x => metrics(x._1).add(x._2))"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "For metrics purpose, this change is a bit too large. I'd try not to increase the complexity for now. I added a comment for this.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T23:50:51Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them.\n+    val writingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+\n+    val metricsNames = metrics.keys.toSeq.sorted\n+    val metricsValues = Seq(writingTime, numFiles, totalNumBytes, totalNumOutput, numPartitions)\n+    metricsNames.zip(metricsValues).foreach(x => metrics(x._1).add(x._2))"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "Seems arbitrary to exclude 0 values. 1ms values also bring the average down, yet you're keeping those.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T10:32:34Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them."
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Excluding 0 values because 0 values can make the average writing time as 0. It seems to me that it doesn't really make sense to show 0 writing time. Seems it indicates the writing doesn't cost time.\r\n\r\n1ms brings the average down, but you still get a meaningful time value.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T13:13:38Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them."
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "It's not clear to me why we need this and the comment is not helpful.\r\nLooks to me like this is currently either null/None, or the map that's defined below, in `def metrics`.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T11:51:13Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "I've explained the purpose of this in below comments.\r\n\r\nOnce a command invokes another command to write data. We have to update the metrics in the physical plan of the first command, not the second.\r\n\r\nBefore we invoke the second command, we have to replace the metrics in the physical plan's metrics with the metrics in the first one (i.e., the external metrics).\r\n\r\nIt seems to me that you miss the relationship that there is two different commands.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T13:10:15Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }"
  }, {
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "Indeed, I missed it at first, but then I got it. See below: https://github.com/apache/spark/pull/18159#discussion_r120359550",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T14:01:50Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "I should have better code comment on this part too. I'll add it in next commit.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T14:17:06Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "Why does this take `metrics` as a parameter, when the trait already has a `metrics` member?\r\n\r\nA big part of this patch is just about passing down metrics through this interface.. It also makes it quite hard to follow. Is there no way we can avoid this? It would be a significant improvement if you could find a solution.\r\n\r\nBasically try to specify the metrics (either the Map you have, or None) together with the corresponding callback function just once, when you first instantiate `FileWritingCommandExec` in `SparkStrategies.scala`. Then you won't need to pass around metrics all over the place and you also won't need the `transform ... withExternalMetrics` hack.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T12:00:12Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them.\n+    val writingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+\n+    val metricsNames = metrics.keys.toSeq.sorted\n+    val metricsValues = Seq(writingTime, numFiles, totalNumBytes, totalNumOutput, numPartitions)\n+    metricsNames.zip(metricsValues).foreach(x => metrics(x._1).add(x._2))\n+\n+    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\n+    SQLMetrics.postDriverMetricUpdates(sparkContext, executionId, metricsNames.map(metrics(_)))\n+  }\n+\n+  def run(\n+    sparkSession: SparkSession,\n+    children: Seq[SparkPlan],\n+    metrics: Map[String, SQLMetric],\n+    metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row]"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "The case is more complicated...\r\n\r\nWe have those commands which don't write the data but invoke another commands to do that. The execution data for showing on UI is bound to the original commands, but the invoked commands. The invoked commands will update the metrics in its physical plans, not the caller's. If we don't pass the metrics or callback function, you won't actually update the correct metrics. That's why we need to pass metrics or callback function here...\r\n\r\n",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T12:58:48Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them.\n+    val writingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+\n+    val metricsNames = metrics.keys.toSeq.sorted\n+    val metricsValues = Seq(writingTime, numFiles, totalNumBytes, totalNumOutput, numPartitions)\n+    metricsNames.zip(metricsValues).foreach(x => metrics(x._1).add(x._2))\n+\n+    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\n+    SQLMetrics.postDriverMetricUpdates(sparkContext, executionId, metricsNames.map(metrics(_)))\n+  }\n+\n+  def run(\n+    sparkSession: SparkSession,\n+    children: Seq[SparkPlan],\n+    metrics: Map[String, SQLMetric],\n+    metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row]"
  }, {
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "I understand that, but I think it can still be simplified. See below: https://github.com/apache/spark/pull/18159#discussion_r120359550",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-06T14:06:32Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+\n+  // The caller of `FileWritingCommand` can replace the metrics location by providing this external\n+  // metrics structure.\n+  private var _externalMetrics: Option[Map[String, SQLMetric]] = None\n+  private[sql] def withExternalMetrics(map: Map[String, SQLMetric]): this.type = {\n+    _externalMetrics = Option(map)\n+    this\n+  }\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `FileWritingCommandExec` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] = _externalMetrics.getOrElse {\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(sparkContext: SparkContext, metrics: Map[String, SQLMetric])\n+      (writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing when calculating average, so excluding them.\n+    val writingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+\n+    val metricsNames = metrics.keys.toSeq.sorted\n+    val metricsValues = Seq(writingTime, numFiles, totalNumBytes, totalNumOutput, numPartitions)\n+    metricsNames.zip(metricsValues).foreach(x => metrics(x._1).add(x._2))\n+\n+    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\n+    SQLMetrics.postDriverMetricUpdates(sparkContext, executionId, metricsNames.map(metrics(_)))\n+  }\n+\n+  def run(\n+    sparkSession: SparkSession,\n+    children: Seq[SparkPlan],\n+    metrics: Map[String, SQLMetric],\n+    metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row]"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think it's simpler to just write 4 lines to set these 4 metrics",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-14T08:49:06Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+  def run(\n+    sparkSession: SparkSession,\n+    children: Seq[SparkPlan],\n+    fileCommandExec: FileWritingCommandExec): Seq[Row]\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `FileWritingCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class FileWritingCommandExec(\n+    cmd: FileWritingCommand,\n+    children: Seq[SparkPlan],\n+    givenMetrics: Option[Map[String, SQLMetric]] = None) extends CommandExec {\n+\n+  override val metrics = givenMetrics.getOrElse {\n+    val sparkContext = sqlContext.sparkContext\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing to zero when calculating average, so excluding them.\n+    val avgWritingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+    // Note: for simplifying metric values assignment, we put the values as the alphabetically\n+    // sorted of the metric keys."
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Ok.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-14T12:08:09Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+  def run(\n+    sparkSession: SparkSession,\n+    children: Seq[SparkPlan],\n+    fileCommandExec: FileWritingCommandExec): Seq[Row]\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `FileWritingCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class FileWritingCommandExec(\n+    cmd: FileWritingCommand,\n+    children: Seq[SparkPlan],\n+    givenMetrics: Option[Map[String, SQLMetric]] = None) extends CommandExec {\n+\n+  override val metrics = givenMetrics.getOrElse {\n+    val sparkContext = sqlContext.sparkContext\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing to zero when calculating average, so excluding them.\n+    val avgWritingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+    // Note: for simplifying metric values assignment, we put the values as the alphabetically\n+    // sorted of the metric keys."
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how do you guarantee the `metrics` contains avg, numFiles, etc. as it's created by `givenMetrics.getOrElse`?",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-14T08:53:36Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+  def run(\n+    sparkSession: SparkSession,\n+    children: Seq[SparkPlan],\n+    fileCommandExec: FileWritingCommandExec): Seq[Row]\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `FileWritingCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class FileWritingCommandExec(\n+    cmd: FileWritingCommand,\n+    children: Seq[SparkPlan],\n+    givenMetrics: Option[Map[String, SQLMetric]] = None) extends CommandExec {\n+\n+  override val metrics = givenMetrics.getOrElse {\n+    val sparkContext = sqlContext.sparkContext\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing to zero when calculating average, so excluding them.\n+    val avgWritingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+    // Note: for simplifying metric values assignment, we put the values as the alphabetically\n+    // sorted of the metric keys.\n+    val metricsNames = metrics.keys.toSeq.sorted"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "`givenMetrics` comes from other `FileWritingCommandExec` or an empty. When it's an empty map, means the wrapped command won't call this callback. But I agree this is loose guarantee. I'll update this in next commit.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-14T13:06:52Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A logical command specialized for writing data out. `FileWritingCommand`s are\n+ * wrapped in `FileWritingCommandExec` during execution.\n+ */\n+trait FileWritingCommand extends logical.Command {\n+  def run(\n+    sparkSession: SparkSession,\n+    children: Seq[SparkPlan],\n+    fileCommandExec: FileWritingCommandExec): Seq[Row]\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `FileWritingCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class FileWritingCommandExec(\n+    cmd: FileWritingCommand,\n+    children: Seq[SparkPlan],\n+    givenMetrics: Option[Map[String, SQLMetric]] = None) extends CommandExec {\n+\n+  override val metrics = givenMetrics.getOrElse {\n+    val sparkContext = sqlContext.sparkContext\n+    Map(\n+      // General metrics.\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\")\n+    )\n+  }\n+\n+  /**\n+   * Callback function that update metrics collected from the writing operation.\n+   */\n+  private[sql] def postDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    // The time for writing individual file can be zero if it's less than 1 ms. Zero values can\n+    // lower actual time of writing to zero when calculating average, so excluding them.\n+    val avgWritingTime =\n+      Utils.average(writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))).toLong\n+    // Note: for simplifying metric values assignment, we put the values as the alphabetically\n+    // sorted of the metric keys.\n+    val metricsNames = metrics.keys.toSeq.sorted"
  }],
  "prId": 18159
}]