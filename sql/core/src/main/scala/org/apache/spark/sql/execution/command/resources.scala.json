[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "ListFilesCommand\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T04:55:20Z",
    "diffHunk": "@@ -46,3 +46,33 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFiles(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "@rxin Thank you very much! I will make the change.\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T05:00:38Z",
    "diffHunk": "@@ -46,3 +46,33 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFiles(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {"
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "ListJarsCommand\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T04:55:26Z",
    "diffHunk": "@@ -46,3 +46,33 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFiles(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"result\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    sparkSession.sparkContext.listFiles(files).map(Row(_))\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJars(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "Will change!\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T05:00:50Z",
    "diffHunk": "@@ -46,3 +46,33 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFiles(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"result\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    sparkSession.sparkContext.listFiles(files).map(Row(_))\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJars(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {"
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about `AttributeReference(\"result\", StringType, nullable = false)() :: Nil`? And it will be good if we can check what's the output name hive produces.\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T06:58:27Z",
    "diffHunk": "@@ -46,3 +51,56 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"Results\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "Thanks! I will update to use `AttributeReference` here. By the way, I check with hive shell, list command does not show an output name. \n\n```\nhive> list files;\n/tmp/328231e3-333c-4b7b-9900-87ef634ef86e_resources/test.txt\n```\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T07:07:23Z",
    "diffHunk": "@@ -46,3 +51,56 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"Results\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ok then we are free to decide the name here, `result` LGTM\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T07:11:41Z",
    "diffHunk": "@@ -46,3 +51,56 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"Results\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes"
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Should also we check the URI like we did in `ListFilesCommand`?\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T07:01:00Z",
    "diffHunk": "@@ -46,3 +51,56 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"Results\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"Results\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "It is a bit difference between listFiles and listJars here. \n`SparkContext.addedFiles` keeps the full file path that is same as the provided one during adding, except when the file path is provided without a protocol, in which case `file:` is added. So to look up for a file resource, i need to keep the file path as close as possible. \n\nOn the other hand, when a jar is added to the resource, `SparkContext.addedJars` contains a jar file path with different directory parent than the provided one. For example:\n\n```\nscala> spark.sql(\"add jar /Users/xinwu/spark/core/src/test/resources/TestUDTF.jar\")\nres6: org.apache.spark.sql.DataFrame = [result: int]\n\nscala> spark.sql(\"list jars\").show(false)\n+---------------------------------------------+\n|result                                       |\n+---------------------------------------------+\n|spark://192.168.1.234:51589/jars/TestUDTF.jar|\n+---------------------------------------------+\n```\n\nSo in order to look up the jar from added resources, I just need the jar file name, instead of the whole full path. This is why I don't need to check URI like I do for `listFiles`.\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-20T07:21:06Z",
    "diffHunk": "@@ -46,3 +51,56 @@ case class AddFile(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"Results\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    val schema = StructType(\n+      StructField(\"Results\", StringType, nullable = false) :: Nil)\n+    schema.toAttributes\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>"
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: \"Returns\" instead of \"Return\".\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T17:28:59Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources."
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "Thanks for catching this. Will update!\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T18:06:56Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources."
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "\"Returns\" instead of \"Return\".\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T17:33:24Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources."
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "Will update!\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T18:08:08Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources."
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Space after `{`.\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T17:37:48Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "Will do.\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T18:09:02Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>"
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "What's the exact semantics of `LIST JARS <paths ...>`? According to the logic above, all added jars with the same file names instead of file paths as provided paths are listed. Namely:\n\n```\nADD JARS /foo/bar.jar /baz/bar.jar;\nLIST JARS /foo/bar.jar;\n```\n\ngives both `/foo/bar.jar` and `/baz/bar.jar`. Is this expected?\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T17:41:47Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>\n+        jarList.filter(_.contains(f))\n+      }.map(Row(_))"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "`ADD JARS` command will consider jar files with the same jar file name are the same jar. For example:\n\n```\nspark-sql> add jar /Users/xinwu/spark-test/data/level1/test1.jar;\nADD JAR /Users/xinwu/spark-test/data/level1/test1.jar\nAdded [/Users/xinwu/spark-test/data/level1/test1.jar] to class path\nAdded resources: [/Users/xinwu/spark-test/data/level1/test1.jar]\n0\nTime taken: 1.928 seconds, Fetched 1 row(s)\nspark-sql> add jar /Users/xinwu/spark-test/data/test1.jar;\nADD JAR /Users/xinwu/spark-test/data/test1.jar\nAdded [/Users/xinwu/spark-test/data/test1.jar] to class path\nAdded resources: [/Users/xinwu/spark-test/data/test1.jar]\n16/05/23 11:11:12 ERROR SparkContext: Error adding jar (java.lang.IllegalArgumentException: requirement failed: JAR test1.jar already registered.), was the --addJars option used?\n0\nTime taken: 0.06 seconds, Fetched 1 row(s)\n\nspark-sql> list jars;\nspark://9.52.238.134:57281/jars/test1.jar, \nTime taken: 0.052 seconds, Fetched 1 row(s)\n```\n\nSo it looks like the jar is added to the command directory `jars`. So for `list jars`, if I specify the full path of the source jar file, it will return empty results. \n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T18:17:56Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>\n+        jarList.filter(_.contains(f))\n+      }.map(Row(_))"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "By the way, currently `ADD FILE/JAR` command only can add a single file or jar at a time. There will be another PR adding the support for adding multiple files or jars at a time. \n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T18:25:24Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>\n+        jarList.filter(_.contains(f))\n+      }.map(Row(_))"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "That makes sense, thanks.\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T19:59:27Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>\n+        jarList.filter(_.contains(f))\n+      }.map(Row(_))"
  }],
  "prId": 13212
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "I wonder whether the following form is more readable...\n\n``` scala\nif (jars.nonEmpty) {\n  for {\n    jarName <- jars.map(f => new Path(f).getName)\n    jarPath <- jarList if jarPath.contains(jarName)\n  } yield Row(jarPath)\n} else {\n  jarList.map(Row(_))\n}\n```\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T20:01:24Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>\n+        jarList.filter(_.contains(f))\n+      }.map(Row(_))\n+    } else {\n+      jarList.map(Row(_))\n+    }\n+  }"
  }, {
    "author": {
      "login": "xwu0226"
    },
    "body": "Yes. this looks nicer. I will update to use this format. Thanks!\n",
    "commit": "5f5c17aa00f2e210a35779b9f7e55cc99b895461",
    "createdAt": "2016-05-23T20:18:05Z",
    "diffHunk": "@@ -46,3 +51,52 @@ case class AddFileCommand(path: String) extends RunnableCommand {\n     Seq.empty[Row]\n   }\n }\n+\n+/**\n+ * Return a list of file paths that are added to resources.\n+ * If file paths are provided, return the ones that are added to resources.\n+ */\n+case class ListFilesCommand(files: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val fileList = sparkSession.sparkContext.listFiles()\n+    if (files.size > 0) {\n+      files.map { f =>\n+        val uri = new URI(f)\n+        val schemeCorrectedPath = uri.getScheme match {\n+          case null | \"local\" => new File(f).getCanonicalFile.toURI.toString\n+          case _ => f\n+        }\n+        new Path(schemeCorrectedPath).toUri.toString\n+      }.collect {\n+        case f if fileList.contains(f) => f\n+      }.map(Row(_))\n+    } else {\n+      fileList.map(Row(_))\n+    }\n+  }\n+}\n+\n+/**\n+ * Return a list of jar files that are added to resources.\n+ * If jar files are provided, return the ones that are added to resources.\n+ */\n+case class ListJarsCommand(jars: Seq[String] = Seq.empty[String]) extends RunnableCommand {\n+  override val output: Seq[Attribute] = {\n+    AttributeReference(\"Results\", StringType, nullable = false)() :: Nil\n+  }\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    val jarList = sparkSession.sparkContext.listJars()\n+    if (jars.size > 0) {\n+      jars.map { f =>\n+        new Path(f).getName\n+      }.flatMap {f =>\n+        jarList.filter(_.contains(f))\n+      }.map(Row(_))\n+    } else {\n+      jarList.map(Row(_))\n+    }\n+  }"
  }],
  "prId": 13212
}]