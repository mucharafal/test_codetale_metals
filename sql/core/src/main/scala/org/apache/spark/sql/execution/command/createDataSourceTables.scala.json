[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "We already have [an assert](https://github.com/cloud-fan/spark/blob/b1dbd0a19a174eaae1aaf114e04f6d3683ea65c4/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala#L133) at the beginning of this function. We need to improve the error message there. That can cover more cases.\r\n\r\nSo far, the error message is pretty confusing.\r\n```\r\nassertion failed\r\njava.lang.AssertionError: assertion failed\r\n\tat scala.Predef$.assert(Predef.scala:156)\r\n```\r\n\r\nWe can add a test case for it.\r\n```Scala\r\n        val df = spark.range(1, 10).toDF(\"id1\")\r\n        df.write.saveAsTable(\"tab1\")\r\n        spark.sql(\"create view view1 as select * from tab1\")\r\n        df.write.mode(SaveMode.Append).format(\"parquet\").saveAsTable(\"view1\")\r\n```",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-17T20:04:47Z",
    "diffHunk": "@@ -157,39 +156,74 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n+          if (existingTable.tableType == CatalogTableType.VIEW) {\n+            throw new AnalysisException(\"Saving data into a view is not allowed.\")"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I'll remove this check as it's unreachable now, and fix the error message in another PR.",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-18T13:16:35Z",
    "diffHunk": "@@ -157,39 +156,74 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n+          if (existingTable.tableType == CatalogTableType.VIEW) {\n+            throw new AnalysisException(\"Saving data into a view is not allowed.\")"
  }],
  "prId": 16313
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Is that possible we can directly use the input parameter [table](https://github.com/cloud-fan/spark/blob/b1dbd0a19a174eaae1aaf114e04f6d3683ea65c4/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala#L125)?",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-17T20:06:05Z",
    "diffHunk": "@@ -157,39 +156,74 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)",
    "line": 32
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "they are different. The input table just contains some user-specified information and the schema is always empty. https://github.com/cloud-fan/spark/blob/b1dbd0a19a174eaae1aaf114e04f6d3683ea65c4/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala#L135",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-18T13:13:51Z",
    "diffHunk": "@@ -157,39 +156,74 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)",
    "line": 32
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "uh, I see. Will do the final review today. ",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-18T20:16:21Z",
    "diffHunk": "@@ -157,39 +156,74 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)",
    "line": 32
  }],
  "prId": 16313
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: the grammar issue.\r\n`Specified partitioning does not match the existing table $tableName.`\r\n->\r\n`Specified partitioning does not match that of the existing table $tableName.`\r\n\r\nFound a [reference link:](http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc00743.1570/html/queryprocessing/queryprocessing137.htm) in sybase adaptive server.  ",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-19T02:28:39Z",
    "diffHunk": "@@ -157,39 +156,71 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n+\n+          if (existingTable.provider.get == DDLUtils.HIVE_PROVIDER) {\n+            throw new AnalysisException(s\"Saving data in the Hive serde table $tableName is \" +\n+              s\"not supported yet. Please use the insertInto() API as an alternative.\")\n+          }\n+\n           // Check if the specified data source match the data source of the existing table.\n-          val existingProvider = DataSource.lookupDataSource(provider)\n+          val existingProvider = DataSource.lookupDataSource(existingTable.provider.get)\n+          val specifiedProvider = DataSource.lookupDataSource(table.provider.get)\n           // TODO: Check that options from the resolved relation match the relation that we are\n           // inserting into (i.e. using the same compression).\n+          if (existingProvider != specifiedProvider) {\n+            throw new AnalysisException(s\"The format of the existing table $tableName is \" +\n+              s\"`${existingProvider.getSimpleName}`. It doesn't match the specified format \" +\n+              s\"`${specifiedProvider.getSimpleName}`.\")\n+          }\n+\n+          if (query.schema.length != existingTable.schema.length) {\n+            throw new AnalysisException(\n+              s\"The column number of the existing table $tableName\" +\n+                s\"(${existingTable.schema.catalogString}) doesn't match the data schema\" +\n+                s\"(${query.schema.catalogString})\")\n+          }\n \n-          // Pass a table identifier with database part, so that `lookupRelation` won't get temp\n-          // views unexpectedly.\n-          EliminateSubqueryAliases(sessionState.catalog.lookupRelation(tableIdentWithDB)) match {\n-            case l @ LogicalRelation(_: InsertableRelation | _: HadoopFsRelation, _, _) =>\n-              // check if the file formats match\n-              l.relation match {\n-                case r: HadoopFsRelation if r.fileFormat.getClass != existingProvider =>\n-                  throw new AnalysisException(\n-                    s\"The file format of the existing table $tableName is \" +\n-                      s\"`${r.fileFormat.getClass.getName}`. It doesn't match the specified \" +\n-                      s\"format `$provider`\")\n-                case _ =>\n-              }\n-              if (query.schema.size != l.schema.size) {\n-                throw new AnalysisException(\n-                  s\"The column number of the existing schema[${l.schema}] \" +\n-                    s\"doesn't match the data schema[${query.schema}]'s\")\n-              }\n-              existingSchema = Some(l.schema)\n-            case s: SimpleCatalogRelation if DDLUtils.isDatasourceTable(s.metadata) =>\n-              existingSchema = Some(s.metadata.schema)\n-            case c: CatalogRelation if c.catalogTable.provider == Some(DDLUtils.HIVE_PROVIDER) =>\n-              throw new AnalysisException(\"Saving data in the Hive serde table \" +\n-                s\"${c.catalogTable.identifier} is not supported yet. Please use the \" +\n-                \"insertInto() API as an alternative..\")\n-            case o =>\n-              throw new AnalysisException(s\"Saving data in ${o.toString} is not supported.\")\n+          val resolver = sessionState.conf.resolver\n+          val tableCols = existingTable.schema.map(_.name)\n+\n+          reorderedColumns = Some(existingTable.schema.map { f =>\n+            query.resolve(Seq(f.name), resolver).getOrElse {\n+              val inputColumns = query.schema.map(_.name).mkString(\", \")\n+              throw new AnalysisException(\n+                s\"cannot resolve '${f.name}' given input columns: [$inputColumns]\")\n+            }\n+          })\n+\n+          // Check if the specified partition columns match the existing table.\n+          val specifiedPartCols = CatalogUtils.normalizePartCols(\n+            tableName, tableCols, table.partitionColumnNames, resolver)\n+          if (specifiedPartCols != existingTable.partitionColumnNames) {\n+            throw new AnalysisException(\n+              s\"\"\"\n+                |Specified partitioning does not match the existing table $tableName."
  }],
  "prId": 16313
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: The same here.",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-19T02:29:01Z",
    "diffHunk": "@@ -157,39 +156,71 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n+\n+          if (existingTable.provider.get == DDLUtils.HIVE_PROVIDER) {\n+            throw new AnalysisException(s\"Saving data in the Hive serde table $tableName is \" +\n+              s\"not supported yet. Please use the insertInto() API as an alternative.\")\n+          }\n+\n           // Check if the specified data source match the data source of the existing table.\n-          val existingProvider = DataSource.lookupDataSource(provider)\n+          val existingProvider = DataSource.lookupDataSource(existingTable.provider.get)\n+          val specifiedProvider = DataSource.lookupDataSource(table.provider.get)\n           // TODO: Check that options from the resolved relation match the relation that we are\n           // inserting into (i.e. using the same compression).\n+          if (existingProvider != specifiedProvider) {\n+            throw new AnalysisException(s\"The format of the existing table $tableName is \" +\n+              s\"`${existingProvider.getSimpleName}`. It doesn't match the specified format \" +\n+              s\"`${specifiedProvider.getSimpleName}`.\")\n+          }\n+\n+          if (query.schema.length != existingTable.schema.length) {\n+            throw new AnalysisException(\n+              s\"The column number of the existing table $tableName\" +\n+                s\"(${existingTable.schema.catalogString}) doesn't match the data schema\" +\n+                s\"(${query.schema.catalogString})\")\n+          }\n \n-          // Pass a table identifier with database part, so that `lookupRelation` won't get temp\n-          // views unexpectedly.\n-          EliminateSubqueryAliases(sessionState.catalog.lookupRelation(tableIdentWithDB)) match {\n-            case l @ LogicalRelation(_: InsertableRelation | _: HadoopFsRelation, _, _) =>\n-              // check if the file formats match\n-              l.relation match {\n-                case r: HadoopFsRelation if r.fileFormat.getClass != existingProvider =>\n-                  throw new AnalysisException(\n-                    s\"The file format of the existing table $tableName is \" +\n-                      s\"`${r.fileFormat.getClass.getName}`. It doesn't match the specified \" +\n-                      s\"format `$provider`\")\n-                case _ =>\n-              }\n-              if (query.schema.size != l.schema.size) {\n-                throw new AnalysisException(\n-                  s\"The column number of the existing schema[${l.schema}] \" +\n-                    s\"doesn't match the data schema[${query.schema}]'s\")\n-              }\n-              existingSchema = Some(l.schema)\n-            case s: SimpleCatalogRelation if DDLUtils.isDatasourceTable(s.metadata) =>\n-              existingSchema = Some(s.metadata.schema)\n-            case c: CatalogRelation if c.catalogTable.provider == Some(DDLUtils.HIVE_PROVIDER) =>\n-              throw new AnalysisException(\"Saving data in the Hive serde table \" +\n-                s\"${c.catalogTable.identifier} is not supported yet. Please use the \" +\n-                \"insertInto() API as an alternative..\")\n-            case o =>\n-              throw new AnalysisException(s\"Saving data in ${o.toString} is not supported.\")\n+          val resolver = sessionState.conf.resolver\n+          val tableCols = existingTable.schema.map(_.name)\n+\n+          reorderedColumns = Some(existingTable.schema.map { f =>\n+            query.resolve(Seq(f.name), resolver).getOrElse {\n+              val inputColumns = query.schema.map(_.name).mkString(\", \")\n+              throw new AnalysisException(\n+                s\"cannot resolve '${f.name}' given input columns: [$inputColumns]\")\n+            }\n+          })\n+\n+          // Check if the specified partition columns match the existing table.\n+          val specifiedPartCols = CatalogUtils.normalizePartCols(\n+            tableName, tableCols, table.partitionColumnNames, resolver)\n+          if (specifiedPartCols != existingTable.partitionColumnNames) {\n+            throw new AnalysisException(\n+              s\"\"\"\n+                |Specified partitioning does not match the existing table $tableName.\n+                |Specified partition columns: [${specifiedPartCols.mkString(\", \")}]\n+                |Existing partition columns: [${existingTable.partitionColumnNames.mkString(\", \")}]\n+              \"\"\".stripMargin)\n+          }\n+\n+          // Check if the specified bucketing match the existing table.\n+          val specifiedBucketSpec = table.bucketSpec.map { bucketSpec =>\n+            CatalogUtils.normalizeBucketSpec(tableName, tableCols, bucketSpec, resolver)\n+          }\n+          if (specifiedBucketSpec != existingTable.bucketSpec) {\n+            val specifiedBucketString =\n+              specifiedBucketSpec.map(_.toString).getOrElse(\"not bucketed\")\n+            val existingBucketString =\n+              existingTable.bucketSpec.map(_.toString).getOrElse(\"not bucketed\")\n+            throw new AnalysisException(\n+              s\"\"\"\n+                |Specified bucketing does not match the existing table $tableName."
  }],
  "prId": 16313
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: the string interpolation`s` is not needed.",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-19T02:56:02Z",
    "diffHunk": "@@ -157,39 +156,71 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n+\n+          if (existingTable.provider.get == DDLUtils.HIVE_PROVIDER) {\n+            throw new AnalysisException(s\"Saving data in the Hive serde table $tableName is \" +\n+              s\"not supported yet. Please use the insertInto() API as an alternative.\")"
  }],
  "prId": 16313
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Now, the checking logics are split two places for CTAS of data source tables using the Append mode. Maybe we can improve the comment to explain `AnalyzeCreateTable` verifies the consistency between the user-specified table schema/definition and the SELECT query. Here, we verifies the consistency between the user-specified table schema/definition and the existing table schema/definition, the consistency between the existing table schema/definition and the SELECT query.",
    "commit": "32857e6c5fa89094b84d4ed78469217af8c515c7",
    "createdAt": "2016-12-19T03:17:09Z",
    "diffHunk": "@@ -157,39 +156,71 @@ case class CreateDataSourceTableAsSelectCommand(\n           // Since the table already exists and the save mode is Ignore, we will just return.\n           return Seq.empty[Row]\n         case SaveMode.Append =>\n+          val existingTable = sessionState.catalog.getTableMetadata(tableIdentWithDB)\n+\n+          if (existingTable.provider.get == DDLUtils.HIVE_PROVIDER) {\n+            throw new AnalysisException(s\"Saving data in the Hive serde table $tableName is \" +\n+              s\"not supported yet. Please use the insertInto() API as an alternative.\")\n+          }\n+\n           // Check if the specified data source match the data source of the existing table."
  }],
  "prId": 16313
}]