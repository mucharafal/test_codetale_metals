[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "This is a behavior change (countFailedValues was false), right?\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-05-06T18:18:38Z",
    "diffHunk": "@@ -19,200 +19,106 @@ package org.apache.spark.sql.execution.metric\n \n import java.text.NumberFormat\n \n-import org.apache.spark.{Accumulable, AccumulableParam, Accumulators, SparkContext}\n+import org.apache.spark.{NewAccumulator, SparkContext}\n import org.apache.spark.scheduler.AccumulableInfo\n import org.apache.spark.util.Utils\n \n-/**\n- * Create a layer for specialized metric. We cannot add `@specialized` to\n- * `Accumulable/AccumulableParam` because it will break Java source compatibility.\n- *\n- * An implementation of SQLMetric should override `+=` and `add` to avoid boxing.\n- */\n-private[sql] abstract class SQLMetric[R <: SQLMetricValue[T], T](\n-    name: String,\n-    val param: SQLMetricParam[R, T]) extends Accumulable[R, T](param.zero, param, Some(name)) {\n \n-  // Provide special identifier as metadata so we can tell that this is a `SQLMetric` later\n-  override def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n-    new AccumulableInfo(id, Some(name), update, value, true, countFailedValues,\n-      Some(SQLMetrics.ACCUM_IDENTIFIER))\n-  }\n-\n-  def reset(): Unit = {\n-    this.value = param.zero\n-  }\n-}\n-\n-/**\n- * Create a layer for specialized metric. We cannot add `@specialized` to\n- * `Accumulable/AccumulableParam` because it will break Java source compatibility.\n- */\n-private[sql] trait SQLMetricParam[R <: SQLMetricValue[T], T] extends AccumulableParam[R, T] {\n-\n-  /**\n-   * A function that defines how we aggregate the final accumulator results among all tasks,\n-   * and represent it in string for a SQL physical operator.\n-   */\n-  val stringValue: Seq[T] => String\n-\n-  def zero: R\n-}\n+class SQLMetric(val metricType: String, initValue: Long = 0L) extends NewAccumulator[Long, Long] {\n+  // This is a workaround for SPARK-11013.\n+  // We may use -1 as initial value of the accumulator, if the accumulator is valid, we will\n+  // update it at the end of task and the value will be at least 0. Then we can filter out the -1\n+  // values before calculate max, min, etc.\n+  private[this] var _value = initValue\n \n-/**\n- * Create a layer for specialized metric. We cannot add `@specialized` to\n- * `Accumulable/AccumulableParam` because it will break Java source compatibility.\n- */\n-private[sql] trait SQLMetricValue[T] extends Serializable {\n+  override def copyAndReset(): SQLMetric = new SQLMetric(metricType, initValue)\n \n-  def value: T\n-\n-  override def toString: String = value.toString\n-}\n-\n-/**\n- * A wrapper of Long to avoid boxing and unboxing when using Accumulator\n- */\n-private[sql] class LongSQLMetricValue(private var _value : Long) extends SQLMetricValue[Long] {\n-\n-  def add(incr: Long): LongSQLMetricValue = {\n-    _value += incr\n-    this\n+  override def merge(other: NewAccumulator[Long, Long]): Unit = other match {\n+    case o: SQLMetric => _value += o.localValue\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n   }\n \n-  // Although there is a boxing here, it's fine because it's only called in SQLListener\n-  override def value: Long = _value\n-\n-  // Needed for SQLListenerSuite\n-  override def equals(other: Any): Boolean = other match {\n-    case o: LongSQLMetricValue => value == o.value\n-    case _ => false\n-  }\n+  override def isZero(): Boolean = _value == initValue\n \n-  override def hashCode(): Int = _value.hashCode()\n-}\n+  override def add(v: Long): Unit = _value += v\n \n-/**\n- * A specialized long Accumulable to avoid boxing and unboxing when using Accumulator's\n- * `+=` and `add`.\n- */\n-private[sql] class LongSQLMetric private[metric](name: String, param: LongSQLMetricParam)\n-  extends SQLMetric[LongSQLMetricValue, Long](name, param) {\n+  def +=(v: Long): Unit = _value += v\n \n-  override def +=(term: Long): Unit = {\n-    localValue.add(term)\n-  }\n+  override def localValue: Long = _value\n \n-  override def add(term: Long): Unit = {\n-    localValue.add(term)\n+  // Provide special identifier as metadata so we can tell that this is a `SQLMetric` later\n+  private[spark] override def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    new AccumulableInfo(id, name, update, value, true, true, Some(SQLMetrics.ACCUM_IDENTIFIER))\n   }\n-}\n-\n-private class LongSQLMetricParam(val stringValue: Seq[Long] => String, initialValue: Long)\n-  extends SQLMetricParam[LongSQLMetricValue, Long] {\n-\n-  override def addAccumulator(r: LongSQLMetricValue, t: Long): LongSQLMetricValue = r.add(t)\n \n-  override def addInPlace(r1: LongSQLMetricValue, r2: LongSQLMetricValue): LongSQLMetricValue =\n-    r1.add(r2.value)\n-\n-  override def zero(initialValue: LongSQLMetricValue): LongSQLMetricValue = zero\n-\n-  override def zero: LongSQLMetricValue = new LongSQLMetricValue(initialValue)\n+  def reset(): Unit = _value = initValue\n }\n \n-private object LongSQLMetricParam\n-  extends LongSQLMetricParam(x => NumberFormat.getInstance().format(x.sum), 0L)\n-\n-private object StatisticsBytesSQLMetricParam extends LongSQLMetricParam(\n-  (values: Seq[Long]) => {\n-    // This is a workaround for SPARK-11013.\n-    // We use -1 as initial value of the accumulator, if the accumulator is valid, we will update\n-    // it at the end of task and the value will be at least 0.\n-    val validValues = values.filter(_ >= 0)\n-    val Seq(sum, min, med, max) = {\n-      val metric = if (validValues.length == 0) {\n-        Seq.fill(4)(0L)\n-      } else {\n-        val sorted = validValues.sorted\n-        Seq(sorted.sum, sorted(0), sorted(validValues.length / 2), sorted(validValues.length - 1))\n-      }\n-      metric.map(Utils.bytesToString)\n-    }\n-    s\"\\n$sum ($min, $med, $max)\"\n-  }, -1L)\n-\n-private object StatisticsTimingSQLMetricParam extends LongSQLMetricParam(\n-  (values: Seq[Long]) => {\n-    // This is a workaround for SPARK-11013.\n-    // We use -1 as initial value of the accumulator, if the accumulator is valid, we will update\n-    // it at the end of task and the value will be at least 0.\n-    val validValues = values.filter(_ >= 0)\n-    val Seq(sum, min, med, max) = {\n-      val metric = if (validValues.length == 0) {\n-        Seq.fill(4)(0L)\n-      } else {\n-        val sorted = validValues.sorted\n-        Seq(sorted.sum, sorted(0), sorted(validValues.length / 2), sorted(validValues.length - 1))\n-      }\n-      metric.map(Utils.msDurationToString)\n-    }\n-    s\"\\n$sum ($min, $med, $max)\"\n-  }, -1L)\n \n private[sql] object SQLMetrics {\n-\n   // Identifier for distinguishing SQL metrics from other accumulators\n   private[sql] val ACCUM_IDENTIFIER = \"sql\"\n \n-  private def createLongMetric(\n-      sc: SparkContext,\n-      name: String,\n-      param: LongSQLMetricParam): LongSQLMetric = {\n-    val acc = new LongSQLMetric(name, param)\n-    // This is an internal accumulator so we need to register it explicitly.\n-    Accumulators.register(acc)\n-    sc.cleaner.foreach(_.registerAccumulatorForCleanup(acc))\n-    acc\n-  }\n+  private[sql] val SUM_METRIC = \"sum\"\n+  private[sql] val SIZE_METRIC = \"size\"\n+  private[sql] val TIMING_METRIC = \"timing\"\n \n-  def createLongMetric(sc: SparkContext, name: String): LongSQLMetric = {\n-    createLongMetric(sc, name, LongSQLMetricParam)\n+  def createMetric(sc: SparkContext, name: String): SQLMetric = {\n+    val acc = new SQLMetric(SUM_METRIC)\n+    acc.register(sc, name = Some(name), countFailedValues = true)",
    "line": 188
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "oh damn this is a mistake! I'm surprised our tests don't cover it, I'll fix it and add regression test\n",
    "commit": "124568b3eeb7e0a657b2fbe4f54bb85543b7ffa3",
    "createdAt": "2016-05-07T03:02:14Z",
    "diffHunk": "@@ -19,200 +19,106 @@ package org.apache.spark.sql.execution.metric\n \n import java.text.NumberFormat\n \n-import org.apache.spark.{Accumulable, AccumulableParam, Accumulators, SparkContext}\n+import org.apache.spark.{NewAccumulator, SparkContext}\n import org.apache.spark.scheduler.AccumulableInfo\n import org.apache.spark.util.Utils\n \n-/**\n- * Create a layer for specialized metric. We cannot add `@specialized` to\n- * `Accumulable/AccumulableParam` because it will break Java source compatibility.\n- *\n- * An implementation of SQLMetric should override `+=` and `add` to avoid boxing.\n- */\n-private[sql] abstract class SQLMetric[R <: SQLMetricValue[T], T](\n-    name: String,\n-    val param: SQLMetricParam[R, T]) extends Accumulable[R, T](param.zero, param, Some(name)) {\n \n-  // Provide special identifier as metadata so we can tell that this is a `SQLMetric` later\n-  override def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n-    new AccumulableInfo(id, Some(name), update, value, true, countFailedValues,\n-      Some(SQLMetrics.ACCUM_IDENTIFIER))\n-  }\n-\n-  def reset(): Unit = {\n-    this.value = param.zero\n-  }\n-}\n-\n-/**\n- * Create a layer for specialized metric. We cannot add `@specialized` to\n- * `Accumulable/AccumulableParam` because it will break Java source compatibility.\n- */\n-private[sql] trait SQLMetricParam[R <: SQLMetricValue[T], T] extends AccumulableParam[R, T] {\n-\n-  /**\n-   * A function that defines how we aggregate the final accumulator results among all tasks,\n-   * and represent it in string for a SQL physical operator.\n-   */\n-  val stringValue: Seq[T] => String\n-\n-  def zero: R\n-}\n+class SQLMetric(val metricType: String, initValue: Long = 0L) extends NewAccumulator[Long, Long] {\n+  // This is a workaround for SPARK-11013.\n+  // We may use -1 as initial value of the accumulator, if the accumulator is valid, we will\n+  // update it at the end of task and the value will be at least 0. Then we can filter out the -1\n+  // values before calculate max, min, etc.\n+  private[this] var _value = initValue\n \n-/**\n- * Create a layer for specialized metric. We cannot add `@specialized` to\n- * `Accumulable/AccumulableParam` because it will break Java source compatibility.\n- */\n-private[sql] trait SQLMetricValue[T] extends Serializable {\n+  override def copyAndReset(): SQLMetric = new SQLMetric(metricType, initValue)\n \n-  def value: T\n-\n-  override def toString: String = value.toString\n-}\n-\n-/**\n- * A wrapper of Long to avoid boxing and unboxing when using Accumulator\n- */\n-private[sql] class LongSQLMetricValue(private var _value : Long) extends SQLMetricValue[Long] {\n-\n-  def add(incr: Long): LongSQLMetricValue = {\n-    _value += incr\n-    this\n+  override def merge(other: NewAccumulator[Long, Long]): Unit = other match {\n+    case o: SQLMetric => _value += o.localValue\n+    case _ => throw new UnsupportedOperationException(\n+      s\"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\")\n   }\n \n-  // Although there is a boxing here, it's fine because it's only called in SQLListener\n-  override def value: Long = _value\n-\n-  // Needed for SQLListenerSuite\n-  override def equals(other: Any): Boolean = other match {\n-    case o: LongSQLMetricValue => value == o.value\n-    case _ => false\n-  }\n+  override def isZero(): Boolean = _value == initValue\n \n-  override def hashCode(): Int = _value.hashCode()\n-}\n+  override def add(v: Long): Unit = _value += v\n \n-/**\n- * A specialized long Accumulable to avoid boxing and unboxing when using Accumulator's\n- * `+=` and `add`.\n- */\n-private[sql] class LongSQLMetric private[metric](name: String, param: LongSQLMetricParam)\n-  extends SQLMetric[LongSQLMetricValue, Long](name, param) {\n+  def +=(v: Long): Unit = _value += v\n \n-  override def +=(term: Long): Unit = {\n-    localValue.add(term)\n-  }\n+  override def localValue: Long = _value\n \n-  override def add(term: Long): Unit = {\n-    localValue.add(term)\n+  // Provide special identifier as metadata so we can tell that this is a `SQLMetric` later\n+  private[spark] override def toInfo(update: Option[Any], value: Option[Any]): AccumulableInfo = {\n+    new AccumulableInfo(id, name, update, value, true, true, Some(SQLMetrics.ACCUM_IDENTIFIER))\n   }\n-}\n-\n-private class LongSQLMetricParam(val stringValue: Seq[Long] => String, initialValue: Long)\n-  extends SQLMetricParam[LongSQLMetricValue, Long] {\n-\n-  override def addAccumulator(r: LongSQLMetricValue, t: Long): LongSQLMetricValue = r.add(t)\n \n-  override def addInPlace(r1: LongSQLMetricValue, r2: LongSQLMetricValue): LongSQLMetricValue =\n-    r1.add(r2.value)\n-\n-  override def zero(initialValue: LongSQLMetricValue): LongSQLMetricValue = zero\n-\n-  override def zero: LongSQLMetricValue = new LongSQLMetricValue(initialValue)\n+  def reset(): Unit = _value = initValue\n }\n \n-private object LongSQLMetricParam\n-  extends LongSQLMetricParam(x => NumberFormat.getInstance().format(x.sum), 0L)\n-\n-private object StatisticsBytesSQLMetricParam extends LongSQLMetricParam(\n-  (values: Seq[Long]) => {\n-    // This is a workaround for SPARK-11013.\n-    // We use -1 as initial value of the accumulator, if the accumulator is valid, we will update\n-    // it at the end of task and the value will be at least 0.\n-    val validValues = values.filter(_ >= 0)\n-    val Seq(sum, min, med, max) = {\n-      val metric = if (validValues.length == 0) {\n-        Seq.fill(4)(0L)\n-      } else {\n-        val sorted = validValues.sorted\n-        Seq(sorted.sum, sorted(0), sorted(validValues.length / 2), sorted(validValues.length - 1))\n-      }\n-      metric.map(Utils.bytesToString)\n-    }\n-    s\"\\n$sum ($min, $med, $max)\"\n-  }, -1L)\n-\n-private object StatisticsTimingSQLMetricParam extends LongSQLMetricParam(\n-  (values: Seq[Long]) => {\n-    // This is a workaround for SPARK-11013.\n-    // We use -1 as initial value of the accumulator, if the accumulator is valid, we will update\n-    // it at the end of task and the value will be at least 0.\n-    val validValues = values.filter(_ >= 0)\n-    val Seq(sum, min, med, max) = {\n-      val metric = if (validValues.length == 0) {\n-        Seq.fill(4)(0L)\n-      } else {\n-        val sorted = validValues.sorted\n-        Seq(sorted.sum, sorted(0), sorted(validValues.length / 2), sorted(validValues.length - 1))\n-      }\n-      metric.map(Utils.msDurationToString)\n-    }\n-    s\"\\n$sum ($min, $med, $max)\"\n-  }, -1L)\n \n private[sql] object SQLMetrics {\n-\n   // Identifier for distinguishing SQL metrics from other accumulators\n   private[sql] val ACCUM_IDENTIFIER = \"sql\"\n \n-  private def createLongMetric(\n-      sc: SparkContext,\n-      name: String,\n-      param: LongSQLMetricParam): LongSQLMetric = {\n-    val acc = new LongSQLMetric(name, param)\n-    // This is an internal accumulator so we need to register it explicitly.\n-    Accumulators.register(acc)\n-    sc.cleaner.foreach(_.registerAccumulatorForCleanup(acc))\n-    acc\n-  }\n+  private[sql] val SUM_METRIC = \"sum\"\n+  private[sql] val SIZE_METRIC = \"size\"\n+  private[sql] val TIMING_METRIC = \"timing\"\n \n-  def createLongMetric(sc: SparkContext, name: String): LongSQLMetric = {\n-    createLongMetric(sc, name, LongSQLMetricParam)\n+  def createMetric(sc: SparkContext, name: String): SQLMetric = {\n+    val acc = new SQLMetric(SUM_METRIC)\n+    acc.register(sc, name = Some(name), countFailedValues = true)",
    "line": 188
  }],
  "prId": 12612
}]