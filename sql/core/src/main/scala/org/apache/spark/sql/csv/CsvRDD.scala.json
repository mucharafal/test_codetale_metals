[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "is it ever possible for \"quote\" to be something that's longer than 1 char?\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T06:36:54Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,"
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "It is. If you look into CsvTokenizer you see I have assumed the general case (similar to delimiter). I will add unit tests with more than one character quotes to show it.\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T07:16:21Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "What are the examples of quotes that are more than 1 char long?\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T07:33:52Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,"
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "One common one is '' (two single quote characters)\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T07:47:12Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Do people actually use this?\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T07:50:13Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,"
  }],
  "prId": 1351
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "We should consolidate all the type related functions in @yhuai's PR on public/schema type. We have many similar code blocks like this in Spark SQL, and many of them are missing stuff (e.g. this one misses Timestamp, Decimal, etc)\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T07:58:47Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,\n+      useHeader: Boolean): LogicalPlan = {\n+\n+    // Constructing schema\n+    // TODO: Infer types based on a sample and/or let user specify types/schema\n+    val firstLine = csv.first()\n+    // Assuming first row is representative and using it to determine number of fields\n+    val firstRow = new CsvTokenizer(Seq(firstLine).iterator, delimiter, quote).next()\n+    val header = if (useHeader) {\n+      firstRow\n+    } else {\n+      firstRow.zipWithIndex.map { case (value, index) => s\"V$index\" }\n+    }\n+\n+    val schemaFields = header.map { fieldName =>\n+      StructField(fieldName.asInstanceOf[String], StringType, nullable = true)\n+    }\n+    val schema = StructType(schemaFields)\n+\n+    val parsedCSV = csv.mapPartitions { iter =>\n+      // When using header, any input line that equals firstLine is assumed to be header\n+      val csvIter = if (useHeader) {\n+        iter.filter(_ != firstLine)\n+      } else {\n+        iter\n+      }\n+      val tokenIter = new CsvTokenizer(csvIter, delimiter, quote)\n+      parseCSV(tokenIter, schema)\n+    }\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), parsedCSV))\n+  }\n+\n+  private def castToType(value: Any, dataType: DataType): Any = dataType match {"
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "I agree.\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T08:00:47Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,\n+      useHeader: Boolean): LogicalPlan = {\n+\n+    // Constructing schema\n+    // TODO: Infer types based on a sample and/or let user specify types/schema\n+    val firstLine = csv.first()\n+    // Assuming first row is representative and using it to determine number of fields\n+    val firstRow = new CsvTokenizer(Seq(firstLine).iterator, delimiter, quote).next()\n+    val header = if (useHeader) {\n+      firstRow\n+    } else {\n+      firstRow.zipWithIndex.map { case (value, index) => s\"V$index\" }\n+    }\n+\n+    val schemaFields = header.map { fieldName =>\n+      StructField(fieldName.asInstanceOf[String], StringType, nullable = true)\n+    }\n+    val schema = StructType(schemaFields)\n+\n+    val parsedCSV = csv.mapPartitions { iter =>\n+      // When using header, any input line that equals firstLine is assumed to be header\n+      val csvIter = if (useHeader) {\n+        iter.filter(_ != firstLine)\n+      } else {\n+        iter\n+      }\n+      val tokenIter = new CsvTokenizer(csvIter, delimiter, quote)\n+      parseCSV(tokenIter, schema)\n+    }\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), parsedCSV))\n+  }\n+\n+  private def castToType(value: Any, dataType: DataType): Any = dataType match {"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Really what we should do here is use a `Projection` of `Cast` expressions.  That way it'll be able to handle all dataTypes and will eventually get JITed by codegen.  @GregOwen should have an example of doing this.\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-11T02:38:48Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,\n+      useHeader: Boolean): LogicalPlan = {\n+\n+    // Constructing schema\n+    // TODO: Infer types based on a sample and/or let user specify types/schema\n+    val firstLine = csv.first()\n+    // Assuming first row is representative and using it to determine number of fields\n+    val firstRow = new CsvTokenizer(Seq(firstLine).iterator, delimiter, quote).next()\n+    val header = if (useHeader) {\n+      firstRow\n+    } else {\n+      firstRow.zipWithIndex.map { case (value, index) => s\"V$index\" }\n+    }\n+\n+    val schemaFields = header.map { fieldName =>\n+      StructField(fieldName.asInstanceOf[String], StringType, nullable = true)\n+    }\n+    val schema = StructType(schemaFields)\n+\n+    val parsedCSV = csv.mapPartitions { iter =>\n+      // When using header, any input line that equals firstLine is assumed to be header\n+      val csvIter = if (useHeader) {\n+        iter.filter(_ != firstLine)\n+      } else {\n+        iter\n+      }\n+      val tokenIter = new CsvTokenizer(csvIter, delimiter, quote)\n+      parseCSV(tokenIter, schema)\n+    }\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), parsedCSV))\n+  }\n+\n+  private def castToType(value: Any, dataType: DataType): Any = dataType match {"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Also we need to add tests for casting.  Right now I'm pretty sure this just throws `ClassCastException`s for non-strings.\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-11T02:46:24Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,\n+      useHeader: Boolean): LogicalPlan = {\n+\n+    // Constructing schema\n+    // TODO: Infer types based on a sample and/or let user specify types/schema\n+    val firstLine = csv.first()\n+    // Assuming first row is representative and using it to determine number of fields\n+    val firstRow = new CsvTokenizer(Seq(firstLine).iterator, delimiter, quote).next()\n+    val header = if (useHeader) {\n+      firstRow\n+    } else {\n+      firstRow.zipWithIndex.map { case (value, index) => s\"V$index\" }\n+    }\n+\n+    val schemaFields = header.map { fieldName =>\n+      StructField(fieldName.asInstanceOf[String], StringType, nullable = true)\n+    }\n+    val schema = StructType(schemaFields)\n+\n+    val parsedCSV = csv.mapPartitions { iter =>\n+      // When using header, any input line that equals firstLine is assumed to be header\n+      val csvIter = if (useHeader) {\n+        iter.filter(_ != firstLine)\n+      } else {\n+        iter\n+      }\n+      val tokenIter = new CsvTokenizer(csvIter, delimiter, quote)\n+      parseCSV(tokenIter, schema)\n+    }\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), parsedCSV))\n+  }\n+\n+  private def castToType(value: Any, dataType: DataType): Any = dataType match {"
  }],
  "prId": 1351
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "We'd better make the GenericMutableRow object as a private member, which is reusable.\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-11T00:50:09Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,\n+      useHeader: Boolean): LogicalPlan = {\n+\n+    // Constructing schema\n+    // TODO: Infer types based on a sample and/or let user specify types/schema\n+    val firstLine = csv.first()\n+    // Assuming first row is representative and using it to determine number of fields\n+    val firstRow = new CsvTokenizer(Seq(firstLine).iterator, delimiter, quote).next()\n+    val header = if (useHeader) {\n+      firstRow\n+    } else {\n+      firstRow.zipWithIndex.map { case (value, index) => s\"V$index\" }\n+    }\n+\n+    val schemaFields = header.map { fieldName =>\n+      StructField(fieldName.asInstanceOf[String], StringType, nullable = true)\n+    }\n+    val schema = StructType(schemaFields)\n+\n+    val parsedCSV = csv.mapPartitions { iter =>\n+      // When using header, any input line that equals firstLine is assumed to be header\n+      val csvIter = if (useHeader) {\n+        iter.filter(_ != firstLine)\n+      } else {\n+        iter\n+      }\n+      val tokenIter = new CsvTokenizer(csvIter, delimiter, quote)\n+      parseCSV(tokenIter, schema)\n+    }\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), parsedCSV))\n+  }\n+\n+  private def castToType(value: Any, dataType: DataType): Any = dataType match {\n+    case StringType => value.asInstanceOf[String]\n+    case BooleanType => value.asInstanceOf[Boolean]\n+    case DoubleType => value.asInstanceOf[Double]\n+    case FloatType => value.asInstanceOf[Float]\n+    case IntegerType => value.asInstanceOf[Int]\n+    case LongType => value.asInstanceOf[Long]\n+    case ShortType => value.asInstanceOf[Short]\n+    case _ => null\n+  }\n+\n+  private def parseCSV(iter: Iterator[Array[String]], schema: StructType): Iterator[Row] = {\n+    val row = new GenericMutableRow(schema.fields.length)"
  }],
  "prId": 1351
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Pass in the value of `schema.fields.zipWithIndex`, instead of the object `schema` for performance concern?\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-11T00:52:23Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.expressions.{GenericMutableRow, AttributeReference, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.{ExistingRdd, SparkLogicalPlan}\n+import org.apache.spark.sql.Logging\n+\n+private[sql] object CsvRDD extends Logging {\n+\n+  private[sql] def inferSchema(\n+      csv: RDD[String],\n+      delimiter: String,\n+      quote: String,\n+      useHeader: Boolean): LogicalPlan = {\n+\n+    // Constructing schema\n+    // TODO: Infer types based on a sample and/or let user specify types/schema\n+    val firstLine = csv.first()\n+    // Assuming first row is representative and using it to determine number of fields\n+    val firstRow = new CsvTokenizer(Seq(firstLine).iterator, delimiter, quote).next()\n+    val header = if (useHeader) {\n+      firstRow\n+    } else {\n+      firstRow.zipWithIndex.map { case (value, index) => s\"V$index\" }\n+    }\n+\n+    val schemaFields = header.map { fieldName =>\n+      StructField(fieldName.asInstanceOf[String], StringType, nullable = true)\n+    }\n+    val schema = StructType(schemaFields)\n+\n+    val parsedCSV = csv.mapPartitions { iter =>\n+      // When using header, any input line that equals firstLine is assumed to be header\n+      val csvIter = if (useHeader) {\n+        iter.filter(_ != firstLine)\n+      } else {\n+        iter\n+      }\n+      val tokenIter = new CsvTokenizer(csvIter, delimiter, quote)\n+      parseCSV(tokenIter, schema)\n+    }\n+\n+    SparkLogicalPlan(ExistingRdd(asAttributes(schema), parsedCSV))\n+  }\n+\n+  private def castToType(value: Any, dataType: DataType): Any = dataType match {\n+    case StringType => value.asInstanceOf[String]\n+    case BooleanType => value.asInstanceOf[Boolean]\n+    case DoubleType => value.asInstanceOf[Double]\n+    case FloatType => value.asInstanceOf[Float]\n+    case IntegerType => value.asInstanceOf[Int]\n+    case LongType => value.asInstanceOf[Long]\n+    case ShortType => value.asInstanceOf[Short]\n+    case _ => null\n+  }\n+\n+  private def parseCSV(iter: Iterator[Array[String]], schema: StructType): Iterator[Row] = {\n+    val row = new GenericMutableRow(schema.fields.length)\n+    iter.map { tokens =>\n+      schema.fields.zipWithIndex.foreach {"
  }],
  "prId": 1351
}]