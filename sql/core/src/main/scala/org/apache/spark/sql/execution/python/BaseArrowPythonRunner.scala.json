[{
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "there seem to be some formatting issues in this file",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-10T22:45:46Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamReader\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+\n+\n+abstract class BaseArrowPythonRunner[T](\n+                         funcs: Seq[ChainedPythonFunctions],"
  }, {
    "author": {
      "login": "d80tb7"
    },
    "body": "I've had a go at reformatting it (and BasePandasGroupExec).  Let me know if you think it's still incorrect as I'm not entirely sure what the standard should be here.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-07-11T21:18:26Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamReader\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+\n+\n+abstract class BaseArrowPythonRunner[T](\n+                         funcs: Seq[ChainedPythonFunctions],"
  }],
  "prId": 24981
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: remove an extra line.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-08-27T20:30:46Z",
    "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamReader\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Common functionality for a udf runner that exchanges data with Python worker via Arrow stream.\n+ */\n+abstract class BaseArrowPythonRunner[T](\n+    funcs: Seq[ChainedPythonFunctions],\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]])\n+  extends BasePythonRunner[T, ColumnarBatch](funcs, evalType, argOffsets) {\n+\n+"
  }],
  "prId": 24981
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "ditto.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-08-27T20:31:09Z",
    "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamReader\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Common functionality for a udf runner that exchanges data with Python worker via Arrow stream.\n+ */\n+abstract class BaseArrowPythonRunner[T](\n+    funcs: Seq[ChainedPythonFunctions],\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]])\n+  extends BasePythonRunner[T, ColumnarBatch](funcs, evalType, argOffsets) {\n+\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      releasedOrClosed: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, releasedOrClosed, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      context.addTaskCompletionListener[Unit] { _ =>\n+        if (reader != null) {\n+          reader.close(false)\n+        }\n+        allocator.close()\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(vectors)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              reader.close(false)\n+              allocator.close()\n+              // Reach end of stream. Call `read()` again to read control data.\n+              read()\n+            }\n+          } else {\n+            stream.readInt() match {\n+              case SpecialLengths.START_ARROW_STREAM =>\n+                reader = new ArrowStreamReader(stream, allocator)\n+                root = reader.getVectorSchemaRoot()\n+                schema = ArrowUtils.fromArrowSchema(root.getSchema())\n+                vectors = root.getFieldVectors().asScala.map { vector =>\n+                  new ArrowColumnVector(vector)\n+                }.toArray[ColumnVector]\n+                read()\n+              case SpecialLengths.TIMING_DATA =>\n+                handleTimingData()\n+                read()\n+              case SpecialLengths.PYTHON_EXCEPTION_THROWN =>\n+                throw handlePythonException()\n+              case SpecialLengths.END_OF_DATA_SECTION =>\n+                handleEndOfDataSection()\n+                null\n+            }\n+          }\n+        } catch handleException\n+      }\n+    }\n+  }\n+}\n+\n+"
  }],
  "prId": 24981
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Why did we do such refactoring in this PR? it should better be separate; otherwise, it's hard to follow the changes.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-09-22T12:45:16Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamReader\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Common functionality for a udf runner that exchanges data with Python worker via Arrow stream.\n+ */\n+abstract class BaseArrowPythonRunner[T](",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "One thing we should consider is, R vectorized code path is matched with Python side. We should think about that before generalizing it - my goal was that deduplicating both code paths.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-09-22T12:45:55Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamReader\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Common functionality for a udf runner that exchanges data with Python worker via Arrow stream.\n+ */\n+abstract class BaseArrowPythonRunner[T](",
    "line": 37
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "I think the changes were pretty straightforward to support the feature, mainly to be able to read back results the same way as group map udfs. I didn't consider this to be major refactoring, so making a complete copy of the python runner seemed a little excessive. Otherwise I would agree to keep things separate.\r\n\r\n>We should think about that before generalizing it - my goal was that deduplicating both code paths.\r\n\r\nThis sounds like a good idea, it shouldn't really matter if writing to Python or R, and would be good to deduplicate.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-09-22T19:00:44Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamReader\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Common functionality for a udf runner that exchanges data with Python worker via Arrow stream.\n+ */\n+abstract class BaseArrowPythonRunner[T](",
    "line": 37
  }],
  "prId": 24981
}]