[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "All apis in execution are non-public so there's no need to document \"experimental\" or \"unstable\" api here. They only have public visibility for debugging.\r\n",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-05T14:02:10Z",
    "diffHunk": "@@ -0,0 +1,629 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.annotation.{DeveloperApi, Experimental, Unstable}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * :: Experimental ::\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[pre]] function can be used to replace [[SparkPlan]] instances with\n+ * versions that support a columnar implementation. After this Spark will insert any transitions\n+ * necessary. This includes transitions from row to columnar [[RowToColumnarExec]] and from\n+ * columnar to row [[ColumnarToRowExec]].  After this the [[post]] function is called to allow\n+ * replacing any of the implementations of the transitions or doing cleanup of the plan, like\n+ * inserting stages to build larger batches for more efficient processing, or stages that\n+ * transition the data to/from an accelerator's memory.\n+ */\n+@Experimental"
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "So how do we want to handle this situation where we are exposing some internal implementation details but with no real guarantees of stability?  Not marking them as anything I felt gave the wrong impression that there was some guarantee.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-05T14:28:51Z",
    "diffHunk": "@@ -0,0 +1,629 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.annotation.{DeveloperApi, Experimental, Unstable}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * :: Experimental ::\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[pre]] function can be used to replace [[SparkPlan]] instances with\n+ * versions that support a columnar implementation. After this Spark will insert any transitions\n+ * necessary. This includes transitions from row to columnar [[RowToColumnarExec]] and from\n+ * columnar to row [[ColumnarToRowExec]].  After this the [[post]] function is called to allow\n+ * replacing any of the implementations of the transitions or doing cleanup of the plan, like\n+ * inserting stages to build larger batches for more efficient processing, or stages that\n+ * transition the data to/from an accelerator's memory.\n+ */\n+@Experimental"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "They are all private. The package.scala file documents that. Also execution and catalyst packages are ignored in the public API docs.\r\n\r\nBasically it's use at your own risk type of internal APIs.\r\n",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-05T14:32:37Z",
    "diffHunk": "@@ -0,0 +1,629 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.annotation.{DeveloperApi, Experimental, Unstable}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * :: Experimental ::\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[pre]] function can be used to replace [[SparkPlan]] instances with\n+ * versions that support a columnar implementation. After this Spark will insert any transitions\n+ * necessary. This includes transitions from row to columnar [[RowToColumnarExec]] and from\n+ * columnar to row [[ColumnarToRowExec]].  After this the [[post]] function is called to allow\n+ * replacing any of the implementations of the transitions or doing cleanup of the plan, like\n+ * inserting stages to build larger batches for more efficient processing, or stages that\n+ * transition the data to/from an accelerator's memory.\n+ */\n+@Experimental"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "Are `pre` and `post` appropriate naming, in particular, `pre`?  IIUC correctly, `pre` is a mandatory function for columnar processing and `post` is an optional function to optimize columnars or to clean up resources. To me, a pair of `pre` and `post` looks dual. (Of course, I am not a native English speaker).\r\n\r\nIn addition, can you create a test case using `ColunarRule` (for example, a simplified version of [this](https://gist.github.com/revans2/c3cad77075c4fa5d9d271308ee2f1b1d), but use `pre` and `post`)? It would help understanding of reviewers and can ensure the behavior of this API.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-06T18:58:48Z",
    "diffHunk": "@@ -0,0 +1,609 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[pre]] function can be used to replace [[SparkPlan]] instances with\n+ * versions that support a columnar implementation. After this Spark will insert any transitions\n+ * necessary. This includes transitions from row to columnar [[RowToColumnarExec]] and from\n+ * columnar to row [[ColumnarToRowExec]].  After this the [[post]] function is called to allow\n+ * replacing any of the implementations of the transitions or doing cleanup of the plan, like\n+ * inserting stages to build larger batches for more efficient processing, or stages that\n+ * transition the data to/from an accelerator's memory.\n+ */\n+class ColumnarRule {",
    "line": 47
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "For me `pre` and `post` made since in relation to the comments, but perhaps not standalone. I'll see if I can make them more descriptive.  I'll also add in a test.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-06T19:44:44Z",
    "diffHunk": "@@ -0,0 +1,609 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[pre]] function can be used to replace [[SparkPlan]] instances with\n+ * versions that support a columnar implementation. After this Spark will insert any transitions\n+ * necessary. This includes transitions from row to columnar [[RowToColumnarExec]] and from\n+ * columnar to row [[ColumnarToRowExec]].  After this the [[post]] function is called to allow\n+ * replacing any of the implementations of the transitions or doing cleanup of the plan, like\n+ * inserting stages to build larger batches for more efficient processing, or stages that\n+ * transition the data to/from an accelerator's memory.\n+ */\n+class ColumnarRule {",
    "line": 47
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "Could this be simplified to a flatMap as done here #24816 ?  I don't think this loads batches eagerly, since it's done on `hasNext`, but a flatMap would be a easier to read.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-14T22:48:32Z",
    "diffHunk": "@@ -0,0 +1,594 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[preColumnarTransitions]] [[Rule]] can be used to replace\n+ * [[SparkPlan]] instances with versions that support a columnar implementation. After this\n+ * Spark will insert any transitions necessary. This includes transitions from row to columnar\n+ * [[RowToColumnarExec]] and from columnar to row [[ColumnarToRowExec]]. At this point the\n+ * [[postColumnarTransitions]] [[Rule]] is called to allow replacing any of the implementations\n+ * of the transitions or doing cleanup of the plan, like inserting stages to build larger batches\n+ * for more efficient processing, or stages that transition the data to/from an accelerator's\n+ * memory.\n+ */\n+class ColumnarRule {\n+  def preColumnarTransitions: Rule[SparkPlan] = plan => plan\n+  def postColumnarTransitions: Rule[SparkPlan] = plan => plan\n+}\n+\n+/**\n+ * Provides a common executor to translate an [[RDD]] of [[ColumnarBatch]] into an [[RDD]] of\n+ * [[InternalRow]]. This is inserted whenever such a transition is determined to be needed.\n+ *\n+ * The implementation is based off of similar implementations in [[ColumnarBatchScan]],\n+ * [[org.apache.spark.sql.execution.python.ArrowEvalPythonExec]], and\n+ * [[MapPartitionsInRWithArrowExec]]. Eventually this should replace those implementations.\n+ */\n+case class ColumnarToRowExec(child: SparkPlan)\n+  extends UnaryExecNode with CodegenSupport {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n+\n+  override lazy val metrics: Map[String, SQLMetric] = Map(\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+    \"numInputBatches\" -> SQLMetrics.createMetric(sparkContext, \"number of input batches\"),\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\")\n+  )\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    val numOutputRows = longMetric(\"numOutputRows\")\n+    val numInputBatches = longMetric(\"numInputBatches\")\n+    val scanTime = longMetric(\"scanTime\")\n+    val batches = child.executeColumnar()\n+    batches.mapPartitions { cbIter =>"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "Could this be used as a generalized replacement for `ArrowWriter` in ArrowWriter.scala, which writes to ArrowColumnVectors?",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-14T22:55:52Z",
    "diffHunk": "@@ -0,0 +1,594 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[preColumnarTransitions]] [[Rule]] can be used to replace\n+ * [[SparkPlan]] instances with versions that support a columnar implementation. After this\n+ * Spark will insert any transitions necessary. This includes transitions from row to columnar\n+ * [[RowToColumnarExec]] and from columnar to row [[ColumnarToRowExec]]. At this point the\n+ * [[postColumnarTransitions]] [[Rule]] is called to allow replacing any of the implementations\n+ * of the transitions or doing cleanup of the plan, like inserting stages to build larger batches\n+ * for more efficient processing, or stages that transition the data to/from an accelerator's\n+ * memory.\n+ */\n+class ColumnarRule {\n+  def preColumnarTransitions: Rule[SparkPlan] = plan => plan\n+  def postColumnarTransitions: Rule[SparkPlan] = plan => plan\n+}\n+\n+/**\n+ * Provides a common executor to translate an [[RDD]] of [[ColumnarBatch]] into an [[RDD]] of\n+ * [[InternalRow]]. This is inserted whenever such a transition is determined to be needed.\n+ *\n+ * The implementation is based off of similar implementations in [[ColumnarBatchScan]],\n+ * [[org.apache.spark.sql.execution.python.ArrowEvalPythonExec]], and\n+ * [[MapPartitionsInRWithArrowExec]]. Eventually this should replace those implementations.\n+ */\n+case class ColumnarToRowExec(child: SparkPlan)\n+  extends UnaryExecNode with CodegenSupport {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n+\n+  override lazy val metrics: Map[String, SQLMetric] = Map(\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+    \"numInputBatches\" -> SQLMetrics.createMetric(sparkContext, \"number of input batches\"),\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\")\n+  )\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    val numOutputRows = longMetric(\"numOutputRows\")\n+    val numInputBatches = longMetric(\"numInputBatches\")\n+    val scanTime = longMetric(\"scanTime\")\n+    val batches = child.executeColumnar()\n+    batches.mapPartitions { cbIter =>\n+      // UnsafeProjection is not serializable so do it on the executor side\n+      val outputProject = UnsafeProjection.create(output, output)\n+      new Iterator[InternalRow] {\n+        var it: java.util.Iterator[InternalRow] = null\n+\n+        def loadNextBatch: Unit = {\n+          if (it != null) {\n+            it = null\n+          }\n+          val batchStartNs = System.nanoTime()\n+          if (cbIter.hasNext) {\n+            val cb = cbIter.next()\n+            it = cb.rowIterator()\n+            numInputBatches += 1\n+            // In order to match the numOutputRows metric in the generated code we update\n+            // numOutputRows for each batch. This is less accurate than doing it at output\n+            // because it will over count the number of rows output in the case of a limit,\n+            // but it is more efficient.\n+            numOutputRows += cb.numRows()\n+          }\n+          scanTime += ((System.nanoTime() - batchStartNs) / (1000 * 1000))\n+        }\n+\n+        override def hasNext: Boolean = {\n+          val itHasNext = it != null && it.hasNext\n+          if (!itHasNext) {\n+            loadNextBatch\n+            it != null && it.hasNext\n+          } else {\n+            itHasNext\n+          }\n+        }\n+\n+        override def next(): InternalRow = {\n+          if (it == null || !it.hasNext) {\n+            loadNextBatch\n+          }\n+          if (it == null) {\n+            throw new NoSuchElementException()\n+          }\n+          it.next()\n+        }\n+        // This is to convert the InternalRow to an UnsafeRow. Even though the type is\n+        // InternalRow some operations downstream operations like collect require it to\n+        // be UnsafeRow\n+      }.map(outputProject)\n+    }\n+  }\n+\n+  /**\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\n+   * This is called once per [[ColumnVector]] in the batch.\n+   *\n+   * This code came unchanged from [[ColumnarBatchScan]] and will hopefully replace it\n+   * at some point.\n+   */\n+  private def genCodeColumnVector(\n+      ctx: CodegenContext,\n+      columnVar: String,\n+      ordinal: String,\n+      dataType: DataType,\n+      nullable: Boolean): ExprCode = {\n+    val javaType = CodeGenerator.javaType(dataType)\n+    val value = CodeGenerator.getValueFromVector(columnVar, dataType, ordinal)\n+    val isNullVar = if (nullable) {\n+      JavaCode.isNullVariable(ctx.freshName(\"isNull\"))\n+    } else {\n+      FalseLiteral\n+    }\n+    val valueVar = ctx.freshName(\"value\")\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\n+    val code = code\"${ctx.registerComment(str)}\" + (if (nullable) {\n+      code\"\"\"\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\n+        $javaType $valueVar = $isNullVar ? ${CodeGenerator.defaultValue(dataType)} : ($value);\n+      \"\"\"\n+    } else {\n+      code\"$javaType $valueVar = $value;\"\n+    })\n+    ExprCode(code, isNullVar, JavaCode.variable(valueVar, dataType))\n+  }\n+\n+  /**\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\n+   * This produces an [[org.apache.spark.sql.catalyst.expressions.UnsafeRow]] for each row in\n+   * each batch.\n+   *\n+   * This code came almost completely unchanged from [[ColumnarBatchScan]] and will\n+   * hopefully replace it at some point.\n+   */\n+  override protected def doProduce(ctx: CodegenContext): String = {\n+    // PhysicalRDD always just has one input\n+    val input = ctx.addMutableState(\"scala.collection.Iterator\", \"input\",\n+      v => s\"$v = inputs[0];\")\n+\n+    // metrics\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\n+    val numInputBatches = metricTerm(ctx, \"numInputBatches\")\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\n+    val scanTimeTotalNs =\n+      ctx.addMutableState(CodeGenerator.JAVA_LONG, \"scanTime\") // init as scanTime = 0\n+\n+    val columnarBatchClz = classOf[ColumnarBatch].getName\n+    val batch = ctx.addMutableState(columnarBatchClz, \"batch\")\n+\n+    val idx = ctx.addMutableState(CodeGenerator.JAVA_INT, \"batchIdx\") // init as batchIdx = 0\n+    val columnVectorClzs = child.vectorTypes.getOrElse(\n+      Seq.fill(output.indices.size)(classOf[ColumnVector].getName))\n+    val (colVars, columnAssigns) = columnVectorClzs.zipWithIndex.map {\n+      case (columnVectorClz, i) =>\n+        val name = ctx.addMutableState(columnVectorClz, s\"colInstance$i\")\n+        (name, s\"$name = ($columnVectorClz) $batch.column($i);\")\n+    }.unzip\n+\n+    val nextBatch = ctx.freshName(\"nextBatch\")\n+    val nextBatchFuncName = ctx.addNewFunction(nextBatch,\n+      s\"\"\"\n+         |private void $nextBatch() throws java.io.IOException {\n+         |  long getBatchStart = System.nanoTime();\n+         |  if ($input.hasNext()) {\n+         |    $batch = ($columnarBatchClz)$input.next();\n+         |    $numOutputRows.add($batch.numRows());\n+         |    $idx = 0;\n+         |    ${columnAssigns.mkString(\"\", \"\\n\", \"\\n\")}\n+         |    ${numInputBatches}.add(1);\n+         |  }\n+         |  $scanTimeTotalNs += System.nanoTime() - getBatchStart;\n+         |}\"\"\".stripMargin)\n+\n+    ctx.currentVars = null\n+    val rowidx = ctx.freshName(\"rowIdx\")\n+    val columnsBatchInput = (output zip colVars).map { case (attr, colVar) =>\n+      genCodeColumnVector(ctx, colVar, rowidx, attr.dataType, attr.nullable)\n+    }\n+    val localIdx = ctx.freshName(\"localIdx\")\n+    val localEnd = ctx.freshName(\"localEnd\")\n+    val numRows = ctx.freshName(\"numRows\")\n+    val shouldStop = if (parent.needStopCheck) {\n+      s\"if (shouldStop()) { $idx = $rowidx + 1; return; }\"\n+    } else {\n+      \"// shouldStop check is eliminated\"\n+    }\n+    s\"\"\"\n+       |if ($batch == null) {\n+       |  $nextBatchFuncName();\n+       |}\n+       |while ($batch != null) {\n+       |  int $numRows = $batch.numRows();\n+       |  int $localEnd = $numRows - $idx;\n+       |  for (int $localIdx = 0; $localIdx < $localEnd; $localIdx++) {\n+       |    int $rowidx = $idx + $localIdx;\n+       |    ${consume(ctx, columnsBatchInput).trim}\n+       |    $shouldStop\n+       |  }\n+       |  $idx = $numRows;\n+       |  $batch = null;\n+       |  $nextBatchFuncName();\n+       |}\n+       |$scanTimeMetric.add($scanTimeTotalNs / (1000 * 1000));\n+       |$scanTimeTotalNs = 0;\n+     \"\"\".stripMargin\n+  }\n+\n+  override def inputRDDs(): Seq[RDD[InternalRow]] = {\n+    child.asInstanceOf[CodegenSupport].inputRDDs()\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to append row based data to an array of\n+ * [[WritableColumnVector]].\n+ */\n+private[execution] class RowToColumnConverter(schema: StructType) extends Serializable {\n+  private val converters = schema.fields.map {\n+    f => RowToColumnConverter.getConverterForType(f.dataType)\n+  }\n+\n+  final def convert(row: InternalRow, vectors: Array[WritableColumnVector]): Unit = {\n+    var idx = 0\n+    while (idx < row.numFields) {\n+      converters(idx).append(row, idx, vectors(idx))\n+      idx += 1\n+    }\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to extract a column from a row and append it to a\n+ * [[WritableColumnVector]].\n+ */\n+private object RowToColumnConverter {",
    "line": 237
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "That is the eventual goal.  In this patch, I wanted the code to not replace any existing code to keep the possible impacts smaller.  \r\n\r\nI am currently working on a separate patch to replace ColumnarBatchScan with ColumnarToRowExec.  The patch is mostly just updating tests to match the new pattern, but I also want to run a lot of benchmark code to be sure that I didn't regress anything.\r\n\r\nAfter that, I was going to start working on the various APIs that use arrow data behind the scenes to process data, R, Python, etc.  That is likely to have a larger impact because the code that does those transitions is spread through a lot more files.  I also don't want to get too far ahead of myself as this patch all of them would be based off of is not even in yet.  ",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-17T12:34:24Z",
    "diffHunk": "@@ -0,0 +1,594 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[preColumnarTransitions]] [[Rule]] can be used to replace\n+ * [[SparkPlan]] instances with versions that support a columnar implementation. After this\n+ * Spark will insert any transitions necessary. This includes transitions from row to columnar\n+ * [[RowToColumnarExec]] and from columnar to row [[ColumnarToRowExec]]. At this point the\n+ * [[postColumnarTransitions]] [[Rule]] is called to allow replacing any of the implementations\n+ * of the transitions or doing cleanup of the plan, like inserting stages to build larger batches\n+ * for more efficient processing, or stages that transition the data to/from an accelerator's\n+ * memory.\n+ */\n+class ColumnarRule {\n+  def preColumnarTransitions: Rule[SparkPlan] = plan => plan\n+  def postColumnarTransitions: Rule[SparkPlan] = plan => plan\n+}\n+\n+/**\n+ * Provides a common executor to translate an [[RDD]] of [[ColumnarBatch]] into an [[RDD]] of\n+ * [[InternalRow]]. This is inserted whenever such a transition is determined to be needed.\n+ *\n+ * The implementation is based off of similar implementations in [[ColumnarBatchScan]],\n+ * [[org.apache.spark.sql.execution.python.ArrowEvalPythonExec]], and\n+ * [[MapPartitionsInRWithArrowExec]]. Eventually this should replace those implementations.\n+ */\n+case class ColumnarToRowExec(child: SparkPlan)\n+  extends UnaryExecNode with CodegenSupport {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n+\n+  override lazy val metrics: Map[String, SQLMetric] = Map(\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+    \"numInputBatches\" -> SQLMetrics.createMetric(sparkContext, \"number of input batches\"),\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\")\n+  )\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    val numOutputRows = longMetric(\"numOutputRows\")\n+    val numInputBatches = longMetric(\"numInputBatches\")\n+    val scanTime = longMetric(\"scanTime\")\n+    val batches = child.executeColumnar()\n+    batches.mapPartitions { cbIter =>\n+      // UnsafeProjection is not serializable so do it on the executor side\n+      val outputProject = UnsafeProjection.create(output, output)\n+      new Iterator[InternalRow] {\n+        var it: java.util.Iterator[InternalRow] = null\n+\n+        def loadNextBatch: Unit = {\n+          if (it != null) {\n+            it = null\n+          }\n+          val batchStartNs = System.nanoTime()\n+          if (cbIter.hasNext) {\n+            val cb = cbIter.next()\n+            it = cb.rowIterator()\n+            numInputBatches += 1\n+            // In order to match the numOutputRows metric in the generated code we update\n+            // numOutputRows for each batch. This is less accurate than doing it at output\n+            // because it will over count the number of rows output in the case of a limit,\n+            // but it is more efficient.\n+            numOutputRows += cb.numRows()\n+          }\n+          scanTime += ((System.nanoTime() - batchStartNs) / (1000 * 1000))\n+        }\n+\n+        override def hasNext: Boolean = {\n+          val itHasNext = it != null && it.hasNext\n+          if (!itHasNext) {\n+            loadNextBatch\n+            it != null && it.hasNext\n+          } else {\n+            itHasNext\n+          }\n+        }\n+\n+        override def next(): InternalRow = {\n+          if (it == null || !it.hasNext) {\n+            loadNextBatch\n+          }\n+          if (it == null) {\n+            throw new NoSuchElementException()\n+          }\n+          it.next()\n+        }\n+        // This is to convert the InternalRow to an UnsafeRow. Even though the type is\n+        // InternalRow some operations downstream operations like collect require it to\n+        // be UnsafeRow\n+      }.map(outputProject)\n+    }\n+  }\n+\n+  /**\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\n+   * This is called once per [[ColumnVector]] in the batch.\n+   *\n+   * This code came unchanged from [[ColumnarBatchScan]] and will hopefully replace it\n+   * at some point.\n+   */\n+  private def genCodeColumnVector(\n+      ctx: CodegenContext,\n+      columnVar: String,\n+      ordinal: String,\n+      dataType: DataType,\n+      nullable: Boolean): ExprCode = {\n+    val javaType = CodeGenerator.javaType(dataType)\n+    val value = CodeGenerator.getValueFromVector(columnVar, dataType, ordinal)\n+    val isNullVar = if (nullable) {\n+      JavaCode.isNullVariable(ctx.freshName(\"isNull\"))\n+    } else {\n+      FalseLiteral\n+    }\n+    val valueVar = ctx.freshName(\"value\")\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\n+    val code = code\"${ctx.registerComment(str)}\" + (if (nullable) {\n+      code\"\"\"\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\n+        $javaType $valueVar = $isNullVar ? ${CodeGenerator.defaultValue(dataType)} : ($value);\n+      \"\"\"\n+    } else {\n+      code\"$javaType $valueVar = $value;\"\n+    })\n+    ExprCode(code, isNullVar, JavaCode.variable(valueVar, dataType))\n+  }\n+\n+  /**\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\n+   * This produces an [[org.apache.spark.sql.catalyst.expressions.UnsafeRow]] for each row in\n+   * each batch.\n+   *\n+   * This code came almost completely unchanged from [[ColumnarBatchScan]] and will\n+   * hopefully replace it at some point.\n+   */\n+  override protected def doProduce(ctx: CodegenContext): String = {\n+    // PhysicalRDD always just has one input\n+    val input = ctx.addMutableState(\"scala.collection.Iterator\", \"input\",\n+      v => s\"$v = inputs[0];\")\n+\n+    // metrics\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\n+    val numInputBatches = metricTerm(ctx, \"numInputBatches\")\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\n+    val scanTimeTotalNs =\n+      ctx.addMutableState(CodeGenerator.JAVA_LONG, \"scanTime\") // init as scanTime = 0\n+\n+    val columnarBatchClz = classOf[ColumnarBatch].getName\n+    val batch = ctx.addMutableState(columnarBatchClz, \"batch\")\n+\n+    val idx = ctx.addMutableState(CodeGenerator.JAVA_INT, \"batchIdx\") // init as batchIdx = 0\n+    val columnVectorClzs = child.vectorTypes.getOrElse(\n+      Seq.fill(output.indices.size)(classOf[ColumnVector].getName))\n+    val (colVars, columnAssigns) = columnVectorClzs.zipWithIndex.map {\n+      case (columnVectorClz, i) =>\n+        val name = ctx.addMutableState(columnVectorClz, s\"colInstance$i\")\n+        (name, s\"$name = ($columnVectorClz) $batch.column($i);\")\n+    }.unzip\n+\n+    val nextBatch = ctx.freshName(\"nextBatch\")\n+    val nextBatchFuncName = ctx.addNewFunction(nextBatch,\n+      s\"\"\"\n+         |private void $nextBatch() throws java.io.IOException {\n+         |  long getBatchStart = System.nanoTime();\n+         |  if ($input.hasNext()) {\n+         |    $batch = ($columnarBatchClz)$input.next();\n+         |    $numOutputRows.add($batch.numRows());\n+         |    $idx = 0;\n+         |    ${columnAssigns.mkString(\"\", \"\\n\", \"\\n\")}\n+         |    ${numInputBatches}.add(1);\n+         |  }\n+         |  $scanTimeTotalNs += System.nanoTime() - getBatchStart;\n+         |}\"\"\".stripMargin)\n+\n+    ctx.currentVars = null\n+    val rowidx = ctx.freshName(\"rowIdx\")\n+    val columnsBatchInput = (output zip colVars).map { case (attr, colVar) =>\n+      genCodeColumnVector(ctx, colVar, rowidx, attr.dataType, attr.nullable)\n+    }\n+    val localIdx = ctx.freshName(\"localIdx\")\n+    val localEnd = ctx.freshName(\"localEnd\")\n+    val numRows = ctx.freshName(\"numRows\")\n+    val shouldStop = if (parent.needStopCheck) {\n+      s\"if (shouldStop()) { $idx = $rowidx + 1; return; }\"\n+    } else {\n+      \"// shouldStop check is eliminated\"\n+    }\n+    s\"\"\"\n+       |if ($batch == null) {\n+       |  $nextBatchFuncName();\n+       |}\n+       |while ($batch != null) {\n+       |  int $numRows = $batch.numRows();\n+       |  int $localEnd = $numRows - $idx;\n+       |  for (int $localIdx = 0; $localIdx < $localEnd; $localIdx++) {\n+       |    int $rowidx = $idx + $localIdx;\n+       |    ${consume(ctx, columnsBatchInput).trim}\n+       |    $shouldStop\n+       |  }\n+       |  $idx = $numRows;\n+       |  $batch = null;\n+       |  $nextBatchFuncName();\n+       |}\n+       |$scanTimeMetric.add($scanTimeTotalNs / (1000 * 1000));\n+       |$scanTimeTotalNs = 0;\n+     \"\"\".stripMargin\n+  }\n+\n+  override def inputRDDs(): Seq[RDD[InternalRow]] = {\n+    child.asInstanceOf[CodegenSupport].inputRDDs()\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to append row based data to an array of\n+ * [[WritableColumnVector]].\n+ */\n+private[execution] class RowToColumnConverter(schema: StructType) extends Serializable {\n+  private val converters = schema.fields.map {\n+    f => RowToColumnConverter.getConverterForType(f.dataType)\n+  }\n+\n+  final def convert(row: InternalRow, vectors: Array[WritableColumnVector]): Unit = {\n+    var idx = 0\n+    while (idx < row.numFields) {\n+      converters(idx).append(row, idx, vectors(idx))\n+      idx += 1\n+    }\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to extract a column from a row and append it to a\n+ * [[WritableColumnVector]].\n+ */\n+private object RowToColumnConverter {",
    "line": 237
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "Could `TypeConverter` take care of writing the nulls so it's not duplicated on each converter?",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-14T23:29:53Z",
    "diffHunk": "@@ -0,0 +1,594 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[preColumnarTransitions]] [[Rule]] can be used to replace\n+ * [[SparkPlan]] instances with versions that support a columnar implementation. After this\n+ * Spark will insert any transitions necessary. This includes transitions from row to columnar\n+ * [[RowToColumnarExec]] and from columnar to row [[ColumnarToRowExec]]. At this point the\n+ * [[postColumnarTransitions]] [[Rule]] is called to allow replacing any of the implementations\n+ * of the transitions or doing cleanup of the plan, like inserting stages to build larger batches\n+ * for more efficient processing, or stages that transition the data to/from an accelerator's\n+ * memory.\n+ */\n+class ColumnarRule {\n+  def preColumnarTransitions: Rule[SparkPlan] = plan => plan\n+  def postColumnarTransitions: Rule[SparkPlan] = plan => plan\n+}\n+\n+/**\n+ * Provides a common executor to translate an [[RDD]] of [[ColumnarBatch]] into an [[RDD]] of\n+ * [[InternalRow]]. This is inserted whenever such a transition is determined to be needed.\n+ *\n+ * The implementation is based off of similar implementations in [[ColumnarBatchScan]],\n+ * [[org.apache.spark.sql.execution.python.ArrowEvalPythonExec]], and\n+ * [[MapPartitionsInRWithArrowExec]]. Eventually this should replace those implementations.\n+ */\n+case class ColumnarToRowExec(child: SparkPlan)\n+  extends UnaryExecNode with CodegenSupport {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n+\n+  override lazy val metrics: Map[String, SQLMetric] = Map(\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+    \"numInputBatches\" -> SQLMetrics.createMetric(sparkContext, \"number of input batches\"),\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\")\n+  )\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    val numOutputRows = longMetric(\"numOutputRows\")\n+    val numInputBatches = longMetric(\"numInputBatches\")\n+    val scanTime = longMetric(\"scanTime\")\n+    val batches = child.executeColumnar()\n+    batches.mapPartitions { cbIter =>\n+      // UnsafeProjection is not serializable so do it on the executor side\n+      val outputProject = UnsafeProjection.create(output, output)\n+      new Iterator[InternalRow] {\n+        var it: java.util.Iterator[InternalRow] = null\n+\n+        def loadNextBatch: Unit = {\n+          if (it != null) {\n+            it = null\n+          }\n+          val batchStartNs = System.nanoTime()\n+          if (cbIter.hasNext) {\n+            val cb = cbIter.next()\n+            it = cb.rowIterator()\n+            numInputBatches += 1\n+            // In order to match the numOutputRows metric in the generated code we update\n+            // numOutputRows for each batch. This is less accurate than doing it at output\n+            // because it will over count the number of rows output in the case of a limit,\n+            // but it is more efficient.\n+            numOutputRows += cb.numRows()\n+          }\n+          scanTime += ((System.nanoTime() - batchStartNs) / (1000 * 1000))\n+        }\n+\n+        override def hasNext: Boolean = {\n+          val itHasNext = it != null && it.hasNext\n+          if (!itHasNext) {\n+            loadNextBatch\n+            it != null && it.hasNext\n+          } else {\n+            itHasNext\n+          }\n+        }\n+\n+        override def next(): InternalRow = {\n+          if (it == null || !it.hasNext) {\n+            loadNextBatch\n+          }\n+          if (it == null) {\n+            throw new NoSuchElementException()\n+          }\n+          it.next()\n+        }\n+        // This is to convert the InternalRow to an UnsafeRow. Even though the type is\n+        // InternalRow some operations downstream operations like collect require it to\n+        // be UnsafeRow\n+      }.map(outputProject)\n+    }\n+  }\n+\n+  /**\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\n+   * This is called once per [[ColumnVector]] in the batch.\n+   *\n+   * This code came unchanged from [[ColumnarBatchScan]] and will hopefully replace it\n+   * at some point.\n+   */\n+  private def genCodeColumnVector(\n+      ctx: CodegenContext,\n+      columnVar: String,\n+      ordinal: String,\n+      dataType: DataType,\n+      nullable: Boolean): ExprCode = {\n+    val javaType = CodeGenerator.javaType(dataType)\n+    val value = CodeGenerator.getValueFromVector(columnVar, dataType, ordinal)\n+    val isNullVar = if (nullable) {\n+      JavaCode.isNullVariable(ctx.freshName(\"isNull\"))\n+    } else {\n+      FalseLiteral\n+    }\n+    val valueVar = ctx.freshName(\"value\")\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\n+    val code = code\"${ctx.registerComment(str)}\" + (if (nullable) {\n+      code\"\"\"\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\n+        $javaType $valueVar = $isNullVar ? ${CodeGenerator.defaultValue(dataType)} : ($value);\n+      \"\"\"\n+    } else {\n+      code\"$javaType $valueVar = $value;\"\n+    })\n+    ExprCode(code, isNullVar, JavaCode.variable(valueVar, dataType))\n+  }\n+\n+  /**\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\n+   * This produces an [[org.apache.spark.sql.catalyst.expressions.UnsafeRow]] for each row in\n+   * each batch.\n+   *\n+   * This code came almost completely unchanged from [[ColumnarBatchScan]] and will\n+   * hopefully replace it at some point.\n+   */\n+  override protected def doProduce(ctx: CodegenContext): String = {\n+    // PhysicalRDD always just has one input\n+    val input = ctx.addMutableState(\"scala.collection.Iterator\", \"input\",\n+      v => s\"$v = inputs[0];\")\n+\n+    // metrics\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\n+    val numInputBatches = metricTerm(ctx, \"numInputBatches\")\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\n+    val scanTimeTotalNs =\n+      ctx.addMutableState(CodeGenerator.JAVA_LONG, \"scanTime\") // init as scanTime = 0\n+\n+    val columnarBatchClz = classOf[ColumnarBatch].getName\n+    val batch = ctx.addMutableState(columnarBatchClz, \"batch\")\n+\n+    val idx = ctx.addMutableState(CodeGenerator.JAVA_INT, \"batchIdx\") // init as batchIdx = 0\n+    val columnVectorClzs = child.vectorTypes.getOrElse(\n+      Seq.fill(output.indices.size)(classOf[ColumnVector].getName))\n+    val (colVars, columnAssigns) = columnVectorClzs.zipWithIndex.map {\n+      case (columnVectorClz, i) =>\n+        val name = ctx.addMutableState(columnVectorClz, s\"colInstance$i\")\n+        (name, s\"$name = ($columnVectorClz) $batch.column($i);\")\n+    }.unzip\n+\n+    val nextBatch = ctx.freshName(\"nextBatch\")\n+    val nextBatchFuncName = ctx.addNewFunction(nextBatch,\n+      s\"\"\"\n+         |private void $nextBatch() throws java.io.IOException {\n+         |  long getBatchStart = System.nanoTime();\n+         |  if ($input.hasNext()) {\n+         |    $batch = ($columnarBatchClz)$input.next();\n+         |    $numOutputRows.add($batch.numRows());\n+         |    $idx = 0;\n+         |    ${columnAssigns.mkString(\"\", \"\\n\", \"\\n\")}\n+         |    ${numInputBatches}.add(1);\n+         |  }\n+         |  $scanTimeTotalNs += System.nanoTime() - getBatchStart;\n+         |}\"\"\".stripMargin)\n+\n+    ctx.currentVars = null\n+    val rowidx = ctx.freshName(\"rowIdx\")\n+    val columnsBatchInput = (output zip colVars).map { case (attr, colVar) =>\n+      genCodeColumnVector(ctx, colVar, rowidx, attr.dataType, attr.nullable)\n+    }\n+    val localIdx = ctx.freshName(\"localIdx\")\n+    val localEnd = ctx.freshName(\"localEnd\")\n+    val numRows = ctx.freshName(\"numRows\")\n+    val shouldStop = if (parent.needStopCheck) {\n+      s\"if (shouldStop()) { $idx = $rowidx + 1; return; }\"\n+    } else {\n+      \"// shouldStop check is eliminated\"\n+    }\n+    s\"\"\"\n+       |if ($batch == null) {\n+       |  $nextBatchFuncName();\n+       |}\n+       |while ($batch != null) {\n+       |  int $numRows = $batch.numRows();\n+       |  int $localEnd = $numRows - $idx;\n+       |  for (int $localIdx = 0; $localIdx < $localEnd; $localIdx++) {\n+       |    int $rowidx = $idx + $localIdx;\n+       |    ${consume(ctx, columnsBatchInput).trim}\n+       |    $shouldStop\n+       |  }\n+       |  $idx = $numRows;\n+       |  $batch = null;\n+       |  $nextBatchFuncName();\n+       |}\n+       |$scanTimeMetric.add($scanTimeTotalNs / (1000 * 1000));\n+       |$scanTimeTotalNs = 0;\n+     \"\"\".stripMargin\n+  }\n+\n+  override def inputRDDs(): Seq[RDD[InternalRow]] = {\n+    child.asInstanceOf[CodegenSupport].inputRDDs()\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to append row based data to an array of\n+ * [[WritableColumnVector]].\n+ */\n+private[execution] class RowToColumnConverter(schema: StructType) extends Serializable {\n+  private val converters = schema.fields.map {\n+    f => RowToColumnConverter.getConverterForType(f.dataType)\n+  }\n+\n+  final def convert(row: InternalRow, vectors: Array[WritableColumnVector]): Unit = {\n+    var idx = 0\n+    while (idx < row.numFields) {\n+      converters(idx).append(row, idx, vectors(idx))\n+      idx += 1\n+    }\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to extract a column from a row and append it to a\n+ * [[WritableColumnVector]].\n+ */\n+private object RowToColumnConverter {\n+  private abstract class TypeConverter extends Serializable {\n+    def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit\n+  }\n+\n+  private def getConverterForType(dataType: DataType): TypeConverter = {\n+    dataType match {\n+      case BooleanType => BooleanConverter\n+      case ByteType => ByteConverter\n+      case ShortType => ShortConverter\n+      case IntegerType => IntConverter\n+      case FloatType => FloatConverter\n+      case LongType => LongConverter\n+      case DoubleType => DoubleConverter\n+      case DateType => IntConverter\n+      case TimestampType => LongConverter\n+      case StringType => StringConverter\n+      case CalendarIntervalType => CalendarConverter\n+      case at: ArrayType => new ArrayConverter(getConverterForType(at.elementType))\n+      case st: StructType => new StructConverter(st.fields.map(\n+        (f) => getConverterForType(f.dataType)))\n+      case dt: DecimalType => new DecimalConverter(dt)\n+      case mt: MapType => new MapConverter(getConverterForType(mt.keyType),\n+        getConverterForType(mt.valueType))\n+      case unknown => throw new UnsupportedOperationException(\n+        s\"Type $unknown not supported\")\n+    }\n+  }\n+\n+  private object BooleanConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      if (row.isNullAt(column)) {"
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "Possibly.  Structs need a different API to append a null, so I would have to special case those within the parent code.  I also was thinking of taking nullability into account to avoid some of the conditionals all together, but yes, I think there is the possibility of reducing the amount of code further.  I'll see what I can come up with.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-17T12:36:59Z",
    "diffHunk": "@@ -0,0 +1,594 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[preColumnarTransitions]] [[Rule]] can be used to replace\n+ * [[SparkPlan]] instances with versions that support a columnar implementation. After this\n+ * Spark will insert any transitions necessary. This includes transitions from row to columnar\n+ * [[RowToColumnarExec]] and from columnar to row [[ColumnarToRowExec]]. At this point the\n+ * [[postColumnarTransitions]] [[Rule]] is called to allow replacing any of the implementations\n+ * of the transitions or doing cleanup of the plan, like inserting stages to build larger batches\n+ * for more efficient processing, or stages that transition the data to/from an accelerator's\n+ * memory.\n+ */\n+class ColumnarRule {\n+  def preColumnarTransitions: Rule[SparkPlan] = plan => plan\n+  def postColumnarTransitions: Rule[SparkPlan] = plan => plan\n+}\n+\n+/**\n+ * Provides a common executor to translate an [[RDD]] of [[ColumnarBatch]] into an [[RDD]] of\n+ * [[InternalRow]]. This is inserted whenever such a transition is determined to be needed.\n+ *\n+ * The implementation is based off of similar implementations in [[ColumnarBatchScan]],\n+ * [[org.apache.spark.sql.execution.python.ArrowEvalPythonExec]], and\n+ * [[MapPartitionsInRWithArrowExec]]. Eventually this should replace those implementations.\n+ */\n+case class ColumnarToRowExec(child: SparkPlan)\n+  extends UnaryExecNode with CodegenSupport {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n+\n+  override lazy val metrics: Map[String, SQLMetric] = Map(\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+    \"numInputBatches\" -> SQLMetrics.createMetric(sparkContext, \"number of input batches\"),\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\")\n+  )\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    val numOutputRows = longMetric(\"numOutputRows\")\n+    val numInputBatches = longMetric(\"numInputBatches\")\n+    val scanTime = longMetric(\"scanTime\")\n+    val batches = child.executeColumnar()\n+    batches.mapPartitions { cbIter =>\n+      // UnsafeProjection is not serializable so do it on the executor side\n+      val outputProject = UnsafeProjection.create(output, output)\n+      new Iterator[InternalRow] {\n+        var it: java.util.Iterator[InternalRow] = null\n+\n+        def loadNextBatch: Unit = {\n+          if (it != null) {\n+            it = null\n+          }\n+          val batchStartNs = System.nanoTime()\n+          if (cbIter.hasNext) {\n+            val cb = cbIter.next()\n+            it = cb.rowIterator()\n+            numInputBatches += 1\n+            // In order to match the numOutputRows metric in the generated code we update\n+            // numOutputRows for each batch. This is less accurate than doing it at output\n+            // because it will over count the number of rows output in the case of a limit,\n+            // but it is more efficient.\n+            numOutputRows += cb.numRows()\n+          }\n+          scanTime += ((System.nanoTime() - batchStartNs) / (1000 * 1000))\n+        }\n+\n+        override def hasNext: Boolean = {\n+          val itHasNext = it != null && it.hasNext\n+          if (!itHasNext) {\n+            loadNextBatch\n+            it != null && it.hasNext\n+          } else {\n+            itHasNext\n+          }\n+        }\n+\n+        override def next(): InternalRow = {\n+          if (it == null || !it.hasNext) {\n+            loadNextBatch\n+          }\n+          if (it == null) {\n+            throw new NoSuchElementException()\n+          }\n+          it.next()\n+        }\n+        // This is to convert the InternalRow to an UnsafeRow. Even though the type is\n+        // InternalRow some operations downstream operations like collect require it to\n+        // be UnsafeRow\n+      }.map(outputProject)\n+    }\n+  }\n+\n+  /**\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\n+   * This is called once per [[ColumnVector]] in the batch.\n+   *\n+   * This code came unchanged from [[ColumnarBatchScan]] and will hopefully replace it\n+   * at some point.\n+   */\n+  private def genCodeColumnVector(\n+      ctx: CodegenContext,\n+      columnVar: String,\n+      ordinal: String,\n+      dataType: DataType,\n+      nullable: Boolean): ExprCode = {\n+    val javaType = CodeGenerator.javaType(dataType)\n+    val value = CodeGenerator.getValueFromVector(columnVar, dataType, ordinal)\n+    val isNullVar = if (nullable) {\n+      JavaCode.isNullVariable(ctx.freshName(\"isNull\"))\n+    } else {\n+      FalseLiteral\n+    }\n+    val valueVar = ctx.freshName(\"value\")\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\n+    val code = code\"${ctx.registerComment(str)}\" + (if (nullable) {\n+      code\"\"\"\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\n+        $javaType $valueVar = $isNullVar ? ${CodeGenerator.defaultValue(dataType)} : ($value);\n+      \"\"\"\n+    } else {\n+      code\"$javaType $valueVar = $value;\"\n+    })\n+    ExprCode(code, isNullVar, JavaCode.variable(valueVar, dataType))\n+  }\n+\n+  /**\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\n+   * This produces an [[org.apache.spark.sql.catalyst.expressions.UnsafeRow]] for each row in\n+   * each batch.\n+   *\n+   * This code came almost completely unchanged from [[ColumnarBatchScan]] and will\n+   * hopefully replace it at some point.\n+   */\n+  override protected def doProduce(ctx: CodegenContext): String = {\n+    // PhysicalRDD always just has one input\n+    val input = ctx.addMutableState(\"scala.collection.Iterator\", \"input\",\n+      v => s\"$v = inputs[0];\")\n+\n+    // metrics\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\n+    val numInputBatches = metricTerm(ctx, \"numInputBatches\")\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\n+    val scanTimeTotalNs =\n+      ctx.addMutableState(CodeGenerator.JAVA_LONG, \"scanTime\") // init as scanTime = 0\n+\n+    val columnarBatchClz = classOf[ColumnarBatch].getName\n+    val batch = ctx.addMutableState(columnarBatchClz, \"batch\")\n+\n+    val idx = ctx.addMutableState(CodeGenerator.JAVA_INT, \"batchIdx\") // init as batchIdx = 0\n+    val columnVectorClzs = child.vectorTypes.getOrElse(\n+      Seq.fill(output.indices.size)(classOf[ColumnVector].getName))\n+    val (colVars, columnAssigns) = columnVectorClzs.zipWithIndex.map {\n+      case (columnVectorClz, i) =>\n+        val name = ctx.addMutableState(columnVectorClz, s\"colInstance$i\")\n+        (name, s\"$name = ($columnVectorClz) $batch.column($i);\")\n+    }.unzip\n+\n+    val nextBatch = ctx.freshName(\"nextBatch\")\n+    val nextBatchFuncName = ctx.addNewFunction(nextBatch,\n+      s\"\"\"\n+         |private void $nextBatch() throws java.io.IOException {\n+         |  long getBatchStart = System.nanoTime();\n+         |  if ($input.hasNext()) {\n+         |    $batch = ($columnarBatchClz)$input.next();\n+         |    $numOutputRows.add($batch.numRows());\n+         |    $idx = 0;\n+         |    ${columnAssigns.mkString(\"\", \"\\n\", \"\\n\")}\n+         |    ${numInputBatches}.add(1);\n+         |  }\n+         |  $scanTimeTotalNs += System.nanoTime() - getBatchStart;\n+         |}\"\"\".stripMargin)\n+\n+    ctx.currentVars = null\n+    val rowidx = ctx.freshName(\"rowIdx\")\n+    val columnsBatchInput = (output zip colVars).map { case (attr, colVar) =>\n+      genCodeColumnVector(ctx, colVar, rowidx, attr.dataType, attr.nullable)\n+    }\n+    val localIdx = ctx.freshName(\"localIdx\")\n+    val localEnd = ctx.freshName(\"localEnd\")\n+    val numRows = ctx.freshName(\"numRows\")\n+    val shouldStop = if (parent.needStopCheck) {\n+      s\"if (shouldStop()) { $idx = $rowidx + 1; return; }\"\n+    } else {\n+      \"// shouldStop check is eliminated\"\n+    }\n+    s\"\"\"\n+       |if ($batch == null) {\n+       |  $nextBatchFuncName();\n+       |}\n+       |while ($batch != null) {\n+       |  int $numRows = $batch.numRows();\n+       |  int $localEnd = $numRows - $idx;\n+       |  for (int $localIdx = 0; $localIdx < $localEnd; $localIdx++) {\n+       |    int $rowidx = $idx + $localIdx;\n+       |    ${consume(ctx, columnsBatchInput).trim}\n+       |    $shouldStop\n+       |  }\n+       |  $idx = $numRows;\n+       |  $batch = null;\n+       |  $nextBatchFuncName();\n+       |}\n+       |$scanTimeMetric.add($scanTimeTotalNs / (1000 * 1000));\n+       |$scanTimeTotalNs = 0;\n+     \"\"\".stripMargin\n+  }\n+\n+  override def inputRDDs(): Seq[RDD[InternalRow]] = {\n+    child.asInstanceOf[CodegenSupport].inputRDDs()\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to append row based data to an array of\n+ * [[WritableColumnVector]].\n+ */\n+private[execution] class RowToColumnConverter(schema: StructType) extends Serializable {\n+  private val converters = schema.fields.map {\n+    f => RowToColumnConverter.getConverterForType(f.dataType)\n+  }\n+\n+  final def convert(row: InternalRow, vectors: Array[WritableColumnVector]): Unit = {\n+    var idx = 0\n+    while (idx < row.numFields) {\n+      converters(idx).append(row, idx, vectors(idx))\n+      idx += 1\n+    }\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to extract a column from a row and append it to a\n+ * [[WritableColumnVector]].\n+ */\n+private object RowToColumnConverter {\n+  private abstract class TypeConverter extends Serializable {\n+    def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit\n+  }\n+\n+  private def getConverterForType(dataType: DataType): TypeConverter = {\n+    dataType match {\n+      case BooleanType => BooleanConverter\n+      case ByteType => ByteConverter\n+      case ShortType => ShortConverter\n+      case IntegerType => IntConverter\n+      case FloatType => FloatConverter\n+      case LongType => LongConverter\n+      case DoubleType => DoubleConverter\n+      case DateType => IntConverter\n+      case TimestampType => LongConverter\n+      case StringType => StringConverter\n+      case CalendarIntervalType => CalendarConverter\n+      case at: ArrayType => new ArrayConverter(getConverterForType(at.elementType))\n+      case st: StructType => new StructConverter(st.fields.map(\n+        (f) => getConverterForType(f.dataType)))\n+      case dt: DecimalType => new DecimalConverter(dt)\n+      case mt: MapType => new MapConverter(getConverterForType(mt.keyType),\n+        getConverterForType(mt.valueType))\n+      case unknown => throw new UnsupportedOperationException(\n+        s\"Type $unknown not supported\")\n+    }\n+  }\n+\n+  private object BooleanConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      if (row.isNullAt(column)) {"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: `cb.close()`?",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-20T15:25:50Z",
    "diffHunk": "@@ -0,0 +1,538 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various\n+ * operators in the plan. The [[preColumnarTransitions]] [[Rule]] can be used to replace\n+ * [[SparkPlan]] instances with versions that support a columnar implementation. After this\n+ * Spark will insert any transitions necessary. This includes transitions from row to columnar\n+ * [[RowToColumnarExec]] and from columnar to row [[ColumnarToRowExec]]. At this point the\n+ * [[postColumnarTransitions]] [[Rule]] is called to allow replacing any of the implementations\n+ * of the transitions or doing cleanup of the plan, like inserting stages to build larger batches\n+ * for more efficient processing, or stages that transition the data to/from an accelerator's\n+ * memory.\n+ */\n+class ColumnarRule {\n+  def preColumnarTransitions: Rule[SparkPlan] = plan => plan\n+  def postColumnarTransitions: Rule[SparkPlan] = plan => plan\n+}\n+\n+/**\n+ * Provides a common executor to translate an [[RDD]] of [[ColumnarBatch]] into an [[RDD]] of\n+ * [[InternalRow]]. This is inserted whenever such a transition is determined to be needed.\n+ *\n+ * The implementation is based off of similar implementations in [[ColumnarBatchScan]],\n+ * [[org.apache.spark.sql.execution.python.ArrowEvalPythonExec]], and\n+ * [[MapPartitionsInRWithArrowExec]]. Eventually this should replace those implementations.\n+ */\n+case class ColumnarToRowExec(child: SparkPlan)\n+  extends UnaryExecNode with CodegenSupport {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n+\n+  override lazy val metrics: Map[String, SQLMetric] = Map(\n+    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+    \"numInputBatches\" -> SQLMetrics.createMetric(sparkContext, \"number of input batches\"),\n+    \"scanTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"scan time\")\n+  )\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    val numOutputRows = longMetric(\"numOutputRows\")\n+    val numInputBatches = longMetric(\"numInputBatches\")\n+    val scanTime = longMetric(\"scanTime\")\n+    // UnsafeProjection is not serializable so do it on the executor side, which is why it is lazy\n+    @transient lazy val outputProject = UnsafeProjection.create(output, output)\n+    val batches = child.executeColumnar()\n+    batches.flatMap(batch => {\n+      val batchStartNs = System.nanoTime()\n+      numInputBatches += 1\n+      // In order to match the numOutputRows metric in the generated code we update\n+      // numOutputRows for each batch. This is less accurate than doing it at output\n+      // because it will over count the number of rows output in the case of a limit,\n+      // but it is more efficient.\n+      numOutputRows += batch.numRows()\n+      val ret = batch.rowIterator().asScala\n+      scanTime += ((System.nanoTime() - batchStartNs) / (1000 * 1000))\n+      ret.map(outputProject)\n+    })\n+  }\n+\n+  /**\n+   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\n+   * This is called once per [[ColumnVector]] in the batch.\n+   *\n+   * This code came unchanged from [[ColumnarBatchScan]] and will hopefully replace it\n+   * at some point.\n+   */\n+  private def genCodeColumnVector(\n+      ctx: CodegenContext,\n+      columnVar: String,\n+      ordinal: String,\n+      dataType: DataType,\n+      nullable: Boolean): ExprCode = {\n+    val javaType = CodeGenerator.javaType(dataType)\n+    val value = CodeGenerator.getValueFromVector(columnVar, dataType, ordinal)\n+    val isNullVar = if (nullable) {\n+      JavaCode.isNullVariable(ctx.freshName(\"isNull\"))\n+    } else {\n+      FalseLiteral\n+    }\n+    val valueVar = ctx.freshName(\"value\")\n+    val str = s\"columnVector[$columnVar, $ordinal, ${dataType.simpleString}]\"\n+    val code = code\"${ctx.registerComment(str)}\" + (if (nullable) {\n+      code\"\"\"\n+        boolean $isNullVar = $columnVar.isNullAt($ordinal);\n+        $javaType $valueVar = $isNullVar ? ${CodeGenerator.defaultValue(dataType)} : ($value);\n+      \"\"\"\n+    } else {\n+      code\"$javaType $valueVar = $value;\"\n+    })\n+    ExprCode(code, isNullVar, JavaCode.variable(valueVar, dataType))\n+  }\n+\n+  /**\n+   * Produce code to process the input iterator as [[ColumnarBatch]]es.\n+   * This produces an [[org.apache.spark.sql.catalyst.expressions.UnsafeRow]] for each row in\n+   * each batch.\n+   *\n+   * This code came almost completely unchanged from [[ColumnarBatchScan]] and will\n+   * hopefully replace it at some point.\n+   */\n+  override protected def doProduce(ctx: CodegenContext): String = {\n+    // PhysicalRDD always just has one input\n+    val input = ctx.addMutableState(\"scala.collection.Iterator\", \"input\",\n+      v => s\"$v = inputs[0];\")\n+\n+    // metrics\n+    val numOutputRows = metricTerm(ctx, \"numOutputRows\")\n+    val numInputBatches = metricTerm(ctx, \"numInputBatches\")\n+    val scanTimeMetric = metricTerm(ctx, \"scanTime\")\n+    val scanTimeTotalNs =\n+      ctx.addMutableState(CodeGenerator.JAVA_LONG, \"scanTime\") // init as scanTime = 0\n+\n+    val columnarBatchClz = classOf[ColumnarBatch].getName\n+    val batch = ctx.addMutableState(columnarBatchClz, \"batch\")\n+\n+    val idx = ctx.addMutableState(CodeGenerator.JAVA_INT, \"batchIdx\") // init as batchIdx = 0\n+    val columnVectorClzs = child.vectorTypes.getOrElse(\n+      Seq.fill(output.indices.size)(classOf[ColumnVector].getName))\n+    val (colVars, columnAssigns) = columnVectorClzs.zipWithIndex.map {\n+      case (columnVectorClz, i) =>\n+        val name = ctx.addMutableState(columnVectorClz, s\"colInstance$i\")\n+        (name, s\"$name = ($columnVectorClz) $batch.column($i);\")\n+    }.unzip\n+\n+    val nextBatch = ctx.freshName(\"nextBatch\")\n+    val nextBatchFuncName = ctx.addNewFunction(nextBatch,\n+      s\"\"\"\n+         |private void $nextBatch() throws java.io.IOException {\n+         |  long getBatchStart = System.nanoTime();\n+         |  if ($input.hasNext()) {\n+         |    $batch = ($columnarBatchClz)$input.next();\n+         |    $numOutputRows.add($batch.numRows());\n+         |    $idx = 0;\n+         |    ${columnAssigns.mkString(\"\", \"\\n\", \"\\n\")}\n+         |    ${numInputBatches}.add(1);\n+         |  }\n+         |  $scanTimeTotalNs += System.nanoTime() - getBatchStart;\n+         |}\"\"\".stripMargin)\n+\n+    ctx.currentVars = null\n+    val rowidx = ctx.freshName(\"rowIdx\")\n+    val columnsBatchInput = (output zip colVars).map { case (attr, colVar) =>\n+      genCodeColumnVector(ctx, colVar, rowidx, attr.dataType, attr.nullable)\n+    }\n+    val localIdx = ctx.freshName(\"localIdx\")\n+    val localEnd = ctx.freshName(\"localEnd\")\n+    val numRows = ctx.freshName(\"numRows\")\n+    val shouldStop = if (parent.needStopCheck) {\n+      s\"if (shouldStop()) { $idx = $rowidx + 1; return; }\"\n+    } else {\n+      \"// shouldStop check is eliminated\"\n+    }\n+    s\"\"\"\n+       |if ($batch == null) {\n+       |  $nextBatchFuncName();\n+       |}\n+       |while ($batch != null) {\n+       |  int $numRows = $batch.numRows();\n+       |  int $localEnd = $numRows - $idx;\n+       |  for (int $localIdx = 0; $localIdx < $localEnd; $localIdx++) {\n+       |    int $rowidx = $idx + $localIdx;\n+       |    ${consume(ctx, columnsBatchInput).trim}\n+       |    $shouldStop\n+       |  }\n+       |  $idx = $numRows;\n+       |  $batch = null;\n+       |  $nextBatchFuncName();\n+       |}\n+       |$scanTimeMetric.add($scanTimeTotalNs / (1000 * 1000));\n+       |$scanTimeTotalNs = 0;\n+     \"\"\".stripMargin\n+  }\n+\n+  override def inputRDDs(): Seq[RDD[InternalRow]] = {\n+    child.asInstanceOf[CodegenSupport].inputRDDs()\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to append row based data to an array of\n+ * [[WritableColumnVector]].\n+ */\n+private[execution] class RowToColumnConverter(schema: StructType) extends Serializable {\n+  private val converters = schema.fields.map {\n+    f => RowToColumnConverter.getConverterForType(f.dataType, f.nullable)\n+  }\n+\n+  final def convert(row: InternalRow, vectors: Array[WritableColumnVector]): Unit = {\n+    var idx = 0\n+    while (idx < row.numFields) {\n+      converters(idx).append(row, idx, vectors(idx))\n+      idx += 1\n+    }\n+  }\n+}\n+\n+/**\n+ * Provides an optimized set of APIs to extract a column from a row and append it to a\n+ * [[WritableColumnVector]].\n+ */\n+private object RowToColumnConverter {\n+  private abstract class TypeConverter extends Serializable {\n+    def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit\n+  }\n+\n+  private final case class BasicNullableTypeConverter(base: TypeConverter) extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      if (row.isNullAt(column)) {\n+        cv.appendNull\n+      } else {\n+        base.append(row, column, cv)\n+      }\n+    }\n+  }\n+\n+  private final case class StructNullableTypeConverter(base: TypeConverter) extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      if (row.isNullAt(column)) {\n+        cv.appendStruct(true)\n+      } else {\n+        base.append(row, column, cv)\n+      }\n+    }\n+  }\n+\n+  private def getConverterForType(dataType: DataType, nullable: Boolean): TypeConverter = {\n+    val core = dataType match {\n+      case BooleanType => BooleanConverter\n+      case ByteType => ByteConverter\n+      case ShortType => ShortConverter\n+      case IntegerType | DateType => IntConverter\n+      case FloatType => FloatConverter\n+      case LongType | TimestampType => LongConverter\n+      case DoubleType => DoubleConverter\n+      case StringType => StringConverter\n+      case CalendarIntervalType => CalendarConverter\n+      case at: ArrayType => new ArrayConverter(getConverterForType(at.elementType, nullable))\n+      case st: StructType => new StructConverter(st.fields.map(\n+        (f) => getConverterForType(f.dataType, f.nullable)))\n+      case dt: DecimalType => new DecimalConverter(dt)\n+      case mt: MapType => new MapConverter(getConverterForType(mt.keyType, nullable),\n+        getConverterForType(mt.valueType, nullable))\n+      case unknown => throw new UnsupportedOperationException(\n+        s\"Type $unknown not supported\")\n+    }\n+\n+    if (nullable) {\n+      dataType match {\n+        case CalendarIntervalType => new StructNullableTypeConverter(core)\n+        case st: StructType => new StructNullableTypeConverter(core)\n+        case _ => new BasicNullableTypeConverter(core)\n+      }\n+    } else {\n+      core\n+    }\n+  }\n+\n+  private object BooleanConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      cv.appendBoolean(row.getBoolean(column))\n+  }\n+\n+  private object ByteConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      cv.appendByte(row.getByte(column))\n+  }\n+\n+  private object ShortConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      cv.appendShort(row.getShort(column))\n+  }\n+\n+  private object IntConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      cv.appendInt(row.getInt(column))\n+  }\n+\n+  private object FloatConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      cv.appendFloat(row.getFloat(column))\n+  }\n+\n+  private object LongConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      cv.appendLong(row.getLong(column))\n+  }\n+\n+  private object DoubleConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit =\n+      cv.appendDouble(row.getDouble(column))\n+  }\n+\n+  private object StringConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      val data = row.getUTF8String(column).getBytes\n+      cv.appendByteArray(data, 0, data.length)\n+    }\n+  }\n+\n+  private object CalendarConverter extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      val c = row.getInterval(column)\n+      cv.appendStruct(false)\n+      cv.getChild(0).appendInt(c.months)\n+      cv.getChild(1).appendLong(c.microseconds)\n+    }\n+  }\n+\n+  private case class ArrayConverter(childConverter: TypeConverter) extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      val values = row.getArray(column)\n+      val numElements = values.numElements()\n+      cv.appendArray(numElements)\n+      val arrData = cv.arrayData()\n+      for (i <- 0 until numElements) {\n+        childConverter.append(values, i, arrData)\n+      }\n+    }\n+  }\n+\n+  private case class StructConverter(childConverters: Array[TypeConverter]) extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      cv.appendStruct(false)\n+      val data = row.getStruct(column, childConverters.length)\n+      for (i <- 0 until childConverters.length) {\n+        childConverters(i).append(data, i, cv.getChild(i))\n+      }\n+    }\n+  }\n+\n+  private case class DecimalConverter(dt: DecimalType) extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      val d = row.getDecimal(column, dt.precision, dt.scale)\n+      if (dt.precision <= Decimal.MAX_INT_DIGITS) {\n+        cv.appendInt(d.toUnscaledLong.toInt)\n+      } else if (dt.precision <= Decimal.MAX_LONG_DIGITS) {\n+        cv.appendLong(d.toUnscaledLong)\n+      } else {\n+        val integer = d.toJavaBigDecimal.unscaledValue\n+        val bytes = integer.toByteArray\n+        cv.appendByteArray(bytes, 0, bytes.length)\n+      }\n+    }\n+  }\n+\n+  private case class MapConverter(keyConverter: TypeConverter, valueConverter: TypeConverter)\n+    extends TypeConverter {\n+    override def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit = {\n+      val m = row.getMap(column)\n+      val keys = cv.getChild(0)\n+      val values = cv.getChild(1)\n+      val numElements = m.numElements()\n+      cv.appendArray(numElements)\n+\n+      val srcKeys = m.keyArray()\n+      val srcValues = m.valueArray()\n+\n+      for (i <- 0 until numElements) {\n+        keyConverter.append(srcKeys, i, keys)\n+        valueConverter.append(srcValues, i, values)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Provides a common executor to translate an [[RDD]] of [[InternalRow]] into an [[RDD]] of\n+ * [[ColumnarBatch]]. This is inserted whenever such a transition is determined to be needed.\n+ *\n+ * This is similar to some of the code in ArrowConverters.scala and\n+ * [[org.apache.spark.sql.execution.arrow.ArrowWriter]]. That code is more specialized\n+ * to convert [[InternalRow]] to Arrow formatted data, but in the future if we make\n+ * [[OffHeapColumnVector]] internally Arrow formatted we may be able to replace much of that code.\n+ *\n+ * This is also similar to\n+ * [[org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate()]] and\n+ * [[org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.toBatch()]] toBatch is only ever\n+ * called from tests and can probably be removed, but populate is used by both Orc and Parquet\n+ * to initialize partition and missing columns. There is some chance that we could replace\n+ * populate with [[RowToColumnConverter]], but the performance requirements are different and it\n+ * would only be to reduce code.\n+ */\n+case class RowToColumnarExec(child: SparkPlan) extends UnaryExecNode {\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    child.execute()\n+  }\n+\n+  override def doExecuteBroadcast[T](): broadcast.Broadcast[T] = {\n+    child.doExecuteBroadcast()\n+  }\n+\n+  override def supportsColumnar: Boolean = true\n+\n+  override lazy val metrics: Map[String, SQLMetric] = Map(\n+    \"numInputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of input rows\"),\n+    \"numOutputBatches\" -> SQLMetrics.createMetric(sparkContext, \"number of output batches\")\n+  )\n+\n+  override def doExecuteColumnar(): RDD[ColumnarBatch] = {\n+    val enableOffHeapColumnVector = sqlContext.conf.offHeapColumnVectorEnabled\n+    val numInputRows = longMetric(\"numInputRows\")\n+    val numOutputBatches = longMetric(\"numOutputBatches\")\n+    // Instead of creating a new config we are reusing columnBatchSize. In the future if we do\n+    // combine with some of the Arrow conversion tools we will need to unify some of the configs.\n+    val numRows = conf.columnBatchSize\n+    val converters = new RowToColumnConverter(schema)\n+    val rowBased = child.execute()\n+    rowBased.mapPartitions(rowIterator => {\n+      new Iterator[ColumnarBatch] {\n+        var cb: ColumnarBatch = null\n+\n+        TaskContext.get().addTaskCompletionListener[Unit] { _ =>\n+          if (cb != null) {\n+            cb.close"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "I still don't understand - why do we need ColumnarRule, i.e. any rule that's column specific?",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-21T04:36:36Z",
    "diffHunk": "@@ -0,0 +1,534 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various",
    "line": 37
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "@rxin \r\nAs I explained [before](https://github.com/apache/spark/pull/24795#issuecomment-499960263) there needs to be at least one rule that operates on the entire physical `SparkPlan` after the exchanges have been inserted into the plan.\r\n\r\nHaving two rules, one that runs prior to inserting columnar transitions and one that runs after it makes it so a plugin does not have to duplicate code for inserting columnar transitions and provides a cleaner API for anyone trying to insert columnar processing, but that is relatively minor.\r\n\r\nIf you want me to split `ColumnarRule` up and rename the individual parts to not be columnar specific I am happy to do it.  If you want me to make it a single rule that runs prior to code generation I can make that work too.  Just let me know which of the two changes you would prefer, and I will make it happen.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-21T12:29:38Z",
    "diffHunk": "@@ -0,0 +1,534 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{broadcast, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder, SpecializedGetters, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+import org.apache.spark.sql.execution.vectorized.{OffHeapColumnVector, OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.vectorized.{ColumnarBatch, ColumnVector}\n+\n+/**\n+ * Holds a user defined rule that can be used to inject columnar implementations of various",
    "line": 37
  }],
  "prId": 24795
}]