[{
  "comments": [{
    "author": {
      "login": "mallman"
    },
    "body": "Based on how you're using this return value above, it looks like you can omit this call to `toMap` and just return a sequence of tuples.\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-19T02:13:48Z",
    "diffHunk": "@@ -276,15 +290,15 @@ object PartitioningAwareFileCatalog extends Logging {\n    */\n   private def listLeafFilesInSerial(\n       paths: Seq[Path],\n-      hadoopConf: Configuration): Seq[FileStatus] = {\n+      hadoopConf: Configuration): Map[Path, Seq[FileStatus]] = {\n     // Dummy jobconf to get to the pathFilter defined in configuration\n     val jobConf = new JobConf(hadoopConf, this.getClass)\n     val filter = FileInputFormat.getInputPathFilter(jobConf)\n \n-    paths.flatMap { path =>\n+    paths.map { path =>\n       val fs = path.getFileSystem(hadoopConf)\n-      listLeafFiles0(fs, path, filter)\n-    }\n+      (path, listLeafFiles0(fs, path, filter))\n+    }.toMap"
  }, {
    "author": {
      "login": "ericl"
    },
    "body": "Done\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-19T19:52:32Z",
    "diffHunk": "@@ -276,15 +290,15 @@ object PartitioningAwareFileCatalog extends Logging {\n    */\n   private def listLeafFilesInSerial(\n       paths: Seq[Path],\n-      hadoopConf: Configuration): Seq[FileStatus] = {\n+      hadoopConf: Configuration): Map[Path, Seq[FileStatus]] = {\n     // Dummy jobconf to get to the pathFilter defined in configuration\n     val jobConf = new JobConf(hadoopConf, this.getClass)\n     val filter = FileInputFormat.getInputPathFilter(jobConf)\n \n-    paths.flatMap { path =>\n+    paths.map { path =>\n       val fs = path.getFileSystem(hadoopConf)\n-      listLeafFiles0(fs, path, filter)\n-    }\n+      (path, listLeafFiles0(fs, path, filter))\n+    }.toMap"
  }],
  "prId": 15539
}, {
  "comments": [{
    "author": {
      "login": "mallman"
    },
    "body": "I believe this should return the same type as `listLeafFilesInSerial`, i.e. `Seq[(Path, Seq[FileStatus])]`.\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-21T00:21:43Z",
    "diffHunk": "@@ -294,7 +308,7 @@ object PartitioningAwareFileCatalog extends Logging {\n   private def listLeafFilesInParallel(\n       paths: Seq[Path],\n       hadoopConf: Configuration,\n-      sparkSession: SparkSession): Seq[FileStatus] = {\n+      sparkSession: SparkSession): Map[Path, Seq[FileStatus]] = {"
  }, {
    "author": {
      "login": "ericl"
    },
    "body": "Updated\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-21T06:23:58Z",
    "diffHunk": "@@ -294,7 +308,7 @@ object PartitioningAwareFileCatalog extends Logging {\n   private def listLeafFilesInParallel(\n       paths: Seq[Path],\n       hadoopConf: Configuration,\n-      sparkSession: SparkSession): Seq[FileStatus] = {\n+      sparkSession: SparkSession): Map[Path, Seq[FileStatus]] = {"
  }],
  "prId": 15539
}]