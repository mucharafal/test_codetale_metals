[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This piece of reasoning is the trickiest part of this entire patch. Is this a valid argument given the current semantics of `guarantees()` and `satisfies()`?\n",
    "commit": "38006e75650f7f0a827dd4b00fc2984786a2244f",
    "createdAt": "2015-08-06T22:57:23Z",
    "diffHunk": "@@ -197,66 +197,108 @@ case class Exchange(newPartitioning: Partitioning, child: SparkPlan) extends Una\n  * of input data meets the\n  * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for\n  * each operator by inserting [[Exchange]] Operators where required.  Also ensure that the\n- * required input partition ordering requirements are met.\n+ * input partition ordering requirements are met.\n  */\n private[sql] case class EnsureRequirements(sqlContext: SQLContext) extends Rule[SparkPlan] {\n   // TODO: Determine the number of partitions.\n-  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+  private def numPartitions: Int = sqlContext.conf.numShufflePartitions\n \n-  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n-    case operator: SparkPlan =>\n-      // Adds Exchange or Sort operators as required\n-      def addOperatorsIfNecessary(\n-          partitioning: Partitioning,\n-          rowOrdering: Seq[SortOrder],\n-          child: SparkPlan): SparkPlan = {\n-\n-        def addShuffleIfNecessary(child: SparkPlan): SparkPlan = {\n-          if (!child.outputPartitioning.guarantees(partitioning)) {\n-            Exchange(partitioning, child)\n-          } else {\n-            child\n-          }\n-        }\n+  /**\n+   * Given a required distribution, returns a partitioning that satisfies that distribution.\n+   */\n+  private def canonicalPartitioning(requiredDistribution: Distribution): Partitioning = {\n+    requiredDistribution match {\n+      case AllTuples => SinglePartition\n+      case ClusteredDistribution(clustering) => HashPartitioning(clustering, numPartitions)\n+      case OrderedDistribution(ordering) => RangePartitioning(ordering, numPartitions)\n+      case dist => sys.error(s\"Do not know how to satisfy distribution $dist\")\n+    }\n+  }\n \n-        def addSortIfNecessary(child: SparkPlan): SparkPlan = {\n+  /**\n+   * Return true if all of the operator's children satisfy their output distribution requirements.\n+   */\n+  private def childPartitioningsSatisfyDistributionRequirements(operator: SparkPlan): Boolean = {\n+    operator.children.zip(operator.requiredChildDistribution).forall {\n+      case (child, distribution) => child.outputPartitioning.satisfies(distribution)\n+    }\n+  }\n \n-          if (rowOrdering.nonEmpty) {\n-            // If child.outputOrdering is [a, b] and rowOrdering is [a], we do not need to sort.\n-            val minSize = Seq(rowOrdering.size, child.outputOrdering.size).min\n-            if (minSize == 0 || rowOrdering.take(minSize) != child.outputOrdering.take(minSize)) {\n-              sqlContext.planner.BasicOperators.getSortOperator(rowOrdering, global = false, child)\n-            } else {\n+  /**\n+   * Given an operator, check whether the operator requires its children to have compatible\n+   * output partitionings and add Exchanges to fix any detected incompatibilities.\n+   */\n+  private def ensureChildPartitioningsAreCompatible(operator: SparkPlan): SparkPlan = {\n+    if (operator.requiresChildPartitioningsToBeCompatible) {\n+      if (!Partitioning.allCompatible(operator.children.map(_.outputPartitioning))) {\n+        val newChildren = operator.children.zip(operator.requiredChildDistribution).map {\n+          case (child, requiredDistribution) =>\n+            val targetPartitioning = canonicalPartitioning(requiredDistribution)\n+            if (child.outputPartitioning.guarantees(targetPartitioning)) {\n               child\n+            } else {\n+              Exchange(targetPartitioning, child)\n             }\n-          } else {\n-            child\n-          }\n         }\n-\n-        addSortIfNecessary(addShuffleIfNecessary(child))\n+        val newOperator = operator.withNewChildren(newChildren)\n+        assert(childPartitioningsSatisfyDistributionRequirements(newOperator))\n+        newOperator\n+      } else {\n+        operator\n       }\n+    } else {\n+      operator\n+    }\n+  }\n \n-      val requirements =\n-        (operator.requiredChildDistribution, operator.requiredChildOrdering, operator.children)\n+  private def ensureDistributionAndOrdering(operator: SparkPlan): SparkPlan = {\n \n-      val fixedChildren = requirements.zipped.map {\n-        case (AllTuples, rowOrdering, child) =>\n-          addOperatorsIfNecessary(SinglePartition, rowOrdering, child)\n-        case (ClusteredDistribution(clustering), rowOrdering, child) =>\n-          addOperatorsIfNecessary(HashPartitioning(clustering, numPartitions), rowOrdering, child)\n-        case (OrderedDistribution(ordering), rowOrdering, child) =>\n-          addOperatorsIfNecessary(RangePartitioning(ordering, numPartitions), rowOrdering, child)\n+    def addShuffleIfNecessary(child: SparkPlan, requiredDistribution: Distribution): SparkPlan = {\n+      // A pre-condition of ensureDistributionAndOrdering is that joins' children have compatible"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "This will be cleaned up tomorrow when I consolidate the shuffle steps and make the assertions / invariants clearer.\n",
    "commit": "38006e75650f7f0a827dd4b00fc2984786a2244f",
    "createdAt": "2015-08-08T06:43:57Z",
    "diffHunk": "@@ -197,66 +197,108 @@ case class Exchange(newPartitioning: Partitioning, child: SparkPlan) extends Una\n  * of input data meets the\n  * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for\n  * each operator by inserting [[Exchange]] Operators where required.  Also ensure that the\n- * required input partition ordering requirements are met.\n+ * input partition ordering requirements are met.\n  */\n private[sql] case class EnsureRequirements(sqlContext: SQLContext) extends Rule[SparkPlan] {\n   // TODO: Determine the number of partitions.\n-  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+  private def numPartitions: Int = sqlContext.conf.numShufflePartitions\n \n-  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n-    case operator: SparkPlan =>\n-      // Adds Exchange or Sort operators as required\n-      def addOperatorsIfNecessary(\n-          partitioning: Partitioning,\n-          rowOrdering: Seq[SortOrder],\n-          child: SparkPlan): SparkPlan = {\n-\n-        def addShuffleIfNecessary(child: SparkPlan): SparkPlan = {\n-          if (!child.outputPartitioning.guarantees(partitioning)) {\n-            Exchange(partitioning, child)\n-          } else {\n-            child\n-          }\n-        }\n+  /**\n+   * Given a required distribution, returns a partitioning that satisfies that distribution.\n+   */\n+  private def canonicalPartitioning(requiredDistribution: Distribution): Partitioning = {\n+    requiredDistribution match {\n+      case AllTuples => SinglePartition\n+      case ClusteredDistribution(clustering) => HashPartitioning(clustering, numPartitions)\n+      case OrderedDistribution(ordering) => RangePartitioning(ordering, numPartitions)\n+      case dist => sys.error(s\"Do not know how to satisfy distribution $dist\")\n+    }\n+  }\n \n-        def addSortIfNecessary(child: SparkPlan): SparkPlan = {\n+  /**\n+   * Return true if all of the operator's children satisfy their output distribution requirements.\n+   */\n+  private def childPartitioningsSatisfyDistributionRequirements(operator: SparkPlan): Boolean = {\n+    operator.children.zip(operator.requiredChildDistribution).forall {\n+      case (child, distribution) => child.outputPartitioning.satisfies(distribution)\n+    }\n+  }\n \n-          if (rowOrdering.nonEmpty) {\n-            // If child.outputOrdering is [a, b] and rowOrdering is [a], we do not need to sort.\n-            val minSize = Seq(rowOrdering.size, child.outputOrdering.size).min\n-            if (minSize == 0 || rowOrdering.take(minSize) != child.outputOrdering.take(minSize)) {\n-              sqlContext.planner.BasicOperators.getSortOperator(rowOrdering, global = false, child)\n-            } else {\n+  /**\n+   * Given an operator, check whether the operator requires its children to have compatible\n+   * output partitionings and add Exchanges to fix any detected incompatibilities.\n+   */\n+  private def ensureChildPartitioningsAreCompatible(operator: SparkPlan): SparkPlan = {\n+    if (operator.requiresChildPartitioningsToBeCompatible) {\n+      if (!Partitioning.allCompatible(operator.children.map(_.outputPartitioning))) {\n+        val newChildren = operator.children.zip(operator.requiredChildDistribution).map {\n+          case (child, requiredDistribution) =>\n+            val targetPartitioning = canonicalPartitioning(requiredDistribution)\n+            if (child.outputPartitioning.guarantees(targetPartitioning)) {\n               child\n+            } else {\n+              Exchange(targetPartitioning, child)\n             }\n-          } else {\n-            child\n-          }\n         }\n-\n-        addSortIfNecessary(addShuffleIfNecessary(child))\n+        val newOperator = operator.withNewChildren(newChildren)\n+        assert(childPartitioningsSatisfyDistributionRequirements(newOperator))\n+        newOperator\n+      } else {\n+        operator\n       }\n+    } else {\n+      operator\n+    }\n+  }\n \n-      val requirements =\n-        (operator.requiredChildDistribution, operator.requiredChildOrdering, operator.children)\n+  private def ensureDistributionAndOrdering(operator: SparkPlan): SparkPlan = {\n \n-      val fixedChildren = requirements.zipped.map {\n-        case (AllTuples, rowOrdering, child) =>\n-          addOperatorsIfNecessary(SinglePartition, rowOrdering, child)\n-        case (ClusteredDistribution(clustering), rowOrdering, child) =>\n-          addOperatorsIfNecessary(HashPartitioning(clustering, numPartitions), rowOrdering, child)\n-        case (OrderedDistribution(ordering), rowOrdering, child) =>\n-          addOperatorsIfNecessary(RangePartitioning(ordering, numPartitions), rowOrdering, child)\n+    def addShuffleIfNecessary(child: SparkPlan, requiredDistribution: Distribution): SparkPlan = {\n+      // A pre-condition of ensureDistributionAndOrdering is that joins' children have compatible"
  }],
  "prId": 7988
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Seems `requiresChildPartitioningsToBeCompatible` is equivalent with `operator.children.length > 1`?\n",
    "commit": "38006e75650f7f0a827dd4b00fc2984786a2244f",
    "createdAt": "2015-08-06T23:47:44Z",
    "diffHunk": "@@ -197,66 +197,108 @@ case class Exchange(newPartitioning: Partitioning, child: SparkPlan) extends Una\n  * of input data meets the\n  * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for\n  * each operator by inserting [[Exchange]] Operators where required.  Also ensure that the\n- * required input partition ordering requirements are met.\n+ * input partition ordering requirements are met.\n  */\n private[sql] case class EnsureRequirements(sqlContext: SQLContext) extends Rule[SparkPlan] {\n   // TODO: Determine the number of partitions.\n-  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+  private def numPartitions: Int = sqlContext.conf.numShufflePartitions\n \n-  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n-    case operator: SparkPlan =>\n-      // Adds Exchange or Sort operators as required\n-      def addOperatorsIfNecessary(\n-          partitioning: Partitioning,\n-          rowOrdering: Seq[SortOrder],\n-          child: SparkPlan): SparkPlan = {\n-\n-        def addShuffleIfNecessary(child: SparkPlan): SparkPlan = {\n-          if (!child.outputPartitioning.guarantees(partitioning)) {\n-            Exchange(partitioning, child)\n-          } else {\n-            child\n-          }\n-        }\n+  /**\n+   * Given a required distribution, returns a partitioning that satisfies that distribution.\n+   */\n+  private def canonicalPartitioning(requiredDistribution: Distribution): Partitioning = {\n+    requiredDistribution match {\n+      case AllTuples => SinglePartition\n+      case ClusteredDistribution(clustering) => HashPartitioning(clustering, numPartitions)\n+      case OrderedDistribution(ordering) => RangePartitioning(ordering, numPartitions)\n+      case dist => sys.error(s\"Do not know how to satisfy distribution $dist\")\n+    }\n+  }\n \n-        def addSortIfNecessary(child: SparkPlan): SparkPlan = {\n+  /**\n+   * Return true if all of the operator's children satisfy their output distribution requirements.\n+   */\n+  private def childPartitioningsSatisfyDistributionRequirements(operator: SparkPlan): Boolean = {\n+    operator.children.zip(operator.requiredChildDistribution).forall {\n+      case (child, distribution) => child.outputPartitioning.satisfies(distribution)\n+    }\n+  }\n \n-          if (rowOrdering.nonEmpty) {\n-            // If child.outputOrdering is [a, b] and rowOrdering is [a], we do not need to sort.\n-            val minSize = Seq(rowOrdering.size, child.outputOrdering.size).min\n-            if (minSize == 0 || rowOrdering.take(minSize) != child.outputOrdering.take(minSize)) {\n-              sqlContext.planner.BasicOperators.getSortOperator(rowOrdering, global = false, child)\n-            } else {\n+  /**\n+   * Given an operator, check whether the operator requires its children to have compatible\n+   * output partitionings and add Exchanges to fix any detected incompatibilities.\n+   */\n+  private def ensureChildPartitioningsAreCompatible(operator: SparkPlan): SparkPlan = {\n+    if (operator.requiresChildPartitioningsToBeCompatible) {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Hmm.  What about `Union`?\n",
    "commit": "38006e75650f7f0a827dd4b00fc2984786a2244f",
    "createdAt": "2015-08-06T23:49:42Z",
    "diffHunk": "@@ -197,66 +197,108 @@ case class Exchange(newPartitioning: Partitioning, child: SparkPlan) extends Una\n  * of input data meets the\n  * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for\n  * each operator by inserting [[Exchange]] Operators where required.  Also ensure that the\n- * required input partition ordering requirements are met.\n+ * input partition ordering requirements are met.\n  */\n private[sql] case class EnsureRequirements(sqlContext: SQLContext) extends Rule[SparkPlan] {\n   // TODO: Determine the number of partitions.\n-  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+  private def numPartitions: Int = sqlContext.conf.numShufflePartitions\n \n-  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n-    case operator: SparkPlan =>\n-      // Adds Exchange or Sort operators as required\n-      def addOperatorsIfNecessary(\n-          partitioning: Partitioning,\n-          rowOrdering: Seq[SortOrder],\n-          child: SparkPlan): SparkPlan = {\n-\n-        def addShuffleIfNecessary(child: SparkPlan): SparkPlan = {\n-          if (!child.outputPartitioning.guarantees(partitioning)) {\n-            Exchange(partitioning, child)\n-          } else {\n-            child\n-          }\n-        }\n+  /**\n+   * Given a required distribution, returns a partitioning that satisfies that distribution.\n+   */\n+  private def canonicalPartitioning(requiredDistribution: Distribution): Partitioning = {\n+    requiredDistribution match {\n+      case AllTuples => SinglePartition\n+      case ClusteredDistribution(clustering) => HashPartitioning(clustering, numPartitions)\n+      case OrderedDistribution(ordering) => RangePartitioning(ordering, numPartitions)\n+      case dist => sys.error(s\"Do not know how to satisfy distribution $dist\")\n+    }\n+  }\n \n-        def addSortIfNecessary(child: SparkPlan): SparkPlan = {\n+  /**\n+   * Return true if all of the operator's children satisfy their output distribution requirements.\n+   */\n+  private def childPartitioningsSatisfyDistributionRequirements(operator: SparkPlan): Boolean = {\n+    operator.children.zip(operator.requiredChildDistribution).forall {\n+      case (child, distribution) => child.outputPartitioning.satisfies(distribution)\n+    }\n+  }\n \n-          if (rowOrdering.nonEmpty) {\n-            // If child.outputOrdering is [a, b] and rowOrdering is [a], we do not need to sort.\n-            val minSize = Seq(rowOrdering.size, child.outputOrdering.size).min\n-            if (minSize == 0 || rowOrdering.take(minSize) != child.outputOrdering.take(minSize)) {\n-              sqlContext.planner.BasicOperators.getSortOperator(rowOrdering, global = false, child)\n-            } else {\n+  /**\n+   * Given an operator, check whether the operator requires its children to have compatible\n+   * output partitionings and add Exchanges to fix any detected incompatibilities.\n+   */\n+  private def ensureChildPartitioningsAreCompatible(operator: SparkPlan): SparkPlan = {\n+    if (operator.requiresChildPartitioningsToBeCompatible) {"
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "The default return value of `requiredChildDistribution` is `Seq.fill(children.size)(UnspecifiedDistribution)`. Union and those broadcast joins should be fine.\n",
    "commit": "38006e75650f7f0a827dd4b00fc2984786a2244f",
    "createdAt": "2015-08-06T23:52:03Z",
    "diffHunk": "@@ -197,66 +197,108 @@ case class Exchange(newPartitioning: Partitioning, child: SparkPlan) extends Una\n  * of input data meets the\n  * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for\n  * each operator by inserting [[Exchange]] Operators where required.  Also ensure that the\n- * required input partition ordering requirements are met.\n+ * input partition ordering requirements are met.\n  */\n private[sql] case class EnsureRequirements(sqlContext: SQLContext) extends Rule[SparkPlan] {\n   // TODO: Determine the number of partitions.\n-  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+  private def numPartitions: Int = sqlContext.conf.numShufflePartitions\n \n-  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n-    case operator: SparkPlan =>\n-      // Adds Exchange or Sort operators as required\n-      def addOperatorsIfNecessary(\n-          partitioning: Partitioning,\n-          rowOrdering: Seq[SortOrder],\n-          child: SparkPlan): SparkPlan = {\n-\n-        def addShuffleIfNecessary(child: SparkPlan): SparkPlan = {\n-          if (!child.outputPartitioning.guarantees(partitioning)) {\n-            Exchange(partitioning, child)\n-          } else {\n-            child\n-          }\n-        }\n+  /**\n+   * Given a required distribution, returns a partitioning that satisfies that distribution.\n+   */\n+  private def canonicalPartitioning(requiredDistribution: Distribution): Partitioning = {\n+    requiredDistribution match {\n+      case AllTuples => SinglePartition\n+      case ClusteredDistribution(clustering) => HashPartitioning(clustering, numPartitions)\n+      case OrderedDistribution(ordering) => RangePartitioning(ordering, numPartitions)\n+      case dist => sys.error(s\"Do not know how to satisfy distribution $dist\")\n+    }\n+  }\n \n-        def addSortIfNecessary(child: SparkPlan): SparkPlan = {\n+  /**\n+   * Return true if all of the operator's children satisfy their output distribution requirements.\n+   */\n+  private def childPartitioningsSatisfyDistributionRequirements(operator: SparkPlan): Boolean = {\n+    operator.children.zip(operator.requiredChildDistribution).forall {\n+      case (child, distribution) => child.outputPartitioning.satisfies(distribution)\n+    }\n+  }\n \n-          if (rowOrdering.nonEmpty) {\n-            // If child.outputOrdering is [a, b] and rowOrdering is [a], we do not need to sort.\n-            val minSize = Seq(rowOrdering.size, child.outputOrdering.size).min\n-            if (minSize == 0 || rowOrdering.take(minSize) != child.outputOrdering.take(minSize)) {\n-              sqlContext.planner.BasicOperators.getSortOperator(rowOrdering, global = false, child)\n-            } else {\n+  /**\n+   * Given an operator, check whether the operator requires its children to have compatible\n+   * output partitionings and add Exchanges to fix any detected incompatibilities.\n+   */\n+  private def ensureChildPartitioningsAreCompatible(operator: SparkPlan): SparkPlan = {\n+    if (operator.requiresChildPartitioningsToBeCompatible) {"
  }],
  "prId": 7988
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This should actually be `guarantees`, not `compatibleWith`.  Will edit tomorrow when I incorporate the implementations of `guarantees` from Yin's patch.\n",
    "commit": "38006e75650f7f0a827dd4b00fc2984786a2244f",
    "createdAt": "2015-08-08T06:42:17Z",
    "diffHunk": "@@ -190,66 +190,111 @@ case class Exchange(newPartitioning: Partitioning, child: SparkPlan) extends Una\n  * of input data meets the\n  * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for\n  * each operator by inserting [[Exchange]] Operators where required.  Also ensure that the\n- * required input partition ordering requirements are met.\n+ * input partition ordering requirements are met.\n  */\n private[sql] case class EnsureRequirements(sqlContext: SQLContext) extends Rule[SparkPlan] {\n   // TODO: Determine the number of partitions.\n-  def numPartitions: Int = sqlContext.conf.numShufflePartitions\n+  private def numPartitions: Int = sqlContext.conf.numShufflePartitions\n \n-  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n-    case operator: SparkPlan =>\n-      // Adds Exchange or Sort operators as required\n-      def addOperatorsIfNecessary(\n-          partitioning: Partitioning,\n-          rowOrdering: Seq[SortOrder],\n-          child: SparkPlan): SparkPlan = {\n-\n-        def addShuffleIfNecessary(child: SparkPlan): SparkPlan = {\n-          if (!child.outputPartitioning.guarantees(partitioning)) {\n-            Exchange(partitioning, child)\n-          } else {\n-            child\n-          }\n-        }\n+  /**\n+   * Given a required distribution, returns a partitioning that satisfies that distribution.\n+   */\n+  private def canonicalPartitioning(requiredDistribution: Distribution): Partitioning = {\n+    requiredDistribution match {\n+      case AllTuples => SinglePartition\n+      case ClusteredDistribution(clustering) => HashPartitioning(clustering, numPartitions)\n+      case OrderedDistribution(ordering) => RangePartitioning(ordering, numPartitions)\n+      case dist => sys.error(s\"Do not know how to satisfy distribution $dist\")\n+    }\n+  }\n \n-        def addSortIfNecessary(child: SparkPlan): SparkPlan = {\n+  /**\n+   * Return true if all of the operator's children satisfy their output distribution requirements.\n+   */\n+  private def childPartitioningsSatisfyDistributionRequirements(operator: SparkPlan): Boolean = {\n+    operator.children.zip(operator.requiredChildDistribution).forall {\n+      case (child, distribution) => child.outputPartitioning.satisfies(distribution)\n+    }\n+  }\n \n-          if (rowOrdering.nonEmpty) {\n-            // If child.outputOrdering is [a, b] and rowOrdering is [a], we do not need to sort.\n-            val minSize = Seq(rowOrdering.size, child.outputOrdering.size).min\n-            if (minSize == 0 || rowOrdering.take(minSize) != child.outputOrdering.take(minSize)) {\n-              sqlContext.planner.BasicOperators.getSortOperator(rowOrdering, global = false, child)\n-            } else {\n+  /**\n+   * Given an operator, check whether the operator requires its children to have compatible\n+   * output partitionings and add Exchanges to fix any detected incompatibilities.\n+   */\n+  private def ensureChildPartitioningsAreCompatible(operator: SparkPlan): SparkPlan = {\n+    // If an operator has multiple children and the operator requires a specific child output\n+    // distribution then we need to ensure that all children have compatible output partitionings.\n+    if (operator.children.length > 1\n+        && operator.requiredChildDistribution.toSet != Set(UnspecifiedDistribution)) {\n+      if (!Partitioning.allCompatible(operator.children.map(_.outputPartitioning))) {\n+        val newChildren = operator.children.zip(operator.requiredChildDistribution).map {\n+          case (child, requiredDistribution) =>\n+            val targetPartitioning = canonicalPartitioning(requiredDistribution)\n+            if (child.outputPartitioning.compatibleWith(targetPartitioning)) {"
  }],
  "prId": 7988
}]