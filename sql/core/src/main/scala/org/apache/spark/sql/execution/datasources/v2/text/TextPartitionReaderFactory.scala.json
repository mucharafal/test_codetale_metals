[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Let's remove the extra new line.",
    "commit": "66b893dcf250e779c7c7cf93d21148fff64c30de",
    "createdAt": "2019-04-01T21:36:22Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.text\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter\n+import org.apache.spark.sql.execution.datasources.{HadoopFileLinesReader, HadoopFileWholeTextReader, PartitionedFile}\n+import org.apache.spark.sql.execution.datasources.text.TextOptions\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Text readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcasted serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Text files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param readSchema Required schema in the batch scan.\n+ * @param textOptions Options for reading a text file.\n+ * */\n+case class TextPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    partitionSchema: StructType,\n+    readSchema: StructType,\n+    textOptions: TextOptions) extends FilePartitionReaderFactory {\n+  private val readDataSchema =\n+    getReadDataSchema(readSchema, partitionSchema, sqlConf.caseSensitiveAnalysis)\n+\n+  assert(\n+    readDataSchema.length <= 1,\n+    \"Text data source only produces a single data column named \\\"value\\\".\")\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val confValue = broadcastedConf.value.value\n+    val reader = if (!textOptions.wholeText) {\n+      new HadoopFileLinesReader(file, textOptions.lineSeparatorInRead, confValue)\n+    } else {\n+      new HadoopFileWholeTextReader(file, confValue)\n+    }\n+    Option(TaskContext.get()).foreach(_.addTaskCompletionListener[Unit](_ => reader.close()))\n+    val iter = if (readDataSchema.isEmpty) {\n+      val emptyUnsafeRow = new UnsafeRow(0)\n+      reader.map(_ => emptyUnsafeRow)\n+    } else {\n+      val unsafeRowWriter = new UnsafeRowWriter(1)\n+\n+      reader.map { line =>\n+        // Writes to an UnsafeRow directly\n+        unsafeRowWriter.reset()\n+        unsafeRowWriter.write(0, line.getBytes, 0, line.getLength)\n+        unsafeRowWriter.getRow()\n+      }\n+    }\n+    val fileReader = new PartitionReaderFromIterator[InternalRow](iter)\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+"
  }],
  "prId": 24207
}]