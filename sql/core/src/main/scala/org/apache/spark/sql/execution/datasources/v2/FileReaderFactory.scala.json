[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why is this class public? Isn't this internal to HadoopFsRelation's v2 implementation?",
    "commit": "67b1748c8b939a6b484bfc868fd311e381d7f8e0",
    "createdAt": "2018-04-05T16:05:55Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.{FilePartition, FilePartitionUtil, PartitionedFile}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, DataReaderFactory}\n+\n+case class FileReaderFactory[T]("
  }],
  "prId": 20933
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "explain why this works, i.e. we use type erase hack to return columnar batch.",
    "commit": "67b1748c8b939a6b484bfc868fd311e381d7f8e0",
    "createdAt": "2018-04-05T16:09:55Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.{FilePartition, FilePartitionUtil, PartitionedFile}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, DataReaderFactory}\n+\n+case class FileReaderFactory[T](\n+    file: FilePartition,\n+    readFunction: (PartitionedFile) => Iterator[InternalRow],\n+    ignoreCorruptFiles: Boolean = false,\n+    ignoreMissingFiles: Boolean = false)\n+  extends DataReaderFactory[T] {\n+  override def createDataReader(): DataReader[T] = {\n+    val taskContext = TaskContext.get()\n+    val iter = FilePartitionUtil.compute(file, taskContext, readFunction,\n+      ignoreCorruptFiles, ignoreMissingFiles)\n+    InternalRowDataReader[T](iter)\n+  }\n+\n+  override def preferredLocations(): Array[String] = {\n+    FilePartitionUtil.getPreferredLocations(file)\n+  }\n+}\n+\n+case class InternalRowDataReader[T](iter: Iterator[InternalRow])\n+  extends DataReader[T] {\n+  override def next(): Boolean = iter.hasNext\n+\n+  override def get(): T = iter.next().asInstanceOf[T]"
  }],
  "prId": 20933
}]