[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Is there any possible message type here? If not, it's better to throw an exception.",
    "commit": "3cb6ceec48b34523ac0c58e39a4825d629b56938",
    "createdAt": "2017-12-13T00:30:51Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes.{Append, Complete, Update}\n+import org.apache.spark.sql.sources.v2.{ContinuousWriteSupport, DataSourceV2, DataSourceV2Options, MicroBatchWriteSupport}\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A sink that stores the results in memory. This [[Sink]] is primarily intended for use in unit\n+ * tests and does not provide durability.\n+ */\n+class MemorySinkV2 extends DataSourceV2\n+  with MicroBatchWriteSupport with ContinuousWriteSupport with Logging {\n+\n+  override def createMicroBatchWriter(\n+      queryId: String,\n+      batchId: Long,\n+      schema: StructType,\n+      mode: OutputMode,\n+      options: DataSourceV2Options): java.util.Optional[DataSourceV2Writer] = {\n+    java.util.Optional.of(new MemoryWriter(this, batchId, mode))\n+  }\n+\n+  override def createContinuousWriter(\n+      queryId: String,\n+      schema: StructType,\n+      mode: OutputMode,\n+      options: DataSourceV2Options): java.util.Optional[ContinuousWriter] = {\n+    java.util.Optional.of(new ContinuousMemoryWriter(this, mode))\n+  }\n+\n+  private case class AddedData(batchId: Long, data: Array[Row])\n+\n+  /** An order list of batches that have been written to this [[Sink]]. */\n+  @GuardedBy(\"this\")\n+  private val batches = new ArrayBuffer[AddedData]()\n+\n+  /** Returns all rows that are stored in this [[Sink]]. */\n+  def allData: Seq[Row] = synchronized {\n+    batches.flatMap(_.data)\n+  }\n+\n+  def latestBatchId: Option[Long] = synchronized {\n+    batches.lastOption.map(_.batchId)\n+  }\n+\n+  def latestBatchData: Seq[Row] = synchronized {\n+    batches.lastOption.toSeq.flatten(_.data)\n+  }\n+\n+  def toDebugString: String = synchronized {\n+    batches.map { case AddedData(batchId, data) =>\n+      val dataStr = try data.mkString(\" \") catch {\n+        case NonFatal(e) => \"[Error converting to string]\"\n+      }\n+      s\"$batchId: $dataStr\"\n+    }.mkString(\"\\n\")\n+  }\n+\n+  def write(batchId: Long, outputMode: OutputMode, newRows: Array[Row]): Unit = {\n+    val notCommitted = synchronized {\n+      latestBatchId.isEmpty || batchId > latestBatchId.get\n+    }\n+    if (notCommitted) {\n+      logDebug(s\"Committing batch $batchId to $this\")\n+      outputMode match {\n+        case Append | Update =>\n+          val rows = AddedData(batchId, newRows)\n+          synchronized { batches += rows }\n+\n+        case Complete =>\n+          val rows = AddedData(batchId, newRows)\n+          synchronized {\n+            batches.clear()\n+            batches += rows\n+          }\n+\n+        case _ =>\n+          throw new IllegalArgumentException(\n+            s\"Output mode $outputMode is not supported by MemorySink\")\n+      }\n+    } else {\n+      logDebug(s\"Skipping already committed batch: $batchId\")\n+    }\n+  }\n+\n+  def clear(): Unit = synchronized {\n+    batches.clear()\n+  }\n+\n+  override def toString(): String = \"MemorySink\"\n+}\n+\n+case class MemoryWriterCommitMessage(partition: Int, data: Seq[Row]) extends WriterCommitMessage {}\n+\n+class MemoryWriter(sink: MemorySinkV2, batchId: Long, outputMode: OutputMode)\n+  extends DataSourceV2Writer with Logging {\n+\n+  override def createWriterFactory: MemoryWriterFactory = MemoryWriterFactory(outputMode)\n+\n+  def commit(messages: Array[WriterCommitMessage]): Unit = {\n+    val newRows = messages.flatMap { message =>\n+      // TODO remove\n+      if (message != null) {\n+        assert(message.isInstanceOf[MemoryWriterCommitMessage])\n+        message.asInstanceOf[MemoryWriterCommitMessage].data\n+      } else {\n+        Seq()\n+      }\n+    }\n+    sink.write(batchId, outputMode, newRows)\n+  }\n+\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {\n+    // Don't accept any of the new input.\n+  }\n+}\n+\n+class ContinuousMemoryWriter(val sink: MemorySinkV2, outputMode: OutputMode)\n+  extends ContinuousWriter {\n+\n+  override def createWriterFactory: MemoryWriterFactory = MemoryWriterFactory(outputMode)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {\n+    val newRows = messages.flatMap {\n+      case message: MemoryWriterCommitMessage => message.data\n+      case _ => Seq()"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I removed the explicit mapping. I don't want to set the expectation that every data source implementation needs to check and throw exceptions here; it's Spark's responsibility to pass in the right type of message.",
    "commit": "3cb6ceec48b34523ac0c58e39a4825d629b56938",
    "createdAt": "2017-12-13T18:20:14Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes.{Append, Complete, Update}\n+import org.apache.spark.sql.sources.v2.{ContinuousWriteSupport, DataSourceV2, DataSourceV2Options, MicroBatchWriteSupport}\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A sink that stores the results in memory. This [[Sink]] is primarily intended for use in unit\n+ * tests and does not provide durability.\n+ */\n+class MemorySinkV2 extends DataSourceV2\n+  with MicroBatchWriteSupport with ContinuousWriteSupport with Logging {\n+\n+  override def createMicroBatchWriter(\n+      queryId: String,\n+      batchId: Long,\n+      schema: StructType,\n+      mode: OutputMode,\n+      options: DataSourceV2Options): java.util.Optional[DataSourceV2Writer] = {\n+    java.util.Optional.of(new MemoryWriter(this, batchId, mode))\n+  }\n+\n+  override def createContinuousWriter(\n+      queryId: String,\n+      schema: StructType,\n+      mode: OutputMode,\n+      options: DataSourceV2Options): java.util.Optional[ContinuousWriter] = {\n+    java.util.Optional.of(new ContinuousMemoryWriter(this, mode))\n+  }\n+\n+  private case class AddedData(batchId: Long, data: Array[Row])\n+\n+  /** An order list of batches that have been written to this [[Sink]]. */\n+  @GuardedBy(\"this\")\n+  private val batches = new ArrayBuffer[AddedData]()\n+\n+  /** Returns all rows that are stored in this [[Sink]]. */\n+  def allData: Seq[Row] = synchronized {\n+    batches.flatMap(_.data)\n+  }\n+\n+  def latestBatchId: Option[Long] = synchronized {\n+    batches.lastOption.map(_.batchId)\n+  }\n+\n+  def latestBatchData: Seq[Row] = synchronized {\n+    batches.lastOption.toSeq.flatten(_.data)\n+  }\n+\n+  def toDebugString: String = synchronized {\n+    batches.map { case AddedData(batchId, data) =>\n+      val dataStr = try data.mkString(\" \") catch {\n+        case NonFatal(e) => \"[Error converting to string]\"\n+      }\n+      s\"$batchId: $dataStr\"\n+    }.mkString(\"\\n\")\n+  }\n+\n+  def write(batchId: Long, outputMode: OutputMode, newRows: Array[Row]): Unit = {\n+    val notCommitted = synchronized {\n+      latestBatchId.isEmpty || batchId > latestBatchId.get\n+    }\n+    if (notCommitted) {\n+      logDebug(s\"Committing batch $batchId to $this\")\n+      outputMode match {\n+        case Append | Update =>\n+          val rows = AddedData(batchId, newRows)\n+          synchronized { batches += rows }\n+\n+        case Complete =>\n+          val rows = AddedData(batchId, newRows)\n+          synchronized {\n+            batches.clear()\n+            batches += rows\n+          }\n+\n+        case _ =>\n+          throw new IllegalArgumentException(\n+            s\"Output mode $outputMode is not supported by MemorySink\")\n+      }\n+    } else {\n+      logDebug(s\"Skipping already committed batch: $batchId\")\n+    }\n+  }\n+\n+  def clear(): Unit = synchronized {\n+    batches.clear()\n+  }\n+\n+  override def toString(): String = \"MemorySink\"\n+}\n+\n+case class MemoryWriterCommitMessage(partition: Int, data: Seq[Row]) extends WriterCommitMessage {}\n+\n+class MemoryWriter(sink: MemorySinkV2, batchId: Long, outputMode: OutputMode)\n+  extends DataSourceV2Writer with Logging {\n+\n+  override def createWriterFactory: MemoryWriterFactory = MemoryWriterFactory(outputMode)\n+\n+  def commit(messages: Array[WriterCommitMessage]): Unit = {\n+    val newRows = messages.flatMap { message =>\n+      // TODO remove\n+      if (message != null) {\n+        assert(message.isInstanceOf[MemoryWriterCommitMessage])\n+        message.asInstanceOf[MemoryWriterCommitMessage].data\n+      } else {\n+        Seq()\n+      }\n+    }\n+    sink.write(batchId, outputMode, newRows)\n+  }\n+\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {\n+    // Don't accept any of the new input.\n+  }\n+}\n+\n+class ContinuousMemoryWriter(val sink: MemorySinkV2, outputMode: OutputMode)\n+  extends ContinuousWriter {\n+\n+  override def createWriterFactory: MemoryWriterFactory = MemoryWriterFactory(outputMode)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {\n+    val newRows = messages.flatMap {\n+      case message: MemoryWriterCommitMessage => message.data\n+      case _ => Seq()"
  }],
  "prId": 19925
}]