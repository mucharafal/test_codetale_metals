[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Do we need to  Invalidate the cache to be consistent with `InsertIntoDataSourceCommand`?\r\n```\r\n    sparkSession.sharedState.cacheManager.invalidateCache(query)\r\n```",
    "commit": "d35fac34247d6d6f2593b5284bbaba906a8dd033",
    "createdAt": "2017-02-16T23:28:23Z",
    "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.spark.sql.{Dataset, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+\n+/**\n+ * Saves the results of `query` in to a data source.\n+ *\n+ * Note that this command is different from [[InsertIntoDataSourceCommand]]. This command will call\n+ * `CreatableRelationProvider.createRelation` to write out the data, while\n+ * [[InsertIntoDataSourceCommand]] calls `InsertableRelation.insert`. Ideally these 2 data source\n+ * interfaces should do the same thing, but as we've already published these 2 interfaces and the\n+ * implementations may have different logic, we have to keep these 2 different commands.\n+ */\n+case class SaveIntoDataSourceCommand(\n+    query: LogicalPlan,\n+    provider: String,\n+    partitionColumns: Seq[String],\n+    options: Map[String, String],\n+    mode: SaveMode) extends RunnableCommand {\n+\n+  override protected def innerChildren: Seq[QueryPlan[_]] = Seq(query)\n+\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    DataSource(\n+      sparkSession,\n+      className = provider,\n+      partitionColumns = partitionColumns,\n+      options = options).write(mode, Dataset.ofRows(sparkSession, query))\n+",
    "line": 49
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "we don't have the `LogicalRelation` to be used as cache key.",
    "commit": "d35fac34247d6d6f2593b5284bbaba906a8dd033",
    "createdAt": "2017-02-16T23:50:37Z",
    "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.spark.sql.{Dataset, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+\n+/**\n+ * Saves the results of `query` in to a data source.\n+ *\n+ * Note that this command is different from [[InsertIntoDataSourceCommand]]. This command will call\n+ * `CreatableRelationProvider.createRelation` to write out the data, while\n+ * [[InsertIntoDataSourceCommand]] calls `InsertableRelation.insert`. Ideally these 2 data source\n+ * interfaces should do the same thing, but as we've already published these 2 interfaces and the\n+ * implementations may have different logic, we have to keep these 2 different commands.\n+ */\n+case class SaveIntoDataSourceCommand(\n+    query: LogicalPlan,\n+    provider: String,\n+    partitionColumns: Seq[String],\n+    options: Map[String, String],\n+    mode: SaveMode) extends RunnableCommand {\n+\n+  override protected def innerChildren: Seq[QueryPlan[_]] = Seq(query)\n+\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    DataSource(\n+      sparkSession,\n+      className = provider,\n+      partitionColumns = partitionColumns,\n+      options = options).write(mode, Dataset.ofRows(sparkSession, query))\n+",
    "line": 49
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": ": )",
    "commit": "d35fac34247d6d6f2593b5284bbaba906a8dd033",
    "createdAt": "2017-02-17T01:04:53Z",
    "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import org.apache.spark.sql.{Dataset, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+\n+/**\n+ * Saves the results of `query` in to a data source.\n+ *\n+ * Note that this command is different from [[InsertIntoDataSourceCommand]]. This command will call\n+ * `CreatableRelationProvider.createRelation` to write out the data, while\n+ * [[InsertIntoDataSourceCommand]] calls `InsertableRelation.insert`. Ideally these 2 data source\n+ * interfaces should do the same thing, but as we've already published these 2 interfaces and the\n+ * implementations may have different logic, we have to keep these 2 different commands.\n+ */\n+case class SaveIntoDataSourceCommand(\n+    query: LogicalPlan,\n+    provider: String,\n+    partitionColumns: Seq[String],\n+    options: Map[String, String],\n+    mode: SaveMode) extends RunnableCommand {\n+\n+  override protected def innerChildren: Seq[QueryPlan[_]] = Seq(query)\n+\n+  override def run(sparkSession: SparkSession): Seq[Row] = {\n+    DataSource(\n+      sparkSession,\n+      className = provider,\n+      partitionColumns = partitionColumns,\n+      options = options).write(mode, Dataset.ofRows(sparkSession, query))\n+",
    "line": 49
  }],
  "prId": 16962
}]