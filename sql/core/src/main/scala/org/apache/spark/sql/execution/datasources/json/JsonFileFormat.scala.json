[{
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "If we employ the blacklist, I think it'd be better that you don't fold these imports.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-02T08:27:26Z",
    "diffHunk": "@@ -30,7 +30,7 @@ import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JacksonParser, JSON\n import org.apache.spark.sql.catalyst.util.CompressionCodecs\n import org.apache.spark.sql.execution.datasources._\n import org.apache.spark.sql.sources._\n-import org.apache.spark.sql.types.{StringType, StructType}\n+import org.apache.spark.sql.types._",
    "line": 5
  }],
  "prId": 21667
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about\r\n\r\n```\r\nthe base class\r\ndef validateDataType(dataType: DataType, isReadPath: Boolean): Boolean = dataType match {\r\n  case BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType |\r\n    StringType | BinaryType | DateType | TimestampType | _: DecimalType => true\r\n  case _ => false\r\n}\r\n\r\njson\r\noverride def validateDataType(dataType: DataType, isReadPath: Boolean): Boolean = {\r\n  case st: StructType => st.forall { f => validateDataType(f.dataType, isReadPath) }\r\n  case ArrayType...\r\n  ...\r\n  case other => super.validateDataType(other)\r\n}\r\n\r\n\r\n```",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-03T16:02:41Z",
    "diffHunk": "@@ -148,6 +144,28 @@ class JsonFileFormat extends TextBasedFileFormat with DataSourceRegister {\n   override def hashCode(): Int = getClass.hashCode()\n \n   override def equals(other: Any): Boolean = other.isInstanceOf[JsonFileFormat]\n+\n+  override def validateDataType(dataType: DataType, isReadPath: Boolean): Unit = dataType match {"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Then the base class could break other existing file formats, right?",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-03T16:10:31Z",
    "diffHunk": "@@ -148,6 +144,28 @@ class JsonFileFormat extends TextBasedFileFormat with DataSourceRegister {\n   override def hashCode(): Int = getClass.hashCode()\n \n   override def equals(other: Any): Boolean = other.isInstanceOf[JsonFileFormat]\n+\n+  override def validateDataType(dataType: DataType, isReadPath: Boolean): Unit = dataType match {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "what do you mean by break? If people use internal API(like avro), they are responsible to update their code for the internal API changes in new Spark releases.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-03T16:51:11Z",
    "diffHunk": "@@ -148,6 +144,28 @@ class JsonFileFormat extends TextBasedFileFormat with DataSourceRegister {\n   override def hashCode(): Int = getClass.hashCode()\n \n   override def equals(other: Any): Boolean = other.isInstanceOf[JsonFileFormat]\n+\n+  override def validateDataType(dataType: DataType, isReadPath: Boolean): Unit = dataType match {"
  }],
  "prId": 21667
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why JSON supports null type but CSV doesn't?",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-05T10:24:56Z",
    "diffHunk": "@@ -148,6 +144,23 @@ class JsonFileFormat extends TextBasedFileFormat with DataSourceRegister {\n   override def hashCode(): Int = getClass.hashCode()\n \n   override def equals(other: Any): Boolean = other.isInstanceOf[JsonFileFormat]\n+\n+  override def supportDataType(dataType: DataType, isReadPath: Boolean): Boolean = dataType match {\n+    case _: AtomicType => true\n+\n+    case st: StructType => st.forall { f => supportDataType(f.dataType, isReadPath) }\n+\n+    case ArrayType(elementType, _) => supportDataType(elementType, isReadPath)\n+\n+    case MapType(keyType, valueType, _) =>\n+      supportDataType(keyType, isReadPath) && supportDataType(valueType, isReadPath)\n+\n+    case udt: UserDefinedType[_] => supportDataType(udt.sqlType, isReadPath)\n+\n+    case _: NullType => true",
    "line": 44
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "currently null type is not handled in UnivocityParser",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-06T06:39:05Z",
    "diffHunk": "@@ -148,6 +144,23 @@ class JsonFileFormat extends TextBasedFileFormat with DataSourceRegister {\n   override def hashCode(): Int = getClass.hashCode()\n \n   override def equals(other: Any): Boolean = other.isInstanceOf[JsonFileFormat]\n+\n+  override def supportDataType(dataType: DataType, isReadPath: Boolean): Boolean = dataType match {\n+    case _: AtomicType => true\n+\n+    case st: StructType => st.forall { f => supportDataType(f.dataType, isReadPath) }\n+\n+    case ArrayType(elementType, _) => supportDataType(elementType, isReadPath)\n+\n+    case MapType(keyType, valueType, _) =>\n+      supportDataType(keyType, isReadPath) && supportDataType(valueType, isReadPath)\n+\n+    case udt: UserDefinedType[_] => supportDataType(udt.sqlType, isReadPath)\n+\n+    case _: NullType => true",
    "line": 44
  }],
  "prId": 21667
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yea, supported types are very specific to datasource's implementation.",
    "commit": "13de60ec3916b8396e6fe04e755eeb7d70c54b3a",
    "createdAt": "2018-07-06T11:09:57Z",
    "diffHunk": "@@ -148,6 +144,23 @@ class JsonFileFormat extends TextBasedFileFormat with DataSourceRegister {\n   override def hashCode(): Int = getClass.hashCode()\n \n   override def equals(other: Any): Boolean = other.isInstanceOf[JsonFileFormat]\n+\n+  override def supportDataType(dataType: DataType, isReadPath: Boolean): Boolean = dataType match {",
    "line": 32
  }],
  "prId": 21667
}]