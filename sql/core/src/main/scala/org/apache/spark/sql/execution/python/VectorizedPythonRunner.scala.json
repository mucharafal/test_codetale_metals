[{
  "comments": [{
    "author": {
      "login": "icexelloss"
    },
    "body": "This class duplicates quite a bit of logic of PythonRDD. I think the only difference is how they serialize/deserialize data (non-arrow vs arrow).  @ueshin @BryanCutler what's your thought on refactoring this and PythonRDD? ",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-06T14:37:00Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(",
    "line": 59
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "I agree with you that we should refactor `PythonRunner`s, but I'm not sure what you mean by refactoring PythonRDD? I think it's already simple enough.",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-07T10:54:34Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(",
    "line": 59
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Yes I meant `PythonRunner` in `PythonRDD.scala`",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-07T15:15:00Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(",
    "line": 59
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "Yes, it is a lot of duplicated code from `PythonRunner` that could be refactored.  I'm guessing you did not use the existing code because of the Arrow stream format?  While I would love to start using that in Spark, I think it would be better to do this at a later time when the required code could be refactored and the Arrow stream format could replace where we currently use the file format.\r\n\r\nAlso, the good part about using the iterator based file format is each iteration can allow Python to communicate back an error code and exit gracefully.  In my own tests with the streaming format if an error occurred after the stream had started, Spark could lock up in a waiting state.  These are the reasons I did not use the streaming format in my implementation.  Would this `VectorizedPythonRunner` be able to handle these types of errors?",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-08T04:36:27Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(",
    "line": 59
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "@icexelloss Ah, I see, thanks! I still agree with refactoring `PythonRunner`.\r\n\r\n@BryanCutler As for the error, do you mean the case like [test_vectorized_udf_exception](https://github.com/apache/spark/pull/19147/files#diff-7c2fe8530271c0635fb99f7b49e0c4a4R3250)? If not, could you please let me know the case and think about it?",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-11T08:19:32Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(",
    "line": 59
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "I was referring to the protocol between Scala and Python that is changed here and could act differently under some circumstances.  Here is the behavior of the `PythonRunner` protocol and `VectorizedPythonRunner` protocol that you introduce here:\r\n\r\n**PythonRunner**\r\nData blocks are framed by a special length integer.  Scala reads each data block one at a time and checks the length code.  If the code is a `PythonException`, the error is read from Python and a `SparkException` is thrown with that being the cause.\r\n\r\n**VectorizedPythonRunner**\r\nA data stream is opened in Scala with `ArrowStreamReader` and batches are transferred until `ArrowStreamReader` returns False indicating there is no more data.  Only at this point are the special length codes checked to handle an error from Python.\r\n\r\nThis behavior change would probably only cause problems if things are not working normally.  For example, what would happen if `pyarrow` was not installed on an executor?  With `PythonRunner` the ImportError would cause a `PythonException` to be transferred and thrown in Scala.  In `VectorizedPythonRunner` I believe the `ArrowStreamReader` would try to read the special length code and then fail somewhere internally to Arrow, not showing the ImportError.\r\n\r\nMy point was that this type of behavior change should probably be implemented in a separate JIRA where we could make sure to handle all of these cases.",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-11T23:06:59Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(",
    "line": 59
  }],
  "prId": 19147
}, {
  "comments": [{
    "author": {
      "login": "icexelloss"
    },
    "body": "A side note: How is performance of ColumnarBatch in terms of converting to a `Iterator[InternalRow]`? As far as I remember it doesn't return unsafe row at the moment, right?",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-06T14:39:05Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(\n+      inputRows: Iterator[InternalRow],\n+      schema: StructType,\n+      partitionIndex: Int,\n+      context: TaskContext): Iterator[InternalRow] = {\n+    val startTime = System.currentTimeMillis\n+    val env = SparkEnv.get\n+    val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\")\n+    envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread\n+    if (reuse_worker) {\n+      envVars.put(\"SPARK_REUSE_WORKER\", \"1\")\n+    }\n+    val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)\n+    // Whether is the worker released into idle pool\n+    @volatile var released = false\n+\n+    // Start a thread to feed the process input from our parent's iterator\n+    val writerThread = new WriterThread(\n+      env, worker, inputRows, schema, partitionIndex, context)\n+\n+    context.addTaskCompletionListener { context =>\n+      writerThread.shutdownOnTaskCompletion()\n+      if (!reuse_worker || !released) {\n+        try {\n+          worker.close()\n+        } catch {\n+          case e: Exception =>\n+            logWarning(\"Failed to close worker socket\", e)\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+\n+    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))\n+\n+    val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+      s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+    val reader = new ArrowStreamReader(stream, allocator)\n+\n+    new Iterator[InternalRow] {\n+      private val root = reader.getVectorSchemaRoot\n+      private val vectors = root.getFieldVectors.asScala.map { vector =>\n+        new ArrowColumnVector(vector)\n+      }.toArray[ColumnVector]\n+\n+      var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          root.close()\n+          allocator.close()\n+        }\n+      }\n+\n+      private[this] var batchLoaded = true\n+      private[this] var currentIter: Iterator[InternalRow] = Iterator.empty\n+\n+      override def hasNext: Boolean = batchLoaded && (currentIter.hasNext || loadNextBatch()) || {\n+        root.close()\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private def loadNextBatch(): Boolean = {\n+        batchLoaded = reader.loadNextBatch()\n+        if (batchLoaded) {\n+          val batch = new ColumnarBatch(schema, vectors, root.getRowCount)",
    "line": 130
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "That's right, it doesn't return unsafe row.\r\nBut I believe it's performant enough because it can return values in row directly from column vectors without copying to unsafe row.",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-07T10:55:08Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(\n+      inputRows: Iterator[InternalRow],\n+      schema: StructType,\n+      partitionIndex: Int,\n+      context: TaskContext): Iterator[InternalRow] = {\n+    val startTime = System.currentTimeMillis\n+    val env = SparkEnv.get\n+    val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\")\n+    envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread\n+    if (reuse_worker) {\n+      envVars.put(\"SPARK_REUSE_WORKER\", \"1\")\n+    }\n+    val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)\n+    // Whether is the worker released into idle pool\n+    @volatile var released = false\n+\n+    // Start a thread to feed the process input from our parent's iterator\n+    val writerThread = new WriterThread(\n+      env, worker, inputRows, schema, partitionIndex, context)\n+\n+    context.addTaskCompletionListener { context =>\n+      writerThread.shutdownOnTaskCompletion()\n+      if (!reuse_worker || !released) {\n+        try {\n+          worker.close()\n+        } catch {\n+          case e: Exception =>\n+            logWarning(\"Failed to close worker socket\", e)\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+\n+    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))\n+\n+    val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+      s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+    val reader = new ArrowStreamReader(stream, allocator)\n+\n+    new Iterator[InternalRow] {\n+      private val root = reader.getVectorSchemaRoot\n+      private val vectors = root.getFieldVectors.asScala.map { vector =>\n+        new ArrowColumnVector(vector)\n+      }.toArray[ColumnVector]\n+\n+      var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          root.close()\n+          allocator.close()\n+        }\n+      }\n+\n+      private[this] var batchLoaded = true\n+      private[this] var currentIter: Iterator[InternalRow] = Iterator.empty\n+\n+      override def hasNext: Boolean = batchLoaded && (currentIter.hasNext || loadNextBatch()) || {\n+        root.close()\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private def loadNextBatch(): Boolean = {\n+        batchLoaded = reader.loadNextBatch()\n+        if (batchLoaded) {\n+          val batch = new ColumnarBatch(schema, vectors, root.getRowCount)",
    "line": 130
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Interesting, does this mean it's more performant than copying the column vectors into unsafe row? Because to get any value out, it has to access the memory region of column vectors any way, therefore copying bytes from column vectors into unsafe row don't improve performance?",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-07T15:13:44Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(\n+      inputRows: Iterator[InternalRow],\n+      schema: StructType,\n+      partitionIndex: Int,\n+      context: TaskContext): Iterator[InternalRow] = {\n+    val startTime = System.currentTimeMillis\n+    val env = SparkEnv.get\n+    val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\")\n+    envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread\n+    if (reuse_worker) {\n+      envVars.put(\"SPARK_REUSE_WORKER\", \"1\")\n+    }\n+    val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)\n+    // Whether is the worker released into idle pool\n+    @volatile var released = false\n+\n+    // Start a thread to feed the process input from our parent's iterator\n+    val writerThread = new WriterThread(\n+      env, worker, inputRows, schema, partitionIndex, context)\n+\n+    context.addTaskCompletionListener { context =>\n+      writerThread.shutdownOnTaskCompletion()\n+      if (!reuse_worker || !released) {\n+        try {\n+          worker.close()\n+        } catch {\n+          case e: Exception =>\n+            logWarning(\"Failed to close worker socket\", e)\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+\n+    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))\n+\n+    val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+      s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+    val reader = new ArrowStreamReader(stream, allocator)\n+\n+    new Iterator[InternalRow] {\n+      private val root = reader.getVectorSchemaRoot\n+      private val vectors = root.getFieldVectors.asScala.map { vector =>\n+        new ArrowColumnVector(vector)\n+      }.toArray[ColumnVector]\n+\n+      var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          root.close()\n+          allocator.close()\n+        }\n+      }\n+\n+      private[this] var batchLoaded = true\n+      private[this] var currentIter: Iterator[InternalRow] = Iterator.empty\n+\n+      override def hasNext: Boolean = batchLoaded && (currentIter.hasNext || loadNextBatch()) || {\n+        root.close()\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private def loadNextBatch(): Boolean = {\n+        batchLoaded = reader.loadNextBatch()\n+        if (batchLoaded) {\n+          val batch = new ColumnarBatch(schema, vectors, root.getRowCount)",
    "line": 130
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Why copying bytes from column vectors into unsafe row can improve performance? Isn't direct access the data from the column vectors faster without the cost of copying bytes?",
    "commit": "803054e9f30b057e1b194b5625cb6216d865f1d4",
    "createdAt": "2017-09-09T15:39:40Z",
    "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream}\n+import java.net.Socket\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark.{SparkEnv, SparkFiles, TaskContext}\n+import org.apache.spark.api.python.{ChainedPythonFunctions, PythonEvalType, PythonException, PythonRDD, SpecialLengths}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonRunner`, but exchange data with Python worker via columnar format.\n+ */\n+class VectorizedPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuse_worker: Boolean,\n+    argOffsets: Array[Array[Int]]) extends Logging {\n+\n+  require(funcs.length == argOffsets.length, \"argOffsets should have the same length as funcs\")\n+\n+  // All the Python functions should have the same exec, version and envvars.\n+  private val envVars = funcs.head.funcs.head.envVars\n+  private val pythonExec = funcs.head.funcs.head.pythonExec\n+  private val pythonVer = funcs.head.funcs.head.pythonVer\n+\n+  // TODO: support accumulator in multiple UDF\n+  private val accumulator = funcs.head.funcs.head.accumulator\n+\n+  // todo: return column batch?\n+  def compute(\n+      inputRows: Iterator[InternalRow],\n+      schema: StructType,\n+      partitionIndex: Int,\n+      context: TaskContext): Iterator[InternalRow] = {\n+    val startTime = System.currentTimeMillis\n+    val env = SparkEnv.get\n+    val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\")\n+    envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread\n+    if (reuse_worker) {\n+      envVars.put(\"SPARK_REUSE_WORKER\", \"1\")\n+    }\n+    val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)\n+    // Whether is the worker released into idle pool\n+    @volatile var released = false\n+\n+    // Start a thread to feed the process input from our parent's iterator\n+    val writerThread = new WriterThread(\n+      env, worker, inputRows, schema, partitionIndex, context)\n+\n+    context.addTaskCompletionListener { context =>\n+      writerThread.shutdownOnTaskCompletion()\n+      if (!reuse_worker || !released) {\n+        try {\n+          worker.close()\n+        } catch {\n+          case e: Exception =>\n+            logWarning(\"Failed to close worker socket\", e)\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+\n+    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))\n+\n+    val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+      s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+    val reader = new ArrowStreamReader(stream, allocator)\n+\n+    new Iterator[InternalRow] {\n+      private val root = reader.getVectorSchemaRoot\n+      private val vectors = root.getFieldVectors.asScala.map { vector =>\n+        new ArrowColumnVector(vector)\n+      }.toArray[ColumnVector]\n+\n+      var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          root.close()\n+          allocator.close()\n+        }\n+      }\n+\n+      private[this] var batchLoaded = true\n+      private[this] var currentIter: Iterator[InternalRow] = Iterator.empty\n+\n+      override def hasNext: Boolean = batchLoaded && (currentIter.hasNext || loadNextBatch()) || {\n+        root.close()\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private def loadNextBatch(): Boolean = {\n+        batchLoaded = reader.loadNextBatch()\n+        if (batchLoaded) {\n+          val batch = new ColumnarBatch(schema, vectors, root.getRowCount)",
    "line": 130
  }],
  "prId": 19147
}]