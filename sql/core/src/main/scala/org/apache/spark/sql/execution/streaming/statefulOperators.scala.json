[{
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "`NUM_RECORDS_IN_PARTITION ` calculate the total number of records in current partiton, and update at the end of sample.",
    "commit": "02d44aa06f025dc1d69a7abbcf59691ce7ee0e4e",
    "createdAt": "2017-03-03T01:41:09Z",
    "diffHunk": "@@ -397,3 +402,110 @@ object StreamingDeduplicateExec {\n   private val EMPTY_ROW =\n     UnsafeProjection.create(Array[DataType](NullType)).apply(InternalRow.apply(null))\n }\n+\n+/**\n+ * Physical operator for executing streaming Sampling.\n+ *\n+ * @param k random sample k elements.\n+ */\n+case class StreamingReservoirSampleExec(\n+    keyExpressions: Seq[Attribute],\n+    child: SparkPlan,\n+    k: Int,\n+    stateId: Option[OperatorStateId] = None,\n+    eventTimeWatermark: Option[Long] = None,\n+    outputMode: Option[OutputMode] = None)\n+  extends UnaryExecNode with StateStoreWriter with WatermarkSupport {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+  ClusteredDistribution(keyExpressions) :: Nil\n+\n+  private val enc = Encoders.STRING.asInstanceOf[ExpressionEncoder[String]]\n+  private val NUM_RECORDS_IN_PARTITION = enc.toRow(\"NUM_RECORDS_IN_PARTITION\")\n+    .asInstanceOf[UnsafeRow]\n+",
    "line": 58
  }],
  "prId": 17141
}, {
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "Here, we transfer the row to (row, numRecordsTillNow), and `numRecordsTillNow` is used to calculate the weight of item.",
    "commit": "02d44aa06f025dc1d69a7abbcf59691ce7ee0e4e",
    "createdAt": "2017-03-03T01:42:52Z",
    "diffHunk": "@@ -397,3 +402,110 @@ object StreamingDeduplicateExec {\n   private val EMPTY_ROW =\n     UnsafeProjection.create(Array[DataType](NullType)).apply(InternalRow.apply(null))\n }\n+\n+/**\n+ * Physical operator for executing streaming Sampling.\n+ *\n+ * @param k random sample k elements.\n+ */\n+case class StreamingReservoirSampleExec(\n+    keyExpressions: Seq[Attribute],\n+    child: SparkPlan,\n+    k: Int,\n+    stateId: Option[OperatorStateId] = None,\n+    eventTimeWatermark: Option[Long] = None,\n+    outputMode: Option[OutputMode] = None)\n+  extends UnaryExecNode with StateStoreWriter with WatermarkSupport {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+  ClusteredDistribution(keyExpressions) :: Nil\n+\n+  private val enc = Encoders.STRING.asInstanceOf[ExpressionEncoder[String]]\n+  private val NUM_RECORDS_IN_PARTITION = enc.toRow(\"NUM_RECORDS_IN_PARTITION\")\n+    .asInstanceOf[UnsafeRow]\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics\n+    val fieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+    val withSumFieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateId.checkpointLocation,\n+      getStateId.operatorId,\n+      getStateId.batchId,\n+      keyExpressions.toStructType,\n+      child.output.toStructType,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+\n+      val numRecordsInPart = store.get(NUM_RECORDS_IN_PARTITION).map(value => {\n+        value.get(0, LongType).asInstanceOf[Long]\n+      }).getOrElse(0L)\n+\n+      val seed = Random.nextLong()\n+      val rand = new XORShiftRandom(seed)\n+      var numSamples = numRecordsInPart\n+      var count = 0\n+\n+      val baseIterator = watermarkPredicate match {\n+        case Some(predicate) => iter.filter((row: InternalRow) => !predicate.eval(row))\n+        case None => iter\n+      }\n+\n+      baseIterator.foreach { r =>\n+        count += 1\n+        if (numSamples < k) {\n+          numSamples += 1\n+          store.put(enc.toRow(numSamples.toString).asInstanceOf[UnsafeRow],\n+            r.asInstanceOf[UnsafeRow])\n+        } else {\n+          val randomIdx = (rand.nextDouble() * (numRecordsInPart + count)).toLong\n+          if (randomIdx <= k) {\n+            val replacementIdx = enc.toRow(randomIdx.toString).asInstanceOf[UnsafeRow]\n+            store.put(replacementIdx, r.asInstanceOf[UnsafeRow])\n+          }\n+        }\n+      }\n+\n+      val numRecordsTillNow = UnsafeProjection.create(Array[DataType](LongType))\n+        .apply(InternalRow.apply(numRecordsInPart + count))\n+      store.put(NUM_RECORDS_IN_PARTITION, numRecordsTillNow)\n+      store.commit()\n+\n+      outputMode match {\n+        case Some(Complete) =>\n+          CompletionIterator[InternalRow, Iterator[InternalRow]](\n+            store.iterator().filter(kv => {\n+              !kv._1.asInstanceOf[UnsafeRow].equals(NUM_RECORDS_IN_PARTITION)\n+            }).map(kv => {\n+              UnsafeProjection.create(withSumFieldTypes).apply(InternalRow.fromSeq(\n+                new JoinedRow(kv._2, numRecordsTillNow)\n+                  .toSeq(withSumFieldTypes)))\n+            }), {})",
    "line": 116
  }],
  "prId": 17141
}, {
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "same",
    "commit": "02d44aa06f025dc1d69a7abbcf59691ce7ee0e4e",
    "createdAt": "2017-03-03T01:43:00Z",
    "diffHunk": "@@ -397,3 +402,110 @@ object StreamingDeduplicateExec {\n   private val EMPTY_ROW =\n     UnsafeProjection.create(Array[DataType](NullType)).apply(InternalRow.apply(null))\n }\n+\n+/**\n+ * Physical operator for executing streaming Sampling.\n+ *\n+ * @param k random sample k elements.\n+ */\n+case class StreamingReservoirSampleExec(\n+    keyExpressions: Seq[Attribute],\n+    child: SparkPlan,\n+    k: Int,\n+    stateId: Option[OperatorStateId] = None,\n+    eventTimeWatermark: Option[Long] = None,\n+    outputMode: Option[OutputMode] = None)\n+  extends UnaryExecNode with StateStoreWriter with WatermarkSupport {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+  ClusteredDistribution(keyExpressions) :: Nil\n+\n+  private val enc = Encoders.STRING.asInstanceOf[ExpressionEncoder[String]]\n+  private val NUM_RECORDS_IN_PARTITION = enc.toRow(\"NUM_RECORDS_IN_PARTITION\")\n+    .asInstanceOf[UnsafeRow]\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics\n+    val fieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+    val withSumFieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateId.checkpointLocation,\n+      getStateId.operatorId,\n+      getStateId.batchId,\n+      keyExpressions.toStructType,\n+      child.output.toStructType,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+\n+      val numRecordsInPart = store.get(NUM_RECORDS_IN_PARTITION).map(value => {\n+        value.get(0, LongType).asInstanceOf[Long]\n+      }).getOrElse(0L)\n+\n+      val seed = Random.nextLong()\n+      val rand = new XORShiftRandom(seed)\n+      var numSamples = numRecordsInPart\n+      var count = 0\n+\n+      val baseIterator = watermarkPredicate match {\n+        case Some(predicate) => iter.filter((row: InternalRow) => !predicate.eval(row))\n+        case None => iter\n+      }\n+\n+      baseIterator.foreach { r =>\n+        count += 1\n+        if (numSamples < k) {\n+          numSamples += 1\n+          store.put(enc.toRow(numSamples.toString).asInstanceOf[UnsafeRow],\n+            r.asInstanceOf[UnsafeRow])\n+        } else {\n+          val randomIdx = (rand.nextDouble() * (numRecordsInPart + count)).toLong\n+          if (randomIdx <= k) {\n+            val replacementIdx = enc.toRow(randomIdx.toString).asInstanceOf[UnsafeRow]\n+            store.put(replacementIdx, r.asInstanceOf[UnsafeRow])\n+          }\n+        }\n+      }\n+\n+      val numRecordsTillNow = UnsafeProjection.create(Array[DataType](LongType))\n+        .apply(InternalRow.apply(numRecordsInPart + count))\n+      store.put(NUM_RECORDS_IN_PARTITION, numRecordsTillNow)\n+      store.commit()\n+\n+      outputMode match {\n+        case Some(Complete) =>\n+          CompletionIterator[InternalRow, Iterator[InternalRow]](\n+            store.iterator().filter(kv => {\n+              !kv._1.asInstanceOf[UnsafeRow].equals(NUM_RECORDS_IN_PARTITION)\n+            }).map(kv => {\n+              UnsafeProjection.create(withSumFieldTypes).apply(InternalRow.fromSeq(\n+                new JoinedRow(kv._2, numRecordsTillNow)\n+                  .toSeq(withSumFieldTypes)))\n+            }), {})\n+        case Some(Update) =>\n+          CompletionIterator[InternalRow, Iterator[InternalRow]](\n+            store.updates()\n+              .filter(update => !update.key.equals(NUM_RECORDS_IN_PARTITION))\n+              .map(update => {\n+                UnsafeProjection.create(withSumFieldTypes).apply(InternalRow.fromSeq(\n+                    new JoinedRow(update.value, numRecordsTillNow)\n+                      .toSeq(withSumFieldTypes)))",
    "line": 124
  }],
  "prId": 17141
}, {
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "here, we do once global weight reservoir sampling.",
    "commit": "02d44aa06f025dc1d69a7abbcf59691ce7ee0e4e",
    "createdAt": "2017-03-03T01:43:37Z",
    "diffHunk": "@@ -397,3 +402,110 @@ object StreamingDeduplicateExec {\n   private val EMPTY_ROW =\n     UnsafeProjection.create(Array[DataType](NullType)).apply(InternalRow.apply(null))\n }\n+\n+/**\n+ * Physical operator for executing streaming Sampling.\n+ *\n+ * @param k random sample k elements.\n+ */\n+case class StreamingReservoirSampleExec(\n+    keyExpressions: Seq[Attribute],\n+    child: SparkPlan,\n+    k: Int,\n+    stateId: Option[OperatorStateId] = None,\n+    eventTimeWatermark: Option[Long] = None,\n+    outputMode: Option[OutputMode] = None)\n+  extends UnaryExecNode with StateStoreWriter with WatermarkSupport {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+  ClusteredDistribution(keyExpressions) :: Nil\n+\n+  private val enc = Encoders.STRING.asInstanceOf[ExpressionEncoder[String]]\n+  private val NUM_RECORDS_IN_PARTITION = enc.toRow(\"NUM_RECORDS_IN_PARTITION\")\n+    .asInstanceOf[UnsafeRow]\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics\n+    val fieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+    val withSumFieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateId.checkpointLocation,\n+      getStateId.operatorId,\n+      getStateId.batchId,\n+      keyExpressions.toStructType,\n+      child.output.toStructType,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+\n+      val numRecordsInPart = store.get(NUM_RECORDS_IN_PARTITION).map(value => {\n+        value.get(0, LongType).asInstanceOf[Long]\n+      }).getOrElse(0L)\n+\n+      val seed = Random.nextLong()\n+      val rand = new XORShiftRandom(seed)\n+      var numSamples = numRecordsInPart\n+      var count = 0\n+\n+      val baseIterator = watermarkPredicate match {\n+        case Some(predicate) => iter.filter((row: InternalRow) => !predicate.eval(row))\n+        case None => iter\n+      }\n+\n+      baseIterator.foreach { r =>\n+        count += 1\n+        if (numSamples < k) {\n+          numSamples += 1\n+          store.put(enc.toRow(numSamples.toString).asInstanceOf[UnsafeRow],\n+            r.asInstanceOf[UnsafeRow])\n+        } else {\n+          val randomIdx = (rand.nextDouble() * (numRecordsInPart + count)).toLong\n+          if (randomIdx <= k) {\n+            val replacementIdx = enc.toRow(randomIdx.toString).asInstanceOf[UnsafeRow]\n+            store.put(replacementIdx, r.asInstanceOf[UnsafeRow])\n+          }\n+        }\n+      }\n+\n+      val numRecordsTillNow = UnsafeProjection.create(Array[DataType](LongType))\n+        .apply(InternalRow.apply(numRecordsInPart + count))\n+      store.put(NUM_RECORDS_IN_PARTITION, numRecordsTillNow)\n+      store.commit()\n+\n+      outputMode match {\n+        case Some(Complete) =>\n+          CompletionIterator[InternalRow, Iterator[InternalRow]](\n+            store.iterator().filter(kv => {\n+              !kv._1.asInstanceOf[UnsafeRow].equals(NUM_RECORDS_IN_PARTITION)\n+            }).map(kv => {\n+              UnsafeProjection.create(withSumFieldTypes).apply(InternalRow.fromSeq(\n+                new JoinedRow(kv._2, numRecordsTillNow)\n+                  .toSeq(withSumFieldTypes)))\n+            }), {})\n+        case Some(Update) =>\n+          CompletionIterator[InternalRow, Iterator[InternalRow]](\n+            store.updates()\n+              .filter(update => !update.key.equals(NUM_RECORDS_IN_PARTITION))\n+              .map(update => {\n+                UnsafeProjection.create(withSumFieldTypes).apply(InternalRow.fromSeq(\n+                    new JoinedRow(update.value, numRecordsTillNow)\n+                      .toSeq(withSumFieldTypes)))\n+              }), {})\n+        case _ =>\n+          throw new UnsupportedOperationException(s\"Invalid output mode: $outputMode \" +\n+            s\"for streaming sampling.\")\n+      }\n+    }.repartition(1).mapPartitions(it => {\n+      SamplingUtils.reservoirSampleWithWeight(\n+        it.map(item => (item, item.getLong(keyExpressions.size))), k)\n+        .map(row =>\n+          UnsafeProjection.create(fieldTypes)\n+            .apply(InternalRow.fromSeq(row.toSeq(fieldTypes)))\n+        ).iterator\n+    })\n+  }",
    "line": 138
  }],
  "prId": 17141
}, {
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "In partiton, we just need to do once normal (without weight) reservoir sampling.",
    "commit": "02d44aa06f025dc1d69a7abbcf59691ce7ee0e4e",
    "createdAt": "2017-03-03T01:44:19Z",
    "diffHunk": "@@ -397,3 +402,110 @@ object StreamingDeduplicateExec {\n   private val EMPTY_ROW =\n     UnsafeProjection.create(Array[DataType](NullType)).apply(InternalRow.apply(null))\n }\n+\n+/**\n+ * Physical operator for executing streaming Sampling.\n+ *\n+ * @param k random sample k elements.\n+ */\n+case class StreamingReservoirSampleExec(\n+    keyExpressions: Seq[Attribute],\n+    child: SparkPlan,\n+    k: Int,\n+    stateId: Option[OperatorStateId] = None,\n+    eventTimeWatermark: Option[Long] = None,\n+    outputMode: Option[OutputMode] = None)\n+  extends UnaryExecNode with StateStoreWriter with WatermarkSupport {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+  ClusteredDistribution(keyExpressions) :: Nil\n+\n+  private val enc = Encoders.STRING.asInstanceOf[ExpressionEncoder[String]]\n+  private val NUM_RECORDS_IN_PARTITION = enc.toRow(\"NUM_RECORDS_IN_PARTITION\")\n+    .asInstanceOf[UnsafeRow]\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics\n+    val fieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+    val withSumFieldTypes = (keyExpressions.map(_.dataType) ++ Seq(LongType)).toArray\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateId.checkpointLocation,\n+      getStateId.operatorId,\n+      getStateId.batchId,\n+      keyExpressions.toStructType,\n+      child.output.toStructType,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+\n+      val numRecordsInPart = store.get(NUM_RECORDS_IN_PARTITION).map(value => {\n+        value.get(0, LongType).asInstanceOf[Long]\n+      }).getOrElse(0L)\n+\n+      val seed = Random.nextLong()\n+      val rand = new XORShiftRandom(seed)\n+      var numSamples = numRecordsInPart\n+      var count = 0\n+\n+      val baseIterator = watermarkPredicate match {\n+        case Some(predicate) => iter.filter((row: InternalRow) => !predicate.eval(row))\n+        case None => iter\n+      }\n+\n+      baseIterator.foreach { r =>\n+        count += 1\n+        if (numSamples < k) {\n+          numSamples += 1\n+          store.put(enc.toRow(numSamples.toString).asInstanceOf[UnsafeRow],\n+            r.asInstanceOf[UnsafeRow])\n+        } else {\n+          val randomIdx = (rand.nextDouble() * (numRecordsInPart + count)).toLong\n+          if (randomIdx <= k) {\n+            val replacementIdx = enc.toRow(randomIdx.toString).asInstanceOf[UnsafeRow]\n+            store.put(replacementIdx, r.asInstanceOf[UnsafeRow])\n+          }\n+        }\n+      }",
    "line": 100
  }],
  "prId": 17141
}]