[{
  "comments": [{
    "author": {
      "login": "gczsjdy"
    },
    "body": "Will there be any difference if we judge by `lower.satisfies(fatherOperator.requiredDistribution)`?",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-03-17T15:04:40Z",
    "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive.rule\n+\n+import org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.exchange.ShuffleExchangeExec\n+\n+/**\n+ * Remove shuffle nodes if the child's output partitions is already the desired partitioning.\n+ *\n+ * This should be the last rule of adaptive optimizer rules, as other rules may change plan\n+ * node's output partitioning and make some shuffle nodes become unnecessary.\n+ */\n+object RemoveRedundantShuffles extends Rule[SparkPlan] {\n+  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case shuffle @ ShuffleExchangeExec(upper: HashPartitioning, child) =>\n+      child.outputPartitioning match {\n+        case lower: HashPartitioning if upper.semanticEquals(lower) => child",
    "line": 35
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is copied from `EnsureRequirements`, but I think there is a  difference: the number of partitions matters in `semanticEquals`. That said, `lower.satisfies(fatherOperator.requiredDistribution)` is more aggressive and may remove user-specified shuffle via something like `df.partitionBy`",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-03-18T06:51:43Z",
    "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive.rule\n+\n+import org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.exchange.ShuffleExchangeExec\n+\n+/**\n+ * Remove shuffle nodes if the child's output partitions is already the desired partitioning.\n+ *\n+ * This should be the last rule of adaptive optimizer rules, as other rules may change plan\n+ * node's output partitioning and make some shuffle nodes become unnecessary.\n+ */\n+object RemoveRedundantShuffles extends Rule[SparkPlan] {\n+  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case shuffle @ ShuffleExchangeExec(upper: HashPartitioning, child) =>\n+      child.outputPartitioning match {\n+        case lower: HashPartitioning if upper.semanticEquals(lower) => child",
    "line": 35
  }],
  "prId": 20303
}]