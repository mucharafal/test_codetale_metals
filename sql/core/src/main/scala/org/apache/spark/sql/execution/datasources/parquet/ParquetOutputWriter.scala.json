[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Why is this down here?\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T00:57:57Z",
    "diffHunk": "@@ -17,125 +17,13 @@\n \n package org.apache.spark.sql.execution.datasources.parquet\n \n-import org.apache.hadoop.conf.Configuration\n import org.apache.hadoop.fs.Path\n import org.apache.hadoop.mapreduce._\n-import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n-import org.apache.parquet.hadoop.{ParquetOutputFormat, ParquetRecordWriter}\n-import org.apache.parquet.hadoop.codec.CodecConfig\n-import org.apache.parquet.hadoop.util.ContextUtil\n+import org.apache.parquet.hadoop.ParquetOutputFormat\n \n import org.apache.spark.sql.Row\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n-import org.apache.spark.sql.internal.SQLConf\n-import org.apache.spark.sql.types.StructType\n-import org.apache.spark.util.SerializableConfiguration\n-\n-\n-/**\n- * A factory for generating OutputWriters for writing parquet files. This implemented is different\n- * from the [[ParquetOutputWriter]] as this does not use any [[OutputCommitter]]. It simply\n- * writes the data to the path used to generate the output writer. Callers of this factory\n- * has to ensure which files are to be considered as committed.\n- */\n-private[parquet] class ParquetOutputWriterFactory(\n-    sqlConf: SQLConf,\n-    dataSchema: StructType,\n-    hadoopConf: Configuration,\n-    options: Map[String, String])\n-  extends OutputWriterFactory {\n-\n-  private val serializableConf: SerializableConfiguration = {\n-    val job = Job.getInstance(hadoopConf)\n-    val conf = ContextUtil.getConfiguration(job)\n-    val parquetOptions = new ParquetOptions(options, sqlConf)\n-\n-    // We're not really using `ParquetOutputFormat[Row]` for writing data here, because we override\n-    // it in `ParquetOutputWriter` to support appending and dynamic partitioning.  The reason why\n-    // we set it here is to setup the output committer class to `ParquetOutputCommitter`, which is\n-    // bundled with `ParquetOutputFormat[Row]`.\n-    job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])\n-\n-    ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n-\n-    // We want to clear this temporary metadata from saving into Parquet file.\n-    // This metadata is only useful for detecting optional columns when pushing down filters.\n-    val dataSchemaToWrite = StructType.removeMetadata(\n-      StructType.metadataKeyForOptionalField,\n-      dataSchema).asInstanceOf[StructType]\n-    ParquetWriteSupport.setSchema(dataSchemaToWrite, conf)\n-\n-    // Sets flags for `CatalystSchemaConverter` (which converts Catalyst schema to Parquet schema)\n-    // and `CatalystWriteSupport` (writing actual rows to Parquet files).\n-    conf.set(\n-      SQLConf.PARQUET_BINARY_AS_STRING.key,\n-      sqlConf.isParquetBinaryAsString.toString)\n-\n-    conf.set(\n-      SQLConf.PARQUET_INT96_AS_TIMESTAMP.key,\n-      sqlConf.isParquetINT96AsTimestamp.toString)\n-\n-    conf.set(\n-      SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key,\n-      sqlConf.writeLegacyParquetFormat.toString)\n-\n-    // Sets compression scheme\n-    conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)\n-    new SerializableConfiguration(conf)\n-  }\n-\n-  /**\n-   * Returns a [[OutputWriter]] that writes data to the give path without using\n-   * [[OutputCommitter]].\n-   */\n-  override def newWriter(path: String): OutputWriter = new OutputWriter {\n-\n-    // Create TaskAttemptContext that is used to pass on Configuration to the ParquetRecordWriter\n-    private val hadoopTaskAttemptId = new TaskAttemptID(new TaskID(new JobID, TaskType.MAP, 0), 0)\n-    private val hadoopAttemptContext = new TaskAttemptContextImpl(\n-      serializableConf.value, hadoopTaskAttemptId)\n-\n-    // Instance of ParquetRecordWriter that does not use OutputCommitter\n-    private val recordWriter = createNoCommitterRecordWriter(path, hadoopAttemptContext)\n-\n-    override def write(row: Row): Unit = {\n-      throw new UnsupportedOperationException(\"call writeInternal\")\n-    }\n-\n-    protected[sql] override def writeInternal(row: InternalRow): Unit = {\n-      recordWriter.write(null, row)\n-    }\n-\n-    override def close(): Unit = recordWriter.close(hadoopAttemptContext)\n-  }\n-\n-  /** Create a [[ParquetRecordWriter]] that writes the given path without using OutputCommitter */\n-  private def createNoCommitterRecordWriter(\n-      path: String,\n-      hadoopAttemptContext: TaskAttemptContext): RecordWriter[Void, InternalRow] = {\n-    // Custom ParquetOutputFormat that disable use of committer and writes to the given path\n-    val outputFormat = new ParquetOutputFormat[InternalRow]() {\n-      override def getOutputCommitter(c: TaskAttemptContext): OutputCommitter = { null }\n-      override def getDefaultWorkFile(c: TaskAttemptContext, ext: String): Path = { new Path(path) }\n-    }\n-    outputFormat.getRecordWriter(hadoopAttemptContext)\n-  }\n-\n-  /** Disable the use of the older API. */\n-  override def newInstance(\n-      path: String,\n-      dataSchema: StructType,\n-      context: TaskAttemptContext): OutputWriter = {\n-    throw new UnsupportedOperationException(\"this version of newInstance not supported for \" +\n-        \"ParquetOutputWriterFactory\")\n-  }\n-\n-  override def getFileExtension(context: TaskAttemptContext): String = {\n-    CodecConfig.from(context).getCodec.getExtension + \".parquet\"\n-  }\n-}\n-\n+import org.apache.spark.sql.execution.datasources.OutputWriter",
    "line": 124
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "It's not. This is the top.\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T01:02:53Z",
    "diffHunk": "@@ -17,125 +17,13 @@\n \n package org.apache.spark.sql.execution.datasources.parquet\n \n-import org.apache.hadoop.conf.Configuration\n import org.apache.hadoop.fs.Path\n import org.apache.hadoop.mapreduce._\n-import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n-import org.apache.parquet.hadoop.{ParquetOutputFormat, ParquetRecordWriter}\n-import org.apache.parquet.hadoop.codec.CodecConfig\n-import org.apache.parquet.hadoop.util.ContextUtil\n+import org.apache.parquet.hadoop.ParquetOutputFormat\n \n import org.apache.spark.sql.Row\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n-import org.apache.spark.sql.internal.SQLConf\n-import org.apache.spark.sql.types.StructType\n-import org.apache.spark.util.SerializableConfiguration\n-\n-\n-/**\n- * A factory for generating OutputWriters for writing parquet files. This implemented is different\n- * from the [[ParquetOutputWriter]] as this does not use any [[OutputCommitter]]. It simply\n- * writes the data to the path used to generate the output writer. Callers of this factory\n- * has to ensure which files are to be considered as committed.\n- */\n-private[parquet] class ParquetOutputWriterFactory(\n-    sqlConf: SQLConf,\n-    dataSchema: StructType,\n-    hadoopConf: Configuration,\n-    options: Map[String, String])\n-  extends OutputWriterFactory {\n-\n-  private val serializableConf: SerializableConfiguration = {\n-    val job = Job.getInstance(hadoopConf)\n-    val conf = ContextUtil.getConfiguration(job)\n-    val parquetOptions = new ParquetOptions(options, sqlConf)\n-\n-    // We're not really using `ParquetOutputFormat[Row]` for writing data here, because we override\n-    // it in `ParquetOutputWriter` to support appending and dynamic partitioning.  The reason why\n-    // we set it here is to setup the output committer class to `ParquetOutputCommitter`, which is\n-    // bundled with `ParquetOutputFormat[Row]`.\n-    job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])\n-\n-    ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n-\n-    // We want to clear this temporary metadata from saving into Parquet file.\n-    // This metadata is only useful for detecting optional columns when pushing down filters.\n-    val dataSchemaToWrite = StructType.removeMetadata(\n-      StructType.metadataKeyForOptionalField,\n-      dataSchema).asInstanceOf[StructType]\n-    ParquetWriteSupport.setSchema(dataSchemaToWrite, conf)\n-\n-    // Sets flags for `CatalystSchemaConverter` (which converts Catalyst schema to Parquet schema)\n-    // and `CatalystWriteSupport` (writing actual rows to Parquet files).\n-    conf.set(\n-      SQLConf.PARQUET_BINARY_AS_STRING.key,\n-      sqlConf.isParquetBinaryAsString.toString)\n-\n-    conf.set(\n-      SQLConf.PARQUET_INT96_AS_TIMESTAMP.key,\n-      sqlConf.isParquetINT96AsTimestamp.toString)\n-\n-    conf.set(\n-      SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key,\n-      sqlConf.writeLegacyParquetFormat.toString)\n-\n-    // Sets compression scheme\n-    conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)\n-    new SerializableConfiguration(conf)\n-  }\n-\n-  /**\n-   * Returns a [[OutputWriter]] that writes data to the give path without using\n-   * [[OutputCommitter]].\n-   */\n-  override def newWriter(path: String): OutputWriter = new OutputWriter {\n-\n-    // Create TaskAttemptContext that is used to pass on Configuration to the ParquetRecordWriter\n-    private val hadoopTaskAttemptId = new TaskAttemptID(new TaskID(new JobID, TaskType.MAP, 0), 0)\n-    private val hadoopAttemptContext = new TaskAttemptContextImpl(\n-      serializableConf.value, hadoopTaskAttemptId)\n-\n-    // Instance of ParquetRecordWriter that does not use OutputCommitter\n-    private val recordWriter = createNoCommitterRecordWriter(path, hadoopAttemptContext)\n-\n-    override def write(row: Row): Unit = {\n-      throw new UnsupportedOperationException(\"call writeInternal\")\n-    }\n-\n-    protected[sql] override def writeInternal(row: InternalRow): Unit = {\n-      recordWriter.write(null, row)\n-    }\n-\n-    override def close(): Unit = recordWriter.close(hadoopAttemptContext)\n-  }\n-\n-  /** Create a [[ParquetRecordWriter]] that writes the given path without using OutputCommitter */\n-  private def createNoCommitterRecordWriter(\n-      path: String,\n-      hadoopAttemptContext: TaskAttemptContext): RecordWriter[Void, InternalRow] = {\n-    // Custom ParquetOutputFormat that disable use of committer and writes to the given path\n-    val outputFormat = new ParquetOutputFormat[InternalRow]() {\n-      override def getOutputCommitter(c: TaskAttemptContext): OutputCommitter = { null }\n-      override def getDefaultWorkFile(c: TaskAttemptContext, ext: String): Path = { new Path(path) }\n-    }\n-    outputFormat.getRecordWriter(hadoopAttemptContext)\n-  }\n-\n-  /** Disable the use of the older API. */\n-  override def newInstance(\n-      path: String,\n-      dataSchema: StructType,\n-      context: TaskAttemptContext): OutputWriter = {\n-    throw new UnsupportedOperationException(\"this version of newInstance not supported for \" +\n-        \"ParquetOutputWriterFactory\")\n-  }\n-\n-  override def getFileExtension(context: TaskAttemptContext): String = {\n-    CodecConfig.from(context).getCodec.getExtension + \".parquet\"\n-  }\n-}\n-\n+import org.apache.spark.sql.execution.datasources.OutputWriter",
    "line": 124
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "oh... i see\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T01:04:18Z",
    "diffHunk": "@@ -17,125 +17,13 @@\n \n package org.apache.spark.sql.execution.datasources.parquet\n \n-import org.apache.hadoop.conf.Configuration\n import org.apache.hadoop.fs.Path\n import org.apache.hadoop.mapreduce._\n-import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n-import org.apache.parquet.hadoop.{ParquetOutputFormat, ParquetRecordWriter}\n-import org.apache.parquet.hadoop.codec.CodecConfig\n-import org.apache.parquet.hadoop.util.ContextUtil\n+import org.apache.parquet.hadoop.ParquetOutputFormat\n \n import org.apache.spark.sql.Row\n import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.execution.datasources.{OutputWriter, OutputWriterFactory}\n-import org.apache.spark.sql.internal.SQLConf\n-import org.apache.spark.sql.types.StructType\n-import org.apache.spark.util.SerializableConfiguration\n-\n-\n-/**\n- * A factory for generating OutputWriters for writing parquet files. This implemented is different\n- * from the [[ParquetOutputWriter]] as this does not use any [[OutputCommitter]]. It simply\n- * writes the data to the path used to generate the output writer. Callers of this factory\n- * has to ensure which files are to be considered as committed.\n- */\n-private[parquet] class ParquetOutputWriterFactory(\n-    sqlConf: SQLConf,\n-    dataSchema: StructType,\n-    hadoopConf: Configuration,\n-    options: Map[String, String])\n-  extends OutputWriterFactory {\n-\n-  private val serializableConf: SerializableConfiguration = {\n-    val job = Job.getInstance(hadoopConf)\n-    val conf = ContextUtil.getConfiguration(job)\n-    val parquetOptions = new ParquetOptions(options, sqlConf)\n-\n-    // We're not really using `ParquetOutputFormat[Row]` for writing data here, because we override\n-    // it in `ParquetOutputWriter` to support appending and dynamic partitioning.  The reason why\n-    // we set it here is to setup the output committer class to `ParquetOutputCommitter`, which is\n-    // bundled with `ParquetOutputFormat[Row]`.\n-    job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])\n-\n-    ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n-\n-    // We want to clear this temporary metadata from saving into Parquet file.\n-    // This metadata is only useful for detecting optional columns when pushing down filters.\n-    val dataSchemaToWrite = StructType.removeMetadata(\n-      StructType.metadataKeyForOptionalField,\n-      dataSchema).asInstanceOf[StructType]\n-    ParquetWriteSupport.setSchema(dataSchemaToWrite, conf)\n-\n-    // Sets flags for `CatalystSchemaConverter` (which converts Catalyst schema to Parquet schema)\n-    // and `CatalystWriteSupport` (writing actual rows to Parquet files).\n-    conf.set(\n-      SQLConf.PARQUET_BINARY_AS_STRING.key,\n-      sqlConf.isParquetBinaryAsString.toString)\n-\n-    conf.set(\n-      SQLConf.PARQUET_INT96_AS_TIMESTAMP.key,\n-      sqlConf.isParquetINT96AsTimestamp.toString)\n-\n-    conf.set(\n-      SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key,\n-      sqlConf.writeLegacyParquetFormat.toString)\n-\n-    // Sets compression scheme\n-    conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)\n-    new SerializableConfiguration(conf)\n-  }\n-\n-  /**\n-   * Returns a [[OutputWriter]] that writes data to the give path without using\n-   * [[OutputCommitter]].\n-   */\n-  override def newWriter(path: String): OutputWriter = new OutputWriter {\n-\n-    // Create TaskAttemptContext that is used to pass on Configuration to the ParquetRecordWriter\n-    private val hadoopTaskAttemptId = new TaskAttemptID(new TaskID(new JobID, TaskType.MAP, 0), 0)\n-    private val hadoopAttemptContext = new TaskAttemptContextImpl(\n-      serializableConf.value, hadoopTaskAttemptId)\n-\n-    // Instance of ParquetRecordWriter that does not use OutputCommitter\n-    private val recordWriter = createNoCommitterRecordWriter(path, hadoopAttemptContext)\n-\n-    override def write(row: Row): Unit = {\n-      throw new UnsupportedOperationException(\"call writeInternal\")\n-    }\n-\n-    protected[sql] override def writeInternal(row: InternalRow): Unit = {\n-      recordWriter.write(null, row)\n-    }\n-\n-    override def close(): Unit = recordWriter.close(hadoopAttemptContext)\n-  }\n-\n-  /** Create a [[ParquetRecordWriter]] that writes the given path without using OutputCommitter */\n-  private def createNoCommitterRecordWriter(\n-      path: String,\n-      hadoopAttemptContext: TaskAttemptContext): RecordWriter[Void, InternalRow] = {\n-    // Custom ParquetOutputFormat that disable use of committer and writes to the given path\n-    val outputFormat = new ParquetOutputFormat[InternalRow]() {\n-      override def getOutputCommitter(c: TaskAttemptContext): OutputCommitter = { null }\n-      override def getDefaultWorkFile(c: TaskAttemptContext, ext: String): Path = { new Path(path) }\n-    }\n-    outputFormat.getRecordWriter(hadoopAttemptContext)\n-  }\n-\n-  /** Disable the use of the older API. */\n-  override def newInstance(\n-      path: String,\n-      dataSchema: StructType,\n-      context: TaskAttemptContext): OutputWriter = {\n-    throw new UnsupportedOperationException(\"this version of newInstance not supported for \" +\n-        \"ParquetOutputWriterFactory\")\n-  }\n-\n-  override def getFileExtension(context: TaskAttemptContext): String = {\n-    CodecConfig.from(context).getCodec.getExtension + \".parquet\"\n-  }\n-}\n-\n+import org.apache.spark.sql.execution.datasources.OutputWriter",
    "line": 124
  }],
  "prId": 15710
}]