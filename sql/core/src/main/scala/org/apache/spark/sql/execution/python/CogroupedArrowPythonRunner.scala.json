[{
  "comments": [{
    "author": {
      "login": "icexelloss"
    },
    "body": "I am thinking if it is better to have allocator to be a member of the class, instead of creating it every time in the function here",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-08-07T19:03:11Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Python UDF Runner for cogrouped udfs.  Although the data is exchanged with the python\n+ * worker via arrow, we cannot use `ArrowPythonRunner` as we need to send more than one\n+ * dataframe.\n+ */\n+class CogroupedArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    leftSchema: StructType,\n+    rightSchema: StructType,\n+    timeZoneId: String,\n+    conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        // For each we first send the number of dataframes in each group then send\n+        // first df, then send second df.  End of data is marked by sending 0.\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(2)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut, \"left\")\n+          writeGroup(nextRight, rightSchema, dataOut, \"right\")\n+        }\n+        dataOut.writeInt(0)\n+      }\n+\n+      def writeGroup(group: Iterator[InternalRow], schema: StructType, dataOut: DataOutputStream,\n+                    name: String) = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema, timeZoneId)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec ($name)\", 0, Long.MaxValue)\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+\n+        Utils.tryWithSafeFinally {\n+          val writer = new ArrowStreamWriter(root, null, dataOut)\n+          val arrowWriter = ArrowWriter.create(root)\n+          writer.start()\n+\n+          while (group.hasNext) {\n+            arrowWriter.write(group.next())\n+          }\n+          arrowWriter.finish()\n+          writer.writeBatch()\n+          writer.end()\n+        }{\n+          root.close()\n+          allocator.close()",
    "line": 107
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "It is a child allocator, so I'm not sure if that makes it cheap to create. The plus side of making a child allocator here is we can be sure it is cleaned up properly at the end of writing",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-08-08T21:58:36Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Python UDF Runner for cogrouped udfs.  Although the data is exchanged with the python\n+ * worker via arrow, we cannot use `ArrowPythonRunner` as we need to send more than one\n+ * dataframe.\n+ */\n+class CogroupedArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    leftSchema: StructType,\n+    rightSchema: StructType,\n+    timeZoneId: String,\n+    conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        // For each we first send the number of dataframes in each group then send\n+        // first df, then send second df.  End of data is marked by sending 0.\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(2)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut, \"left\")\n+          writeGroup(nextRight, rightSchema, dataOut, \"right\")\n+        }\n+        dataOut.writeInt(0)\n+      }\n+\n+      def writeGroup(group: Iterator[InternalRow], schema: StructType, dataOut: DataOutputStream,\n+                    name: String) = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema, timeZoneId)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec ($name)\", 0, Long.MaxValue)\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+\n+        Utils.tryWithSafeFinally {\n+          val writer = new ArrowStreamWriter(root, null, dataOut)\n+          val arrowWriter = ArrowWriter.create(root)\n+          writer.start()\n+\n+          while (group.hasNext) {\n+            arrowWriter.write(group.next())\n+          }\n+          arrowWriter.finish()\n+          writer.writeBatch()\n+          writer.end()\n+        }{\n+          root.close()\n+          allocator.close()",
    "line": 107
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Oh good point. I missed it is a child allocator already. Yeah the current approach looks good.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-08-08T22:11:23Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Python UDF Runner for cogrouped udfs.  Although the data is exchanged with the python\n+ * worker via arrow, we cannot use `ArrowPythonRunner` as we need to send more than one\n+ * dataframe.\n+ */\n+class CogroupedArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    leftSchema: StructType,\n+    rightSchema: StructType,\n+    timeZoneId: String,\n+    conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        // For each we first send the number of dataframes in each group then send\n+        // first df, then send second df.  End of data is marked by sending 0.\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(2)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut, \"left\")\n+          writeGroup(nextRight, rightSchema, dataOut, \"right\")\n+        }\n+        dataOut.writeInt(0)\n+      }\n+\n+      def writeGroup(group: Iterator[InternalRow], schema: StructType, dataOut: DataOutputStream,\n+                    name: String) = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema, timeZoneId)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec ($name)\", 0, Long.MaxValue)\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+\n+        Utils.tryWithSafeFinally {\n+          val writer = new ArrowStreamWriter(root, null, dataOut)\n+          val arrowWriter = ArrowWriter.create(root)\n+          writer.start()\n+\n+          while (group.hasNext) {\n+            arrowWriter.write(group.next())\n+          }\n+          arrowWriter.finish()\n+          writer.writeBatch()\n+          writer.end()\n+        }{\n+          root.close()\n+          allocator.close()",
    "line": 107
  }],
  "prId": 24981
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: style.",
    "commit": "1b966fda46c5334cf7963bae0bece159c9568622",
    "createdAt": "2019-08-27T20:36:20Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.ArrowWriter\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.ArrowUtils\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * Python UDF Runner for cogrouped udfs.  Although the data is exchanged with the python\n+ * worker via arrow, we cannot use `ArrowPythonRunner` as we need to send more than one\n+ * dataframe.\n+ */\n+class CogroupedArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    leftSchema: StructType,\n+    rightSchema: StructType,\n+    timeZoneId: String,\n+    conf: Map[String, String])\n+  extends BaseArrowPythonRunner[(Iterator[InternalRow], Iterator[InternalRow])](\n+    funcs, evalType, argOffsets) {\n+\n+  protected def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[(Iterator[InternalRow], Iterator[InternalRow])],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      protected override def writeCommand(dataOut: DataOutputStream): Unit = {\n+\n+        // Write config for the worker as a number of key -> value pairs of strings\n+        dataOut.writeInt(conf.size)\n+        for ((k, v) <- conf) {\n+          PythonRDD.writeUTF(k, dataOut)\n+          PythonRDD.writeUTF(v, dataOut)\n+        }\n+\n+        PythonUDFRunner.writeUDFs(dataOut, funcs, argOffsets)\n+      }\n+\n+      protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        // For each we first send the number of dataframes in each group then send\n+        // first df, then send second df.  End of data is marked by sending 0.\n+        while (inputIterator.hasNext) {\n+          dataOut.writeInt(2)\n+          val (nextLeft, nextRight) = inputIterator.next()\n+          writeGroup(nextLeft, leftSchema, dataOut, \"left\")\n+          writeGroup(nextRight, rightSchema, dataOut, \"right\")\n+        }\n+        dataOut.writeInt(0)\n+      }\n+\n+      def writeGroup(group: Iterator[InternalRow], schema: StructType, dataOut: DataOutputStream,\n+                    name: String) = {"
  }],
  "prId": 24981
}]