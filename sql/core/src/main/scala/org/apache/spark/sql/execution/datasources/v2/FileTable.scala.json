[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @cloud-fan .\r\nShould we change `FileTable` signature to accept `paths` additionally for merging `DataSourceOptions` and `CaseInsensitiveStringMap`?",
    "commit": "4599659fa944abcd51feafb284f2f623d46b5976",
    "createdAt": "2019-03-08T17:06:47Z",
    "diffHunk": "@@ -22,23 +22,27 @@ import org.apache.hadoop.fs.FileStatus\n \n import org.apache.spark.sql.{AnalysisException, SparkSession}\n import org.apache.spark.sql.execution.datasources._\n-import org.apache.spark.sql.sources.v2.{DataSourceOptions, SupportsBatchRead, SupportsBatchWrite, Table}\n+import org.apache.spark.sql.sources.v2.{SupportsBatchRead, SupportsBatchWrite, Table}\n import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n \n abstract class FileTable(\n     sparkSession: SparkSession,\n-    options: DataSourceOptions,\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],",
    "line": 13
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "it's not a big deal. I did this because we need paths in the `OrcDataSourceV2` as well, so we can calculate the paths only once in the `OrcDataSourceV2`.",
    "commit": "4599659fa944abcd51feafb284f2f623d46b5976",
    "createdAt": "2019-03-09T03:57:04Z",
    "diffHunk": "@@ -22,23 +22,27 @@ import org.apache.hadoop.fs.FileStatus\n \n import org.apache.spark.sql.{AnalysisException, SparkSession}\n import org.apache.spark.sql.execution.datasources._\n-import org.apache.spark.sql.sources.v2.{DataSourceOptions, SupportsBatchRead, SupportsBatchWrite, Table}\n+import org.apache.spark.sql.sources.v2.{SupportsBatchRead, SupportsBatchWrite, Table}\n import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n \n abstract class FileTable(\n     sparkSession: SparkSession,\n-    options: DataSourceOptions,\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],",
    "line": 13
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Got it. Thanks!",
    "commit": "4599659fa944abcd51feafb284f2f623d46b5976",
    "createdAt": "2019-03-10T00:54:25Z",
    "diffHunk": "@@ -22,23 +22,27 @@ import org.apache.hadoop.fs.FileStatus\n \n import org.apache.spark.sql.{AnalysisException, SparkSession}\n import org.apache.spark.sql.execution.datasources._\n-import org.apache.spark.sql.sources.v2.{DataSourceOptions, SupportsBatchRead, SupportsBatchWrite, Table}\n+import org.apache.spark.sql.sources.v2.{SupportsBatchRead, SupportsBatchWrite, Table}\n import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n \n abstract class FileTable(\n     sparkSession: SparkSession,\n-    options: DataSourceOptions,\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],",
    "line": 13
  }],
  "prId": 24025
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "According to https://github.com/apache/spark/pull/24025/files#diff-f70bda59304588cc3abfa3a9840653f4R214 , is this going to be removed by TODO later?",
    "commit": "4599659fa944abcd51feafb284f2f623d46b5976",
    "createdAt": "2019-03-08T17:15:34Z",
    "diffHunk": "@@ -22,23 +22,27 @@ import org.apache.hadoop.fs.FileStatus\n \n import org.apache.spark.sql.{AnalysisException, SparkSession}\n import org.apache.spark.sql.execution.datasources._\n-import org.apache.spark.sql.sources.v2.{DataSourceOptions, SupportsBatchRead, SupportsBatchWrite, Table}\n+import org.apache.spark.sql.sources.v2.{SupportsBatchRead, SupportsBatchWrite, Table}\n import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n \n abstract class FileTable(\n     sparkSession: SparkSession,\n-    options: DataSourceOptions,\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n     userSpecifiedSchema: Option[StructType])\n   extends Table with SupportsBatchRead with SupportsBatchWrite {\n+\n   lazy val fileIndex: PartitioningAwareFileIndex = {\n-    val filePaths = options.paths()\n-    val hadoopConf =\n-      sparkSession.sessionState.newHadoopConfWithOptions(options.asMap().asScala.toMap)\n-    val rootPathsSpecified = DataSource.checkAndGlobPathIfNecessary(filePaths, hadoopConf,\n-      checkEmptyGlobPath = true, checkFilesExist = options.checkFilesExist())\n+    val scalaMap = options.asScala.toMap\n+    val hadoopConf = sparkSession.sessionState.newHadoopConfWithOptions(scalaMap)\n+    // This is an internal config so must be present.\n+    val checkFilesExist = options.get(\"check_files_exist\").toBoolean",
    "line": 26
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "yea please refer to the discussion [here](https://github.com/apache/spark/pull/23383#discussion_r248717465)",
    "commit": "4599659fa944abcd51feafb284f2f623d46b5976",
    "createdAt": "2019-03-09T04:04:23Z",
    "diffHunk": "@@ -22,23 +22,27 @@ import org.apache.hadoop.fs.FileStatus\n \n import org.apache.spark.sql.{AnalysisException, SparkSession}\n import org.apache.spark.sql.execution.datasources._\n-import org.apache.spark.sql.sources.v2.{DataSourceOptions, SupportsBatchRead, SupportsBatchWrite, Table}\n+import org.apache.spark.sql.sources.v2.{SupportsBatchRead, SupportsBatchWrite, Table}\n import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n \n abstract class FileTable(\n     sparkSession: SparkSession,\n-    options: DataSourceOptions,\n+    options: CaseInsensitiveStringMap,\n+    paths: Seq[String],\n     userSpecifiedSchema: Option[StructType])\n   extends Table with SupportsBatchRead with SupportsBatchWrite {\n+\n   lazy val fileIndex: PartitioningAwareFileIndex = {\n-    val filePaths = options.paths()\n-    val hadoopConf =\n-      sparkSession.sessionState.newHadoopConfWithOptions(options.asMap().asScala.toMap)\n-    val rootPathsSpecified = DataSource.checkAndGlobPathIfNecessary(filePaths, hadoopConf,\n-      checkEmptyGlobPath = true, checkFilesExist = options.checkFilesExist())\n+    val scalaMap = options.asScala.toMap\n+    val hadoopConf = sparkSession.sessionState.newHadoopConfWithOptions(scalaMap)\n+    // This is an internal config so must be present.\n+    val checkFilesExist = options.get(\"check_files_exist\").toBoolean",
    "line": 26
  }],
  "prId": 24025
}]