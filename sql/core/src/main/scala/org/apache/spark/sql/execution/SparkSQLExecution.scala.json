[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "I think it's slightly clearer if this is called `withNewExecutionId` to be more consistent with `withExecutionId` below. At first I was a little confused what this does just from the name.\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T05:40:25Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.util.Utils\n+\n+private[sql] object SparkSQLExecution {\n+\n+  val EXECUTION_ID_KEY = \"spark.sql.execution.id\"\n+\n+  private val _nextExecutionId = new AtomicLong(0)\n+\n+  private def nextExecutionId: Long = _nextExecutionId.getAndIncrement\n+\n+  /**\n+   * Wrap a DataFrame action to track all Spark jobs in the body so that we can connect them with\n+   * an execution.\n+   */\n+  def withNewExecution[T](sqlContext: SQLContext, df: DataFrame)(body: => T): T = {"
  }],
  "prId": 7774
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "you can probably just call this `SQLExecution` to be more concise\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T05:44:53Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.util.Utils\n+\n+private[sql] object SparkSQLExecution {"
  }],
  "prId": 7774
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Not sure if I fully understand this. What if the execution runs multiple actions, like\n\n```\ndef someOperation: T = withNewExecution {\n  someDataFrame.collect() // this calls withNewExecution internally\n  sc.parallelize(1 to 10).count() // this doesn't\n}\n```\n\nIn this case the last job should be associated with `someOperation`, no?\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T05:55:22Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.util.Utils\n+\n+private[sql] object SparkSQLExecution {\n+\n+  val EXECUTION_ID_KEY = \"spark.sql.execution.id\"\n+\n+  private val _nextExecutionId = new AtomicLong(0)\n+\n+  private def nextExecutionId: Long = _nextExecutionId.getAndIncrement\n+\n+  /**\n+   * Wrap a DataFrame action to track all Spark jobs in the body so that we can connect them with\n+   * an execution.\n+   */\n+  def withNewExecution[T](sqlContext: SQLContext, df: DataFrame)(body: => T): T = {\n+    val sc = sqlContext.sparkContext\n+    val oldExecutionId = sc.getLocalProperty(EXECUTION_ID_KEY)\n+    try {\n+      if (oldExecutionId == null) {\n+        val executionId = SparkSQLExecution.nextExecutionId\n+        sc.setLocalProperty(EXECUTION_ID_KEY, executionId.toString)\n+        val callSite = Utils.getCallSite()\n+        sqlContext.listener.onExecutionStart(\n+          executionId, callSite.shortForm, callSite.longForm, df, System.currentTimeMillis())\n+        val r = body\n+        sqlContext.listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+        r\n+      } else {\n+        // Don't support nested `withNewExecution`. This is an example of the nested\n+        // `withNewExecution`:\n+        //\n+        // class DataFrame {\n+        //   def foo: T = withNewExecution { something.createNewDataFrame().collect() }\n+        // }\n+        //\n+        // Note: `collect` will call withNewExecution\n+        // In this case, only the \"executedPlan\" for \"collect\" will be executed. The \"executedPlan\"\n+        // for the outer DataFrame won't be executed. So it's meaningless to create a new Execution\n+        // for the outer DataFrame. Even if we track it, since its \"executedPlan\" doesn't run,\n+        // all accumulator metrics will be 0. It will confuse people if we show them in Web UI.\n+        //\n+        // A real case is the `DataFrame.count` method.\n+        throw new IllegalArgumentException(s\"$EXECUTION_ID_KEY is already set\")"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Ah, I get it now. It's because a DF operation might call other DF operations internally (e.g. count calls collect) but the user only cares about the top level operation (i.e. count, not collect). I think the proper semantics should be to inherit the existing execution ID if it already exists. I'm not sure if I see why we should throw an exception though.\n\nBy the way, I'm confused about one thing, since you didn't wrap `count` in `withNewExecution` how did your screenshot display `count` in the call site instead of `collect`?\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-02T18:00:09Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.util.Utils\n+\n+private[sql] object SparkSQLExecution {\n+\n+  val EXECUTION_ID_KEY = \"spark.sql.execution.id\"\n+\n+  private val _nextExecutionId = new AtomicLong(0)\n+\n+  private def nextExecutionId: Long = _nextExecutionId.getAndIncrement\n+\n+  /**\n+   * Wrap a DataFrame action to track all Spark jobs in the body so that we can connect them with\n+   * an execution.\n+   */\n+  def withNewExecution[T](sqlContext: SQLContext, df: DataFrame)(body: => T): T = {\n+    val sc = sqlContext.sparkContext\n+    val oldExecutionId = sc.getLocalProperty(EXECUTION_ID_KEY)\n+    try {\n+      if (oldExecutionId == null) {\n+        val executionId = SparkSQLExecution.nextExecutionId\n+        sc.setLocalProperty(EXECUTION_ID_KEY, executionId.toString)\n+        val callSite = Utils.getCallSite()\n+        sqlContext.listener.onExecutionStart(\n+          executionId, callSite.shortForm, callSite.longForm, df, System.currentTimeMillis())\n+        val r = body\n+        sqlContext.listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+        r\n+      } else {\n+        // Don't support nested `withNewExecution`. This is an example of the nested\n+        // `withNewExecution`:\n+        //\n+        // class DataFrame {\n+        //   def foo: T = withNewExecution { something.createNewDataFrame().collect() }\n+        // }\n+        //\n+        // Note: `collect` will call withNewExecution\n+        // In this case, only the \"executedPlan\" for \"collect\" will be executed. The \"executedPlan\"\n+        // for the outer DataFrame won't be executed. So it's meaningless to create a new Execution\n+        // for the outer DataFrame. Even if we track it, since its \"executedPlan\" doesn't run,\n+        // all accumulator metrics will be 0. It will confuse people if we show them in Web UI.\n+        //\n+        // A real case is the `DataFrame.count` method.\n+        throw new IllegalArgumentException(s\"$EXECUTION_ID_KEY is already set\")"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "inheriting the existing execution ID doesn't make sense in the following case (although we don't have such case now):\n\n```\ndef someOperation: T = withNewExecution {\n  someDataFrame.collect() // this calls withNewExecution internally\n  someDataFrame.map...filter...count() // Another query calls withNewExecution\n}\n```\n\nIn this case, I want to show there are two queries in the UI along with their explanations.\n\nGenerally, since `withNewExecutionId` is only an internal API, and all methods in DataFrame should be covered in unit tests, I think this exception should only be seen when changing DataFrame improperly.\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-03T06:15:36Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.util.Utils\n+\n+private[sql] object SparkSQLExecution {\n+\n+  val EXECUTION_ID_KEY = \"spark.sql.execution.id\"\n+\n+  private val _nextExecutionId = new AtomicLong(0)\n+\n+  private def nextExecutionId: Long = _nextExecutionId.getAndIncrement\n+\n+  /**\n+   * Wrap a DataFrame action to track all Spark jobs in the body so that we can connect them with\n+   * an execution.\n+   */\n+  def withNewExecution[T](sqlContext: SQLContext, df: DataFrame)(body: => T): T = {\n+    val sc = sqlContext.sparkContext\n+    val oldExecutionId = sc.getLocalProperty(EXECUTION_ID_KEY)\n+    try {\n+      if (oldExecutionId == null) {\n+        val executionId = SparkSQLExecution.nextExecutionId\n+        sc.setLocalProperty(EXECUTION_ID_KEY, executionId.toString)\n+        val callSite = Utils.getCallSite()\n+        sqlContext.listener.onExecutionStart(\n+          executionId, callSite.shortForm, callSite.longForm, df, System.currentTimeMillis())\n+        val r = body\n+        sqlContext.listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+        r\n+      } else {\n+        // Don't support nested `withNewExecution`. This is an example of the nested\n+        // `withNewExecution`:\n+        //\n+        // class DataFrame {\n+        //   def foo: T = withNewExecution { something.createNewDataFrame().collect() }\n+        // }\n+        //\n+        // Note: `collect` will call withNewExecution\n+        // In this case, only the \"executedPlan\" for \"collect\" will be executed. The \"executedPlan\"\n+        // for the outer DataFrame won't be executed. So it's meaningless to create a new Execution\n+        // for the outer DataFrame. Even if we track it, since its \"executedPlan\" doesn't run,\n+        // all accumulator metrics will be 0. It will confuse people if we show them in Web UI.\n+        //\n+        // A real case is the `DataFrame.count` method.\n+        throw new IllegalArgumentException(s\"$EXECUTION_ID_KEY is already set\")"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Hm, but in your example, isn't `someOperation` the user-facing operation? I would think that the user doesn't care about the internal `collect` and `count` since these are implementation details.\n\nFor instance, `df.count()` calls `collect()`, so we should display `count()` on the UI, and `collect()` should just inherit the execution ID of `count()`. Maybe I'm missing something.\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-03T20:34:39Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.util.Utils\n+\n+private[sql] object SparkSQLExecution {\n+\n+  val EXECUTION_ID_KEY = \"spark.sql.execution.id\"\n+\n+  private val _nextExecutionId = new AtomicLong(0)\n+\n+  private def nextExecutionId: Long = _nextExecutionId.getAndIncrement\n+\n+  /**\n+   * Wrap a DataFrame action to track all Spark jobs in the body so that we can connect them with\n+   * an execution.\n+   */\n+  def withNewExecution[T](sqlContext: SQLContext, df: DataFrame)(body: => T): T = {\n+    val sc = sqlContext.sparkContext\n+    val oldExecutionId = sc.getLocalProperty(EXECUTION_ID_KEY)\n+    try {\n+      if (oldExecutionId == null) {\n+        val executionId = SparkSQLExecution.nextExecutionId\n+        sc.setLocalProperty(EXECUTION_ID_KEY, executionId.toString)\n+        val callSite = Utils.getCallSite()\n+        sqlContext.listener.onExecutionStart(\n+          executionId, callSite.shortForm, callSite.longForm, df, System.currentTimeMillis())\n+        val r = body\n+        sqlContext.listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+        r\n+      } else {\n+        // Don't support nested `withNewExecution`. This is an example of the nested\n+        // `withNewExecution`:\n+        //\n+        // class DataFrame {\n+        //   def foo: T = withNewExecution { something.createNewDataFrame().collect() }\n+        // }\n+        //\n+        // Note: `collect` will call withNewExecution\n+        // In this case, only the \"executedPlan\" for \"collect\" will be executed. The \"executedPlan\"\n+        // for the outer DataFrame won't be executed. So it's meaningless to create a new Execution\n+        // for the outer DataFrame. Even if we track it, since its \"executedPlan\" doesn't run,\n+        // all accumulator metrics will be 0. It will confuse people if we show them in Web UI.\n+        //\n+        // A real case is the `DataFrame.count` method.\n+        throw new IllegalArgumentException(s\"$EXECUTION_ID_KEY is already set\")"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "By the way, I would recommend that we keep the changes here in this patch and address this in a future issue if necessary, since it's all internal anyway so we can change it any time.\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-03T20:35:10Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.util.Utils\n+\n+private[sql] object SparkSQLExecution {\n+\n+  val EXECUTION_ID_KEY = \"spark.sql.execution.id\"\n+\n+  private val _nextExecutionId = new AtomicLong(0)\n+\n+  private def nextExecutionId: Long = _nextExecutionId.getAndIncrement\n+\n+  /**\n+   * Wrap a DataFrame action to track all Spark jobs in the body so that we can connect them with\n+   * an execution.\n+   */\n+  def withNewExecution[T](sqlContext: SQLContext, df: DataFrame)(body: => T): T = {\n+    val sc = sqlContext.sparkContext\n+    val oldExecutionId = sc.getLocalProperty(EXECUTION_ID_KEY)\n+    try {\n+      if (oldExecutionId == null) {\n+        val executionId = SparkSQLExecution.nextExecutionId\n+        sc.setLocalProperty(EXECUTION_ID_KEY, executionId.toString)\n+        val callSite = Utils.getCallSite()\n+        sqlContext.listener.onExecutionStart(\n+          executionId, callSite.shortForm, callSite.longForm, df, System.currentTimeMillis())\n+        val r = body\n+        sqlContext.listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+        r\n+      } else {\n+        // Don't support nested `withNewExecution`. This is an example of the nested\n+        // `withNewExecution`:\n+        //\n+        // class DataFrame {\n+        //   def foo: T = withNewExecution { something.createNewDataFrame().collect() }\n+        // }\n+        //\n+        // Note: `collect` will call withNewExecution\n+        // In this case, only the \"executedPlan\" for \"collect\" will be executed. The \"executedPlan\"\n+        // for the outer DataFrame won't be executed. So it's meaningless to create a new Execution\n+        // for the outer DataFrame. Even if we track it, since its \"executedPlan\" doesn't run,\n+        // all accumulator metrics will be 0. It will confuse people if we show them in Web UI.\n+        //\n+        // A real case is the `DataFrame.count` method.\n+        throw new IllegalArgumentException(s\"$EXECUTION_ID_KEY is already set\")"
  }],
  "prId": 7774
}]