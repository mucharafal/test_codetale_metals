[{
  "comments": [{
    "author": {
      "login": "NathanHowell"
    },
    "body": "Should this return `None` instead of throwing an exception? The JSON parser does, though I'm not sure which approach is recommended. They should be consistent though.",
    "commit": "22eb29eddfde484fbd887a32bf8be0b681c4d714",
    "createdAt": "2017-02-23T08:43:24Z",
    "diffHunk": "@@ -43,23 +37,26 @@ class CSVFileFormat extends TextBasedFileFormat with DataSourceRegister {\n \n   override def shortName(): String = \"csv\"\n \n-  override def toString: String = \"CSV\"\n-\n-  override def hashCode(): Int = getClass.hashCode()\n-\n-  override def equals(other: Any): Boolean = other.isInstanceOf[CSVFileFormat]\n+  override def isSplitable(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      path: Path): Boolean = {\n+    val parsedOptions =\n+      new CSVOptions(options, sparkSession.sessionState.conf.sessionLocalTimeZone)\n+    val csvDataSource = CSVDataSource(parsedOptions)\n+    csvDataSource.isSplitable && super.isSplitable(sparkSession, options, path)\n+  }\n \n   override def inferSchema(\n       sparkSession: SparkSession,\n       options: Map[String, String],\n       files: Seq[FileStatus]): Option[StructType] = {\n     require(files.nonEmpty, \"Cannot infer schema from an empty set of files\")",
    "line": 46
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think returning `None` is better, the upper-level(DataSource) will handle it and throw exception",
    "commit": "22eb29eddfde484fbd887a32bf8be0b681c4d714",
    "createdAt": "2017-02-23T08:54:55Z",
    "diffHunk": "@@ -43,23 +37,26 @@ class CSVFileFormat extends TextBasedFileFormat with DataSourceRegister {\n \n   override def shortName(): String = \"csv\"\n \n-  override def toString: String = \"CSV\"\n-\n-  override def hashCode(): Int = getClass.hashCode()\n-\n-  override def equals(other: Any): Boolean = other.isInstanceOf[CSVFileFormat]\n+  override def isSplitable(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      path: Path): Boolean = {\n+    val parsedOptions =\n+      new CSVOptions(options, sparkSession.sessionState.conf.sessionLocalTimeZone)\n+    val csvDataSource = CSVDataSource(parsedOptions)\n+    csvDataSource.isSplitable && super.isSplitable(sparkSession, options, path)\n+  }\n \n   override def inferSchema(\n       sparkSession: SparkSession,\n       options: Map[String, String],\n       files: Seq[FileStatus]): Option[StructType] = {\n     require(files.nonEmpty, \"Cannot infer schema from an empty set of files\")",
    "line": 46
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "This is not related with this JIRA itself. I would like to avoid other additional changes.",
    "commit": "22eb29eddfde484fbd887a32bf8be0b681c4d714",
    "createdAt": "2017-02-23T09:02:12Z",
    "diffHunk": "@@ -43,23 +37,26 @@ class CSVFileFormat extends TextBasedFileFormat with DataSourceRegister {\n \n   override def shortName(): String = \"csv\"\n \n-  override def toString: String = \"CSV\"\n-\n-  override def hashCode(): Int = getClass.hashCode()\n-\n-  override def equals(other: Any): Boolean = other.isInstanceOf[CSVFileFormat]\n+  override def isSplitable(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      path: Path): Boolean = {\n+    val parsedOptions =\n+      new CSVOptions(options, sparkSession.sessionState.conf.sessionLocalTimeZone)\n+    val csvDataSource = CSVDataSource(parsedOptions)\n+    csvDataSource.isSplitable && super.isSplitable(sparkSession, options, path)\n+  }\n \n   override def inferSchema(\n       sparkSession: SparkSession,\n       options: Map[String, String],\n       files: Seq[FileStatus]): Option[StructType] = {\n     require(files.nonEmpty, \"Cannot infer schema from an empty set of files\")",
    "line": 46
  }],
  "prId": 16976
}]