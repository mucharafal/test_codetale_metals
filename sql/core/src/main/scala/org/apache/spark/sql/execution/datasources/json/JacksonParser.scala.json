[{
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "Should the other error cases be handled this way too? The String -> float/double conversions.\n",
    "commit": "ad714338cfbf1d55dd454a7e4ee91a70d7bd373c",
    "createdAt": "2015-12-14T23:15:38Z",
    "diffHunk": "@@ -174,7 +176,11 @@ object JacksonParser {\n         convertField(factory, parser, udt.sqlType)\n \n       case (token, dataType) =>\n-        sys.error(s\"Failed to parse a value for data type $dataType (current token: $token).\")\n+        // We cannot parse this token based on the given data type. So, we throw a",
    "line": 32
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "yeah, it is a good point. I added the change.\n",
    "commit": "ad714338cfbf1d55dd454a7e4ee91a70d7bd373c",
    "createdAt": "2015-12-16T22:59:15Z",
    "diffHunk": "@@ -174,7 +176,11 @@ object JacksonParser {\n         convertField(factory, parser, udt.sqlType)\n \n       case (token, dataType) =>\n-        sys.error(s\"Failed to parse a value for data type $dataType (current token: $token).\")\n+        // We cannot parse this token based on the given data type. So, we throw a",
    "line": 32
  }],
  "prId": 10288
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this should technically extend RuntimeException\n",
    "commit": "ad714338cfbf1d55dd454a7e4ee91a70d7bd373c",
    "createdAt": "2015-12-17T07:13:05Z",
    "diffHunk": "@@ -31,6 +31,8 @@ import org.apache.spark.sql.types._\n import org.apache.spark.unsafe.types.UTF8String\n import org.apache.spark.util.Utils\n \n+private[json] class SparkSQLJsonProcessingException(msg: String) extends Exception(msg)",
    "line": 4
  }],
  "prId": 10288
}]