[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "now each option will be redacted in https://github.com/apache/spark/pull/20647/files#diff-439c1c6b47c91729f0d2354af8fdb1bfR82",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-02-27T03:04:58Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    entries ++= options"
  }],
  "prId": 20647
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Now users can match password by `password:.+` to redact password.",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-02-28T05:01:15Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    entries ++= options\n+\n+    val outputStr = Utils.truncatedString(output, \"[\", \", \", \"]\")\n+\n+    val entriesStr = if (entries.nonEmpty) {\n+      Utils.truncatedString(entries.map {\n+        case (key, value) => StringUtils.abbreviate(redact(key + \":\" + value), 100)"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Why not redact the map? That would work with the default redaction pattern. If I understand correctly, this will require customization.",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-02-28T16:52:47Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    entries ++= options\n+\n+    val outputStr = Utils.truncatedString(output, \"[\", \", \", \"]\")\n+\n+    val entriesStr = if (entries.nonEmpty) {\n+      Utils.truncatedString(entries.map {\n+        case (key, value) => StringUtils.abbreviate(redact(key + \":\" + value), 100)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "If you look at the `SQLConf.stringRedationPattern`, the default pattern is empty. So it needs customization anyway, and this implementation is easier to customize. If we redact the entire map, I'm not sure how to write a regex to precisely match the password part.",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-02-28T18:02:04Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    entries ++= options\n+\n+    val outputStr = Utils.truncatedString(output, \"[\", \", \", \"]\")\n+\n+    val entriesStr = if (entries.nonEmpty) {\n+      Utils.truncatedString(entries.map {\n+        case (key, value) => StringUtils.abbreviate(redact(key + \":\" + value), 100)"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Yes, part of the problem is that we're using that pattern and not the secret redaction pattern, which defaults to `\"(?i)secret|password|url|user|username\".r`: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/package.scala#L328-L335\r\n\r\nIf you used the map redaction method, it would use the default pattern to match keys that should be redacted: https://github.com/apache/spark/blob/a5a4b83/core/src/main/scala/org/apache/spark/util/Utils.scala#L2723-L2729",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-02-28T18:51:34Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    entries ++= options\n+\n+    val outputStr = Utils.truncatedString(output, \"[\", \", \", \"]\")\n+\n+    val entriesStr = if (entries.nonEmpty) {\n+      Utils.truncatedString(entries.map {\n+        case (key, value) => StringUtils.abbreviate(redact(key + \":\" + value), 100)"
  }],
  "prId": 20647
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "For followup, there are 2 proposals:\r\n1. define some standard options and only display standard options, if they are specified.\r\n2. Create a new mix-in interface to allow data source implementations to decide which options they want to show during explain.",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-03-01T05:51:58Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc."
  }],
  "prId": 20647
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: Why redact options twice? Is there anything in entries but not options that needs to be redacted?",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-03-01T16:38:21Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    if (options.nonEmpty) {\n+      entries += \"Options\" -> Utils.redact(options).map {\n+        case (k, v) => s\"$k=$v\"\n+      }.mkString(\"[\", \",\", \"]\")\n+    }\n+\n+    val outputStr = Utils.truncatedString(output, \"[\", \", \", \"]\")\n+\n+    val entriesStr = if (entries.nonEmpty) {\n+      Utils.truncatedString(entries.map {\n+        case (key, value) => key + \": \" + StringUtils.abbreviate(redact(value), 100)"
  }],
  "prId": 20647
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Should there be spaces? I don't see spaces added to the components, so it looks like this might look squished together.",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-03-01T16:40:48Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    if (options.nonEmpty) {\n+      entries += \"Options\" -> Utils.redact(options).map {\n+        case (k, v) => s\"$k=$v\"\n+      }.mkString(\"[\", \",\", \"]\")\n+    }\n+\n+    val outputStr = Utils.truncatedString(output, \"[\", \", \", \"]\")\n+\n+    val entriesStr = if (entries.nonEmpty) {\n+      Utils.truncatedString(entries.map {\n+        case (key, value) => key + \": \" + StringUtils.abbreviate(redact(value), 100)\n+      }, \" (\", \", \", \")\")\n+    } else {\n+      \"\"\n+    }\n+\n+    s\"$sourceName$outputStr$entriesStr\"",
    "line": 92
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`outputStr` doesn't need space, we want `Relation[a: int]` instead of `Relation [a: int]`. This is also what data source v1 explains.\r\n\r\n`entriesStr` has space. It's added via the paramter of mkString: `\" (\", \", \", \")\"`. It's mostly to avoid the extra space if `entriesStr` is empty.",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-03-02T05:36:45Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")\n+    }\n+\n+    // TODO: we should only display some standard options like path, table, etc.\n+    if (options.nonEmpty) {\n+      entries += \"Options\" -> Utils.redact(options).map {\n+        case (k, v) => s\"$k=$v\"\n+      }.mkString(\"[\", \",\", \"]\")\n+    }\n+\n+    val outputStr = Utils.truncatedString(output, \"[\", \", \", \"]\")\n+\n+    val entriesStr = if (entries.nonEmpty) {\n+      Utils.truncatedString(entries.map {\n+        case (key, value) => key + \": \" + StringUtils.abbreviate(redact(value), 100)\n+      }, \" (\", \", \", \")\")\n+    } else {\n+      \"\"\n+    }\n+\n+    s\"$sourceName$outputStr$entriesStr\"",
    "line": 92
  }],
  "prId": 20647
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Nit: Does this need \"Pushed\"?",
    "commit": "1a96d143afe02bdc3c3da3563ca1e3aa96a9145e",
    "createdAt": "2018-03-01T16:41:08Z",
    "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.commons.lang3.StringUtils\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A trait that can be used by data source v2 related query plans(both logical and physical), to\n+ * provide a string format of the data source information for explain.\n+ */\n+trait DataSourceV2StringFormat {\n+\n+  /**\n+   * The instance of this data source implementation. Note that we only consider its class in\n+   * equals/hashCode, not the instance itself.\n+   */\n+  def source: DataSourceV2\n+\n+  /**\n+   * The output of the data source reader, w.r.t. column pruning.\n+   */\n+  def output: Seq[Attribute]\n+\n+  /**\n+   * The options for this data source reader.\n+   */\n+  def options: Map[String, String]\n+\n+  /**\n+   * The created data source reader. Here we use it to get the filters that has been pushed down\n+   * so far, itself doesn't take part in the equals/hashCode.\n+   */\n+  def reader: DataSourceReader\n+\n+  private lazy val filters = reader match {\n+    case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n+    case s: SupportsPushDownFilters => s.pushedFilters().toSet\n+    case _ => Set.empty\n+  }\n+\n+  private def sourceName: String = source match {\n+    case registered: DataSourceRegister => registered.shortName()\n+    case _ => source.getClass.getSimpleName.stripSuffix(\"$\")\n+  }\n+\n+  def metadataString: String = {\n+    val entries = scala.collection.mutable.ArrayBuffer.empty[(String, String)]\n+\n+    if (filters.nonEmpty) {\n+      entries += \"Pushed Filters\" -> filters.mkString(\"[\", \", \", \"]\")"
  }],
  "prId": 20647
}]