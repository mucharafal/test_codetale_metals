[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This method and below methods are moved from the old `DataSourceResolution`.",
    "commit": "9ad516ea9a0ec7525308c9ca64d36d4069531ae0",
    "createdAt": "2019-09-17T18:39:06Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.{AnalysisException, SaveMode}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{BucketSpec, CatalogTable, CatalogTableType, CatalogUtils}\n+import org.apache.spark.sql.catalyst.plans.logical.{CreateTableAsSelect, CreateV2Table, DropTable, LogicalPlan, ReplaceTable, ReplaceTableAsSelect, ShowNamespaces, ShowTables}\n+import org.apache.spark.sql.catalyst.plans.logical.sql.{CreateTableAsSelectStatement, CreateTableStatement, DropTableStatement, DropViewStatement, ReplaceTableAsSelectStatement, ReplaceTableStatement, ShowNamespacesStatement, ShowTablesStatement}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, Identifier, LookupCatalog}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.execution.command.{DropTableCommand, ShowTablesCommand}\n+import org.apache.spark.sql.execution.datasources.{CreateTable, DataSource}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * Resolves catalogs from the multi-part identifiers in DDL/DML commands.\n+ *\n+ * For each SQL statement, this rule has 2 different code paths for the session catalog and other\n+ * catalogs.\n+ */\n+class ResolveCatalogs(val catalogManager: CatalogManager, conf: SQLConf)\n+  extends Rule[LogicalPlan] with LookupCatalog {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp {\n+    // For CREATE TABLE [AS SELECT], we should use the v1 command if the catalog is resolved to the\n+    // session catalog and the table provider is not v2.\n+    case c: CreateTableStatement =>\n+      c.tableName match {\n+        case CatalogAndRestNameParts(Right(sessionCatalog), _) if !isV2Provider(c.provider) =>\n+          val tableDesc = buildCatalogTable(c.tableName.toV1Identifier, c.tableSchema,\n+            c.partitioning, c.bucketSpec, c.properties, c.provider, c.options, c.location,\n+            c.comment, c.ifNotExists)\n+          val mode = if (c.ifNotExists) SaveMode.Ignore else SaveMode.ErrorIfExists\n+          CreateTable(tableDesc, mode, None)\n+\n+        case _ =>\n+          val (catalog, ident) = getCatalogAndIdent(c.tableName)\n+          CreateV2Table(\n+            catalog.asTableCatalog,\n+            ident,\n+            c.tableSchema,\n+            // convert the bucket spec and add it as a transform\n+            c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+            convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+            ignoreIfExists = c.ifNotExists)\n+      }\n+\n+    case c: CreateTableAsSelectStatement =>\n+      c.tableName match {\n+        case CatalogAndRestNameParts(Right(sessionCatalog), _) if !isV2Provider(c.provider) =>\n+          val tableDesc = buildCatalogTable(c.tableName.toV1Identifier, new StructType,\n+            c.partitioning, c.bucketSpec, c.properties, c.provider, c.options, c.location,\n+            c.comment, c.ifNotExists)\n+          val mode = if (c.ifNotExists) SaveMode.Ignore else SaveMode.ErrorIfExists\n+          CreateTable(tableDesc, mode, Some(c.asSelect))\n+\n+        case _ =>\n+          val (catalog, ident) = getCatalogAndIdent(c.tableName)\n+          CreateTableAsSelect(\n+            catalog.asTableCatalog,\n+            ident,\n+            // convert the bucket spec and add it as a transform\n+            c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+            c.asSelect,\n+            convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+            writeOptions = c.options.filterKeys(_ != \"path\"),\n+            ignoreIfExists = c.ifNotExists)\n+      }\n+\n+    // For REPLACE TABLE [AS SELECT], we should fail if the catalog is resolved to the\n+    // session catalog and the table provider is not v2.\n+    case c: ReplaceTableStatement =>\n+      c.tableName match {\n+        case CatalogAndRestNameParts(Right(sessionCatalog), _) if !isV2Provider(c.provider) =>\n+          throw new AnalysisException(\"REPLACE TABLE is only supported with v2 tables.\")\n+\n+        case _ =>\n+          val (catalog, ident) = getCatalogAndIdent(c.tableName)\n+          ReplaceTable(\n+            catalog.asTableCatalog,\n+            ident,\n+            c.tableSchema,\n+            // convert the bucket spec and add it as a transform\n+            c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+            convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+            orCreate = c.orCreate)\n+      }\n+\n+    case c: ReplaceTableAsSelectStatement =>\n+      c.tableName match {\n+        case CatalogAndRestNameParts(Right(sessionCatalog), _) if !isV2Provider(c.provider) =>\n+          throw new AnalysisException(\"REPLACE TABLE AS SELECT is only supported with v2 tables.\")\n+\n+        case _ =>\n+          val (catalog, ident) = getCatalogAndIdent(c.tableName)\n+          ReplaceTableAsSelect(\n+            catalog.asTableCatalog,\n+            ident,\n+            // convert the bucket spec and add it as a transform\n+            c.partitioning ++ c.bucketSpec.map(_.asTransform),\n+            c.asSelect,\n+            convertTableProperties(c.properties, c.options, c.location, c.comment, c.provider),\n+            writeOptions = c.options.filterKeys(_ != \"path\"),\n+            orCreate = c.orCreate)\n+      }\n+\n+    case DropTableStatement(\n+         CatalogAndRestNameParts(Left(catalog), restNameParts), ifExists, purge) =>\n+      DropTable(catalog.asTableCatalog, restNameParts.toIdentifier, ifExists)\n+\n+    case d @ DropTableStatement(\n+         CatalogAndRestNameParts(Right(sessionCatalog), restNameParts), ifExists, purge) =>\n+      DropTableCommand(d.tableName.toV1Identifier, ifExists, isView = false, purge = purge)\n+\n+    case DropViewStatement(\n+         CatalogAndRestNameParts(Left(catalog), restNameParts), _) =>\n+      throw new AnalysisException(\n+        s\"Can not specify catalog `${catalog.name}` for view ${restNameParts.quoted} \" +\n+          s\"because view support in catalog has not been implemented yet\")\n+\n+    case DropViewStatement(\n+         CatalogAndRestNameParts(Right(sessionCatalog), restNameParts), ifExists) =>\n+      DropTableCommand(restNameParts.toV1Identifier, ifExists, isView = true, purge = false)\n+\n+    case ShowNamespacesStatement(\n+         Some(CatalogAndRestNameParts(Left(catalog), restNameParts)), pattern) =>\n+      val namespace = if (restNameParts.isEmpty) None else Some(restNameParts)\n+      ShowNamespaces(catalog.asNamespaceCatalog, namespace, pattern)\n+\n+    case ShowNamespacesStatement(\n+         Some(CatalogAndRestNameParts(Right(sessionCatalog), restNameParts)), pattern) =>\n+      throw new AnalysisException(\n+        \"SHOW NAMESPACES is not supported with the session catalog.\")\n+\n+    case ShowNamespacesStatement(None, pattern) =>\n+      if (defaultCatalog.isDefined) {\n+        ShowNamespaces(defaultCatalog.get.asNamespaceCatalog, None, pattern)\n+      } else {\n+        throw new AnalysisException(\n+          \"SHOW NAMESPACES is not supported with the session catalog.\")\n+      }\n+\n+    case ShowTablesStatement(\n+         Some(CatalogAndRestNameParts(Left(catalog), restNameParts)), pattern) =>\n+      ShowTables(catalog.asTableCatalog, restNameParts, pattern)\n+\n+    case ShowTablesStatement(\n+         Some(CatalogAndRestNameParts(Right(sessionCatalog), restNameParts)), pattern) =>\n+      if (restNameParts.length != 1) {\n+        throw new AnalysisException(\n+          s\"The database name is not valid: ${restNameParts.quoted}\")\n+      }\n+      ShowTablesCommand(Some(restNameParts.head), pattern)\n+\n+    case ShowTablesStatement(None, pattern) =>\n+      if (defaultCatalog.isDefined) {\n+        ShowTables(defaultCatalog.get.asTableCatalog, catalogManager.currentNamespace, pattern)\n+      } else {\n+        ShowTablesCommand(None, pattern)\n+      }\n+  }\n+\n+  private def convertTableProperties("
  }],
  "prId": 25747
}]