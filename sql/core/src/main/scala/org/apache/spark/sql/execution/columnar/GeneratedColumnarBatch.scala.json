[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @kiszk .\r\nIs there any reason having only two types, `int` and `double`?\r\nThe PR looks more general to me.",
    "commit": "c183032a0edc3837bd0e697a15b298b47a2ab5ab",
    "createdAt": "2017-06-05T04:09:28Z",
    "diffHunk": "@@ -0,0 +1,220 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution.columnar\r\n+\r\n+import org.apache.spark.sql.catalyst.InternalRow\r\n+import org.apache.spark.sql.catalyst.expressions.Attribute\r\n+import org.apache.spark.sql.catalyst.expressions.codegen._\r\n+import org.apache.spark.sql.execution.vectorized.ColumnarBatch\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.storage.StorageLevel\r\n+import org.apache.spark.storage.StorageLevel._\r\n+\r\n+\r\n+/**\r\n+ * A helper class to expose the scala iterator to Java.\r\n+ */\r\n+abstract class ColumnarBatchIterator extends Iterator[ColumnarBatch]\r\n+\r\n+\r\n+/**\r\n+ * Generate code to batch [[InternalRow]]s into [[ColumnarBatch]]es.\r\n+ */\r\n+class GenerateColumnarBatch(\r\n+    schema: StructType,\r\n+    batchSize: Int,\r\n+    storageLevel: StorageLevel)\r\n+  extends CodeGenerator[Iterator[InternalRow], Iterator[CachedColumnarBatch]] {\r\n+\r\n+  protected def canonicalize(in: Iterator[InternalRow]): Iterator[InternalRow] = in\r\n+\r\n+  protected def bind(\r\n+    in: Iterator[InternalRow], inputSchema: Seq[Attribute]): Iterator[InternalRow] = {\r\n+    in\r\n+  }\r\n+\r\n+  protected def create(rowIterator: Iterator[InternalRow]): Iterator[CachedColumnarBatch] = {\r\n+    import scala.collection.JavaConverters._\r\n+    val ctx = newCodeGenContext()\r\n+    val columnStatsCls = classOf[ColumnStats].getName\r\n+    val rowVar = ctx.freshName(\"row\")\r\n+    val batchVar = ctx.freshName(\"columnarBatch\")\r\n+    val rowNumVar = ctx.freshName(\"rowNum\")\r\n+    val numBytesVar = ctx.freshName(\"bytesInBatch\")\r\n+    ctx.addMutableState(\"long\", numBytesVar, s\"$numBytesVar = 0;\")\r\n+    val rowIterVar = ctx.addReferenceObj(\r\n+      \"rowIterator\", rowIterator.asJava, classOf[java.util.Iterator[_]].getName)\r\n+    val schemas = StructType(\r\n+      schema.fields.map(s => StructField(s.name,\r\n+        s.dataType match {\r\n+          case udt: UserDefinedType[_] => udt.sqlType\r\n+          case other => other\r\n+        }, s.nullable))\r\n+    )\r\n+    val schemaVar = ctx.addReferenceObj(\"schema\", schemas, classOf[StructType].getName)\r\n+    val maxNumBytes = ColumnBuilder.MAX_BATCH_SIZE_IN_BYTE\r\n+    val numColumns = schema.fields.length\r\n+\r\n+    val colStatVars = (0 to numColumns - 1).map(i => ctx.freshName(\"colStat\" + i))\r\n+    val colStatCode = ctx.splitExpressions(\r\n+      (schemas.fields zip colStatVars).zipWithIndex.map {\r\n+        case ((field, varName), i) =>\r\n+          val columnStatsCls = field.dataType match {\r\n+            case IntegerType => classOf[IntColumnStats].getName\r\n+            case DoubleType => classOf[DoubleColumnStats].getName\r\n+            case others => throw new UnsupportedOperationException(s\"$others is not supported yet\")\r\n+          }\r\n+          ctx.addMutableState(columnStatsCls, varName, \"\")\r\n+          s\"$varName = new $columnStatsCls(); statsArray[$i] = $varName;\\n\"\r\n+      },\r\n+      \"apply\",\r\n+      Seq.empty\r\n+    )\r\n+\r\n+    val populateColumnVectorsCode = ctx.splitExpressions(\r\n+      (schemas.fields zip colStatVars).zipWithIndex.map {\r\n+        case ((field, colStatVar), i) =>\r\n+          GenerateColumnarBatch.putColumnCode(ctx, field.dataType, field.nullable,\r\n+            batchVar, rowVar, rowNumVar, colStatVar, i, numBytesVar).trim + \"\\n\"\r\n+      },\r\n+      \"apply\",\r\n+      Seq((\"InternalRow\", rowVar), (\"ColumnarBatch\", batchVar), (\"int\", rowNumVar))\r\n+    )\r\n+\r\n+    val code = s\"\"\"\r\n+      import org.apache.spark.memory.MemoryMode;\r\n+      import org.apache.spark.sql.catalyst.InternalRow;\r\n+      import org.apache.spark.sql.execution.columnar.CachedColumnarBatch;\r\n+      import org.apache.spark.sql.execution.columnar.GenerateColumnarBatch;\r\n+      import org.apache.spark.sql.execution.vectorized.ColumnarBatch;\r\n+      import org.apache.spark.sql.execution.vectorized.ColumnVector;\r\n+\r\n+      public GeneratedColumnarBatchIterator generate(Object[] references) {\r\n+        return new GeneratedColumnarBatchIterator(references);\r\n+      }\r\n+\r\n+      class GeneratedColumnarBatchIterator extends ${classOf[ColumnarBatchIterator].getName} {\r\n+        private Object[] references;\r\n+        ${ctx.declareMutableStates()}\r\n+\r\n+        public GeneratedColumnarBatchIterator(Object[] references) {\r\n+          this.references = references;\r\n+          ${ctx.initMutableStates()}\r\n+        }\r\n+\r\n+        ${ctx.declareAddedFunctions()}\r\n+\r\n+        $columnStatsCls[] statsArray = new $columnStatsCls[$numColumns];\r\n+        private void allocateColumnStats() {\r\n+          ${colStatCode.trim}\r\n+        }\r\n+\r\n+        @Override\r\n+        public boolean hasNext() {\r\n+          return $rowIterVar.hasNext();\r\n+        }\r\n+\r\n+        @Override\r\n+        public CachedColumnarBatch next() {\r\n+          ColumnarBatch $batchVar =\r\n+          ColumnarBatch.allocate($schemaVar, MemoryMode.ON_HEAP, $batchSize);\r\n+          allocateColumnStats();\r\n+          int $rowNumVar = 0;\r\n+          $numBytesVar = 0;\r\n+          while ($rowIterVar.hasNext() && $rowNumVar < $batchSize && $numBytesVar < $maxNumBytes) {\r\n+            InternalRow $rowVar = (InternalRow) $rowIterVar.next();\r\n+            $populateColumnVectorsCode\r\n+            $rowNumVar += 1;\r\n+          }\r\n+          $batchVar.setNumRows($rowNumVar);\r\n+          return CachedColumnarBatch.apply(\r\n+            $batchVar, GenerateColumnarBatch.generateStats(statsArray));\r\n+        }\r\n+      }\r\n+      \"\"\"\r\n+    val formattedCode = CodeFormatter.stripOverlappingComments(\r\n+      new CodeAndComment(code, ctx.getPlaceHolderToComments()))\r\n+    CodeGenerator.compile(formattedCode).generate(ctx.references.toArray)\r\n+      .asInstanceOf[Iterator[CachedColumnarBatch]]\r\n+  }\r\n+}\r\n+\r\n+\r\n+private[sql] object GenerateColumnarBatch {\r\n+  def compressStorageLevel(storageLevel: StorageLevel, useCompression: Boolean): StorageLevel = {\r\n+    if (!useCompression) return storageLevel\r\n+    storageLevel match {\r\n+      case MEMORY_ONLY => MEMORY_ONLY_SER\r\n+      case MEMORY_ONLY_2 => MEMORY_ONLY_SER_2\r\n+      case MEMORY_AND_DISK => MEMORY_AND_DISK_SER\r\n+      case MEMORY_AND_DISK_2 => MEMORY_AND_DISK_SER_2\r\n+      case sl => sl\r\n+    }\r\n+  }\r\n+\r\n+  def isCompress(storageLevel: StorageLevel) : Boolean = {\r\n+    (storageLevel == MEMORY_ONLY_SER || storageLevel == MEMORY_ONLY_SER_2 ||\r\n+      storageLevel == MEMORY_AND_DISK_SER || storageLevel == MEMORY_AND_DISK_SER_2)\r\n+  }\r\n+\r\n+  private val typeToName = Map[AbstractDataType, String](\r",
    "line": 175
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "As I described in the description, this is for ease of review.\r\n\r\n>As the first step, for ease of review, I supported only integer and double data types with whole-stage codegen. Another PR will address an execution path without whole-stage codegen\r\n",
    "commit": "c183032a0edc3837bd0e697a15b298b47a2ab5ab",
    "createdAt": "2017-06-05T05:17:29Z",
    "diffHunk": "@@ -0,0 +1,220 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution.columnar\r\n+\r\n+import org.apache.spark.sql.catalyst.InternalRow\r\n+import org.apache.spark.sql.catalyst.expressions.Attribute\r\n+import org.apache.spark.sql.catalyst.expressions.codegen._\r\n+import org.apache.spark.sql.execution.vectorized.ColumnarBatch\r\n+import org.apache.spark.sql.types._\r\n+import org.apache.spark.storage.StorageLevel\r\n+import org.apache.spark.storage.StorageLevel._\r\n+\r\n+\r\n+/**\r\n+ * A helper class to expose the scala iterator to Java.\r\n+ */\r\n+abstract class ColumnarBatchIterator extends Iterator[ColumnarBatch]\r\n+\r\n+\r\n+/**\r\n+ * Generate code to batch [[InternalRow]]s into [[ColumnarBatch]]es.\r\n+ */\r\n+class GenerateColumnarBatch(\r\n+    schema: StructType,\r\n+    batchSize: Int,\r\n+    storageLevel: StorageLevel)\r\n+  extends CodeGenerator[Iterator[InternalRow], Iterator[CachedColumnarBatch]] {\r\n+\r\n+  protected def canonicalize(in: Iterator[InternalRow]): Iterator[InternalRow] = in\r\n+\r\n+  protected def bind(\r\n+    in: Iterator[InternalRow], inputSchema: Seq[Attribute]): Iterator[InternalRow] = {\r\n+    in\r\n+  }\r\n+\r\n+  protected def create(rowIterator: Iterator[InternalRow]): Iterator[CachedColumnarBatch] = {\r\n+    import scala.collection.JavaConverters._\r\n+    val ctx = newCodeGenContext()\r\n+    val columnStatsCls = classOf[ColumnStats].getName\r\n+    val rowVar = ctx.freshName(\"row\")\r\n+    val batchVar = ctx.freshName(\"columnarBatch\")\r\n+    val rowNumVar = ctx.freshName(\"rowNum\")\r\n+    val numBytesVar = ctx.freshName(\"bytesInBatch\")\r\n+    ctx.addMutableState(\"long\", numBytesVar, s\"$numBytesVar = 0;\")\r\n+    val rowIterVar = ctx.addReferenceObj(\r\n+      \"rowIterator\", rowIterator.asJava, classOf[java.util.Iterator[_]].getName)\r\n+    val schemas = StructType(\r\n+      schema.fields.map(s => StructField(s.name,\r\n+        s.dataType match {\r\n+          case udt: UserDefinedType[_] => udt.sqlType\r\n+          case other => other\r\n+        }, s.nullable))\r\n+    )\r\n+    val schemaVar = ctx.addReferenceObj(\"schema\", schemas, classOf[StructType].getName)\r\n+    val maxNumBytes = ColumnBuilder.MAX_BATCH_SIZE_IN_BYTE\r\n+    val numColumns = schema.fields.length\r\n+\r\n+    val colStatVars = (0 to numColumns - 1).map(i => ctx.freshName(\"colStat\" + i))\r\n+    val colStatCode = ctx.splitExpressions(\r\n+      (schemas.fields zip colStatVars).zipWithIndex.map {\r\n+        case ((field, varName), i) =>\r\n+          val columnStatsCls = field.dataType match {\r\n+            case IntegerType => classOf[IntColumnStats].getName\r\n+            case DoubleType => classOf[DoubleColumnStats].getName\r\n+            case others => throw new UnsupportedOperationException(s\"$others is not supported yet\")\r\n+          }\r\n+          ctx.addMutableState(columnStatsCls, varName, \"\")\r\n+          s\"$varName = new $columnStatsCls(); statsArray[$i] = $varName;\\n\"\r\n+      },\r\n+      \"apply\",\r\n+      Seq.empty\r\n+    )\r\n+\r\n+    val populateColumnVectorsCode = ctx.splitExpressions(\r\n+      (schemas.fields zip colStatVars).zipWithIndex.map {\r\n+        case ((field, colStatVar), i) =>\r\n+          GenerateColumnarBatch.putColumnCode(ctx, field.dataType, field.nullable,\r\n+            batchVar, rowVar, rowNumVar, colStatVar, i, numBytesVar).trim + \"\\n\"\r\n+      },\r\n+      \"apply\",\r\n+      Seq((\"InternalRow\", rowVar), (\"ColumnarBatch\", batchVar), (\"int\", rowNumVar))\r\n+    )\r\n+\r\n+    val code = s\"\"\"\r\n+      import org.apache.spark.memory.MemoryMode;\r\n+      import org.apache.spark.sql.catalyst.InternalRow;\r\n+      import org.apache.spark.sql.execution.columnar.CachedColumnarBatch;\r\n+      import org.apache.spark.sql.execution.columnar.GenerateColumnarBatch;\r\n+      import org.apache.spark.sql.execution.vectorized.ColumnarBatch;\r\n+      import org.apache.spark.sql.execution.vectorized.ColumnVector;\r\n+\r\n+      public GeneratedColumnarBatchIterator generate(Object[] references) {\r\n+        return new GeneratedColumnarBatchIterator(references);\r\n+      }\r\n+\r\n+      class GeneratedColumnarBatchIterator extends ${classOf[ColumnarBatchIterator].getName} {\r\n+        private Object[] references;\r\n+        ${ctx.declareMutableStates()}\r\n+\r\n+        public GeneratedColumnarBatchIterator(Object[] references) {\r\n+          this.references = references;\r\n+          ${ctx.initMutableStates()}\r\n+        }\r\n+\r\n+        ${ctx.declareAddedFunctions()}\r\n+\r\n+        $columnStatsCls[] statsArray = new $columnStatsCls[$numColumns];\r\n+        private void allocateColumnStats() {\r\n+          ${colStatCode.trim}\r\n+        }\r\n+\r\n+        @Override\r\n+        public boolean hasNext() {\r\n+          return $rowIterVar.hasNext();\r\n+        }\r\n+\r\n+        @Override\r\n+        public CachedColumnarBatch next() {\r\n+          ColumnarBatch $batchVar =\r\n+          ColumnarBatch.allocate($schemaVar, MemoryMode.ON_HEAP, $batchSize);\r\n+          allocateColumnStats();\r\n+          int $rowNumVar = 0;\r\n+          $numBytesVar = 0;\r\n+          while ($rowIterVar.hasNext() && $rowNumVar < $batchSize && $numBytesVar < $maxNumBytes) {\r\n+            InternalRow $rowVar = (InternalRow) $rowIterVar.next();\r\n+            $populateColumnVectorsCode\r\n+            $rowNumVar += 1;\r\n+          }\r\n+          $batchVar.setNumRows($rowNumVar);\r\n+          return CachedColumnarBatch.apply(\r\n+            $batchVar, GenerateColumnarBatch.generateStats(statsArray));\r\n+        }\r\n+      }\r\n+      \"\"\"\r\n+    val formattedCode = CodeFormatter.stripOverlappingComments(\r\n+      new CodeAndComment(code, ctx.getPlaceHolderToComments()))\r\n+    CodeGenerator.compile(formattedCode).generate(ctx.references.toArray)\r\n+      .asInstanceOf[Iterator[CachedColumnarBatch]]\r\n+  }\r\n+}\r\n+\r\n+\r\n+private[sql] object GenerateColumnarBatch {\r\n+  def compressStorageLevel(storageLevel: StorageLevel, useCompression: Boolean): StorageLevel = {\r\n+    if (!useCompression) return storageLevel\r\n+    storageLevel match {\r\n+      case MEMORY_ONLY => MEMORY_ONLY_SER\r\n+      case MEMORY_ONLY_2 => MEMORY_ONLY_SER_2\r\n+      case MEMORY_AND_DISK => MEMORY_AND_DISK_SER\r\n+      case MEMORY_AND_DISK_2 => MEMORY_AND_DISK_SER_2\r\n+      case sl => sl\r\n+    }\r\n+  }\r\n+\r\n+  def isCompress(storageLevel: StorageLevel) : Boolean = {\r\n+    (storageLevel == MEMORY_ONLY_SER || storageLevel == MEMORY_ONLY_SER_2 ||\r\n+      storageLevel == MEMORY_AND_DISK_SER || storageLevel == MEMORY_AND_DISK_SER_2)\r\n+  }\r\n+\r\n+  private val typeToName = Map[AbstractDataType, String](\r",
    "line": 175
  }],
  "prId": 18066
}]