[{
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "Any reason not do implmeent the h = h \\* 37 + v hash function?\n",
    "commit": "9b5ee1d9d648ae454cc0749047528509a032c7ab",
    "createdAt": "2016-04-13T07:48:05Z",
    "diffHunk": "@@ -103,7 +119,7 @@ class ColumnarAggMapCodeGenerator(\n     s\"\"\"\n        |// TODO: Improve this hash function\n        |private long hash($groupingKeySignature) {\n-       |  return ${groupingKeys.map(_._2).mkString(\" ^ \")};\n+       |  return ${groupingKeys.map(_._2).mkString(\" | \")};",
    "line": 104
  }, {
    "author": {
      "login": "sameeragarwal"
    },
    "body": "No particular reason, was planning to do this as part of a separate small PR (along with the benchmarks). Please let me know if you'd prefer it here instead\n",
    "commit": "9b5ee1d9d648ae454cc0749047528509a032c7ab",
    "createdAt": "2016-04-13T22:33:04Z",
    "diffHunk": "@@ -103,7 +119,7 @@ class ColumnarAggMapCodeGenerator(\n     s\"\"\"\n        |// TODO: Improve this hash function\n        |private long hash($groupingKeySignature) {\n-       |  return ${groupingKeys.map(_._2).mkString(\" ^ \")};\n+       |  return ${groupingKeys.map(_._2).mkString(\" | \")};",
    "line": 104
  }],
  "prId": 12345
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "Can we not make either of these public. Also, have a comment that explains why you have two batches their relation.\n",
    "commit": "9b5ee1d9d648ae454cc0749047528509a032c7ab",
    "createdAt": "2016-04-13T07:50:18Z",
    "diffHunk": "@@ -65,27 +65,43 @@ class ColumnarAggMapCodeGenerator(\n           .mkString(\"\\n\")};\n       \"\"\".stripMargin\n \n+    val generatedAggBufferSchema: String =\n+      s\"\"\"\n+         |new org.apache.spark.sql.types.StructType()\n+         |${bufferSchema.map(key =>\n+        s\"\"\".add(\"${key.name}\", org.apache.spark.sql.types.DataTypes.${key.dataType})\"\"\")\n+        .mkString(\"\\n\")};\n+      \"\"\".stripMargin\n+\n     s\"\"\"\n-       |  private org.apache.spark.sql.execution.vectorized.ColumnarBatch batch;\n+       |  public org.apache.spark.sql.execution.vectorized.ColumnarBatch batch;"
  }],
  "prId": 12345
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "Let's leave a TODO to fix this. There should be a nicer way to get a projection of a batch instead of this.\n",
    "commit": "9b5ee1d9d648ae454cc0749047528509a032c7ab",
    "createdAt": "2016-04-14T00:36:03Z",
    "diffHunk": "@@ -65,27 +69,43 @@ class ColumnarAggMapCodeGenerator(\n           .mkString(\"\\n\")};\n       \"\"\".stripMargin\n \n+    val generatedAggBufferSchema: String =\n+      s\"\"\"\n+         |new org.apache.spark.sql.types.StructType()\n+         |${bufferSchema.map(key =>\n+        s\"\"\".add(\"${key.name}\", org.apache.spark.sql.types.DataTypes.${key.dataType})\"\"\")\n+        .mkString(\"\\n\")};\n+      \"\"\".stripMargin\n+\n     s\"\"\"\n        |  private org.apache.spark.sql.execution.vectorized.ColumnarBatch batch;\n+       |  private org.apache.spark.sql.execution.vectorized.ColumnarBatch aggregateBufferBatch;\n        |  private int[] buckets;\n        |  private int numBuckets;\n        |  private int maxSteps;\n        |  private int numRows = 0;\n        |  private org.apache.spark.sql.types.StructType schema = $generatedSchema\n+       |  private org.apache.spark.sql.types.StructType aggregateBufferSchema =\n+       |    $generatedAggBufferSchema\n        |\n-       |  public $generatedClassName(int capacity, double loadFactor, int maxSteps) {\n-       |    assert (capacity > 0 && ((capacity & (capacity - 1)) == 0));\n-       |    this.maxSteps = maxSteps;\n-       |    numBuckets = (int) (capacity / loadFactor);\n+       |  public $generatedClassName() {\n+       |    // TODO: These should be generated based on the schema\n+       |    int DEFAULT_CAPACITY = 1 << 16;\n+       |    double DEFAULT_LOAD_FACTOR = 0.25;\n+       |    int DEFAULT_MAX_STEPS = 2;\n+       |    assert (DEFAULT_CAPACITY > 0 && ((DEFAULT_CAPACITY & (DEFAULT_CAPACITY - 1)) == 0));\n+       |    this.maxSteps = DEFAULT_MAX_STEPS;\n+       |    numBuckets = (int) (DEFAULT_CAPACITY / DEFAULT_LOAD_FACTOR);\n        |    batch = org.apache.spark.sql.execution.vectorized.ColumnarBatch.allocate(schema,\n-       |      org.apache.spark.memory.MemoryMode.ON_HEAP, capacity);\n+       |      org.apache.spark.memory.MemoryMode.ON_HEAP, DEFAULT_CAPACITY);\n+       |    aggregateBufferBatch = org.apache.spark.sql.execution.vectorized.ColumnarBatch.allocate(\n+       |      aggregateBufferSchema, org.apache.spark.memory.MemoryMode.ON_HEAP, DEFAULT_CAPACITY);"
  }],
  "prId": 12345
}, {
  "comments": [{
    "author": {
      "login": "nongli"
    },
    "body": "You chose to have this not handle null keys right? Comment that.\n",
    "commit": "9b5ee1d9d648ae454cc0749047528509a032c7ab",
    "createdAt": "2016-04-14T00:43:16Z",
    "diffHunk": "@@ -21,10 +21,10 @@ import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\n import org.apache.spark.sql.types.StructType\n \n /**\n- * This is a helper object to generate an append-only single-key/single value aggregate hash\n- * map that can act as a 'cache' for extremely fast key-value lookups while evaluating aggregates\n- * (and fall back to the `BytesToBytesMap` if a given key isn't found). This is 'codegened' in\n- * TungstenAggregate to speed up aggregates w/ key.\n+ * This is a helper class to generate an append-only aggregate hash map that can act as a 'cache'\n+ * for extremely fast key-value lookups while evaluating aggregates (and fall back to the\n+ * `BytesToBytesMap` if a given key isn't found). This is 'codegened' in TungstenAggregate to speed\n+ * up aggregates w/ key."
  }],
  "prId": 12345
}]