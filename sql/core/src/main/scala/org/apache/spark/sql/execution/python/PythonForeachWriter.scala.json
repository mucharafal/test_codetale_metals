[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`s` seems not needed",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-05T01:06:23Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+import java.util.concurrent.TimeUnit\n+import java.util.concurrent.locks.ReentrantLock\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.TaskMemoryManager\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{NextIterator, Utils}\n+\n+class PythonForeachWriter(func: PythonFunction, schema: StructType)\n+  extends ForeachWriter[UnsafeRow] {\n+\n+  private lazy val context = TaskContext.get()\n+  private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer(\n+    context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length)\n+  private lazy val inputRowIterator = buffer.iterator\n+\n+  private lazy val inputByteIterator = {\n+    EvaluatePython.registerPicklers()\n+    val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) }\n+    new SerDeUtil.AutoBatchedPickler(objIterator)\n+  }\n+\n+  private lazy val pythonRunner = {\n+    val conf = SparkEnv.get.conf\n+    val bufferSize = conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = conf.getBoolean(\"spark.python.worker.reuse\", true)\n+    PythonRunner(func, bufferSize, reuseWorker)\n+  }\n+\n+  private lazy val outputIterator =\n+    pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n+\n+  override def open(partitionId: Long, version: Long): Boolean = {\n+    outputIterator  // initialize everything\n+    TaskContext.get.addTaskCompletionListener { _ => buffer.close() }\n+    true\n+  }\n+\n+  override def process(value: UnsafeRow): Unit = {\n+    buffer.add(value)\n+  }\n+\n+  override def close(errorOrNull: Throwable): Unit = {\n+    buffer.allRowsAdded()\n+    if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one\n+  }\n+}\n+\n+object PythonForeachWriter {\n+\n+  /**\n+   * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeahWriter.\n+   * It is designed to be used with only 1 writer thread (i.e. JVM task thread) and only 1 reader\n+   * thread (i.e. PythonRunner writing thread that reads from the buffer and writes to the Python\n+   * worker stdin). Adds to the buffer are non-blocking, and reads through the buffer's iterator\n+   * are blocking, that is, it blocks until new data is available or all data has been added.\n+   *\n+   * Internally, it uses a [[HybridRowQueue]] to buffer the rows in a practically unlimited queue\n+   * across memory and local disk. However, HybridRowQueue is designed to be used only with\n+   * EvalPythonExec where the reader is always behind the the writer, that is, the reader does not\n+   * try to read n+1 rows if the writer has only written n rows at any point of time. This\n+   * assumption is not true for PythonForeachWriter where rows may be added at a different rate as\n+   * they are consumed by the python worker. Hence, to maintain the invariant of the reader being\n+   * behind the writer while using HybridRowQueue, the buffer does the following\n+   * - Keeps a count of the rows in the HybridRowQueue\n+   * - Blocks the buffer's consuming iterator when the count is 0 so that the reader does not\n+   *   try to read more rows than what has been written.\n+   *\n+   * The implementation of the blocking iterator (ReentrantLock, Condition, etc.) has been borrowed\n+   * from that of ArrayBlockingQueue.\n+   */\n+  class UnsafeRowBuffer(taskMemoryManager: TaskMemoryManager, tempDir: File, numFields: Int)\n+      extends Logging {\n+    private val queue = HybridRowQueue(taskMemoryManager, tempDir, numFields)\n+    private val lock = new ReentrantLock()\n+    private val unblockRemove = lock.newCondition()\n+\n+    // All of these are guarded by `lock`\n+    private var count = 0L\n+    private var allAdded = false\n+    private var exception: Throwable = null\n+\n+    val iterator = new NextIterator[UnsafeRow] {\n+      override protected def getNext(): UnsafeRow = {\n+        val row = remove()\n+        if (row == null) finished = true\n+        row\n+      }\n+      override protected def close(): Unit = { }\n+    }\n+\n+    def add(row: UnsafeRow): Unit = withLock {\n+      assert(queue.add(row), s\"Failed to add row to HybridRowQueue while sending data to Python\" +",
    "line": 117
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: PythonForeachWriter",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T04:56:20Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+import java.util.concurrent.TimeUnit\n+import java.util.concurrent.locks.ReentrantLock\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.TaskMemoryManager\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{NextIterator, Utils}\n+\n+class PythonForeachWriter(func: PythonFunction, schema: StructType)\n+  extends ForeachWriter[UnsafeRow] {\n+\n+  private lazy val context = TaskContext.get()\n+  private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer(\n+    context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length)\n+  private lazy val inputRowIterator = buffer.iterator\n+\n+  private lazy val inputByteIterator = {\n+    EvaluatePython.registerPicklers()\n+    val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) }\n+    new SerDeUtil.AutoBatchedPickler(objIterator)\n+  }\n+\n+  private lazy val pythonRunner = {\n+    val conf = SparkEnv.get.conf\n+    val bufferSize = conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = conf.getBoolean(\"spark.python.worker.reuse\", true)\n+    PythonRunner(func, bufferSize, reuseWorker)\n+  }\n+\n+  private lazy val outputIterator =\n+    pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n+\n+  override def open(partitionId: Long, version: Long): Boolean = {\n+    outputIterator  // initialize everything\n+    TaskContext.get.addTaskCompletionListener { _ => buffer.close() }\n+    true\n+  }\n+\n+  override def process(value: UnsafeRow): Unit = {\n+    buffer.add(value)\n+  }\n+\n+  override def close(errorOrNull: Throwable): Unit = {\n+    buffer.allRowsAdded()\n+    if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one\n+  }\n+}\n+\n+object PythonForeachWriter {\n+\n+  /**\n+   * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeahWriter."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "done.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T21:20:28Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+import java.util.concurrent.TimeUnit\n+import java.util.concurrent.locks.ReentrantLock\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.TaskMemoryManager\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{NextIterator, Utils}\n+\n+class PythonForeachWriter(func: PythonFunction, schema: StructType)\n+  extends ForeachWriter[UnsafeRow] {\n+\n+  private lazy val context = TaskContext.get()\n+  private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer(\n+    context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length)\n+  private lazy val inputRowIterator = buffer.iterator\n+\n+  private lazy val inputByteIterator = {\n+    EvaluatePython.registerPicklers()\n+    val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) }\n+    new SerDeUtil.AutoBatchedPickler(objIterator)\n+  }\n+\n+  private lazy val pythonRunner = {\n+    val conf = SparkEnv.get.conf\n+    val bufferSize = conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = conf.getBoolean(\"spark.python.worker.reuse\", true)\n+    PythonRunner(func, bufferSize, reuseWorker)\n+  }\n+\n+  private lazy val outputIterator =\n+    pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n+\n+  override def open(partitionId: Long, version: Long): Boolean = {\n+    outputIterator  // initialize everything\n+    TaskContext.get.addTaskCompletionListener { _ => buffer.close() }\n+    true\n+  }\n+\n+  override def process(value: UnsafeRow): Unit = {\n+    buffer.add(value)\n+  }\n+\n+  override def close(errorOrNull: Throwable): Unit = {\n+    buffer.allRowsAdded()\n+    if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one\n+  }\n+}\n+\n+object PythonForeachWriter {\n+\n+  /**\n+   * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeahWriter."
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "looks `s` can be removed",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-15T02:50:09Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io.File\n+import java.util.concurrent.TimeUnit\n+import java.util.concurrent.locks.ReentrantLock\n+\n+import org.apache.spark.{SparkEnv, TaskContext}\n+import org.apache.spark.api.python._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.memory.TaskMemoryManager\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{NextIterator, Utils}\n+\n+class PythonForeachWriter(func: PythonFunction, schema: StructType)\n+  extends ForeachWriter[UnsafeRow] {\n+\n+  private lazy val context = TaskContext.get()\n+  private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer(\n+    context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length)\n+  private lazy val inputRowIterator = buffer.iterator\n+\n+  private lazy val inputByteIterator = {\n+    EvaluatePython.registerPicklers()\n+    val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) }\n+    new SerDeUtil.AutoBatchedPickler(objIterator)\n+  }\n+\n+  private lazy val pythonRunner = {\n+    val conf = SparkEnv.get.conf\n+    val bufferSize = conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = conf.getBoolean(\"spark.python.worker.reuse\", true)\n+    PythonRunner(func, bufferSize, reuseWorker)\n+  }\n+\n+  private lazy val outputIterator =\n+    pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n+\n+  override def open(partitionId: Long, version: Long): Boolean = {\n+    outputIterator  // initialize everything\n+    TaskContext.get.addTaskCompletionListener { _ => buffer.close() }\n+    true\n+  }\n+\n+  override def process(value: UnsafeRow): Unit = {\n+    buffer.add(value)\n+  }\n+\n+  override def close(errorOrNull: Throwable): Unit = {\n+    buffer.allRowsAdded()\n+    if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one\n+  }\n+}\n+\n+object PythonForeachWriter {\n+\n+  /**\n+   * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeachWriter.\n+   * It is designed to be used with only 1 writer thread (i.e. JVM task thread) and only 1 reader\n+   * thread (i.e. PythonRunner writing thread that reads from the buffer and writes to the Python\n+   * worker stdin). Adds to the buffer are non-blocking, and reads through the buffer's iterator\n+   * are blocking, that is, it blocks until new data is available or all data has been added.\n+   *\n+   * Internally, it uses a [[HybridRowQueue]] to buffer the rows in a practically unlimited queue\n+   * across memory and local disk. However, HybridRowQueue is designed to be used only with\n+   * EvalPythonExec where the reader is always behind the the writer, that is, the reader does not\n+   * try to read n+1 rows if the writer has only written n rows at any point of time. This\n+   * assumption is not true for PythonForeachWriter where rows may be added at a different rate as\n+   * they are consumed by the python worker. Hence, to maintain the invariant of the reader being\n+   * behind the writer while using HybridRowQueue, the buffer does the following\n+   * - Keeps a count of the rows in the HybridRowQueue\n+   * - Blocks the buffer's consuming iterator when the count is 0 so that the reader does not\n+   *   try to read more rows than what has been written.\n+   *\n+   * The implementation of the blocking iterator (ReentrantLock, Condition, etc.) has been borrowed\n+   * from that of ArrayBlockingQueue.\n+   */\n+  class UnsafeRowBuffer(taskMemoryManager: TaskMemoryManager, tempDir: File, numFields: Int)\n+      extends Logging {\n+    private val queue = HybridRowQueue(taskMemoryManager, tempDir, numFields)\n+    private val lock = new ReentrantLock()\n+    private val unblockRemove = lock.newCondition()\n+\n+    // All of these are guarded by `lock`\n+    private var count = 0L\n+    private var allAdded = false\n+    private var exception: Throwable = null\n+\n+    val iterator = new NextIterator[UnsafeRow] {\n+      override protected def getNext(): UnsafeRow = {\n+        val row = remove()\n+        if (row == null) finished = true\n+        row\n+      }\n+      override protected def close(): Unit = { }\n+    }\n+\n+    def add(row: UnsafeRow): Unit = withLock {\n+      assert(queue.add(row), s\"Failed to add row to HybridRowQueue while sending data to Python\" +\n+        s\"[count = $count, allAdded = $allAdded, exception = $exception]\")\n+      count += 1\n+      unblockRemove.signal()\n+      logTrace(s\"Added $row, $count left\")\n+    }\n+\n+    private def remove(): UnsafeRow = withLock {\n+      while (count == 0 && !allAdded && exception == null) {\n+        unblockRemove.await(100, TimeUnit.MILLISECONDS)\n+      }\n+\n+      // If there was any error in the adding thread, then rethrow it in the removing thread\n+      if (exception != null) throw exception\n+\n+      if (count > 0) {\n+        val row = queue.remove()\n+        assert(row != null, s\"HybridRowQueue.remove() returned null \" +"
  }],
  "prId": 21477
}]