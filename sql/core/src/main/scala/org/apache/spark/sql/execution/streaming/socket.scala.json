[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why do we still need StreamSourceProvider?",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-31T01:21:57Z",
    "diffHunk": "@@ -47,130 +48,141 @@ object TextSocketSource {\n  * This source will *not* work in production applications due to multiple reasons, including no\n  * support for fault recovery and keeping all of the text read in memory forever.\n  */\n-class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext)\n-  extends Source with Logging {\n-\n-  @GuardedBy(\"this\")\n-  private var socket: Socket = null\n-\n-  @GuardedBy(\"this\")\n-  private var readThread: Thread = null\n-\n-  /**\n-   * All batches from `lastCommittedOffset + 1` to `currentOffset`, inclusive.\n-   * Stored in a ListBuffer to facilitate removing committed batches.\n-   */\n-  @GuardedBy(\"this\")\n-  protected val batches = new ListBuffer[(String, Timestamp)]\n-\n-  @GuardedBy(\"this\")\n-  protected var currentOffset: LongOffset = new LongOffset(-1)\n-\n-  @GuardedBy(\"this\")\n-  protected var lastOffsetCommitted : LongOffset = new LongOffset(-1)\n+class TextSocketSource(\n+    protected val host: String,\n+    protected val port: Int,\n+    includeTimestamp: Boolean,\n+    sqlContext: SQLContext)\n+  extends Source with TextSocketReader with Logging {\n \n   initialize()\n \n-  private def initialize(): Unit = synchronized {\n-    socket = new Socket(host, port)\n-    val reader = new BufferedReader(new InputStreamReader(socket.getInputStream))\n-    readThread = new Thread(s\"TextSocketSource($host, $port)\") {\n-      setDaemon(true)\n-\n-      override def run(): Unit = {\n-        try {\n-          while (true) {\n-            val line = reader.readLine()\n-            if (line == null) {\n-              // End of file reached\n-              logWarning(s\"Stream closed by $host:$port\")\n-              return\n-            }\n-            TextSocketSource.this.synchronized {\n-              val newData = (line,\n-                Timestamp.valueOf(\n-                  TextSocketSource.DATE_FORMAT.format(Calendar.getInstance().getTime()))\n-                )\n-              currentOffset = currentOffset + 1\n-              batches.append(newData)\n-            }\n-          }\n-        } catch {\n-          case e: IOException =>\n-        }\n-      }\n-    }\n-    readThread.start()\n-  }\n-\n   /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n-\n-  override def getOffset: Option[Offset] = synchronized {\n-    if (currentOffset.offset == -1) {\n-      None\n-    } else {\n-      Some(currentOffset)\n-    }\n-  }\n+  override def schema: StructType =\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+\n+  override def getOffset: Option[Offset] = getOffsetInternal.map(LongOffset(_))\n \n   /** Returns the data that is between the offsets (`start`, `end`]. */\n-  override def getBatch(start: Option[Offset], end: Offset): DataFrame = synchronized {\n-    val startOrdinal =\n-      start.flatMap(LongOffset.convert).getOrElse(LongOffset(-1)).offset.toInt + 1\n-    val endOrdinal = LongOffset.convert(end).getOrElse(LongOffset(-1)).offset.toInt + 1\n-\n-    // Internal buffer only holds the batches after lastOffsetCommitted\n-    val rawList = synchronized {\n-      val sliceStart = startOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      val sliceEnd = endOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      batches.slice(sliceStart, sliceEnd)\n-    }\n+  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {\n+    val rawList = getBatchInternal(start.flatMap(LongOffset.convert).map(_.offset),\n+      LongOffset.convert(end).map(_.offset))\n \n     val rdd = sqlContext.sparkContext\n       .parallelize(rawList)\n       .map { case (v, ts) => InternalRow(UTF8String.fromString(v), ts.getTime) }\n     sqlContext.internalCreateDataFrame(rdd, schema, isStreaming = true)\n   }\n \n-  override def commit(end: Offset): Unit = synchronized {\n+  override def commit(end: Offset): Unit = {\n     val newOffset = LongOffset.convert(end).getOrElse(\n       sys.error(s\"TextSocketStream.commit() received an offset ($end) that did not \" +\n         s\"originate with an instance of this class\")\n     )\n \n-    val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt\n+    commitInternal(newOffset.offset)\n+  }\n \n-    if (offsetDiff < 0) {\n-      sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")\n-    }\n+  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+}\n+\n+case class TextSocketOffset(offset: Long) extends V2Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with TextSocketReader with Logging {\n+\n+  private var startOffset: TextSocketOffset = _\n+  private var endOffset: TextSocketOffset = _\n+\n+  protected val host: String = options.get(\"host\").get()\n+  protected val port: Int = options.get(\"port\").get().toInt\n \n-    batches.trimStart(offsetDiff)\n-    lastOffsetCommitted = newOffset\n+  initialize()\n+\n+  override def setOffsetRange(start: Optional[V2Offset], end: Optional[V2Offset]): Unit = {\n+    startOffset = start.orElse(TextSocketOffset(-1L)).asInstanceOf[TextSocketOffset]\n+    endOffset = end.orElse(\n+      TextSocketOffset(getOffsetInternal.getOrElse(-1L))).asInstanceOf[TextSocketOffset]\n   }\n \n-  /** Stop this source. */\n-  override def stop(): Unit = synchronized {\n-    if (socket != null) {\n-      try {\n-        // Unfortunately, BufferedReader.readLine() cannot be interrupted, so the only way to\n-        // stop the readThread is to close the socket.\n-        socket.close()\n-      } catch {\n-        case e: IOException =>\n-      }\n-      socket = null\n+  override def getStartOffset(): V2Offset = {\n+    Option(startOffset).getOrElse(throw new IllegalStateException(\"start offset not set\"))\n+  }\n+\n+  override def getEndOffset(): V2Offset = {\n+    Option(endOffset).getOrElse(throw new IllegalStateException(\"end offset not set\"))\n+  }\n+\n+  override def deserializeOffset(json: String): V2Offset = {\n+    TextSocketOffset(json.toLong)\n+  }\n+\n+  override def readSchema(): StructType = {\n+    val includeTimestamp = options.getBoolean(\"includeTimestamp\", false)\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+  }\n+\n+  override def createReadTasks(): JList[ReadTask[Row]] = {\n+    val rawList = getBatchInternal(Option(startOffset.offset), Option(endOffset.offset))\n+\n+    assert(SparkSession.getActiveSession.isDefined)\n+    val spark = SparkSession.getActiveSession.get\n+    val numPartitions = spark.sparkContext.defaultParallelism\n+\n+    val slices = Array.fill(numPartitions)(new ListBuffer[(String, Timestamp)])\n+    rawList.zipWithIndex.foreach { case (r, idx) =>\n+      slices(idx % numPartitions).append(r)\n     }\n+\n+    (0 until numPartitions).map { i =>\n+      val slice = slices(i)\n+      new ReadTask[Row] {\n+        override def createDataReader(): DataReader[Row] = new DataReader[Row] {\n+          private var currentIdx = -1\n+\n+          override def next(): Boolean = {\n+            currentIdx += 1\n+            currentIdx < slice.size\n+          }\n+\n+          override def get(): Row = {\n+            Row(slice(currentIdx)._1, slice(currentIdx)._2)\n+          }\n+\n+          override def close(): Unit = {}\n+        }\n+      }\n+    }.toList.asJava\n   }\n \n-  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+  override def commit(end: V2Offset): Unit = {\n+    val newOffset = end.asInstanceOf[TextSocketOffset]\n+    commitInternal(newOffset.offset)\n+  }\n+\n+  override def toString: String = s\"TextSocketMicroBatchReader[host: $host, port: $port]\"\n }\n \n-class TextSocketSourceProvider extends StreamSourceProvider with DataSourceRegister with Logging {\n-  private def parseIncludeTimestamp(params: Map[String, String]): Boolean = {\n-    Try(params.getOrElse(\"includeTimestamp\", \"false\").toBoolean) match {\n-      case Success(bool) => bool\n+class TextSocketSourceProvider extends DataSourceV2\n+  with MicroBatchReadSupport with StreamSourceProvider with DataSourceRegister with Logging {"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "If I don't misunderstand @jose-torres 's intention, basically he wanted this socket source to work also in V1 code path.",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-31T01:42:21Z",
    "diffHunk": "@@ -47,130 +48,141 @@ object TextSocketSource {\n  * This source will *not* work in production applications due to multiple reasons, including no\n  * support for fault recovery and keeping all of the text read in memory forever.\n  */\n-class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext)\n-  extends Source with Logging {\n-\n-  @GuardedBy(\"this\")\n-  private var socket: Socket = null\n-\n-  @GuardedBy(\"this\")\n-  private var readThread: Thread = null\n-\n-  /**\n-   * All batches from `lastCommittedOffset + 1` to `currentOffset`, inclusive.\n-   * Stored in a ListBuffer to facilitate removing committed batches.\n-   */\n-  @GuardedBy(\"this\")\n-  protected val batches = new ListBuffer[(String, Timestamp)]\n-\n-  @GuardedBy(\"this\")\n-  protected var currentOffset: LongOffset = new LongOffset(-1)\n-\n-  @GuardedBy(\"this\")\n-  protected var lastOffsetCommitted : LongOffset = new LongOffset(-1)\n+class TextSocketSource(\n+    protected val host: String,\n+    protected val port: Int,\n+    includeTimestamp: Boolean,\n+    sqlContext: SQLContext)\n+  extends Source with TextSocketReader with Logging {\n \n   initialize()\n \n-  private def initialize(): Unit = synchronized {\n-    socket = new Socket(host, port)\n-    val reader = new BufferedReader(new InputStreamReader(socket.getInputStream))\n-    readThread = new Thread(s\"TextSocketSource($host, $port)\") {\n-      setDaemon(true)\n-\n-      override def run(): Unit = {\n-        try {\n-          while (true) {\n-            val line = reader.readLine()\n-            if (line == null) {\n-              // End of file reached\n-              logWarning(s\"Stream closed by $host:$port\")\n-              return\n-            }\n-            TextSocketSource.this.synchronized {\n-              val newData = (line,\n-                Timestamp.valueOf(\n-                  TextSocketSource.DATE_FORMAT.format(Calendar.getInstance().getTime()))\n-                )\n-              currentOffset = currentOffset + 1\n-              batches.append(newData)\n-            }\n-          }\n-        } catch {\n-          case e: IOException =>\n-        }\n-      }\n-    }\n-    readThread.start()\n-  }\n-\n   /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n-\n-  override def getOffset: Option[Offset] = synchronized {\n-    if (currentOffset.offset == -1) {\n-      None\n-    } else {\n-      Some(currentOffset)\n-    }\n-  }\n+  override def schema: StructType =\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+\n+  override def getOffset: Option[Offset] = getOffsetInternal.map(LongOffset(_))\n \n   /** Returns the data that is between the offsets (`start`, `end`]. */\n-  override def getBatch(start: Option[Offset], end: Offset): DataFrame = synchronized {\n-    val startOrdinal =\n-      start.flatMap(LongOffset.convert).getOrElse(LongOffset(-1)).offset.toInt + 1\n-    val endOrdinal = LongOffset.convert(end).getOrElse(LongOffset(-1)).offset.toInt + 1\n-\n-    // Internal buffer only holds the batches after lastOffsetCommitted\n-    val rawList = synchronized {\n-      val sliceStart = startOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      val sliceEnd = endOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      batches.slice(sliceStart, sliceEnd)\n-    }\n+  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {\n+    val rawList = getBatchInternal(start.flatMap(LongOffset.convert).map(_.offset),\n+      LongOffset.convert(end).map(_.offset))\n \n     val rdd = sqlContext.sparkContext\n       .parallelize(rawList)\n       .map { case (v, ts) => InternalRow(UTF8String.fromString(v), ts.getTime) }\n     sqlContext.internalCreateDataFrame(rdd, schema, isStreaming = true)\n   }\n \n-  override def commit(end: Offset): Unit = synchronized {\n+  override def commit(end: Offset): Unit = {\n     val newOffset = LongOffset.convert(end).getOrElse(\n       sys.error(s\"TextSocketStream.commit() received an offset ($end) that did not \" +\n         s\"originate with an instance of this class\")\n     )\n \n-    val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt\n+    commitInternal(newOffset.offset)\n+  }\n \n-    if (offsetDiff < 0) {\n-      sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")\n-    }\n+  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+}\n+\n+case class TextSocketOffset(offset: Long) extends V2Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with TextSocketReader with Logging {\n+\n+  private var startOffset: TextSocketOffset = _\n+  private var endOffset: TextSocketOffset = _\n+\n+  protected val host: String = options.get(\"host\").get()\n+  protected val port: Int = options.get(\"port\").get().toInt\n \n-    batches.trimStart(offsetDiff)\n-    lastOffsetCommitted = newOffset\n+  initialize()\n+\n+  override def setOffsetRange(start: Optional[V2Offset], end: Optional[V2Offset]): Unit = {\n+    startOffset = start.orElse(TextSocketOffset(-1L)).asInstanceOf[TextSocketOffset]\n+    endOffset = end.orElse(\n+      TextSocketOffset(getOffsetInternal.getOrElse(-1L))).asInstanceOf[TextSocketOffset]\n   }\n \n-  /** Stop this source. */\n-  override def stop(): Unit = synchronized {\n-    if (socket != null) {\n-      try {\n-        // Unfortunately, BufferedReader.readLine() cannot be interrupted, so the only way to\n-        // stop the readThread is to close the socket.\n-        socket.close()\n-      } catch {\n-        case e: IOException =>\n-      }\n-      socket = null\n+  override def getStartOffset(): V2Offset = {\n+    Option(startOffset).getOrElse(throw new IllegalStateException(\"start offset not set\"))\n+  }\n+\n+  override def getEndOffset(): V2Offset = {\n+    Option(endOffset).getOrElse(throw new IllegalStateException(\"end offset not set\"))\n+  }\n+\n+  override def deserializeOffset(json: String): V2Offset = {\n+    TextSocketOffset(json.toLong)\n+  }\n+\n+  override def readSchema(): StructType = {\n+    val includeTimestamp = options.getBoolean(\"includeTimestamp\", false)\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+  }\n+\n+  override def createReadTasks(): JList[ReadTask[Row]] = {\n+    val rawList = getBatchInternal(Option(startOffset.offset), Option(endOffset.offset))\n+\n+    assert(SparkSession.getActiveSession.isDefined)\n+    val spark = SparkSession.getActiveSession.get\n+    val numPartitions = spark.sparkContext.defaultParallelism\n+\n+    val slices = Array.fill(numPartitions)(new ListBuffer[(String, Timestamp)])\n+    rawList.zipWithIndex.foreach { case (r, idx) =>\n+      slices(idx % numPartitions).append(r)\n     }\n+\n+    (0 until numPartitions).map { i =>\n+      val slice = slices(i)\n+      new ReadTask[Row] {\n+        override def createDataReader(): DataReader[Row] = new DataReader[Row] {\n+          private var currentIdx = -1\n+\n+          override def next(): Boolean = {\n+            currentIdx += 1\n+            currentIdx < slice.size\n+          }\n+\n+          override def get(): Row = {\n+            Row(slice(currentIdx)._1, slice(currentIdx)._2)\n+          }\n+\n+          override def close(): Unit = {}\n+        }\n+      }\n+    }.toList.asJava\n   }\n \n-  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+  override def commit(end: V2Offset): Unit = {\n+    val newOffset = end.asInstanceOf[TextSocketOffset]\n+    commitInternal(newOffset.offset)\n+  }\n+\n+  override def toString: String = s\"TextSocketMicroBatchReader[host: $host, port: $port]\"\n }\n \n-class TextSocketSourceProvider extends StreamSourceProvider with DataSourceRegister with Logging {\n-  private def parseIncludeTimestamp(params: Map[String, String]): Boolean = {\n-    Try(params.getOrElse(\"includeTimestamp\", \"false\").toBoolean) match {\n-      case Success(bool) => bool\n+class TextSocketSourceProvider extends DataSourceV2\n+  with MicroBatchReadSupport with StreamSourceProvider with DataSourceRegister with Logging {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "aah, i see earlier comments.\r\n",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-31T02:13:37Z",
    "diffHunk": "@@ -47,130 +48,141 @@ object TextSocketSource {\n  * This source will *not* work in production applications due to multiple reasons, including no\n  * support for fault recovery and keeping all of the text read in memory forever.\n  */\n-class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext)\n-  extends Source with Logging {\n-\n-  @GuardedBy(\"this\")\n-  private var socket: Socket = null\n-\n-  @GuardedBy(\"this\")\n-  private var readThread: Thread = null\n-\n-  /**\n-   * All batches from `lastCommittedOffset + 1` to `currentOffset`, inclusive.\n-   * Stored in a ListBuffer to facilitate removing committed batches.\n-   */\n-  @GuardedBy(\"this\")\n-  protected val batches = new ListBuffer[(String, Timestamp)]\n-\n-  @GuardedBy(\"this\")\n-  protected var currentOffset: LongOffset = new LongOffset(-1)\n-\n-  @GuardedBy(\"this\")\n-  protected var lastOffsetCommitted : LongOffset = new LongOffset(-1)\n+class TextSocketSource(\n+    protected val host: String,\n+    protected val port: Int,\n+    includeTimestamp: Boolean,\n+    sqlContext: SQLContext)\n+  extends Source with TextSocketReader with Logging {\n \n   initialize()\n \n-  private def initialize(): Unit = synchronized {\n-    socket = new Socket(host, port)\n-    val reader = new BufferedReader(new InputStreamReader(socket.getInputStream))\n-    readThread = new Thread(s\"TextSocketSource($host, $port)\") {\n-      setDaemon(true)\n-\n-      override def run(): Unit = {\n-        try {\n-          while (true) {\n-            val line = reader.readLine()\n-            if (line == null) {\n-              // End of file reached\n-              logWarning(s\"Stream closed by $host:$port\")\n-              return\n-            }\n-            TextSocketSource.this.synchronized {\n-              val newData = (line,\n-                Timestamp.valueOf(\n-                  TextSocketSource.DATE_FORMAT.format(Calendar.getInstance().getTime()))\n-                )\n-              currentOffset = currentOffset + 1\n-              batches.append(newData)\n-            }\n-          }\n-        } catch {\n-          case e: IOException =>\n-        }\n-      }\n-    }\n-    readThread.start()\n-  }\n-\n   /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n-\n-  override def getOffset: Option[Offset] = synchronized {\n-    if (currentOffset.offset == -1) {\n-      None\n-    } else {\n-      Some(currentOffset)\n-    }\n-  }\n+  override def schema: StructType =\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+\n+  override def getOffset: Option[Offset] = getOffsetInternal.map(LongOffset(_))\n \n   /** Returns the data that is between the offsets (`start`, `end`]. */\n-  override def getBatch(start: Option[Offset], end: Offset): DataFrame = synchronized {\n-    val startOrdinal =\n-      start.flatMap(LongOffset.convert).getOrElse(LongOffset(-1)).offset.toInt + 1\n-    val endOrdinal = LongOffset.convert(end).getOrElse(LongOffset(-1)).offset.toInt + 1\n-\n-    // Internal buffer only holds the batches after lastOffsetCommitted\n-    val rawList = synchronized {\n-      val sliceStart = startOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      val sliceEnd = endOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      batches.slice(sliceStart, sliceEnd)\n-    }\n+  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {\n+    val rawList = getBatchInternal(start.flatMap(LongOffset.convert).map(_.offset),\n+      LongOffset.convert(end).map(_.offset))\n \n     val rdd = sqlContext.sparkContext\n       .parallelize(rawList)\n       .map { case (v, ts) => InternalRow(UTF8String.fromString(v), ts.getTime) }\n     sqlContext.internalCreateDataFrame(rdd, schema, isStreaming = true)\n   }\n \n-  override def commit(end: Offset): Unit = synchronized {\n+  override def commit(end: Offset): Unit = {\n     val newOffset = LongOffset.convert(end).getOrElse(\n       sys.error(s\"TextSocketStream.commit() received an offset ($end) that did not \" +\n         s\"originate with an instance of this class\")\n     )\n \n-    val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt\n+    commitInternal(newOffset.offset)\n+  }\n \n-    if (offsetDiff < 0) {\n-      sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")\n-    }\n+  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+}\n+\n+case class TextSocketOffset(offset: Long) extends V2Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with TextSocketReader with Logging {\n+\n+  private var startOffset: TextSocketOffset = _\n+  private var endOffset: TextSocketOffset = _\n+\n+  protected val host: String = options.get(\"host\").get()\n+  protected val port: Int = options.get(\"port\").get().toInt\n \n-    batches.trimStart(offsetDiff)\n-    lastOffsetCommitted = newOffset\n+  initialize()\n+\n+  override def setOffsetRange(start: Optional[V2Offset], end: Optional[V2Offset]): Unit = {\n+    startOffset = start.orElse(TextSocketOffset(-1L)).asInstanceOf[TextSocketOffset]\n+    endOffset = end.orElse(\n+      TextSocketOffset(getOffsetInternal.getOrElse(-1L))).asInstanceOf[TextSocketOffset]\n   }\n \n-  /** Stop this source. */\n-  override def stop(): Unit = synchronized {\n-    if (socket != null) {\n-      try {\n-        // Unfortunately, BufferedReader.readLine() cannot be interrupted, so the only way to\n-        // stop the readThread is to close the socket.\n-        socket.close()\n-      } catch {\n-        case e: IOException =>\n-      }\n-      socket = null\n+  override def getStartOffset(): V2Offset = {\n+    Option(startOffset).getOrElse(throw new IllegalStateException(\"start offset not set\"))\n+  }\n+\n+  override def getEndOffset(): V2Offset = {\n+    Option(endOffset).getOrElse(throw new IllegalStateException(\"end offset not set\"))\n+  }\n+\n+  override def deserializeOffset(json: String): V2Offset = {\n+    TextSocketOffset(json.toLong)\n+  }\n+\n+  override def readSchema(): StructType = {\n+    val includeTimestamp = options.getBoolean(\"includeTimestamp\", false)\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+  }\n+\n+  override def createReadTasks(): JList[ReadTask[Row]] = {\n+    val rawList = getBatchInternal(Option(startOffset.offset), Option(endOffset.offset))\n+\n+    assert(SparkSession.getActiveSession.isDefined)\n+    val spark = SparkSession.getActiveSession.get\n+    val numPartitions = spark.sparkContext.defaultParallelism\n+\n+    val slices = Array.fill(numPartitions)(new ListBuffer[(String, Timestamp)])\n+    rawList.zipWithIndex.foreach { case (r, idx) =>\n+      slices(idx % numPartitions).append(r)\n     }\n+\n+    (0 until numPartitions).map { i =>\n+      val slice = slices(i)\n+      new ReadTask[Row] {\n+        override def createDataReader(): DataReader[Row] = new DataReader[Row] {\n+          private var currentIdx = -1\n+\n+          override def next(): Boolean = {\n+            currentIdx += 1\n+            currentIdx < slice.size\n+          }\n+\n+          override def get(): Row = {\n+            Row(slice(currentIdx)._1, slice(currentIdx)._2)\n+          }\n+\n+          override def close(): Unit = {}\n+        }\n+      }\n+    }.toList.asJava\n   }\n \n-  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+  override def commit(end: V2Offset): Unit = {\n+    val newOffset = end.asInstanceOf[TextSocketOffset]\n+    commitInternal(newOffset.offset)\n+  }\n+\n+  override def toString: String = s\"TextSocketMicroBatchReader[host: $host, port: $port]\"\n }\n \n-class TextSocketSourceProvider extends StreamSourceProvider with DataSourceRegister with Logging {\n-  private def parseIncludeTimestamp(params: Map[String, String]): Boolean = {\n-    Try(params.getOrElse(\"includeTimestamp\", \"false\").toBoolean) match {\n-      case Success(bool) => bool\n+class TextSocketSourceProvider extends DataSourceV2\n+  with MicroBatchReadSupport with StreamSourceProvider with DataSourceRegister with Logging {"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "TD and I discussed this offline. It should be fine to remove the V1 StreamSourceProvider implementation, because:\r\n\r\n* this isn't a production-quality source, so users shouldn't need to fall back to it\r\n* this source won't be particularly useful at exercising the V1 execution pipeline once we transition all sources to V2",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-31T03:10:08Z",
    "diffHunk": "@@ -47,130 +48,141 @@ object TextSocketSource {\n  * This source will *not* work in production applications due to multiple reasons, including no\n  * support for fault recovery and keeping all of the text read in memory forever.\n  */\n-class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext)\n-  extends Source with Logging {\n-\n-  @GuardedBy(\"this\")\n-  private var socket: Socket = null\n-\n-  @GuardedBy(\"this\")\n-  private var readThread: Thread = null\n-\n-  /**\n-   * All batches from `lastCommittedOffset + 1` to `currentOffset`, inclusive.\n-   * Stored in a ListBuffer to facilitate removing committed batches.\n-   */\n-  @GuardedBy(\"this\")\n-  protected val batches = new ListBuffer[(String, Timestamp)]\n-\n-  @GuardedBy(\"this\")\n-  protected var currentOffset: LongOffset = new LongOffset(-1)\n-\n-  @GuardedBy(\"this\")\n-  protected var lastOffsetCommitted : LongOffset = new LongOffset(-1)\n+class TextSocketSource(\n+    protected val host: String,\n+    protected val port: Int,\n+    includeTimestamp: Boolean,\n+    sqlContext: SQLContext)\n+  extends Source with TextSocketReader with Logging {\n \n   initialize()\n \n-  private def initialize(): Unit = synchronized {\n-    socket = new Socket(host, port)\n-    val reader = new BufferedReader(new InputStreamReader(socket.getInputStream))\n-    readThread = new Thread(s\"TextSocketSource($host, $port)\") {\n-      setDaemon(true)\n-\n-      override def run(): Unit = {\n-        try {\n-          while (true) {\n-            val line = reader.readLine()\n-            if (line == null) {\n-              // End of file reached\n-              logWarning(s\"Stream closed by $host:$port\")\n-              return\n-            }\n-            TextSocketSource.this.synchronized {\n-              val newData = (line,\n-                Timestamp.valueOf(\n-                  TextSocketSource.DATE_FORMAT.format(Calendar.getInstance().getTime()))\n-                )\n-              currentOffset = currentOffset + 1\n-              batches.append(newData)\n-            }\n-          }\n-        } catch {\n-          case e: IOException =>\n-        }\n-      }\n-    }\n-    readThread.start()\n-  }\n-\n   /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n-\n-  override def getOffset: Option[Offset] = synchronized {\n-    if (currentOffset.offset == -1) {\n-      None\n-    } else {\n-      Some(currentOffset)\n-    }\n-  }\n+  override def schema: StructType =\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+\n+  override def getOffset: Option[Offset] = getOffsetInternal.map(LongOffset(_))\n \n   /** Returns the data that is between the offsets (`start`, `end`]. */\n-  override def getBatch(start: Option[Offset], end: Offset): DataFrame = synchronized {\n-    val startOrdinal =\n-      start.flatMap(LongOffset.convert).getOrElse(LongOffset(-1)).offset.toInt + 1\n-    val endOrdinal = LongOffset.convert(end).getOrElse(LongOffset(-1)).offset.toInt + 1\n-\n-    // Internal buffer only holds the batches after lastOffsetCommitted\n-    val rawList = synchronized {\n-      val sliceStart = startOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      val sliceEnd = endOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      batches.slice(sliceStart, sliceEnd)\n-    }\n+  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {\n+    val rawList = getBatchInternal(start.flatMap(LongOffset.convert).map(_.offset),\n+      LongOffset.convert(end).map(_.offset))\n \n     val rdd = sqlContext.sparkContext\n       .parallelize(rawList)\n       .map { case (v, ts) => InternalRow(UTF8String.fromString(v), ts.getTime) }\n     sqlContext.internalCreateDataFrame(rdd, schema, isStreaming = true)\n   }\n \n-  override def commit(end: Offset): Unit = synchronized {\n+  override def commit(end: Offset): Unit = {\n     val newOffset = LongOffset.convert(end).getOrElse(\n       sys.error(s\"TextSocketStream.commit() received an offset ($end) that did not \" +\n         s\"originate with an instance of this class\")\n     )\n \n-    val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt\n+    commitInternal(newOffset.offset)\n+  }\n \n-    if (offsetDiff < 0) {\n-      sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")\n-    }\n+  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+}\n+\n+case class TextSocketOffset(offset: Long) extends V2Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with TextSocketReader with Logging {\n+\n+  private var startOffset: TextSocketOffset = _\n+  private var endOffset: TextSocketOffset = _\n+\n+  protected val host: String = options.get(\"host\").get()\n+  protected val port: Int = options.get(\"port\").get().toInt\n \n-    batches.trimStart(offsetDiff)\n-    lastOffsetCommitted = newOffset\n+  initialize()\n+\n+  override def setOffsetRange(start: Optional[V2Offset], end: Optional[V2Offset]): Unit = {\n+    startOffset = start.orElse(TextSocketOffset(-1L)).asInstanceOf[TextSocketOffset]\n+    endOffset = end.orElse(\n+      TextSocketOffset(getOffsetInternal.getOrElse(-1L))).asInstanceOf[TextSocketOffset]\n   }\n \n-  /** Stop this source. */\n-  override def stop(): Unit = synchronized {\n-    if (socket != null) {\n-      try {\n-        // Unfortunately, BufferedReader.readLine() cannot be interrupted, so the only way to\n-        // stop the readThread is to close the socket.\n-        socket.close()\n-      } catch {\n-        case e: IOException =>\n-      }\n-      socket = null\n+  override def getStartOffset(): V2Offset = {\n+    Option(startOffset).getOrElse(throw new IllegalStateException(\"start offset not set\"))\n+  }\n+\n+  override def getEndOffset(): V2Offset = {\n+    Option(endOffset).getOrElse(throw new IllegalStateException(\"end offset not set\"))\n+  }\n+\n+  override def deserializeOffset(json: String): V2Offset = {\n+    TextSocketOffset(json.toLong)\n+  }\n+\n+  override def readSchema(): StructType = {\n+    val includeTimestamp = options.getBoolean(\"includeTimestamp\", false)\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+  }\n+\n+  override def createReadTasks(): JList[ReadTask[Row]] = {\n+    val rawList = getBatchInternal(Option(startOffset.offset), Option(endOffset.offset))\n+\n+    assert(SparkSession.getActiveSession.isDefined)\n+    val spark = SparkSession.getActiveSession.get\n+    val numPartitions = spark.sparkContext.defaultParallelism\n+\n+    val slices = Array.fill(numPartitions)(new ListBuffer[(String, Timestamp)])\n+    rawList.zipWithIndex.foreach { case (r, idx) =>\n+      slices(idx % numPartitions).append(r)\n     }\n+\n+    (0 until numPartitions).map { i =>\n+      val slice = slices(i)\n+      new ReadTask[Row] {\n+        override def createDataReader(): DataReader[Row] = new DataReader[Row] {\n+          private var currentIdx = -1\n+\n+          override def next(): Boolean = {\n+            currentIdx += 1\n+            currentIdx < slice.size\n+          }\n+\n+          override def get(): Row = {\n+            Row(slice(currentIdx)._1, slice(currentIdx)._2)\n+          }\n+\n+          override def close(): Unit = {}\n+        }\n+      }\n+    }.toList.asJava\n   }\n \n-  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+  override def commit(end: V2Offset): Unit = {\n+    val newOffset = end.asInstanceOf[TextSocketOffset]\n+    commitInternal(newOffset.offset)\n+  }\n+\n+  override def toString: String = s\"TextSocketMicroBatchReader[host: $host, port: $port]\"\n }\n \n-class TextSocketSourceProvider extends StreamSourceProvider with DataSourceRegister with Logging {\n-  private def parseIncludeTimestamp(params: Map[String, String]): Boolean = {\n-    Try(params.getOrElse(\"includeTimestamp\", \"false\").toBoolean) match {\n-      case Success(bool) => bool\n+class TextSocketSourceProvider extends DataSourceV2\n+  with MicroBatchReadSupport with StreamSourceProvider with DataSourceRegister with Logging {"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "OK, I will update the patch accordingly.",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-31T03:14:13Z",
    "diffHunk": "@@ -47,130 +48,141 @@ object TextSocketSource {\n  * This source will *not* work in production applications due to multiple reasons, including no\n  * support for fault recovery and keeping all of the text read in memory forever.\n  */\n-class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext)\n-  extends Source with Logging {\n-\n-  @GuardedBy(\"this\")\n-  private var socket: Socket = null\n-\n-  @GuardedBy(\"this\")\n-  private var readThread: Thread = null\n-\n-  /**\n-   * All batches from `lastCommittedOffset + 1` to `currentOffset`, inclusive.\n-   * Stored in a ListBuffer to facilitate removing committed batches.\n-   */\n-  @GuardedBy(\"this\")\n-  protected val batches = new ListBuffer[(String, Timestamp)]\n-\n-  @GuardedBy(\"this\")\n-  protected var currentOffset: LongOffset = new LongOffset(-1)\n-\n-  @GuardedBy(\"this\")\n-  protected var lastOffsetCommitted : LongOffset = new LongOffset(-1)\n+class TextSocketSource(\n+    protected val host: String,\n+    protected val port: Int,\n+    includeTimestamp: Boolean,\n+    sqlContext: SQLContext)\n+  extends Source with TextSocketReader with Logging {\n \n   initialize()\n \n-  private def initialize(): Unit = synchronized {\n-    socket = new Socket(host, port)\n-    val reader = new BufferedReader(new InputStreamReader(socket.getInputStream))\n-    readThread = new Thread(s\"TextSocketSource($host, $port)\") {\n-      setDaemon(true)\n-\n-      override def run(): Unit = {\n-        try {\n-          while (true) {\n-            val line = reader.readLine()\n-            if (line == null) {\n-              // End of file reached\n-              logWarning(s\"Stream closed by $host:$port\")\n-              return\n-            }\n-            TextSocketSource.this.synchronized {\n-              val newData = (line,\n-                Timestamp.valueOf(\n-                  TextSocketSource.DATE_FORMAT.format(Calendar.getInstance().getTime()))\n-                )\n-              currentOffset = currentOffset + 1\n-              batches.append(newData)\n-            }\n-          }\n-        } catch {\n-          case e: IOException =>\n-        }\n-      }\n-    }\n-    readThread.start()\n-  }\n-\n   /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n-\n-  override def getOffset: Option[Offset] = synchronized {\n-    if (currentOffset.offset == -1) {\n-      None\n-    } else {\n-      Some(currentOffset)\n-    }\n-  }\n+  override def schema: StructType =\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+\n+  override def getOffset: Option[Offset] = getOffsetInternal.map(LongOffset(_))\n \n   /** Returns the data that is between the offsets (`start`, `end`]. */\n-  override def getBatch(start: Option[Offset], end: Offset): DataFrame = synchronized {\n-    val startOrdinal =\n-      start.flatMap(LongOffset.convert).getOrElse(LongOffset(-1)).offset.toInt + 1\n-    val endOrdinal = LongOffset.convert(end).getOrElse(LongOffset(-1)).offset.toInt + 1\n-\n-    // Internal buffer only holds the batches after lastOffsetCommitted\n-    val rawList = synchronized {\n-      val sliceStart = startOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      val sliceEnd = endOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      batches.slice(sliceStart, sliceEnd)\n-    }\n+  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {\n+    val rawList = getBatchInternal(start.flatMap(LongOffset.convert).map(_.offset),\n+      LongOffset.convert(end).map(_.offset))\n \n     val rdd = sqlContext.sparkContext\n       .parallelize(rawList)\n       .map { case (v, ts) => InternalRow(UTF8String.fromString(v), ts.getTime) }\n     sqlContext.internalCreateDataFrame(rdd, schema, isStreaming = true)\n   }\n \n-  override def commit(end: Offset): Unit = synchronized {\n+  override def commit(end: Offset): Unit = {\n     val newOffset = LongOffset.convert(end).getOrElse(\n       sys.error(s\"TextSocketStream.commit() received an offset ($end) that did not \" +\n         s\"originate with an instance of this class\")\n     )\n \n-    val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt\n+    commitInternal(newOffset.offset)\n+  }\n \n-    if (offsetDiff < 0) {\n-      sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")\n-    }\n+  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+}\n+\n+case class TextSocketOffset(offset: Long) extends V2Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with TextSocketReader with Logging {\n+\n+  private var startOffset: TextSocketOffset = _\n+  private var endOffset: TextSocketOffset = _\n+\n+  protected val host: String = options.get(\"host\").get()\n+  protected val port: Int = options.get(\"port\").get().toInt\n \n-    batches.trimStart(offsetDiff)\n-    lastOffsetCommitted = newOffset\n+  initialize()\n+\n+  override def setOffsetRange(start: Optional[V2Offset], end: Optional[V2Offset]): Unit = {\n+    startOffset = start.orElse(TextSocketOffset(-1L)).asInstanceOf[TextSocketOffset]\n+    endOffset = end.orElse(\n+      TextSocketOffset(getOffsetInternal.getOrElse(-1L))).asInstanceOf[TextSocketOffset]\n   }\n \n-  /** Stop this source. */\n-  override def stop(): Unit = synchronized {\n-    if (socket != null) {\n-      try {\n-        // Unfortunately, BufferedReader.readLine() cannot be interrupted, so the only way to\n-        // stop the readThread is to close the socket.\n-        socket.close()\n-      } catch {\n-        case e: IOException =>\n-      }\n-      socket = null\n+  override def getStartOffset(): V2Offset = {\n+    Option(startOffset).getOrElse(throw new IllegalStateException(\"start offset not set\"))\n+  }\n+\n+  override def getEndOffset(): V2Offset = {\n+    Option(endOffset).getOrElse(throw new IllegalStateException(\"end offset not set\"))\n+  }\n+\n+  override def deserializeOffset(json: String): V2Offset = {\n+    TextSocketOffset(json.toLong)\n+  }\n+\n+  override def readSchema(): StructType = {\n+    val includeTimestamp = options.getBoolean(\"includeTimestamp\", false)\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+  }\n+\n+  override def createReadTasks(): JList[ReadTask[Row]] = {\n+    val rawList = getBatchInternal(Option(startOffset.offset), Option(endOffset.offset))\n+\n+    assert(SparkSession.getActiveSession.isDefined)\n+    val spark = SparkSession.getActiveSession.get\n+    val numPartitions = spark.sparkContext.defaultParallelism\n+\n+    val slices = Array.fill(numPartitions)(new ListBuffer[(String, Timestamp)])\n+    rawList.zipWithIndex.foreach { case (r, idx) =>\n+      slices(idx % numPartitions).append(r)\n     }\n+\n+    (0 until numPartitions).map { i =>\n+      val slice = slices(i)\n+      new ReadTask[Row] {\n+        override def createDataReader(): DataReader[Row] = new DataReader[Row] {\n+          private var currentIdx = -1\n+\n+          override def next(): Boolean = {\n+            currentIdx += 1\n+            currentIdx < slice.size\n+          }\n+\n+          override def get(): Row = {\n+            Row(slice(currentIdx)._1, slice(currentIdx)._2)\n+          }\n+\n+          override def close(): Unit = {}\n+        }\n+      }\n+    }.toList.asJava\n   }\n \n-  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+  override def commit(end: V2Offset): Unit = {\n+    val newOffset = end.asInstanceOf[TextSocketOffset]\n+    commitInternal(newOffset.offset)\n+  }\n+\n+  override def toString: String = s\"TextSocketMicroBatchReader[host: $host, port: $port]\"\n }\n \n-class TextSocketSourceProvider extends StreamSourceProvider with DataSourceRegister with Logging {\n-  private def parseIncludeTimestamp(params: Map[String, String]): Boolean = {\n-    Try(params.getOrElse(\"includeTimestamp\", \"false\").toBoolean) match {\n-      case Success(bool) => bool\n+class TextSocketSourceProvider extends DataSourceV2\n+  with MicroBatchReadSupport with StreamSourceProvider with DataSourceRegister with Logging {"
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I would wait for my PR #20445 to go in where I migrate LongOffset to use OffsetV2",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-31T03:18:38Z",
    "diffHunk": "@@ -47,130 +48,141 @@ object TextSocketSource {\n  * This source will *not* work in production applications due to multiple reasons, including no\n  * support for fault recovery and keeping all of the text read in memory forever.\n  */\n-class TextSocketSource(host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext)\n-  extends Source with Logging {\n-\n-  @GuardedBy(\"this\")\n-  private var socket: Socket = null\n-\n-  @GuardedBy(\"this\")\n-  private var readThread: Thread = null\n-\n-  /**\n-   * All batches from `lastCommittedOffset + 1` to `currentOffset`, inclusive.\n-   * Stored in a ListBuffer to facilitate removing committed batches.\n-   */\n-  @GuardedBy(\"this\")\n-  protected val batches = new ListBuffer[(String, Timestamp)]\n-\n-  @GuardedBy(\"this\")\n-  protected var currentOffset: LongOffset = new LongOffset(-1)\n-\n-  @GuardedBy(\"this\")\n-  protected var lastOffsetCommitted : LongOffset = new LongOffset(-1)\n+class TextSocketSource(\n+    protected val host: String,\n+    protected val port: Int,\n+    includeTimestamp: Boolean,\n+    sqlContext: SQLContext)\n+  extends Source with TextSocketReader with Logging {\n \n   initialize()\n \n-  private def initialize(): Unit = synchronized {\n-    socket = new Socket(host, port)\n-    val reader = new BufferedReader(new InputStreamReader(socket.getInputStream))\n-    readThread = new Thread(s\"TextSocketSource($host, $port)\") {\n-      setDaemon(true)\n-\n-      override def run(): Unit = {\n-        try {\n-          while (true) {\n-            val line = reader.readLine()\n-            if (line == null) {\n-              // End of file reached\n-              logWarning(s\"Stream closed by $host:$port\")\n-              return\n-            }\n-            TextSocketSource.this.synchronized {\n-              val newData = (line,\n-                Timestamp.valueOf(\n-                  TextSocketSource.DATE_FORMAT.format(Calendar.getInstance().getTime()))\n-                )\n-              currentOffset = currentOffset + 1\n-              batches.append(newData)\n-            }\n-          }\n-        } catch {\n-          case e: IOException =>\n-        }\n-      }\n-    }\n-    readThread.start()\n-  }\n-\n   /** Returns the schema of the data from this source */\n-  override def schema: StructType = if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP\n-  else TextSocketSource.SCHEMA_REGULAR\n-\n-  override def getOffset: Option[Offset] = synchronized {\n-    if (currentOffset.offset == -1) {\n-      None\n-    } else {\n-      Some(currentOffset)\n-    }\n-  }\n+  override def schema: StructType =\n+    if (includeTimestamp) TextSocketSource.SCHEMA_TIMESTAMP else TextSocketSource.SCHEMA_REGULAR\n+\n+  override def getOffset: Option[Offset] = getOffsetInternal.map(LongOffset(_))\n \n   /** Returns the data that is between the offsets (`start`, `end`]. */\n-  override def getBatch(start: Option[Offset], end: Offset): DataFrame = synchronized {\n-    val startOrdinal =\n-      start.flatMap(LongOffset.convert).getOrElse(LongOffset(-1)).offset.toInt + 1\n-    val endOrdinal = LongOffset.convert(end).getOrElse(LongOffset(-1)).offset.toInt + 1\n-\n-    // Internal buffer only holds the batches after lastOffsetCommitted\n-    val rawList = synchronized {\n-      val sliceStart = startOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      val sliceEnd = endOrdinal - lastOffsetCommitted.offset.toInt - 1\n-      batches.slice(sliceStart, sliceEnd)\n-    }\n+  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {\n+    val rawList = getBatchInternal(start.flatMap(LongOffset.convert).map(_.offset),\n+      LongOffset.convert(end).map(_.offset))\n \n     val rdd = sqlContext.sparkContext\n       .parallelize(rawList)\n       .map { case (v, ts) => InternalRow(UTF8String.fromString(v), ts.getTime) }\n     sqlContext.internalCreateDataFrame(rdd, schema, isStreaming = true)\n   }\n \n-  override def commit(end: Offset): Unit = synchronized {\n+  override def commit(end: Offset): Unit = {\n     val newOffset = LongOffset.convert(end).getOrElse(\n       sys.error(s\"TextSocketStream.commit() received an offset ($end) that did not \" +\n         s\"originate with an instance of this class\")\n     )\n \n-    val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt\n+    commitInternal(newOffset.offset)\n+  }\n \n-    if (offsetDiff < 0) {\n-      sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")\n-    }\n+  override def toString: String = s\"TextSocketSource[host: $host, port: $port]\"\n+}\n+\n+case class TextSocketOffset(offset: Long) extends V2Offset {"
  }],
  "prId": 20382
}]