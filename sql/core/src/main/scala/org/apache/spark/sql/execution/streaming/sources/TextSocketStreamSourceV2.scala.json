[{
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "The intent is for the V2 and V1 source to live in the same register, so existing queries can start using the V2 source with no change needed. This also allows the V2 implementation to be validated by passing all the old tests.\r\n\r\nRateSourceV2 is a bad example; it only exists because I didn't have time to write a fully compatible rate source. I'll work on fixing it.",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-24T17:02:57Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "@jose-torres , you mean that instead of creating a new V2 socket source, modifying current V1 socket source to make it work with V2, am I understanding correctly?",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-25T05:32:29Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "The idea is that the existing TextSocketSourceProvider will have the MicroBatchReadSupport implementation here, in addition to the StreamSourceProvider implementation it already has.",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-25T05:53:00Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "I see, thanks for the clarify. Let me change it.",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-25T05:54:53Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {"
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "To match the old parallelize behavior, the default number of partitions should be sparkContext.defaultParallelism.",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-24T17:07:21Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {\n+  override def shortName(): String = \"socketv2\"\n+\n+  override def createMicroBatchReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceV2Options): MicroBatchReader = {\n+    logWarning(\"The socket source should not be used for production applications! \" +\n+      \"It does not support recovery.\")\n+    if (!options.get(TextSocketSourceProviderV2.HOST).isPresent) {\n+      throw new AnalysisException(\"Set a host to read from with option(\\\"host\\\", ...).\")\n+    }\n+    if (!options.get(TextSocketSourceProviderV2.PORT).isPresent) {\n+      throw new AnalysisException(\"Set a port to read from with option(\\\"port\\\", ...).\")\n+    }\n+    if (schema.isPresent) {\n+      throw new AnalysisException(\"The socket source does not support a user-specified schema.\")\n+    }\n+\n+    if (options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).isPresent) {\n+      Try(options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).get().toBoolean) match {\n+        case Success(bool) =>\n+        case Failure(_) =>\n+          throw new AnalysisException(\n+            \"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\")\n+      }\n+    }\n+\n+    new TextSocketStreamMicroBatchReader(options)\n+  }\n+}\n+\n+case class TextSocketStreamOffset(offset: Long) extends Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketStreamMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with Logging {\n+\n+  import TextSocketSourceProviderV2._\n+\n+  private var start: TextSocketStreamOffset = _\n+  private var end: TextSocketStreamOffset = _\n+\n+  private val host = options.get(HOST).get()\n+  private val port = options.get(PORT).get().toInt\n+  private val includeTimestamp = options.getBoolean(INCLUDE_TIMESTAMP, false)\n+  private val numPartitions = options.getInt(NUM_PARTITIONS, 1)"
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "Is it possible to initialize in the constructor?",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-24T17:08:50Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {\n+  override def shortName(): String = \"socketv2\"\n+\n+  override def createMicroBatchReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceV2Options): MicroBatchReader = {\n+    logWarning(\"The socket source should not be used for production applications! \" +\n+      \"It does not support recovery.\")\n+    if (!options.get(TextSocketSourceProviderV2.HOST).isPresent) {\n+      throw new AnalysisException(\"Set a host to read from with option(\\\"host\\\", ...).\")\n+    }\n+    if (!options.get(TextSocketSourceProviderV2.PORT).isPresent) {\n+      throw new AnalysisException(\"Set a port to read from with option(\\\"port\\\", ...).\")\n+    }\n+    if (schema.isPresent) {\n+      throw new AnalysisException(\"The socket source does not support a user-specified schema.\")\n+    }\n+\n+    if (options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).isPresent) {\n+      Try(options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).get().toBoolean) match {\n+        case Success(bool) =>\n+        case Failure(_) =>\n+          throw new AnalysisException(\n+            \"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\")\n+      }\n+    }\n+\n+    new TextSocketStreamMicroBatchReader(options)\n+  }\n+}\n+\n+case class TextSocketStreamOffset(offset: Long) extends Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketStreamMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with Logging {\n+\n+  import TextSocketSourceProviderV2._\n+\n+  private var start: TextSocketStreamOffset = _\n+  private var end: TextSocketStreamOffset = _\n+\n+  private val host = options.get(HOST).get()\n+  private val port = options.get(PORT).get().toInt\n+  private val includeTimestamp = options.getBoolean(INCLUDE_TIMESTAMP, false)\n+  private val numPartitions = options.getInt(NUM_PARTITIONS, 1)\n+\n+  @GuardedBy(\"this\")\n+  private var socket: Socket = _\n+\n+  @GuardedBy(\"this\")\n+  private var readThread: Thread = _\n+\n+  @GuardedBy(\"this\")\n+  private val batches = new ListBuffer[(String, Timestamp)]\n+\n+  private val currentOffset = new AtomicLong(-1L)\n+\n+  private var initialized = false\n+\n+  @GuardedBy(\"this\")\n+  private var lastOffsetCommitted: Long = -1L\n+\n+  override def setOffsetRange(start: Optional[Offset], end: Optional[Offset]): Unit = {\n+    if (!initialized) {"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "This is what I want to bring out. Originally I initialized this in constructor like old socket source. But I found that `MicroBatchReader` will be created in two different places with two objects. So initializing in constructor will create two sock threads and connectors. This is different from V1 source. In V1 source, we only created source once, but with V2 `MicroBatchReader` we will create two objects in two different places (one for schema), which means such side-affect actions in constructor will have two copies. Ideally we should only create this `MicroBatchReader` once.",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-25T01:01:04Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {\n+  override def shortName(): String = \"socketv2\"\n+\n+  override def createMicroBatchReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceV2Options): MicroBatchReader = {\n+    logWarning(\"The socket source should not be used for production applications! \" +\n+      \"It does not support recovery.\")\n+    if (!options.get(TextSocketSourceProviderV2.HOST).isPresent) {\n+      throw new AnalysisException(\"Set a host to read from with option(\\\"host\\\", ...).\")\n+    }\n+    if (!options.get(TextSocketSourceProviderV2.PORT).isPresent) {\n+      throw new AnalysisException(\"Set a port to read from with option(\\\"port\\\", ...).\")\n+    }\n+    if (schema.isPresent) {\n+      throw new AnalysisException(\"The socket source does not support a user-specified schema.\")\n+    }\n+\n+    if (options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).isPresent) {\n+      Try(options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).get().toBoolean) match {\n+        case Success(bool) =>\n+        case Failure(_) =>\n+          throw new AnalysisException(\n+            \"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\")\n+      }\n+    }\n+\n+    new TextSocketStreamMicroBatchReader(options)\n+  }\n+}\n+\n+case class TextSocketStreamOffset(offset: Long) extends Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketStreamMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with Logging {\n+\n+  import TextSocketSourceProviderV2._\n+\n+  private var start: TextSocketStreamOffset = _\n+  private var end: TextSocketStreamOffset = _\n+\n+  private val host = options.get(HOST).get()\n+  private val port = options.get(PORT).get().toInt\n+  private val includeTimestamp = options.getBoolean(INCLUDE_TIMESTAMP, false)\n+  private val numPartitions = options.getInt(NUM_PARTITIONS, 1)\n+\n+  @GuardedBy(\"this\")\n+  private var socket: Socket = _\n+\n+  @GuardedBy(\"this\")\n+  private var readThread: Thread = _\n+\n+  @GuardedBy(\"this\")\n+  private val batches = new ListBuffer[(String, Timestamp)]\n+\n+  private val currentOffset = new AtomicLong(-1L)\n+\n+  private var initialized = false\n+\n+  @GuardedBy(\"this\")\n+  private var lastOffsetCommitted: Long = -1L\n+\n+  override def setOffsetRange(start: Optional[Offset], end: Optional[Offset]): Unit = {\n+    if (!initialized) {"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I don't think this will solve that problem, since each reader will just have its own initialize bit.\r\n\r\nIn general, I think it's fine if we do a bit of extra work. V1 sources do have to support being created multiple times (in e.g. restart scenarios), and the lifecycles of the two V2 readers being created here don't overlap. (We should be closing the tempReader created in DataStreamReader, though.)",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-25T01:39:26Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {\n+  override def shortName(): String = \"socketv2\"\n+\n+  override def createMicroBatchReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceV2Options): MicroBatchReader = {\n+    logWarning(\"The socket source should not be used for production applications! \" +\n+      \"It does not support recovery.\")\n+    if (!options.get(TextSocketSourceProviderV2.HOST).isPresent) {\n+      throw new AnalysisException(\"Set a host to read from with option(\\\"host\\\", ...).\")\n+    }\n+    if (!options.get(TextSocketSourceProviderV2.PORT).isPresent) {\n+      throw new AnalysisException(\"Set a port to read from with option(\\\"port\\\", ...).\")\n+    }\n+    if (schema.isPresent) {\n+      throw new AnalysisException(\"The socket source does not support a user-specified schema.\")\n+    }\n+\n+    if (options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).isPresent) {\n+      Try(options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).get().toBoolean) match {\n+        case Success(bool) =>\n+        case Failure(_) =>\n+          throw new AnalysisException(\n+            \"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\")\n+      }\n+    }\n+\n+    new TextSocketStreamMicroBatchReader(options)\n+  }\n+}\n+\n+case class TextSocketStreamOffset(offset: Long) extends Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketStreamMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with Logging {\n+\n+  import TextSocketSourceProviderV2._\n+\n+  private var start: TextSocketStreamOffset = _\n+  private var end: TextSocketStreamOffset = _\n+\n+  private val host = options.get(HOST).get()\n+  private val port = options.get(PORT).get().toInt\n+  private val includeTimestamp = options.getBoolean(INCLUDE_TIMESTAMP, false)\n+  private val numPartitions = options.getInt(NUM_PARTITIONS, 1)\n+\n+  @GuardedBy(\"this\")\n+  private var socket: Socket = _\n+\n+  @GuardedBy(\"this\")\n+  private var readThread: Thread = _\n+\n+  @GuardedBy(\"this\")\n+  private val batches = new ListBuffer[(String, Timestamp)]\n+\n+  private val currentOffset = new AtomicLong(-1L)\n+\n+  private var initialized = false\n+\n+  @GuardedBy(\"this\")\n+  private var lastOffsetCommitted: Long = -1L\n+\n+  override def setOffsetRange(start: Optional[Offset], end: Optional[Offset]): Unit = {\n+    if (!initialized) {"
  }],
  "prId": 20382
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "nit: conversion to int is unnecessary",
    "commit": "762f1da952eae99fc7b377a08267c0d4cdaf00ee",
    "createdAt": "2018-01-24T17:17:45Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io.{BufferedReader, InputStreamReader, IOException}\n+import java.net.Socket\n+import java.sql.Timestamp\n+import java.text.SimpleDateFormat\n+import java.util._\n+import java.util.{List => JList}\n+import java.util.concurrent.atomic.AtomicLong\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ListBuffer\n+import scala.util.{Failure, Success, Try}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{DataSourceV2, DataSourceV2Options}\n+import org.apache.spark.sql.sources.v2.reader.{DataReader, ReadTask}\n+import org.apache.spark.sql.sources.v2.streaming.MicroBatchReadSupport\n+import org.apache.spark.sql.sources.v2.streaming.reader.{MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}\n+\n+\n+object TextSocketSourceProviderV2 {\n+  val HOST = \"host\"\n+  val PORT = \"port\"\n+  val INCLUDE_TIMESTAMP = \"includeTimestamp\"\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val SCHEMA_REGULAR = StructType(StructField(\"value\", StringType) :: Nil)\n+  val SCHEMA_TIMESTAMP = StructType(StructField(\"value\", StringType) ::\n+    StructField(\"timestamp\", TimestampType) :: Nil)\n+  val DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US)\n+}\n+\n+class TextSocketSourceProviderV2 extends DataSourceV2\n+    with MicroBatchReadSupport with DataSourceRegister with Logging {\n+  override def shortName(): String = \"socketv2\"\n+\n+  override def createMicroBatchReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceV2Options): MicroBatchReader = {\n+    logWarning(\"The socket source should not be used for production applications! \" +\n+      \"It does not support recovery.\")\n+    if (!options.get(TextSocketSourceProviderV2.HOST).isPresent) {\n+      throw new AnalysisException(\"Set a host to read from with option(\\\"host\\\", ...).\")\n+    }\n+    if (!options.get(TextSocketSourceProviderV2.PORT).isPresent) {\n+      throw new AnalysisException(\"Set a port to read from with option(\\\"port\\\", ...).\")\n+    }\n+    if (schema.isPresent) {\n+      throw new AnalysisException(\"The socket source does not support a user-specified schema.\")\n+    }\n+\n+    if (options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).isPresent) {\n+      Try(options.get(TextSocketSourceProviderV2.INCLUDE_TIMESTAMP).get().toBoolean) match {\n+        case Success(bool) =>\n+        case Failure(_) =>\n+          throw new AnalysisException(\n+            \"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\")\n+      }\n+    }\n+\n+    new TextSocketStreamMicroBatchReader(options)\n+  }\n+}\n+\n+case class TextSocketStreamOffset(offset: Long) extends Offset {\n+  override def json(): String = offset.toString\n+}\n+\n+class TextSocketStreamMicroBatchReader(options: DataSourceV2Options)\n+  extends MicroBatchReader with Logging {\n+\n+  import TextSocketSourceProviderV2._\n+\n+  private var start: TextSocketStreamOffset = _\n+  private var end: TextSocketStreamOffset = _\n+\n+  private val host = options.get(HOST).get()\n+  private val port = options.get(PORT).get().toInt\n+  private val includeTimestamp = options.getBoolean(INCLUDE_TIMESTAMP, false)\n+  private val numPartitions = options.getInt(NUM_PARTITIONS, 1)\n+\n+  @GuardedBy(\"this\")\n+  private var socket: Socket = _\n+\n+  @GuardedBy(\"this\")\n+  private var readThread: Thread = _\n+\n+  @GuardedBy(\"this\")\n+  private val batches = new ListBuffer[(String, Timestamp)]\n+\n+  private val currentOffset = new AtomicLong(-1L)\n+\n+  private var initialized = false\n+\n+  @GuardedBy(\"this\")\n+  private var lastOffsetCommitted: Long = -1L\n+\n+  override def setOffsetRange(start: Optional[Offset], end: Optional[Offset]): Unit = {\n+    if (!initialized) {\n+      initialize()\n+      initialized = true\n+    }\n+\n+    this.start = start.orElse(TextSocketStreamOffset(-1L)).asInstanceOf[TextSocketStreamOffset]\n+    this.end =\n+      end.orElse(TextSocketStreamOffset(currentOffset.get())).asInstanceOf[TextSocketStreamOffset]\n+  }\n+\n+  override def getStartOffset(): Offset = {\n+    Option(start).getOrElse(throw new IllegalStateException(\"start offset not set\"))\n+  }\n+\n+  override def getEndOffset(): Offset = {\n+    Option(end).getOrElse(throw new IllegalStateException(\"end offset not set\"))\n+  }\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    TextSocketStreamOffset(json.toLong)\n+  }\n+\n+  override def readSchema(): StructType = {\n+    if (includeTimestamp) {\n+      SCHEMA_TIMESTAMP\n+    } else {\n+      SCHEMA_REGULAR\n+    }\n+  }\n+\n+  override def createReadTasks(): JList[ReadTask[Row]] = {\n+    val startOrdinal = start.offset.toInt + 1\n+    val endOrdinal = end.offset.toInt + 1\n+    val sliceStart = startOrdinal - lastOffsetCommitted.toInt - 1\n+    val sliceEnd = endOrdinal - lastOffsetCommitted.toInt - 1\n+\n+    val rawList = TextSocketStreamMicroBatchReader.this.synchronized {\n+      batches.slice(sliceStart, sliceEnd)\n+    }\n+\n+    val slices = Array.fill(numPartitions)(new ListBuffer[(String, Timestamp)])\n+    rawList.zipWithIndex.foreach { case (r, idx) =>\n+      slices(idx % numPartitions).append(r)\n+    }\n+\n+    (0 until numPartitions).map { i =>\n+      val slice = slices(i)\n+      new ReadTask[Row] {\n+        override def createDataReader(): DataReader[Row] = new DataReader[Row] {\n+          private var currentIdx = -1\n+\n+          override def next(): Boolean = {\n+            currentIdx += 1\n+            currentIdx < slice.size\n+          }\n+\n+          override def get(): Row = {\n+            Row(slice(currentIdx)._1, slice(currentIdx)._2)\n+          }\n+\n+          override def close(): Unit = {}\n+        }\n+      }\n+    }.toList.asJava\n+  }\n+\n+  override def commit(end: Offset): Unit = synchronized {\n+    val newOffset = end.asInstanceOf[TextSocketStreamOffset]\n+    val offsetDiff = (newOffset.offset - lastOffsetCommitted).toInt"
  }],
  "prId": 20382
}]