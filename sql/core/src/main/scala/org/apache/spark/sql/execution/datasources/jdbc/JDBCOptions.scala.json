[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "It does not make sense to the other data sources except MySQL. If needed, we can introduce a dialect specific checking APIs in [JdbcDialect.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala)",
    "commit": "97b6d7bc74b1895db0c772b4c0de726c6be2c3f0",
    "createdAt": "2017-07-03T16:48:42Z",
    "diffHunk": "@@ -103,10 +103,11 @@ class JDBCOptions(\n       s\" and '$JDBC_NUM_PARTITIONS' are required.\")\n   val fetchSize = {\n     val size = parameters.getOrElse(JDBC_BATCH_FETCH_SIZE, \"0\").toInt\n-    require(size >= 0,\n+    require(size >= 0 || size == Integer.MIN_VALUE,",
    "line": 5
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "+1 for @gatorsmile 's suggestion.",
    "commit": "97b6d7bc74b1895db0c772b4c0de726c6be2c3f0",
    "createdAt": "2017-07-05T02:34:23Z",
    "diffHunk": "@@ -103,10 +103,11 @@ class JDBCOptions(\n       s\" and '$JDBC_NUM_PARTITIONS' are required.\")\n   val fetchSize = {\n     val size = parameters.getOrElse(JDBC_BATCH_FETCH_SIZE, \"0\").toInt\n-    require(size >= 0,\n+    require(size >= 0 || size == Integer.MIN_VALUE,",
    "line": 5
  }],
  "prId": 18515
}]