[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "does this actually do anything?\n",
    "commit": "ff22a2c883e8236f911bd583b7e9a4da66d6e980",
    "createdAt": "2016-01-15T08:23:48Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.csv\n+\n+import java.io.{ByteArrayOutputStream, OutputStreamWriter, StringReader}\n+\n+import com.univocity.parsers.csv.{CsvParser, CsvParserSettings, CsvWriter, CsvWriterSettings}\n+\n+import org.apache.spark.Logging\n+\n+/**\n+  * Read and parse CSV-like input\n+  *\n+  * @param params Parameters object\n+  * @param headers headers for the columns\n+  */\n+private[sql] abstract class CsvReader(params: CSVParameters, headers: Seq[String]) {\n+\n+  protected lazy val parser: CsvParser = {\n+    val settings = new CsvParserSettings()\n+    val format = settings.getFormat\n+    format.setDelimiter(params.delimiter)\n+    format.setLineSeparator(params.rowSeparator)\n+    format.setQuote(params.quote)\n+    format.setQuoteEscape(params.escape)\n+    format.setComment(params.comment)\n+    settings.setIgnoreLeadingWhitespaces(params.ignoreLeadingWhiteSpaceFlag)\n+    settings.setIgnoreTrailingWhitespaces(params.ignoreTrailingWhiteSpaceFlag)\n+    settings.setReadInputOnSeparateThread(false)\n+    settings.setInputBufferSize(params.inputBufferSize)\n+    settings.setMaxColumns(params.maxColumns)\n+    settings.setNullValue(params.nullValue)\n+    settings.setMaxCharsPerColumn(params.maxCharsPerColumn)\n+    if (headers != null) settings.setHeaders(headers: _*)\n+\n+    new CsvParser(settings)\n+  }\n+}\n+\n+/**\n+  * Converts a sequence of string to CSV string\n+  *\n+  * @param params Parameters object for configuration\n+  * @param headers headers for columns\n+  */\n+private[sql] class LineCsvWriter(params: CSVParameters, headers: Seq[String]) extends Logging {\n+  private val writerSettings = new CsvWriterSettings\n+  private val format = writerSettings.getFormat\n+\n+  format.setDelimiter(params.delimiter)\n+  format.setLineSeparator(params.rowSeparator)\n+  format.setQuote(params.quote)\n+  format.setQuoteEscape(params.escape)\n+  format.setComment(params.comment)\n+\n+  writerSettings.setNullValue(params.nullValue)\n+  writerSettings.setEmptyValue(params.nullValue)\n+  writerSettings.setSkipEmptyLines(true)\n+  writerSettings.setQuoteAllFields(false)\n+  writerSettings.setHeaders(headers: _*)\n+\n+  def writeRow(row: Seq[String], includeHeader: Boolean): String = {\n+    val buffer = new ByteArrayOutputStream()\n+    val outputWriter = new OutputStreamWriter(buffer)\n+    val writer = new CsvWriter(outputWriter, writerSettings)\n+\n+    if (includeHeader) {\n+      writer.writeHeaders()\n+    }\n+    writer.writeRow(row.toArray: _*)\n+    writer.close()\n+    buffer.toString.stripLineEnd\n+  }\n+}\n+\n+/**\n+  * Parser for parsing a line at a time. Not efficient for bulk data.\n+  *\n+  * @param params Parameters object\n+  */\n+private[sql] class LineCsvReader(params: CSVParameters)\n+  extends CsvReader(params, null) {\n+  /**\n+    * parse a line\n+    *\n+    * @param line a String with no newline at the end\n+    * @return array of strings where each string is a field in the CSV record\n+    */\n+  def parseLine(line: String): Array[String] = {\n+    parser.beginParsing(new StringReader(line))\n+    val parsed = parser.parseNext()\n+    parser.stopParsing()\n+    parsed\n+  }\n+}\n+\n+/**\n+  * Parser for parsing lines in bulk. Use this when efficiency is desired.\n+  *\n+  * @param iter iterator over lines in the file\n+  * @param params Parameters object\n+  * @param headers headers for the columns\n+  */\n+private[sql] class BulkCsvReader(\n+    iter: Iterator[String],\n+    params: CSVParameters,\n+    headers: Seq[String])\n+  extends CsvReader(params, headers) with Iterator[Array[String]] {\n+\n+  private val reader = new StringIteratorReader(iter)\n+  parser.beginParsing(reader)\n+  private var nextRecord = parser.parseNext()\n+\n+  /**\n+    * get the next parsed line.\n+    * @return array of strings where each string is a field in the CSV record\n+    */\n+  override def next(): Array[String] = {\n+    val curRecord = nextRecord\n+    if(curRecord != null) {\n+      nextRecord = parser.parseNext()\n+    } else {\n+      throw new NoSuchElementException(\"next record is null\")\n+    }\n+    curRecord\n+  }\n+\n+  override def hasNext: Boolean = nextRecord != null\n+\n+}\n+\n+/**\n+  * A Reader that \"reads\" from a sequence of lines. Spark's textFile method removes newlines at\n+  * end of each line Univocity parser requires a Reader that provides access to the data to be\n+  * parsed and needs the newlines to be present\n+  * @param iter iterator over RDD[String]\n+  */\n+private class StringIteratorReader(val iter: Iterator[String]) extends java.io.Reader {\n+\n+  private var next: Long = 0\n+  private var length: Long = 0  // length of input so far\n+  private var start: Long = 0\n+  private var str: String = null   // current string from iter\n+\n+  /**\n+    * fetch next string from iter, if done with current one\n+    * pretend there is a new line at the end of every string we get from from iter\n+    */\n+  private def refill(): Unit = {\n+    if (length == next) {\n+      if (iter.hasNext) {\n+        str = iter.next()\n+        start = length\n+        length += (str.length + 1) // allowance for newline removed by SparkContext.textFile()\n+      } else {\n+        str = null\n+      }\n+    }\n+  }\n+\n+  /**\n+    * read the next character, if at end of string pretend there is a new line\n+    */\n+  override def read(): Int = {\n+    refill()\n+    if (next >= length) {\n+      -1\n+    } else {\n+      val cur = next - start\n+      next += 1\n+      if (cur == str.length) '\\n' else str.charAt(cur.toInt)\n+    }\n+  }\n+\n+  /**\n+    * read from str into cbuf\n+    */\n+  override def read(cbuf: Array[Char], off: Int, len: Int): Int = {\n+    refill()\n+    var n = 0\n+    if ((off < 0) || (off > cbuf.length) || (len < 0) ||\n+      ((off + len) > cbuf.length) || ((off + len) < 0)) {\n+      throw new IndexOutOfBoundsException()\n+    } else if (len == 0) {\n+      n = 0\n+    } else {\n+      if (next >= length) {   // end of input\n+        n = -1\n+      } else {\n+        n = Math.min(length - next, len).toInt // lesser of amount of input available or buf size\n+        if (n == length - next) {\n+          str.getChars((next - start).toInt, (next - start + n - 1).toInt, cbuf, off)",
    "line": 207
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "Yes, this call copies the content of `src` to `cbuf` \n",
    "commit": "ff22a2c883e8236f911bd583b7e9a4da66d6e980",
    "createdAt": "2016-01-15T20:03:34Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.csv\n+\n+import java.io.{ByteArrayOutputStream, OutputStreamWriter, StringReader}\n+\n+import com.univocity.parsers.csv.{CsvParser, CsvParserSettings, CsvWriter, CsvWriterSettings}\n+\n+import org.apache.spark.Logging\n+\n+/**\n+  * Read and parse CSV-like input\n+  *\n+  * @param params Parameters object\n+  * @param headers headers for the columns\n+  */\n+private[sql] abstract class CsvReader(params: CSVParameters, headers: Seq[String]) {\n+\n+  protected lazy val parser: CsvParser = {\n+    val settings = new CsvParserSettings()\n+    val format = settings.getFormat\n+    format.setDelimiter(params.delimiter)\n+    format.setLineSeparator(params.rowSeparator)\n+    format.setQuote(params.quote)\n+    format.setQuoteEscape(params.escape)\n+    format.setComment(params.comment)\n+    settings.setIgnoreLeadingWhitespaces(params.ignoreLeadingWhiteSpaceFlag)\n+    settings.setIgnoreTrailingWhitespaces(params.ignoreTrailingWhiteSpaceFlag)\n+    settings.setReadInputOnSeparateThread(false)\n+    settings.setInputBufferSize(params.inputBufferSize)\n+    settings.setMaxColumns(params.maxColumns)\n+    settings.setNullValue(params.nullValue)\n+    settings.setMaxCharsPerColumn(params.maxCharsPerColumn)\n+    if (headers != null) settings.setHeaders(headers: _*)\n+\n+    new CsvParser(settings)\n+  }\n+}\n+\n+/**\n+  * Converts a sequence of string to CSV string\n+  *\n+  * @param params Parameters object for configuration\n+  * @param headers headers for columns\n+  */\n+private[sql] class LineCsvWriter(params: CSVParameters, headers: Seq[String]) extends Logging {\n+  private val writerSettings = new CsvWriterSettings\n+  private val format = writerSettings.getFormat\n+\n+  format.setDelimiter(params.delimiter)\n+  format.setLineSeparator(params.rowSeparator)\n+  format.setQuote(params.quote)\n+  format.setQuoteEscape(params.escape)\n+  format.setComment(params.comment)\n+\n+  writerSettings.setNullValue(params.nullValue)\n+  writerSettings.setEmptyValue(params.nullValue)\n+  writerSettings.setSkipEmptyLines(true)\n+  writerSettings.setQuoteAllFields(false)\n+  writerSettings.setHeaders(headers: _*)\n+\n+  def writeRow(row: Seq[String], includeHeader: Boolean): String = {\n+    val buffer = new ByteArrayOutputStream()\n+    val outputWriter = new OutputStreamWriter(buffer)\n+    val writer = new CsvWriter(outputWriter, writerSettings)\n+\n+    if (includeHeader) {\n+      writer.writeHeaders()\n+    }\n+    writer.writeRow(row.toArray: _*)\n+    writer.close()\n+    buffer.toString.stripLineEnd\n+  }\n+}\n+\n+/**\n+  * Parser for parsing a line at a time. Not efficient for bulk data.\n+  *\n+  * @param params Parameters object\n+  */\n+private[sql] class LineCsvReader(params: CSVParameters)\n+  extends CsvReader(params, null) {\n+  /**\n+    * parse a line\n+    *\n+    * @param line a String with no newline at the end\n+    * @return array of strings where each string is a field in the CSV record\n+    */\n+  def parseLine(line: String): Array[String] = {\n+    parser.beginParsing(new StringReader(line))\n+    val parsed = parser.parseNext()\n+    parser.stopParsing()\n+    parsed\n+  }\n+}\n+\n+/**\n+  * Parser for parsing lines in bulk. Use this when efficiency is desired.\n+  *\n+  * @param iter iterator over lines in the file\n+  * @param params Parameters object\n+  * @param headers headers for the columns\n+  */\n+private[sql] class BulkCsvReader(\n+    iter: Iterator[String],\n+    params: CSVParameters,\n+    headers: Seq[String])\n+  extends CsvReader(params, headers) with Iterator[Array[String]] {\n+\n+  private val reader = new StringIteratorReader(iter)\n+  parser.beginParsing(reader)\n+  private var nextRecord = parser.parseNext()\n+\n+  /**\n+    * get the next parsed line.\n+    * @return array of strings where each string is a field in the CSV record\n+    */\n+  override def next(): Array[String] = {\n+    val curRecord = nextRecord\n+    if(curRecord != null) {\n+      nextRecord = parser.parseNext()\n+    } else {\n+      throw new NoSuchElementException(\"next record is null\")\n+    }\n+    curRecord\n+  }\n+\n+  override def hasNext: Boolean = nextRecord != null\n+\n+}\n+\n+/**\n+  * A Reader that \"reads\" from a sequence of lines. Spark's textFile method removes newlines at\n+  * end of each line Univocity parser requires a Reader that provides access to the data to be\n+  * parsed and needs the newlines to be present\n+  * @param iter iterator over RDD[String]\n+  */\n+private class StringIteratorReader(val iter: Iterator[String]) extends java.io.Reader {\n+\n+  private var next: Long = 0\n+  private var length: Long = 0  // length of input so far\n+  private var start: Long = 0\n+  private var str: String = null   // current string from iter\n+\n+  /**\n+    * fetch next string from iter, if done with current one\n+    * pretend there is a new line at the end of every string we get from from iter\n+    */\n+  private def refill(): Unit = {\n+    if (length == next) {\n+      if (iter.hasNext) {\n+        str = iter.next()\n+        start = length\n+        length += (str.length + 1) // allowance for newline removed by SparkContext.textFile()\n+      } else {\n+        str = null\n+      }\n+    }\n+  }\n+\n+  /**\n+    * read the next character, if at end of string pretend there is a new line\n+    */\n+  override def read(): Int = {\n+    refill()\n+    if (next >= length) {\n+      -1\n+    } else {\n+      val cur = next - start\n+      next += 1\n+      if (cur == str.length) '\\n' else str.charAt(cur.toInt)\n+    }\n+  }\n+\n+  /**\n+    * read from str into cbuf\n+    */\n+  override def read(cbuf: Array[Char], off: Int, len: Int): Int = {\n+    refill()\n+    var n = 0\n+    if ((off < 0) || (off > cbuf.length) || (len < 0) ||\n+      ((off + len) > cbuf.length) || ((off + len) < 0)) {\n+      throw new IndexOutOfBoundsException()\n+    } else if (len == 0) {\n+      n = 0\n+    } else {\n+      if (next >= length) {   // end of input\n+        n = -1\n+      } else {\n+        n = Math.min(length - next, len).toInt // lesser of amount of input available or buf size\n+        if (n == length - next) {\n+          str.getChars((next - start).toInt, (next - start + n - 1).toInt, cbuf, off)",
    "line": 207
  }],
  "prId": 10766
}]