[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "It would be good to add a description of what these functions do instead of having empty auto-generated scala doc.  In most cases, I think its okay to omit the `@param` notations, unless its not obvious what the parameters do or it is a public user-facing API.\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:43:31Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.spark.sql.execution.{ExistingRdd, LeafNode, UnaryNode, SparkPlan}\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.{TaskContext, SerializableWritable}\r\n+import org.apache.spark.rdd.RDD\r\n+\r\n+import _root_.parquet.hadoop.util.ContextUtil\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, FileOutputCommitter}\r\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\r\n+import org.apache.hadoop.io.{Writable, NullWritable}\r\n+import org.apache.hadoop.mapreduce.{TaskID, TaskAttemptContext, Job}\r\n+\r\n+import org.apache.hadoop.hive.ql.io.orc.{OrcFile, OrcSerde, OrcInputFormat, OrcOutputFormat}\r\n+import org.apache.hadoop.hive.serde2.objectinspector._\r\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\r\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\r\n+\r\n+import java.io.IOException\r\n+import java.text.SimpleDateFormat\r\n+import java.util.{Locale, Date}\r\n+import scala.collection.JavaConversions._\r\n+import org.apache.hadoop.mapred.{SparkHadoopMapRedUtil, Reporter, JobConf}\r\n+\r\n+/**\r\n+ * orc table scan operator. Imports the file that backs the given\r\n+ * [[org.apache.spark.sql.orc.OrcRelation]] as a ``RDD[Row]``.\r\n+ */\r\n+case class OrcTableScan(\r\n+   output: Seq[Attribute],\r\n+   relation: OrcRelation,\r\n+   columnPruningPred: Option[Expression])\r\n+  extends LeafNode {\r\n+\r\n+  @transient\r\n+  lazy val serde: OrcSerde = initSerde\r\n+\r\n+  @transient\r\n+  lazy val getFieldValue: Seq[Product => Any] = {\r\n+    val inspector = serde.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+    output.map(attr => {\r\n+      val ref = inspector.getStructFieldRef(attr.name.toLowerCase(Locale.ENGLISH))\r\n+      row: Product => {\r\n+        val fieldData = row.productElement(1)\r\n+        val data = inspector.getStructFieldData(fieldData, ref)\r\n+        unwrapData(data, ref.getFieldObjectInspector)\r\n+      }\r\n+    })\r\n+  }\r\n+\r\n+  private def initSerde(): OrcSerde = {\r\n+    val serde = new OrcSerde\r\n+    serde.initialize(null, relation.prop)\r\n+    serde\r\n+  }\r\n+\r\n+  def unwrapData(data: Any, oi: ObjectInspector): Any = oi match {\r\n+    case pi: PrimitiveObjectInspector => pi.getPrimitiveJavaObject(data)\r\n+    case li: ListObjectInspector =>\r\n+      Option(li.getList(data))\r\n+        .map(_.map(unwrapData(_, li.getListElementObjectInspector)).toSeq)\r\n+        .orNull\r\n+    case mi: MapObjectInspector =>\r\n+      Option(mi.getMap(data)).map(\r\n+        _.map {\r\n+          case (k, v) =>\r\n+            (unwrapData(k, mi.getMapKeyObjectInspector),\r\n+              unwrapData(v, mi.getMapValueObjectInspector))\r\n+        }.toMap).orNull\r\n+    case si: StructObjectInspector =>\r\n+      val allRefs = si.getAllStructFieldRefs\r\n+      new GenericRow(\r\n+        allRefs.map(r =>\r\n+          unwrapData(si.getStructFieldData(data, r), r.getFieldObjectInspector)).toArray)\r\n+  }\r\n+\r\n+  override def execute(): RDD[Row] = {\r\n+    val sc = sqlContext.sparkContext\r\n+    val job = new Job(sc.hadoopConfiguration)\r\n+\r\n+    val conf: Configuration = ContextUtil.getConfiguration(job)\r\n+    val fileList = FileSystemHelper.listFiles(relation.path, conf)\r\n+\r\n+    // add all paths in the directory but skip \"hidden\" ones such\r\n+    // as \"_SUCCESS\"\r\n+    for (path <- fileList if !path.getName.startsWith(\"_\")) {\r\n+      FileInputFormat.addInputPath(job, path)\r\n+    }\r\n+    val serialConf = sc.broadcast(new SerializableWritable(conf))\r\n+\r\n+    setColumnIds(output, relation, conf)\r\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\r\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[Void, Row]]]\r\n+\r\n+    val rowRdd = productToRowRdd(sc.hadoopRDD[Void, Row](\r\n+      serialConf.value.value.asInstanceOf[JobConf], inputClass, classOf[Void], classOf[Row]))\r\n+    rowRdd\r\n+  }\r\n+\r\n+  /**\r\n+   * @param output\r\n+   * @param relation\r\n+   * @param conf\r"
  }],
  "prId": 2576
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Indent 4 spaces from `private`.\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:44:09Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.spark.sql.execution.{ExistingRdd, LeafNode, UnaryNode, SparkPlan}\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.{TaskContext, SerializableWritable}\r\n+import org.apache.spark.rdd.RDD\r\n+\r\n+import _root_.parquet.hadoop.util.ContextUtil\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, FileOutputCommitter}\r\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\r\n+import org.apache.hadoop.io.{Writable, NullWritable}\r\n+import org.apache.hadoop.mapreduce.{TaskID, TaskAttemptContext, Job}\r\n+\r\n+import org.apache.hadoop.hive.ql.io.orc.{OrcFile, OrcSerde, OrcInputFormat, OrcOutputFormat}\r\n+import org.apache.hadoop.hive.serde2.objectinspector._\r\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\r\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\r\n+\r\n+import java.io.IOException\r\n+import java.text.SimpleDateFormat\r\n+import java.util.{Locale, Date}\r\n+import scala.collection.JavaConversions._\r\n+import org.apache.hadoop.mapred.{SparkHadoopMapRedUtil, Reporter, JobConf}\r\n+\r\n+/**\r\n+ * orc table scan operator. Imports the file that backs the given\r\n+ * [[org.apache.spark.sql.orc.OrcRelation]] as a ``RDD[Row]``.\r\n+ */\r\n+case class OrcTableScan(\r\n+   output: Seq[Attribute],\r\n+   relation: OrcRelation,\r\n+   columnPruningPred: Option[Expression])\r\n+  extends LeafNode {\r\n+\r\n+  @transient\r\n+  lazy val serde: OrcSerde = initSerde\r\n+\r\n+  @transient\r\n+  lazy val getFieldValue: Seq[Product => Any] = {\r\n+    val inspector = serde.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+    output.map(attr => {\r\n+      val ref = inspector.getStructFieldRef(attr.name.toLowerCase(Locale.ENGLISH))\r\n+      row: Product => {\r\n+        val fieldData = row.productElement(1)\r\n+        val data = inspector.getStructFieldData(fieldData, ref)\r\n+        unwrapData(data, ref.getFieldObjectInspector)\r\n+      }\r\n+    })\r\n+  }\r\n+\r\n+  private def initSerde(): OrcSerde = {\r\n+    val serde = new OrcSerde\r\n+    serde.initialize(null, relation.prop)\r\n+    serde\r\n+  }\r\n+\r\n+  def unwrapData(data: Any, oi: ObjectInspector): Any = oi match {\r\n+    case pi: PrimitiveObjectInspector => pi.getPrimitiveJavaObject(data)\r\n+    case li: ListObjectInspector =>\r\n+      Option(li.getList(data))\r\n+        .map(_.map(unwrapData(_, li.getListElementObjectInspector)).toSeq)\r\n+        .orNull\r\n+    case mi: MapObjectInspector =>\r\n+      Option(mi.getMap(data)).map(\r\n+        _.map {\r\n+          case (k, v) =>\r\n+            (unwrapData(k, mi.getMapKeyObjectInspector),\r\n+              unwrapData(v, mi.getMapValueObjectInspector))\r\n+        }.toMap).orNull\r\n+    case si: StructObjectInspector =>\r\n+      val allRefs = si.getAllStructFieldRefs\r\n+      new GenericRow(\r\n+        allRefs.map(r =>\r\n+          unwrapData(si.getStructFieldData(data, r), r.getFieldObjectInspector)).toArray)\r\n+  }\r\n+\r\n+  override def execute(): RDD[Row] = {\r\n+    val sc = sqlContext.sparkContext\r\n+    val job = new Job(sc.hadoopConfiguration)\r\n+\r\n+    val conf: Configuration = ContextUtil.getConfiguration(job)\r\n+    val fileList = FileSystemHelper.listFiles(relation.path, conf)\r\n+\r\n+    // add all paths in the directory but skip \"hidden\" ones such\r\n+    // as \"_SUCCESS\"\r\n+    for (path <- fileList if !path.getName.startsWith(\"_\")) {\r\n+      FileInputFormat.addInputPath(job, path)\r\n+    }\r\n+    val serialConf = sc.broadcast(new SerializableWritable(conf))\r\n+\r\n+    setColumnIds(output, relation, conf)\r\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\r\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[Void, Row]]]\r\n+\r\n+    val rowRdd = productToRowRdd(sc.hadoopRDD[Void, Row](\r\n+      serialConf.value.value.asInstanceOf[JobConf], inputClass, classOf[Void], classOf[Row]))\r\n+    rowRdd\r\n+  }\r\n+\r\n+  /**\r\n+   * @param output\r\n+   * @param relation\r\n+   * @param conf\r\n+   */\r\n+  def setColumnIds(output: Seq[Attribute], relation: OrcRelation, conf: Configuration) {\r\n+    val idBuff = new StringBuilder()\r\n+\r\n+    output.map(att => {\r\n+      val realName = att.name.toLowerCase(Locale.ENGLISH)\r\n+      val id = relation.fieldIdCache.getOrElse(realName, null)\r\n+      if (null != id) {\r\n+        idBuff.append(id)\r\n+        idBuff.append(\",\")\r\n+      }\r\n+    })\r\n+    if (idBuff.length > 0) {\r\n+      idBuff.setLength(idBuff.length - 1)\r\n+    }\r\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, idBuff.toString())\r\n+  }\r\n+\r\n+  /**\r\n+   *\r\n+   * @param data\r\n+   * @tparam A\r\n+   * @return\r\n+   */\r\n+  def productToRowRdd[A <: Product](data: RDD[A]): RDD[Row] = {\r\n+    data.mapPartitions {\r\n+      iterator =>\r\n+        if (iterator.isEmpty) {\r\n+          Iterator.empty\r\n+        } else {\r\n+          val bufferedIterator = iterator.buffered\r\n+          bufferedIterator.map {r =>\r\n+              val values = getFieldValue.map(_(r))\r\n+              new GenericRow(values.map {\r\n+                case n: String if n.toLowerCase == \"null\" => \"\"\r\n+                case varchar: HiveVarchar => varchar.getValue\r\n+                case decimal: HiveDecimal =>\r\n+                  BigDecimal(decimal.bigDecimalValue)\r\n+                case null => \"\"\r\n+                case other => other\r\n+\r\n+              }.toArray)\r\n+          }\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * Applies a (candidate) projection.\r\n+   *\r\n+   * @param prunedAttributes The list of attributes to be used in the projection.\r\n+   * @return Pruned TableScan.\r\n+   */\r\n+  def pruneColumns(prunedAttributes: Seq[Attribute]): OrcTableScan = {\r\n+    OrcTableScan(prunedAttributes, relation, columnPruningPred)\r\n+  }\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ * @param relation\r\n+ * @param child\r\n+ * @param overwrite\r\n+ */\r\n+private[sql] case class InsertIntoOrcTable(\r\n+    relation: OrcRelation,\r\n+    child: SparkPlan,\r\n+    overwrite: Boolean = false)\r\n+  extends UnaryNode with SparkHadoopMapRedUtil with org.apache.spark.Logging {\r\n+\r\n+  override def output = child.output\r\n+\r\n+  val intputClass: Class[_] = getInputClass\r\n+\r\n+  @transient val sc = sqlContext.sparkContext\r\n+\r\n+  val inspector = sc.broadcast(getInspector(intputClass))\r\n+\r\n+  @transient lazy val orcSerde = initFieldInfo\r\n+\r\n+  private def getInputClass: Class[_] = {\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val productClass = existRdd.rdd.firstParent.elementClassTag.runtimeClass\r\n+    logInfo(\"productClass is \" + productClass)\r\n+    val clazz = productClass\r\n+    if (null == relation.rowClass) {\r\n+      relation.rowClass = clazz\r\n+    }\r\n+    clazz\r\n+  }\r\n+\r\n+  private def getInspector(clazz: Class[_]): ObjectInspector = {\r\n+    val inspector = ObjectInspectorFactory.getReflectionObjectInspector(clazz,\r\n+      ObjectInspectorFactory.ObjectInspectorOptions.JAVA)\r\n+    inspector\r\n+  }\r\n+\r\n+  private def initFieldInfo(): OrcSerde = {\r\n+    val serde: OrcSerde = new OrcSerde\r\n+    serde\r\n+  }\r\n+\r\n+  /**\r\n+   * Inserts all rows into the Orc file.\r\n+   */\r\n+  override def execute() = {\r\n+    val childRdd = child.execute()\r\n+    assert(childRdd != null)\r\n+\r\n+    val job = new Job(sqlContext.sparkContext.hadoopConfiguration)\r\n+    // TODO: move that to function in object\r\n+    val conf = job.getConfiguration\r\n+\r\n+    val fspath = new Path(relation.path)\r\n+    val fs = fspath.getFileSystem(conf)\r\n+\r\n+    if (overwrite) {\r\n+      try {\r\n+        fs.delete(fspath, true)\r\n+      } catch {\r\n+        case e: IOException =>\r\n+          throw new IOException(\r\n+            s\"Unable to clear output directory ${fspath.toString} prior\"\r\n+              + s\" to InsertIntoOrcTable:\\n${e.toString}\")\r\n+      }\r\n+    }\r\n+\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val parentRdd = existRdd.rdd.firstParent[Product]\r\n+    val writableRdd = parentRdd.map(obj => {\r\n+      orcSerde.serialize(obj, inspector.value)\r\n+    })\r\n+\r\n+    saveAsHadoopFile(writableRdd, relation.rowClass, relation.path, conf)\r\n+\r\n+    // We return the child RDD to allow chaining (alternatively, one could return nothing).\r\n+    childRdd\r\n+  }\r\n+\r\n+\r\n+  // based on ``saveAsNewAPIHadoopFile`` in [[PairRDDFunctions]]\r\n+  // TODO: Maybe PairRDDFunctions should use Product2 instead of Tuple2?\r\n+  // .. then we could use the default one and could use [[MutablePair]]\r\n+  // instead of ``Tuple2``\r\n+  private def saveAsHadoopFile(\r\n+                                rdd: RDD[Writable],\r"
  }],
  "prId": 2576
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Indent 4 spaces from `def`.\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:44:22Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.spark.sql.execution.{ExistingRdd, LeafNode, UnaryNode, SparkPlan}\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.{TaskContext, SerializableWritable}\r\n+import org.apache.spark.rdd.RDD\r\n+\r\n+import _root_.parquet.hadoop.util.ContextUtil\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, FileOutputCommitter}\r\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\r\n+import org.apache.hadoop.io.{Writable, NullWritable}\r\n+import org.apache.hadoop.mapreduce.{TaskID, TaskAttemptContext, Job}\r\n+\r\n+import org.apache.hadoop.hive.ql.io.orc.{OrcFile, OrcSerde, OrcInputFormat, OrcOutputFormat}\r\n+import org.apache.hadoop.hive.serde2.objectinspector._\r\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\r\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\r\n+\r\n+import java.io.IOException\r\n+import java.text.SimpleDateFormat\r\n+import java.util.{Locale, Date}\r\n+import scala.collection.JavaConversions._\r\n+import org.apache.hadoop.mapred.{SparkHadoopMapRedUtil, Reporter, JobConf}\r\n+\r\n+/**\r\n+ * orc table scan operator. Imports the file that backs the given\r\n+ * [[org.apache.spark.sql.orc.OrcRelation]] as a ``RDD[Row]``.\r\n+ */\r\n+case class OrcTableScan(\r\n+   output: Seq[Attribute],\r\n+   relation: OrcRelation,\r\n+   columnPruningPred: Option[Expression])\r\n+  extends LeafNode {\r\n+\r\n+  @transient\r\n+  lazy val serde: OrcSerde = initSerde\r\n+\r\n+  @transient\r\n+  lazy val getFieldValue: Seq[Product => Any] = {\r\n+    val inspector = serde.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+    output.map(attr => {\r\n+      val ref = inspector.getStructFieldRef(attr.name.toLowerCase(Locale.ENGLISH))\r\n+      row: Product => {\r\n+        val fieldData = row.productElement(1)\r\n+        val data = inspector.getStructFieldData(fieldData, ref)\r\n+        unwrapData(data, ref.getFieldObjectInspector)\r\n+      }\r\n+    })\r\n+  }\r\n+\r\n+  private def initSerde(): OrcSerde = {\r\n+    val serde = new OrcSerde\r\n+    serde.initialize(null, relation.prop)\r\n+    serde\r\n+  }\r\n+\r\n+  def unwrapData(data: Any, oi: ObjectInspector): Any = oi match {\r\n+    case pi: PrimitiveObjectInspector => pi.getPrimitiveJavaObject(data)\r\n+    case li: ListObjectInspector =>\r\n+      Option(li.getList(data))\r\n+        .map(_.map(unwrapData(_, li.getListElementObjectInspector)).toSeq)\r\n+        .orNull\r\n+    case mi: MapObjectInspector =>\r\n+      Option(mi.getMap(data)).map(\r\n+        _.map {\r\n+          case (k, v) =>\r\n+            (unwrapData(k, mi.getMapKeyObjectInspector),\r\n+              unwrapData(v, mi.getMapValueObjectInspector))\r\n+        }.toMap).orNull\r\n+    case si: StructObjectInspector =>\r\n+      val allRefs = si.getAllStructFieldRefs\r\n+      new GenericRow(\r\n+        allRefs.map(r =>\r\n+          unwrapData(si.getStructFieldData(data, r), r.getFieldObjectInspector)).toArray)\r\n+  }\r\n+\r\n+  override def execute(): RDD[Row] = {\r\n+    val sc = sqlContext.sparkContext\r\n+    val job = new Job(sc.hadoopConfiguration)\r\n+\r\n+    val conf: Configuration = ContextUtil.getConfiguration(job)\r\n+    val fileList = FileSystemHelper.listFiles(relation.path, conf)\r\n+\r\n+    // add all paths in the directory but skip \"hidden\" ones such\r\n+    // as \"_SUCCESS\"\r\n+    for (path <- fileList if !path.getName.startsWith(\"_\")) {\r\n+      FileInputFormat.addInputPath(job, path)\r\n+    }\r\n+    val serialConf = sc.broadcast(new SerializableWritable(conf))\r\n+\r\n+    setColumnIds(output, relation, conf)\r\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\r\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[Void, Row]]]\r\n+\r\n+    val rowRdd = productToRowRdd(sc.hadoopRDD[Void, Row](\r\n+      serialConf.value.value.asInstanceOf[JobConf], inputClass, classOf[Void], classOf[Row]))\r\n+    rowRdd\r\n+  }\r\n+\r\n+  /**\r\n+   * @param output\r\n+   * @param relation\r\n+   * @param conf\r\n+   */\r\n+  def setColumnIds(output: Seq[Attribute], relation: OrcRelation, conf: Configuration) {\r\n+    val idBuff = new StringBuilder()\r\n+\r\n+    output.map(att => {\r\n+      val realName = att.name.toLowerCase(Locale.ENGLISH)\r\n+      val id = relation.fieldIdCache.getOrElse(realName, null)\r\n+      if (null != id) {\r\n+        idBuff.append(id)\r\n+        idBuff.append(\",\")\r\n+      }\r\n+    })\r\n+    if (idBuff.length > 0) {\r\n+      idBuff.setLength(idBuff.length - 1)\r\n+    }\r\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, idBuff.toString())\r\n+  }\r\n+\r\n+  /**\r\n+   *\r\n+   * @param data\r\n+   * @tparam A\r\n+   * @return\r\n+   */\r\n+  def productToRowRdd[A <: Product](data: RDD[A]): RDD[Row] = {\r\n+    data.mapPartitions {\r\n+      iterator =>\r\n+        if (iterator.isEmpty) {\r\n+          Iterator.empty\r\n+        } else {\r\n+          val bufferedIterator = iterator.buffered\r\n+          bufferedIterator.map {r =>\r\n+              val values = getFieldValue.map(_(r))\r\n+              new GenericRow(values.map {\r\n+                case n: String if n.toLowerCase == \"null\" => \"\"\r\n+                case varchar: HiveVarchar => varchar.getValue\r\n+                case decimal: HiveDecimal =>\r\n+                  BigDecimal(decimal.bigDecimalValue)\r\n+                case null => \"\"\r\n+                case other => other\r\n+\r\n+              }.toArray)\r\n+          }\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * Applies a (candidate) projection.\r\n+   *\r\n+   * @param prunedAttributes The list of attributes to be used in the projection.\r\n+   * @return Pruned TableScan.\r\n+   */\r\n+  def pruneColumns(prunedAttributes: Seq[Attribute]): OrcTableScan = {\r\n+    OrcTableScan(prunedAttributes, relation, columnPruningPred)\r\n+  }\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ * @param relation\r\n+ * @param child\r\n+ * @param overwrite\r\n+ */\r\n+private[sql] case class InsertIntoOrcTable(\r\n+    relation: OrcRelation,\r\n+    child: SparkPlan,\r\n+    overwrite: Boolean = false)\r\n+  extends UnaryNode with SparkHadoopMapRedUtil with org.apache.spark.Logging {\r\n+\r\n+  override def output = child.output\r\n+\r\n+  val intputClass: Class[_] = getInputClass\r\n+\r\n+  @transient val sc = sqlContext.sparkContext\r\n+\r\n+  val inspector = sc.broadcast(getInspector(intputClass))\r\n+\r\n+  @transient lazy val orcSerde = initFieldInfo\r\n+\r\n+  private def getInputClass: Class[_] = {\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val productClass = existRdd.rdd.firstParent.elementClassTag.runtimeClass\r\n+    logInfo(\"productClass is \" + productClass)\r\n+    val clazz = productClass\r\n+    if (null == relation.rowClass) {\r\n+      relation.rowClass = clazz\r\n+    }\r\n+    clazz\r\n+  }\r\n+\r\n+  private def getInspector(clazz: Class[_]): ObjectInspector = {\r\n+    val inspector = ObjectInspectorFactory.getReflectionObjectInspector(clazz,\r\n+      ObjectInspectorFactory.ObjectInspectorOptions.JAVA)\r\n+    inspector\r\n+  }\r\n+\r\n+  private def initFieldInfo(): OrcSerde = {\r\n+    val serde: OrcSerde = new OrcSerde\r\n+    serde\r\n+  }\r\n+\r\n+  /**\r\n+   * Inserts all rows into the Orc file.\r\n+   */\r\n+  override def execute() = {\r\n+    val childRdd = child.execute()\r\n+    assert(childRdd != null)\r\n+\r\n+    val job = new Job(sqlContext.sparkContext.hadoopConfiguration)\r\n+    // TODO: move that to function in object\r\n+    val conf = job.getConfiguration\r\n+\r\n+    val fspath = new Path(relation.path)\r\n+    val fs = fspath.getFileSystem(conf)\r\n+\r\n+    if (overwrite) {\r\n+      try {\r\n+        fs.delete(fspath, true)\r\n+      } catch {\r\n+        case e: IOException =>\r\n+          throw new IOException(\r\n+            s\"Unable to clear output directory ${fspath.toString} prior\"\r\n+              + s\" to InsertIntoOrcTable:\\n${e.toString}\")\r\n+      }\r\n+    }\r\n+\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val parentRdd = existRdd.rdd.firstParent[Product]\r\n+    val writableRdd = parentRdd.map(obj => {\r\n+      orcSerde.serialize(obj, inspector.value)\r\n+    })\r\n+\r\n+    saveAsHadoopFile(writableRdd, relation.rowClass, relation.path, conf)\r\n+\r\n+    // We return the child RDD to allow chaining (alternatively, one could return nothing).\r\n+    childRdd\r\n+  }\r\n+\r\n+\r\n+  // based on ``saveAsNewAPIHadoopFile`` in [[PairRDDFunctions]]\r\n+  // TODO: Maybe PairRDDFunctions should use Product2 instead of Tuple2?\r\n+  // .. then we could use the default one and could use [[MutablePair]]\r\n+  // instead of ``Tuple2``\r\n+  private def saveAsHadoopFile(\r\n+                                rdd: RDD[Writable],\r\n+                                rowClass: Class[_],\r\n+                                path: String,\r\n+                                @transient conf: Configuration) {\r\n+    val job = new Job(conf)\r\n+    val keyType = classOf[Void]\r\n+    job.setOutputKeyClass(keyType)\r\n+    job.setOutputValueClass(classOf[Writable])\r\n+    FileOutputFormat.setOutputPath(job, new Path(path))\r\n+\r\n+    val wrappedConf = new SerializableWritable(job.getConfiguration)\r\n+\r\n+    val formatter = new SimpleDateFormat(\"yyyyMMddHHmm\")\r\n+    val jobtrackerID = formatter.format(new Date())\r\n+    val stageId = sqlContext.sparkContext.newRddId()\r\n+\r\n+    val taskIdOffset =\r\n+      if (overwrite) {\r\n+        1\r\n+      } else {\r\n+        FileSystemHelper\r\n+          .findMaxTaskId(FileOutputFormat.getOutputPath(job).toString, job.getConfiguration) + 1\r\n+      }\r\n+\r\n+    def getWriter(\r\n+                   outFormat: OrcOutputFormat,\r"
  }],
  "prId": 2576
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Fill this in please.\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:46:08Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.spark.sql.execution.{ExistingRdd, LeafNode, UnaryNode, SparkPlan}\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.{TaskContext, SerializableWritable}\r\n+import org.apache.spark.rdd.RDD\r\n+\r\n+import _root_.parquet.hadoop.util.ContextUtil\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, FileOutputCommitter}\r\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\r\n+import org.apache.hadoop.io.{Writable, NullWritable}\r\n+import org.apache.hadoop.mapreduce.{TaskID, TaskAttemptContext, Job}\r\n+\r\n+import org.apache.hadoop.hive.ql.io.orc.{OrcFile, OrcSerde, OrcInputFormat, OrcOutputFormat}\r\n+import org.apache.hadoop.hive.serde2.objectinspector._\r\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\r\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\r\n+\r\n+import java.io.IOException\r\n+import java.text.SimpleDateFormat\r\n+import java.util.{Locale, Date}\r\n+import scala.collection.JavaConversions._\r\n+import org.apache.hadoop.mapred.{SparkHadoopMapRedUtil, Reporter, JobConf}\r\n+\r\n+/**\r\n+ * orc table scan operator. Imports the file that backs the given\r\n+ * [[org.apache.spark.sql.orc.OrcRelation]] as a ``RDD[Row]``.\r\n+ */\r\n+case class OrcTableScan(\r\n+   output: Seq[Attribute],\r\n+   relation: OrcRelation,\r\n+   columnPruningPred: Option[Expression])\r\n+  extends LeafNode {\r\n+\r\n+  @transient\r\n+  lazy val serde: OrcSerde = initSerde\r\n+\r\n+  @transient\r\n+  lazy val getFieldValue: Seq[Product => Any] = {\r\n+    val inspector = serde.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+    output.map(attr => {\r\n+      val ref = inspector.getStructFieldRef(attr.name.toLowerCase(Locale.ENGLISH))\r\n+      row: Product => {\r\n+        val fieldData = row.productElement(1)\r\n+        val data = inspector.getStructFieldData(fieldData, ref)\r\n+        unwrapData(data, ref.getFieldObjectInspector)\r\n+      }\r\n+    })\r\n+  }\r\n+\r\n+  private def initSerde(): OrcSerde = {\r\n+    val serde = new OrcSerde\r\n+    serde.initialize(null, relation.prop)\r\n+    serde\r\n+  }\r\n+\r\n+  def unwrapData(data: Any, oi: ObjectInspector): Any = oi match {\r\n+    case pi: PrimitiveObjectInspector => pi.getPrimitiveJavaObject(data)\r\n+    case li: ListObjectInspector =>\r\n+      Option(li.getList(data))\r\n+        .map(_.map(unwrapData(_, li.getListElementObjectInspector)).toSeq)\r\n+        .orNull\r\n+    case mi: MapObjectInspector =>\r\n+      Option(mi.getMap(data)).map(\r\n+        _.map {\r\n+          case (k, v) =>\r\n+            (unwrapData(k, mi.getMapKeyObjectInspector),\r\n+              unwrapData(v, mi.getMapValueObjectInspector))\r\n+        }.toMap).orNull\r\n+    case si: StructObjectInspector =>\r\n+      val allRefs = si.getAllStructFieldRefs\r\n+      new GenericRow(\r\n+        allRefs.map(r =>\r\n+          unwrapData(si.getStructFieldData(data, r), r.getFieldObjectInspector)).toArray)\r\n+  }\r\n+\r\n+  override def execute(): RDD[Row] = {\r\n+    val sc = sqlContext.sparkContext\r\n+    val job = new Job(sc.hadoopConfiguration)\r\n+\r\n+    val conf: Configuration = ContextUtil.getConfiguration(job)\r\n+    val fileList = FileSystemHelper.listFiles(relation.path, conf)\r\n+\r\n+    // add all paths in the directory but skip \"hidden\" ones such\r\n+    // as \"_SUCCESS\"\r\n+    for (path <- fileList if !path.getName.startsWith(\"_\")) {\r\n+      FileInputFormat.addInputPath(job, path)\r\n+    }\r\n+    val serialConf = sc.broadcast(new SerializableWritable(conf))\r\n+\r\n+    setColumnIds(output, relation, conf)\r\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\r\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[Void, Row]]]\r\n+\r\n+    val rowRdd = productToRowRdd(sc.hadoopRDD[Void, Row](\r\n+      serialConf.value.value.asInstanceOf[JobConf], inputClass, classOf[Void], classOf[Row]))\r\n+    rowRdd\r\n+  }\r\n+\r\n+  /**\r\n+   * @param output\r\n+   * @param relation\r\n+   * @param conf\r\n+   */\r\n+  def setColumnIds(output: Seq[Attribute], relation: OrcRelation, conf: Configuration) {\r\n+    val idBuff = new StringBuilder()\r\n+\r\n+    output.map(att => {\r\n+      val realName = att.name.toLowerCase(Locale.ENGLISH)\r\n+      val id = relation.fieldIdCache.getOrElse(realName, null)\r\n+      if (null != id) {\r\n+        idBuff.append(id)\r\n+        idBuff.append(\",\")\r\n+      }\r\n+    })\r\n+    if (idBuff.length > 0) {\r\n+      idBuff.setLength(idBuff.length - 1)\r\n+    }\r\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, idBuff.toString())\r\n+  }\r\n+\r\n+  /**\r\n+   *\r\n+   * @param data\r\n+   * @tparam A\r\n+   * @return\r\n+   */\r\n+  def productToRowRdd[A <: Product](data: RDD[A]): RDD[Row] = {\r\n+    data.mapPartitions {\r\n+      iterator =>\r\n+        if (iterator.isEmpty) {\r\n+          Iterator.empty\r\n+        } else {\r\n+          val bufferedIterator = iterator.buffered\r\n+          bufferedIterator.map {r =>\r\n+              val values = getFieldValue.map(_(r))\r\n+              new GenericRow(values.map {\r\n+                case n: String if n.toLowerCase == \"null\" => \"\"\r\n+                case varchar: HiveVarchar => varchar.getValue\r\n+                case decimal: HiveDecimal =>\r\n+                  BigDecimal(decimal.bigDecimalValue)\r\n+                case null => \"\"\r\n+                case other => other\r\n+\r\n+              }.toArray)\r\n+          }\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * Applies a (candidate) projection.\r\n+   *\r\n+   * @param prunedAttributes The list of attributes to be used in the projection.\r\n+   * @return Pruned TableScan.\r\n+   */\r\n+  def pruneColumns(prunedAttributes: Seq[Attribute]): OrcTableScan = {\r\n+    OrcTableScan(prunedAttributes, relation, columnPruningPred)\r\n+  }\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ * @param relation\r\n+ * @param child\r\n+ * @param overwrite\r\n+ */\r\n+private[sql] case class InsertIntoOrcTable(\r\n+    relation: OrcRelation,\r\n+    child: SparkPlan,\r\n+    overwrite: Boolean = false)\r\n+  extends UnaryNode with SparkHadoopMapRedUtil with org.apache.spark.Logging {\r\n+\r\n+  override def output = child.output\r\n+\r\n+  val intputClass: Class[_] = getInputClass\r\n+\r\n+  @transient val sc = sqlContext.sparkContext\r\n+\r\n+  val inspector = sc.broadcast(getInspector(intputClass))\r\n+\r\n+  @transient lazy val orcSerde = initFieldInfo\r\n+\r\n+  private def getInputClass: Class[_] = {\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val productClass = existRdd.rdd.firstParent.elementClassTag.runtimeClass\r\n+    logInfo(\"productClass is \" + productClass)\r\n+    val clazz = productClass\r\n+    if (null == relation.rowClass) {\r\n+      relation.rowClass = clazz\r\n+    }\r\n+    clazz\r\n+  }\r\n+\r\n+  private def getInspector(clazz: Class[_]): ObjectInspector = {\r\n+    val inspector = ObjectInspectorFactory.getReflectionObjectInspector(clazz,\r\n+      ObjectInspectorFactory.ObjectInspectorOptions.JAVA)\r\n+    inspector\r\n+  }\r\n+\r\n+  private def initFieldInfo(): OrcSerde = {\r\n+    val serde: OrcSerde = new OrcSerde\r\n+    serde\r\n+  }\r\n+\r\n+  /**\r\n+   * Inserts all rows into the Orc file.\r\n+   */\r\n+  override def execute() = {\r\n+    val childRdd = child.execute()\r\n+    assert(childRdd != null)\r\n+\r\n+    val job = new Job(sqlContext.sparkContext.hadoopConfiguration)\r\n+    // TODO: move that to function in object\r\n+    val conf = job.getConfiguration\r\n+\r\n+    val fspath = new Path(relation.path)\r\n+    val fs = fspath.getFileSystem(conf)\r\n+\r\n+    if (overwrite) {\r\n+      try {\r\n+        fs.delete(fspath, true)\r\n+      } catch {\r\n+        case e: IOException =>\r\n+          throw new IOException(\r\n+            s\"Unable to clear output directory ${fspath.toString} prior\"\r\n+              + s\" to InsertIntoOrcTable:\\n${e.toString}\")\r\n+      }\r\n+    }\r\n+\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val parentRdd = existRdd.rdd.firstParent[Product]\r\n+    val writableRdd = parentRdd.map(obj => {\r\n+      orcSerde.serialize(obj, inspector.value)\r\n+    })\r\n+\r\n+    saveAsHadoopFile(writableRdd, relation.rowClass, relation.path, conf)\r\n+\r\n+    // We return the child RDD to allow chaining (alternatively, one could return nothing).\r\n+    childRdd\r\n+  }\r\n+\r\n+\r\n+  // based on ``saveAsNewAPIHadoopFile`` in [[PairRDDFunctions]]\r\n+  // TODO: Maybe PairRDDFunctions should use Product2 instead of Tuple2?\r\n+  // .. then we could use the default one and could use [[MutablePair]]\r\n+  // instead of ``Tuple2``\r\n+  private def saveAsHadoopFile(\r\n+                                rdd: RDD[Writable],\r\n+                                rowClass: Class[_],\r\n+                                path: String,\r\n+                                @transient conf: Configuration) {\r\n+    val job = new Job(conf)\r\n+    val keyType = classOf[Void]\r\n+    job.setOutputKeyClass(keyType)\r\n+    job.setOutputValueClass(classOf[Writable])\r\n+    FileOutputFormat.setOutputPath(job, new Path(path))\r\n+\r\n+    val wrappedConf = new SerializableWritable(job.getConfiguration)\r\n+\r\n+    val formatter = new SimpleDateFormat(\"yyyyMMddHHmm\")\r\n+    val jobtrackerID = formatter.format(new Date())\r\n+    val stageId = sqlContext.sparkContext.newRddId()\r\n+\r\n+    val taskIdOffset =\r\n+      if (overwrite) {\r\n+        1\r\n+      } else {\r\n+        FileSystemHelper\r\n+          .findMaxTaskId(FileOutputFormat.getOutputPath(job).toString, job.getConfiguration) + 1\r\n+      }\r\n+\r\n+    def getWriter(\r\n+                   outFormat: OrcOutputFormat,\r\n+                   conf: Configuration,\r\n+                   path: Path,\r\n+                   reporter: Reporter) = {\r\n+      val fs = path.getFileSystem(conf)\r\n+      outFormat.getRecordWriter(fs, conf.asInstanceOf[JobConf], path.toUri.getPath, reporter).\r\n+        asInstanceOf[org.apache.hadoop.mapred.RecordWriter[NullWritable, Writable]]\r\n+    }\r\n+\r\n+    def getCommitterAndWriter(offset: Int, context: TaskAttemptContext) = {\r\n+      val outFormat = new OrcOutputFormat\r\n+\r\n+      val taskId: TaskID = context.getTaskAttemptID.getTaskID\r\n+      val partition: Int = taskId.getId\r\n+      val filename = s\"part-r-${partition + offset}.orc\"\r\n+      val output: Path = FileOutputFormat.getOutputPath(context)\r\n+      val committer = new FileOutputCommitter(output, context)\r\n+      val path = new Path(committer.getWorkPath, filename)\r\n+      val writer = getWriter(outFormat, wrappedConf.value, path, Reporter.NULL)\r\n+      (committer, writer)\r\n+    }\r\n+\r\n+    def writeShard(context: TaskContext, iter: Iterator[Writable]): Int = {\r\n+      // Hadoop wants a 32-bit task attempt ID, so if ours is bigger than Int.MaxValue, roll it\r\n+      // around by taking a mod. We expect that no task will be attempted 2 billion times.\r\n+      val attemptNumber = (context.attemptId % Int.MaxValue).toInt\r\n+      /* \"reduce task\" <split #> <attempt # = spark task #> */\r\n+      val attemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = false, context.partitionId,\r\n+        attemptNumber)\r\n+      val hadoopContext = newTaskAttemptContext(wrappedConf.value.asInstanceOf[JobConf], attemptId)\r\n+      val workerAndComitter = getCommitterAndWriter(taskIdOffset, hadoopContext)\r\n+      val writer = workerAndComitter._2\r\n+\r\n+      while (iter.hasNext) {\r\n+        val row = iter.next()\r\n+        writer.write(NullWritable.get(), row)\r\n+      }\r\n+\r\n+      writer.close(Reporter.NULL)\r\n+      workerAndComitter._1.commitTask(hadoopContext)\r\n+      return 1\r\n+    }\r\n+\r\n+    /* apparently we need a TaskAttemptID to construct an OutputCommitter;\r\n+     * however we're only going to use this local OutputCommitter for\r\n+     * setupJob/commitJob, so we just use a dummy \"map\" task.\r\n+     */\r\n+    val jobAttemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = true, 0, 0)\r\n+    val jobTaskContext = newTaskAttemptContext(\r\n+      wrappedConf.value.asInstanceOf[JobConf], jobAttemptId)\r\n+    val workerAndComitter = getCommitterAndWriter(taskIdOffset, jobTaskContext)\r\n+    workerAndComitter._1.setupJob(jobTaskContext)\r\n+    sc.runJob(rdd, writeShard _)\r\n+    workerAndComitter._1.commitJob(jobTaskContext)\r\n+  }\r\n+\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ */\r"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Actually is this just copied from `parquet`?  We should probably just generalize the code there and move it up to SQL (keeping it `private[sql]`) instead of duplicating it.  That is unless there is some real difference in functionality (in which case you should explain that here).\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:49:10Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.spark.sql.execution.{ExistingRdd, LeafNode, UnaryNode, SparkPlan}\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.{TaskContext, SerializableWritable}\r\n+import org.apache.spark.rdd.RDD\r\n+\r\n+import _root_.parquet.hadoop.util.ContextUtil\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, FileOutputCommitter}\r\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\r\n+import org.apache.hadoop.io.{Writable, NullWritable}\r\n+import org.apache.hadoop.mapreduce.{TaskID, TaskAttemptContext, Job}\r\n+\r\n+import org.apache.hadoop.hive.ql.io.orc.{OrcFile, OrcSerde, OrcInputFormat, OrcOutputFormat}\r\n+import org.apache.hadoop.hive.serde2.objectinspector._\r\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\r\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\r\n+\r\n+import java.io.IOException\r\n+import java.text.SimpleDateFormat\r\n+import java.util.{Locale, Date}\r\n+import scala.collection.JavaConversions._\r\n+import org.apache.hadoop.mapred.{SparkHadoopMapRedUtil, Reporter, JobConf}\r\n+\r\n+/**\r\n+ * orc table scan operator. Imports the file that backs the given\r\n+ * [[org.apache.spark.sql.orc.OrcRelation]] as a ``RDD[Row]``.\r\n+ */\r\n+case class OrcTableScan(\r\n+   output: Seq[Attribute],\r\n+   relation: OrcRelation,\r\n+   columnPruningPred: Option[Expression])\r\n+  extends LeafNode {\r\n+\r\n+  @transient\r\n+  lazy val serde: OrcSerde = initSerde\r\n+\r\n+  @transient\r\n+  lazy val getFieldValue: Seq[Product => Any] = {\r\n+    val inspector = serde.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+    output.map(attr => {\r\n+      val ref = inspector.getStructFieldRef(attr.name.toLowerCase(Locale.ENGLISH))\r\n+      row: Product => {\r\n+        val fieldData = row.productElement(1)\r\n+        val data = inspector.getStructFieldData(fieldData, ref)\r\n+        unwrapData(data, ref.getFieldObjectInspector)\r\n+      }\r\n+    })\r\n+  }\r\n+\r\n+  private def initSerde(): OrcSerde = {\r\n+    val serde = new OrcSerde\r\n+    serde.initialize(null, relation.prop)\r\n+    serde\r\n+  }\r\n+\r\n+  def unwrapData(data: Any, oi: ObjectInspector): Any = oi match {\r\n+    case pi: PrimitiveObjectInspector => pi.getPrimitiveJavaObject(data)\r\n+    case li: ListObjectInspector =>\r\n+      Option(li.getList(data))\r\n+        .map(_.map(unwrapData(_, li.getListElementObjectInspector)).toSeq)\r\n+        .orNull\r\n+    case mi: MapObjectInspector =>\r\n+      Option(mi.getMap(data)).map(\r\n+        _.map {\r\n+          case (k, v) =>\r\n+            (unwrapData(k, mi.getMapKeyObjectInspector),\r\n+              unwrapData(v, mi.getMapValueObjectInspector))\r\n+        }.toMap).orNull\r\n+    case si: StructObjectInspector =>\r\n+      val allRefs = si.getAllStructFieldRefs\r\n+      new GenericRow(\r\n+        allRefs.map(r =>\r\n+          unwrapData(si.getStructFieldData(data, r), r.getFieldObjectInspector)).toArray)\r\n+  }\r\n+\r\n+  override def execute(): RDD[Row] = {\r\n+    val sc = sqlContext.sparkContext\r\n+    val job = new Job(sc.hadoopConfiguration)\r\n+\r\n+    val conf: Configuration = ContextUtil.getConfiguration(job)\r\n+    val fileList = FileSystemHelper.listFiles(relation.path, conf)\r\n+\r\n+    // add all paths in the directory but skip \"hidden\" ones such\r\n+    // as \"_SUCCESS\"\r\n+    for (path <- fileList if !path.getName.startsWith(\"_\")) {\r\n+      FileInputFormat.addInputPath(job, path)\r\n+    }\r\n+    val serialConf = sc.broadcast(new SerializableWritable(conf))\r\n+\r\n+    setColumnIds(output, relation, conf)\r\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\r\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[Void, Row]]]\r\n+\r\n+    val rowRdd = productToRowRdd(sc.hadoopRDD[Void, Row](\r\n+      serialConf.value.value.asInstanceOf[JobConf], inputClass, classOf[Void], classOf[Row]))\r\n+    rowRdd\r\n+  }\r\n+\r\n+  /**\r\n+   * @param output\r\n+   * @param relation\r\n+   * @param conf\r\n+   */\r\n+  def setColumnIds(output: Seq[Attribute], relation: OrcRelation, conf: Configuration) {\r\n+    val idBuff = new StringBuilder()\r\n+\r\n+    output.map(att => {\r\n+      val realName = att.name.toLowerCase(Locale.ENGLISH)\r\n+      val id = relation.fieldIdCache.getOrElse(realName, null)\r\n+      if (null != id) {\r\n+        idBuff.append(id)\r\n+        idBuff.append(\",\")\r\n+      }\r\n+    })\r\n+    if (idBuff.length > 0) {\r\n+      idBuff.setLength(idBuff.length - 1)\r\n+    }\r\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, idBuff.toString())\r\n+  }\r\n+\r\n+  /**\r\n+   *\r\n+   * @param data\r\n+   * @tparam A\r\n+   * @return\r\n+   */\r\n+  def productToRowRdd[A <: Product](data: RDD[A]): RDD[Row] = {\r\n+    data.mapPartitions {\r\n+      iterator =>\r\n+        if (iterator.isEmpty) {\r\n+          Iterator.empty\r\n+        } else {\r\n+          val bufferedIterator = iterator.buffered\r\n+          bufferedIterator.map {r =>\r\n+              val values = getFieldValue.map(_(r))\r\n+              new GenericRow(values.map {\r\n+                case n: String if n.toLowerCase == \"null\" => \"\"\r\n+                case varchar: HiveVarchar => varchar.getValue\r\n+                case decimal: HiveDecimal =>\r\n+                  BigDecimal(decimal.bigDecimalValue)\r\n+                case null => \"\"\r\n+                case other => other\r\n+\r\n+              }.toArray)\r\n+          }\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * Applies a (candidate) projection.\r\n+   *\r\n+   * @param prunedAttributes The list of attributes to be used in the projection.\r\n+   * @return Pruned TableScan.\r\n+   */\r\n+  def pruneColumns(prunedAttributes: Seq[Attribute]): OrcTableScan = {\r\n+    OrcTableScan(prunedAttributes, relation, columnPruningPred)\r\n+  }\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ * @param relation\r\n+ * @param child\r\n+ * @param overwrite\r\n+ */\r\n+private[sql] case class InsertIntoOrcTable(\r\n+    relation: OrcRelation,\r\n+    child: SparkPlan,\r\n+    overwrite: Boolean = false)\r\n+  extends UnaryNode with SparkHadoopMapRedUtil with org.apache.spark.Logging {\r\n+\r\n+  override def output = child.output\r\n+\r\n+  val intputClass: Class[_] = getInputClass\r\n+\r\n+  @transient val sc = sqlContext.sparkContext\r\n+\r\n+  val inspector = sc.broadcast(getInspector(intputClass))\r\n+\r\n+  @transient lazy val orcSerde = initFieldInfo\r\n+\r\n+  private def getInputClass: Class[_] = {\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val productClass = existRdd.rdd.firstParent.elementClassTag.runtimeClass\r\n+    logInfo(\"productClass is \" + productClass)\r\n+    val clazz = productClass\r\n+    if (null == relation.rowClass) {\r\n+      relation.rowClass = clazz\r\n+    }\r\n+    clazz\r\n+  }\r\n+\r\n+  private def getInspector(clazz: Class[_]): ObjectInspector = {\r\n+    val inspector = ObjectInspectorFactory.getReflectionObjectInspector(clazz,\r\n+      ObjectInspectorFactory.ObjectInspectorOptions.JAVA)\r\n+    inspector\r\n+  }\r\n+\r\n+  private def initFieldInfo(): OrcSerde = {\r\n+    val serde: OrcSerde = new OrcSerde\r\n+    serde\r\n+  }\r\n+\r\n+  /**\r\n+   * Inserts all rows into the Orc file.\r\n+   */\r\n+  override def execute() = {\r\n+    val childRdd = child.execute()\r\n+    assert(childRdd != null)\r\n+\r\n+    val job = new Job(sqlContext.sparkContext.hadoopConfiguration)\r\n+    // TODO: move that to function in object\r\n+    val conf = job.getConfiguration\r\n+\r\n+    val fspath = new Path(relation.path)\r\n+    val fs = fspath.getFileSystem(conf)\r\n+\r\n+    if (overwrite) {\r\n+      try {\r\n+        fs.delete(fspath, true)\r\n+      } catch {\r\n+        case e: IOException =>\r\n+          throw new IOException(\r\n+            s\"Unable to clear output directory ${fspath.toString} prior\"\r\n+              + s\" to InsertIntoOrcTable:\\n${e.toString}\")\r\n+      }\r\n+    }\r\n+\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val parentRdd = existRdd.rdd.firstParent[Product]\r\n+    val writableRdd = parentRdd.map(obj => {\r\n+      orcSerde.serialize(obj, inspector.value)\r\n+    })\r\n+\r\n+    saveAsHadoopFile(writableRdd, relation.rowClass, relation.path, conf)\r\n+\r\n+    // We return the child RDD to allow chaining (alternatively, one could return nothing).\r\n+    childRdd\r\n+  }\r\n+\r\n+\r\n+  // based on ``saveAsNewAPIHadoopFile`` in [[PairRDDFunctions]]\r\n+  // TODO: Maybe PairRDDFunctions should use Product2 instead of Tuple2?\r\n+  // .. then we could use the default one and could use [[MutablePair]]\r\n+  // instead of ``Tuple2``\r\n+  private def saveAsHadoopFile(\r\n+                                rdd: RDD[Writable],\r\n+                                rowClass: Class[_],\r\n+                                path: String,\r\n+                                @transient conf: Configuration) {\r\n+    val job = new Job(conf)\r\n+    val keyType = classOf[Void]\r\n+    job.setOutputKeyClass(keyType)\r\n+    job.setOutputValueClass(classOf[Writable])\r\n+    FileOutputFormat.setOutputPath(job, new Path(path))\r\n+\r\n+    val wrappedConf = new SerializableWritable(job.getConfiguration)\r\n+\r\n+    val formatter = new SimpleDateFormat(\"yyyyMMddHHmm\")\r\n+    val jobtrackerID = formatter.format(new Date())\r\n+    val stageId = sqlContext.sparkContext.newRddId()\r\n+\r\n+    val taskIdOffset =\r\n+      if (overwrite) {\r\n+        1\r\n+      } else {\r\n+        FileSystemHelper\r\n+          .findMaxTaskId(FileOutputFormat.getOutputPath(job).toString, job.getConfiguration) + 1\r\n+      }\r\n+\r\n+    def getWriter(\r\n+                   outFormat: OrcOutputFormat,\r\n+                   conf: Configuration,\r\n+                   path: Path,\r\n+                   reporter: Reporter) = {\r\n+      val fs = path.getFileSystem(conf)\r\n+      outFormat.getRecordWriter(fs, conf.asInstanceOf[JobConf], path.toUri.getPath, reporter).\r\n+        asInstanceOf[org.apache.hadoop.mapred.RecordWriter[NullWritable, Writable]]\r\n+    }\r\n+\r\n+    def getCommitterAndWriter(offset: Int, context: TaskAttemptContext) = {\r\n+      val outFormat = new OrcOutputFormat\r\n+\r\n+      val taskId: TaskID = context.getTaskAttemptID.getTaskID\r\n+      val partition: Int = taskId.getId\r\n+      val filename = s\"part-r-${partition + offset}.orc\"\r\n+      val output: Path = FileOutputFormat.getOutputPath(context)\r\n+      val committer = new FileOutputCommitter(output, context)\r\n+      val path = new Path(committer.getWorkPath, filename)\r\n+      val writer = getWriter(outFormat, wrappedConf.value, path, Reporter.NULL)\r\n+      (committer, writer)\r\n+    }\r\n+\r\n+    def writeShard(context: TaskContext, iter: Iterator[Writable]): Int = {\r\n+      // Hadoop wants a 32-bit task attempt ID, so if ours is bigger than Int.MaxValue, roll it\r\n+      // around by taking a mod. We expect that no task will be attempted 2 billion times.\r\n+      val attemptNumber = (context.attemptId % Int.MaxValue).toInt\r\n+      /* \"reduce task\" <split #> <attempt # = spark task #> */\r\n+      val attemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = false, context.partitionId,\r\n+        attemptNumber)\r\n+      val hadoopContext = newTaskAttemptContext(wrappedConf.value.asInstanceOf[JobConf], attemptId)\r\n+      val workerAndComitter = getCommitterAndWriter(taskIdOffset, hadoopContext)\r\n+      val writer = workerAndComitter._2\r\n+\r\n+      while (iter.hasNext) {\r\n+        val row = iter.next()\r\n+        writer.write(NullWritable.get(), row)\r\n+      }\r\n+\r\n+      writer.close(Reporter.NULL)\r\n+      workerAndComitter._1.commitTask(hadoopContext)\r\n+      return 1\r\n+    }\r\n+\r\n+    /* apparently we need a TaskAttemptID to construct an OutputCommitter;\r\n+     * however we're only going to use this local OutputCommitter for\r\n+     * setupJob/commitJob, so we just use a dummy \"map\" task.\r\n+     */\r\n+    val jobAttemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = true, 0, 0)\r\n+    val jobTaskContext = newTaskAttemptContext(\r\n+      wrappedConf.value.asInstanceOf[JobConf], jobAttemptId)\r\n+    val workerAndComitter = getCommitterAndWriter(taskIdOffset, jobTaskContext)\r\n+    workerAndComitter._1.setupJob(jobTaskContext)\r\n+    sc.runJob(rdd, writeShard _)\r\n+    workerAndComitter._1.commitJob(jobTaskContext)\r\n+  }\r\n+\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ */\r"
  }, {
    "author": {
      "login": "scwf"
    },
    "body": "you mean `saveAsHadoopFile`, right? Here ORC's has different input para `rdd: RDD[Writable]` while parquet use `rdd: RDD[Row]`\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-02T03:30:37Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.spark.sql.execution.{ExistingRdd, LeafNode, UnaryNode, SparkPlan}\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.{TaskContext, SerializableWritable}\r\n+import org.apache.spark.rdd.RDD\r\n+\r\n+import _root_.parquet.hadoop.util.ContextUtil\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, FileOutputCommitter}\r\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\r\n+import org.apache.hadoop.io.{Writable, NullWritable}\r\n+import org.apache.hadoop.mapreduce.{TaskID, TaskAttemptContext, Job}\r\n+\r\n+import org.apache.hadoop.hive.ql.io.orc.{OrcFile, OrcSerde, OrcInputFormat, OrcOutputFormat}\r\n+import org.apache.hadoop.hive.serde2.objectinspector._\r\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\r\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\r\n+\r\n+import java.io.IOException\r\n+import java.text.SimpleDateFormat\r\n+import java.util.{Locale, Date}\r\n+import scala.collection.JavaConversions._\r\n+import org.apache.hadoop.mapred.{SparkHadoopMapRedUtil, Reporter, JobConf}\r\n+\r\n+/**\r\n+ * orc table scan operator. Imports the file that backs the given\r\n+ * [[org.apache.spark.sql.orc.OrcRelation]] as a ``RDD[Row]``.\r\n+ */\r\n+case class OrcTableScan(\r\n+   output: Seq[Attribute],\r\n+   relation: OrcRelation,\r\n+   columnPruningPred: Option[Expression])\r\n+  extends LeafNode {\r\n+\r\n+  @transient\r\n+  lazy val serde: OrcSerde = initSerde\r\n+\r\n+  @transient\r\n+  lazy val getFieldValue: Seq[Product => Any] = {\r\n+    val inspector = serde.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+    output.map(attr => {\r\n+      val ref = inspector.getStructFieldRef(attr.name.toLowerCase(Locale.ENGLISH))\r\n+      row: Product => {\r\n+        val fieldData = row.productElement(1)\r\n+        val data = inspector.getStructFieldData(fieldData, ref)\r\n+        unwrapData(data, ref.getFieldObjectInspector)\r\n+      }\r\n+    })\r\n+  }\r\n+\r\n+  private def initSerde(): OrcSerde = {\r\n+    val serde = new OrcSerde\r\n+    serde.initialize(null, relation.prop)\r\n+    serde\r\n+  }\r\n+\r\n+  def unwrapData(data: Any, oi: ObjectInspector): Any = oi match {\r\n+    case pi: PrimitiveObjectInspector => pi.getPrimitiveJavaObject(data)\r\n+    case li: ListObjectInspector =>\r\n+      Option(li.getList(data))\r\n+        .map(_.map(unwrapData(_, li.getListElementObjectInspector)).toSeq)\r\n+        .orNull\r\n+    case mi: MapObjectInspector =>\r\n+      Option(mi.getMap(data)).map(\r\n+        _.map {\r\n+          case (k, v) =>\r\n+            (unwrapData(k, mi.getMapKeyObjectInspector),\r\n+              unwrapData(v, mi.getMapValueObjectInspector))\r\n+        }.toMap).orNull\r\n+    case si: StructObjectInspector =>\r\n+      val allRefs = si.getAllStructFieldRefs\r\n+      new GenericRow(\r\n+        allRefs.map(r =>\r\n+          unwrapData(si.getStructFieldData(data, r), r.getFieldObjectInspector)).toArray)\r\n+  }\r\n+\r\n+  override def execute(): RDD[Row] = {\r\n+    val sc = sqlContext.sparkContext\r\n+    val job = new Job(sc.hadoopConfiguration)\r\n+\r\n+    val conf: Configuration = ContextUtil.getConfiguration(job)\r\n+    val fileList = FileSystemHelper.listFiles(relation.path, conf)\r\n+\r\n+    // add all paths in the directory but skip \"hidden\" ones such\r\n+    // as \"_SUCCESS\"\r\n+    for (path <- fileList if !path.getName.startsWith(\"_\")) {\r\n+      FileInputFormat.addInputPath(job, path)\r\n+    }\r\n+    val serialConf = sc.broadcast(new SerializableWritable(conf))\r\n+\r\n+    setColumnIds(output, relation, conf)\r\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\r\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[Void, Row]]]\r\n+\r\n+    val rowRdd = productToRowRdd(sc.hadoopRDD[Void, Row](\r\n+      serialConf.value.value.asInstanceOf[JobConf], inputClass, classOf[Void], classOf[Row]))\r\n+    rowRdd\r\n+  }\r\n+\r\n+  /**\r\n+   * @param output\r\n+   * @param relation\r\n+   * @param conf\r\n+   */\r\n+  def setColumnIds(output: Seq[Attribute], relation: OrcRelation, conf: Configuration) {\r\n+    val idBuff = new StringBuilder()\r\n+\r\n+    output.map(att => {\r\n+      val realName = att.name.toLowerCase(Locale.ENGLISH)\r\n+      val id = relation.fieldIdCache.getOrElse(realName, null)\r\n+      if (null != id) {\r\n+        idBuff.append(id)\r\n+        idBuff.append(\",\")\r\n+      }\r\n+    })\r\n+    if (idBuff.length > 0) {\r\n+      idBuff.setLength(idBuff.length - 1)\r\n+    }\r\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, idBuff.toString())\r\n+  }\r\n+\r\n+  /**\r\n+   *\r\n+   * @param data\r\n+   * @tparam A\r\n+   * @return\r\n+   */\r\n+  def productToRowRdd[A <: Product](data: RDD[A]): RDD[Row] = {\r\n+    data.mapPartitions {\r\n+      iterator =>\r\n+        if (iterator.isEmpty) {\r\n+          Iterator.empty\r\n+        } else {\r\n+          val bufferedIterator = iterator.buffered\r\n+          bufferedIterator.map {r =>\r\n+              val values = getFieldValue.map(_(r))\r\n+              new GenericRow(values.map {\r\n+                case n: String if n.toLowerCase == \"null\" => \"\"\r\n+                case varchar: HiveVarchar => varchar.getValue\r\n+                case decimal: HiveDecimal =>\r\n+                  BigDecimal(decimal.bigDecimalValue)\r\n+                case null => \"\"\r\n+                case other => other\r\n+\r\n+              }.toArray)\r\n+          }\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * Applies a (candidate) projection.\r\n+   *\r\n+   * @param prunedAttributes The list of attributes to be used in the projection.\r\n+   * @return Pruned TableScan.\r\n+   */\r\n+  def pruneColumns(prunedAttributes: Seq[Attribute]): OrcTableScan = {\r\n+    OrcTableScan(prunedAttributes, relation, columnPruningPred)\r\n+  }\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ * @param relation\r\n+ * @param child\r\n+ * @param overwrite\r\n+ */\r\n+private[sql] case class InsertIntoOrcTable(\r\n+    relation: OrcRelation,\r\n+    child: SparkPlan,\r\n+    overwrite: Boolean = false)\r\n+  extends UnaryNode with SparkHadoopMapRedUtil with org.apache.spark.Logging {\r\n+\r\n+  override def output = child.output\r\n+\r\n+  val intputClass: Class[_] = getInputClass\r\n+\r\n+  @transient val sc = sqlContext.sparkContext\r\n+\r\n+  val inspector = sc.broadcast(getInspector(intputClass))\r\n+\r\n+  @transient lazy val orcSerde = initFieldInfo\r\n+\r\n+  private def getInputClass: Class[_] = {\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val productClass = existRdd.rdd.firstParent.elementClassTag.runtimeClass\r\n+    logInfo(\"productClass is \" + productClass)\r\n+    val clazz = productClass\r\n+    if (null == relation.rowClass) {\r\n+      relation.rowClass = clazz\r\n+    }\r\n+    clazz\r\n+  }\r\n+\r\n+  private def getInspector(clazz: Class[_]): ObjectInspector = {\r\n+    val inspector = ObjectInspectorFactory.getReflectionObjectInspector(clazz,\r\n+      ObjectInspectorFactory.ObjectInspectorOptions.JAVA)\r\n+    inspector\r\n+  }\r\n+\r\n+  private def initFieldInfo(): OrcSerde = {\r\n+    val serde: OrcSerde = new OrcSerde\r\n+    serde\r\n+  }\r\n+\r\n+  /**\r\n+   * Inserts all rows into the Orc file.\r\n+   */\r\n+  override def execute() = {\r\n+    val childRdd = child.execute()\r\n+    assert(childRdd != null)\r\n+\r\n+    val job = new Job(sqlContext.sparkContext.hadoopConfiguration)\r\n+    // TODO: move that to function in object\r\n+    val conf = job.getConfiguration\r\n+\r\n+    val fspath = new Path(relation.path)\r\n+    val fs = fspath.getFileSystem(conf)\r\n+\r\n+    if (overwrite) {\r\n+      try {\r\n+        fs.delete(fspath, true)\r\n+      } catch {\r\n+        case e: IOException =>\r\n+          throw new IOException(\r\n+            s\"Unable to clear output directory ${fspath.toString} prior\"\r\n+              + s\" to InsertIntoOrcTable:\\n${e.toString}\")\r\n+      }\r\n+    }\r\n+\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val parentRdd = existRdd.rdd.firstParent[Product]\r\n+    val writableRdd = parentRdd.map(obj => {\r\n+      orcSerde.serialize(obj, inspector.value)\r\n+    })\r\n+\r\n+    saveAsHadoopFile(writableRdd, relation.rowClass, relation.path, conf)\r\n+\r\n+    // We return the child RDD to allow chaining (alternatively, one could return nothing).\r\n+    childRdd\r\n+  }\r\n+\r\n+\r\n+  // based on ``saveAsNewAPIHadoopFile`` in [[PairRDDFunctions]]\r\n+  // TODO: Maybe PairRDDFunctions should use Product2 instead of Tuple2?\r\n+  // .. then we could use the default one and could use [[MutablePair]]\r\n+  // instead of ``Tuple2``\r\n+  private def saveAsHadoopFile(\r\n+                                rdd: RDD[Writable],\r\n+                                rowClass: Class[_],\r\n+                                path: String,\r\n+                                @transient conf: Configuration) {\r\n+    val job = new Job(conf)\r\n+    val keyType = classOf[Void]\r\n+    job.setOutputKeyClass(keyType)\r\n+    job.setOutputValueClass(classOf[Writable])\r\n+    FileOutputFormat.setOutputPath(job, new Path(path))\r\n+\r\n+    val wrappedConf = new SerializableWritable(job.getConfiguration)\r\n+\r\n+    val formatter = new SimpleDateFormat(\"yyyyMMddHHmm\")\r\n+    val jobtrackerID = formatter.format(new Date())\r\n+    val stageId = sqlContext.sparkContext.newRddId()\r\n+\r\n+    val taskIdOffset =\r\n+      if (overwrite) {\r\n+        1\r\n+      } else {\r\n+        FileSystemHelper\r\n+          .findMaxTaskId(FileOutputFormat.getOutputPath(job).toString, job.getConfiguration) + 1\r\n+      }\r\n+\r\n+    def getWriter(\r\n+                   outFormat: OrcOutputFormat,\r\n+                   conf: Configuration,\r\n+                   path: Path,\r\n+                   reporter: Reporter) = {\r\n+      val fs = path.getFileSystem(conf)\r\n+      outFormat.getRecordWriter(fs, conf.asInstanceOf[JobConf], path.toUri.getPath, reporter).\r\n+        asInstanceOf[org.apache.hadoop.mapred.RecordWriter[NullWritable, Writable]]\r\n+    }\r\n+\r\n+    def getCommitterAndWriter(offset: Int, context: TaskAttemptContext) = {\r\n+      val outFormat = new OrcOutputFormat\r\n+\r\n+      val taskId: TaskID = context.getTaskAttemptID.getTaskID\r\n+      val partition: Int = taskId.getId\r\n+      val filename = s\"part-r-${partition + offset}.orc\"\r\n+      val output: Path = FileOutputFormat.getOutputPath(context)\r\n+      val committer = new FileOutputCommitter(output, context)\r\n+      val path = new Path(committer.getWorkPath, filename)\r\n+      val writer = getWriter(outFormat, wrappedConf.value, path, Reporter.NULL)\r\n+      (committer, writer)\r\n+    }\r\n+\r\n+    def writeShard(context: TaskContext, iter: Iterator[Writable]): Int = {\r\n+      // Hadoop wants a 32-bit task attempt ID, so if ours is bigger than Int.MaxValue, roll it\r\n+      // around by taking a mod. We expect that no task will be attempted 2 billion times.\r\n+      val attemptNumber = (context.attemptId % Int.MaxValue).toInt\r\n+      /* \"reduce task\" <split #> <attempt # = spark task #> */\r\n+      val attemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = false, context.partitionId,\r\n+        attemptNumber)\r\n+      val hadoopContext = newTaskAttemptContext(wrappedConf.value.asInstanceOf[JobConf], attemptId)\r\n+      val workerAndComitter = getCommitterAndWriter(taskIdOffset, hadoopContext)\r\n+      val writer = workerAndComitter._2\r\n+\r\n+      while (iter.hasNext) {\r\n+        val row = iter.next()\r\n+        writer.write(NullWritable.get(), row)\r\n+      }\r\n+\r\n+      writer.close(Reporter.NULL)\r\n+      workerAndComitter._1.commitTask(hadoopContext)\r\n+      return 1\r\n+    }\r\n+\r\n+    /* apparently we need a TaskAttemptID to construct an OutputCommitter;\r\n+     * however we're only going to use this local OutputCommitter for\r\n+     * setupJob/commitJob, so we just use a dummy \"map\" task.\r\n+     */\r\n+    val jobAttemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = true, 0, 0)\r\n+    val jobTaskContext = newTaskAttemptContext(\r\n+      wrappedConf.value.asInstanceOf[JobConf], jobAttemptId)\r\n+    val workerAndComitter = getCommitterAndWriter(taskIdOffset, jobTaskContext)\r\n+    workerAndComitter._1.setupJob(jobTaskContext)\r\n+    sc.runJob(rdd, writeShard _)\r\n+    workerAndComitter._1.commitJob(jobTaskContext)\r\n+  }\r\n+\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ */\r"
  }, {
    "author": {
      "login": "scwf"
    },
    "body": "aha, get it, you mean `FileSystemHelper`, yes, ok, i will modify this\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-02T03:32:59Z",
    "diffHunk": "@@ -0,0 +1,418 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.spark.sql.execution.{ExistingRdd, LeafNode, UnaryNode, SparkPlan}\r\n+import org.apache.spark.sql.catalyst.expressions._\r\n+import org.apache.spark.{TaskContext, SerializableWritable}\r\n+import org.apache.spark.rdd.RDD\r\n+\r\n+import _root_.parquet.hadoop.util.ContextUtil\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, FileOutputCommitter}\r\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\r\n+import org.apache.hadoop.io.{Writable, NullWritable}\r\n+import org.apache.hadoop.mapreduce.{TaskID, TaskAttemptContext, Job}\r\n+\r\n+import org.apache.hadoop.hive.ql.io.orc.{OrcFile, OrcSerde, OrcInputFormat, OrcOutputFormat}\r\n+import org.apache.hadoop.hive.serde2.objectinspector._\r\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\r\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\r\n+\r\n+import java.io.IOException\r\n+import java.text.SimpleDateFormat\r\n+import java.util.{Locale, Date}\r\n+import scala.collection.JavaConversions._\r\n+import org.apache.hadoop.mapred.{SparkHadoopMapRedUtil, Reporter, JobConf}\r\n+\r\n+/**\r\n+ * orc table scan operator. Imports the file that backs the given\r\n+ * [[org.apache.spark.sql.orc.OrcRelation]] as a ``RDD[Row]``.\r\n+ */\r\n+case class OrcTableScan(\r\n+   output: Seq[Attribute],\r\n+   relation: OrcRelation,\r\n+   columnPruningPred: Option[Expression])\r\n+  extends LeafNode {\r\n+\r\n+  @transient\r\n+  lazy val serde: OrcSerde = initSerde\r\n+\r\n+  @transient\r\n+  lazy val getFieldValue: Seq[Product => Any] = {\r\n+    val inspector = serde.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+    output.map(attr => {\r\n+      val ref = inspector.getStructFieldRef(attr.name.toLowerCase(Locale.ENGLISH))\r\n+      row: Product => {\r\n+        val fieldData = row.productElement(1)\r\n+        val data = inspector.getStructFieldData(fieldData, ref)\r\n+        unwrapData(data, ref.getFieldObjectInspector)\r\n+      }\r\n+    })\r\n+  }\r\n+\r\n+  private def initSerde(): OrcSerde = {\r\n+    val serde = new OrcSerde\r\n+    serde.initialize(null, relation.prop)\r\n+    serde\r\n+  }\r\n+\r\n+  def unwrapData(data: Any, oi: ObjectInspector): Any = oi match {\r\n+    case pi: PrimitiveObjectInspector => pi.getPrimitiveJavaObject(data)\r\n+    case li: ListObjectInspector =>\r\n+      Option(li.getList(data))\r\n+        .map(_.map(unwrapData(_, li.getListElementObjectInspector)).toSeq)\r\n+        .orNull\r\n+    case mi: MapObjectInspector =>\r\n+      Option(mi.getMap(data)).map(\r\n+        _.map {\r\n+          case (k, v) =>\r\n+            (unwrapData(k, mi.getMapKeyObjectInspector),\r\n+              unwrapData(v, mi.getMapValueObjectInspector))\r\n+        }.toMap).orNull\r\n+    case si: StructObjectInspector =>\r\n+      val allRefs = si.getAllStructFieldRefs\r\n+      new GenericRow(\r\n+        allRefs.map(r =>\r\n+          unwrapData(si.getStructFieldData(data, r), r.getFieldObjectInspector)).toArray)\r\n+  }\r\n+\r\n+  override def execute(): RDD[Row] = {\r\n+    val sc = sqlContext.sparkContext\r\n+    val job = new Job(sc.hadoopConfiguration)\r\n+\r\n+    val conf: Configuration = ContextUtil.getConfiguration(job)\r\n+    val fileList = FileSystemHelper.listFiles(relation.path, conf)\r\n+\r\n+    // add all paths in the directory but skip \"hidden\" ones such\r\n+    // as \"_SUCCESS\"\r\n+    for (path <- fileList if !path.getName.startsWith(\"_\")) {\r\n+      FileInputFormat.addInputPath(job, path)\r\n+    }\r\n+    val serialConf = sc.broadcast(new SerializableWritable(conf))\r\n+\r\n+    setColumnIds(output, relation, conf)\r\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\r\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[Void, Row]]]\r\n+\r\n+    val rowRdd = productToRowRdd(sc.hadoopRDD[Void, Row](\r\n+      serialConf.value.value.asInstanceOf[JobConf], inputClass, classOf[Void], classOf[Row]))\r\n+    rowRdd\r\n+  }\r\n+\r\n+  /**\r\n+   * @param output\r\n+   * @param relation\r\n+   * @param conf\r\n+   */\r\n+  def setColumnIds(output: Seq[Attribute], relation: OrcRelation, conf: Configuration) {\r\n+    val idBuff = new StringBuilder()\r\n+\r\n+    output.map(att => {\r\n+      val realName = att.name.toLowerCase(Locale.ENGLISH)\r\n+      val id = relation.fieldIdCache.getOrElse(realName, null)\r\n+      if (null != id) {\r\n+        idBuff.append(id)\r\n+        idBuff.append(\",\")\r\n+      }\r\n+    })\r\n+    if (idBuff.length > 0) {\r\n+      idBuff.setLength(idBuff.length - 1)\r\n+    }\r\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, idBuff.toString())\r\n+  }\r\n+\r\n+  /**\r\n+   *\r\n+   * @param data\r\n+   * @tparam A\r\n+   * @return\r\n+   */\r\n+  def productToRowRdd[A <: Product](data: RDD[A]): RDD[Row] = {\r\n+    data.mapPartitions {\r\n+      iterator =>\r\n+        if (iterator.isEmpty) {\r\n+          Iterator.empty\r\n+        } else {\r\n+          val bufferedIterator = iterator.buffered\r\n+          bufferedIterator.map {r =>\r\n+              val values = getFieldValue.map(_(r))\r\n+              new GenericRow(values.map {\r\n+                case n: String if n.toLowerCase == \"null\" => \"\"\r\n+                case varchar: HiveVarchar => varchar.getValue\r\n+                case decimal: HiveDecimal =>\r\n+                  BigDecimal(decimal.bigDecimalValue)\r\n+                case null => \"\"\r\n+                case other => other\r\n+\r\n+              }.toArray)\r\n+          }\r\n+        }\r\n+    }\r\n+  }\r\n+\r\n+  /**\r\n+   * Applies a (candidate) projection.\r\n+   *\r\n+   * @param prunedAttributes The list of attributes to be used in the projection.\r\n+   * @return Pruned TableScan.\r\n+   */\r\n+  def pruneColumns(prunedAttributes: Seq[Attribute]): OrcTableScan = {\r\n+    OrcTableScan(prunedAttributes, relation, columnPruningPred)\r\n+  }\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ * @param relation\r\n+ * @param child\r\n+ * @param overwrite\r\n+ */\r\n+private[sql] case class InsertIntoOrcTable(\r\n+    relation: OrcRelation,\r\n+    child: SparkPlan,\r\n+    overwrite: Boolean = false)\r\n+  extends UnaryNode with SparkHadoopMapRedUtil with org.apache.spark.Logging {\r\n+\r\n+  override def output = child.output\r\n+\r\n+  val intputClass: Class[_] = getInputClass\r\n+\r\n+  @transient val sc = sqlContext.sparkContext\r\n+\r\n+  val inspector = sc.broadcast(getInspector(intputClass))\r\n+\r\n+  @transient lazy val orcSerde = initFieldInfo\r\n+\r\n+  private def getInputClass: Class[_] = {\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val productClass = existRdd.rdd.firstParent.elementClassTag.runtimeClass\r\n+    logInfo(\"productClass is \" + productClass)\r\n+    val clazz = productClass\r\n+    if (null == relation.rowClass) {\r\n+      relation.rowClass = clazz\r\n+    }\r\n+    clazz\r\n+  }\r\n+\r\n+  private def getInspector(clazz: Class[_]): ObjectInspector = {\r\n+    val inspector = ObjectInspectorFactory.getReflectionObjectInspector(clazz,\r\n+      ObjectInspectorFactory.ObjectInspectorOptions.JAVA)\r\n+    inspector\r\n+  }\r\n+\r\n+  private def initFieldInfo(): OrcSerde = {\r\n+    val serde: OrcSerde = new OrcSerde\r\n+    serde\r\n+  }\r\n+\r\n+  /**\r\n+   * Inserts all rows into the Orc file.\r\n+   */\r\n+  override def execute() = {\r\n+    val childRdd = child.execute()\r\n+    assert(childRdd != null)\r\n+\r\n+    val job = new Job(sqlContext.sparkContext.hadoopConfiguration)\r\n+    // TODO: move that to function in object\r\n+    val conf = job.getConfiguration\r\n+\r\n+    val fspath = new Path(relation.path)\r\n+    val fs = fspath.getFileSystem(conf)\r\n+\r\n+    if (overwrite) {\r\n+      try {\r\n+        fs.delete(fspath, true)\r\n+      } catch {\r\n+        case e: IOException =>\r\n+          throw new IOException(\r\n+            s\"Unable to clear output directory ${fspath.toString} prior\"\r\n+              + s\" to InsertIntoOrcTable:\\n${e.toString}\")\r\n+      }\r\n+    }\r\n+\r\n+    val existRdd = child.asInstanceOf[ExistingRdd]\r\n+    val parentRdd = existRdd.rdd.firstParent[Product]\r\n+    val writableRdd = parentRdd.map(obj => {\r\n+      orcSerde.serialize(obj, inspector.value)\r\n+    })\r\n+\r\n+    saveAsHadoopFile(writableRdd, relation.rowClass, relation.path, conf)\r\n+\r\n+    // We return the child RDD to allow chaining (alternatively, one could return nothing).\r\n+    childRdd\r\n+  }\r\n+\r\n+\r\n+  // based on ``saveAsNewAPIHadoopFile`` in [[PairRDDFunctions]]\r\n+  // TODO: Maybe PairRDDFunctions should use Product2 instead of Tuple2?\r\n+  // .. then we could use the default one and could use [[MutablePair]]\r\n+  // instead of ``Tuple2``\r\n+  private def saveAsHadoopFile(\r\n+                                rdd: RDD[Writable],\r\n+                                rowClass: Class[_],\r\n+                                path: String,\r\n+                                @transient conf: Configuration) {\r\n+    val job = new Job(conf)\r\n+    val keyType = classOf[Void]\r\n+    job.setOutputKeyClass(keyType)\r\n+    job.setOutputValueClass(classOf[Writable])\r\n+    FileOutputFormat.setOutputPath(job, new Path(path))\r\n+\r\n+    val wrappedConf = new SerializableWritable(job.getConfiguration)\r\n+\r\n+    val formatter = new SimpleDateFormat(\"yyyyMMddHHmm\")\r\n+    val jobtrackerID = formatter.format(new Date())\r\n+    val stageId = sqlContext.sparkContext.newRddId()\r\n+\r\n+    val taskIdOffset =\r\n+      if (overwrite) {\r\n+        1\r\n+      } else {\r\n+        FileSystemHelper\r\n+          .findMaxTaskId(FileOutputFormat.getOutputPath(job).toString, job.getConfiguration) + 1\r\n+      }\r\n+\r\n+    def getWriter(\r\n+                   outFormat: OrcOutputFormat,\r\n+                   conf: Configuration,\r\n+                   path: Path,\r\n+                   reporter: Reporter) = {\r\n+      val fs = path.getFileSystem(conf)\r\n+      outFormat.getRecordWriter(fs, conf.asInstanceOf[JobConf], path.toUri.getPath, reporter).\r\n+        asInstanceOf[org.apache.hadoop.mapred.RecordWriter[NullWritable, Writable]]\r\n+    }\r\n+\r\n+    def getCommitterAndWriter(offset: Int, context: TaskAttemptContext) = {\r\n+      val outFormat = new OrcOutputFormat\r\n+\r\n+      val taskId: TaskID = context.getTaskAttemptID.getTaskID\r\n+      val partition: Int = taskId.getId\r\n+      val filename = s\"part-r-${partition + offset}.orc\"\r\n+      val output: Path = FileOutputFormat.getOutputPath(context)\r\n+      val committer = new FileOutputCommitter(output, context)\r\n+      val path = new Path(committer.getWorkPath, filename)\r\n+      val writer = getWriter(outFormat, wrappedConf.value, path, Reporter.NULL)\r\n+      (committer, writer)\r\n+    }\r\n+\r\n+    def writeShard(context: TaskContext, iter: Iterator[Writable]): Int = {\r\n+      // Hadoop wants a 32-bit task attempt ID, so if ours is bigger than Int.MaxValue, roll it\r\n+      // around by taking a mod. We expect that no task will be attempted 2 billion times.\r\n+      val attemptNumber = (context.attemptId % Int.MaxValue).toInt\r\n+      /* \"reduce task\" <split #> <attempt # = spark task #> */\r\n+      val attemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = false, context.partitionId,\r\n+        attemptNumber)\r\n+      val hadoopContext = newTaskAttemptContext(wrappedConf.value.asInstanceOf[JobConf], attemptId)\r\n+      val workerAndComitter = getCommitterAndWriter(taskIdOffset, hadoopContext)\r\n+      val writer = workerAndComitter._2\r\n+\r\n+      while (iter.hasNext) {\r\n+        val row = iter.next()\r\n+        writer.write(NullWritable.get(), row)\r\n+      }\r\n+\r\n+      writer.close(Reporter.NULL)\r\n+      workerAndComitter._1.commitTask(hadoopContext)\r\n+      return 1\r\n+    }\r\n+\r\n+    /* apparently we need a TaskAttemptID to construct an OutputCommitter;\r\n+     * however we're only going to use this local OutputCommitter for\r\n+     * setupJob/commitJob, so we just use a dummy \"map\" task.\r\n+     */\r\n+    val jobAttemptId = newTaskAttemptID(jobtrackerID, stageId, isMap = true, 0, 0)\r\n+    val jobTaskContext = newTaskAttemptContext(\r\n+      wrappedConf.value.asInstanceOf[JobConf], jobAttemptId)\r\n+    val workerAndComitter = getCommitterAndWriter(taskIdOffset, jobTaskContext)\r\n+    workerAndComitter._1.setupJob(jobTaskContext)\r\n+    sc.runJob(rdd, writeShard _)\r\n+    workerAndComitter._1.commitJob(jobTaskContext)\r\n+  }\r\n+\r\n+}\r\n+\r\n+/**\r\n+ *\r\n+ */\r"
  }],
  "prId": 2576
}]