[{
  "comments": [{
    "author": {
      "login": "wangyum"
    },
    "body": "Add a new line.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-07-30T05:12:24Z",
    "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.adaptive.rule.CoalescedShuffleReaderExec\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec, BuildLeft, BuildRight}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    if (!conf.optimizedLocalShuffleReaderEnabled) {\n+      return plan\n+    }\n+\n+    plan.transformUp {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.buildSide match {\n+          case BuildLeft if (bhj.right.isInstanceOf[CoalescedShuffleReaderExec]) =>\n+            bhj.right.asInstanceOf[CoalescedShuffleReaderExec].isLocal = true\n+          case BuildRight if (bhj.left.isInstanceOf[CoalescedShuffleReaderExec]) =>\n+            bhj.left.asInstanceOf[CoalescedShuffleReaderExec].isLocal = true\n+          case _ => None\n+        }\n+        bhj\n+    }\n+  }\n+}"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: this should be a verb, `OptimizeLocalShuffleReader `",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-18T07:41:48Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "if possible let's not add mutable states to the plan",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-18T07:42:38Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Why don't we traverse the tree once?\r\n```\r\ndef isShuffleStage(plan: SparkPlan): Boolean = plan match {\r\n  case _: ShuffleQueryStageExec => true\r\n  case ReusedQueryStageExec(_: ShuffleQueryStageExec) => true\r\n  case _ => false\r\n}\r\n\r\ndef canUseLocalShuffleReaderLeft(j: BroadcastHashJoinExec): Boolean = {\r\n  j.buildSide = BuildLeft && isShuffleStage(j.left)\r\n}\r\n\r\ndef canUseLocalShuffleReaderRight ...\r\n...\r\nplan transformDown {\r\n  case join: BroadcastHashJoinExec if canUseLocalShuffleReaderLeft(join) =>\r\n    val localShuffleReader = ...\r\n    join.copy(left = localShuffleReader)\r\n\r\n  ...\r\n}\r\n```",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-18T07:50:20Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "let's not strip the `ReusedQueryStageExec`",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-18T07:52:36Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {\n+      case stage: ShuffleQueryStageExec if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) if (stage.isLocalShuffle) =>"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shouldn't this be `child.outputPartitioning`?",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-18T08:06:51Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {\n+      case stage: ShuffleQueryStageExec if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+    }\n+\n+    val afterEnsureRequirements = EnsureRequirements(conf).apply(newPlan)\n+    val numExchanges = afterEnsureRequirements.collect {\n+      case e: ShuffleExchangeExec => e\n+    }.length\n+    if (numExchanges > 0) {\n+      logWarning(\"Local shuffle reader optimization is not applied due\" +\n+        \" to additional shuffles will be introduced.\")\n+      revertLocalShuffleReader(newPlan)\n+    } else {\n+      newPlan\n+    }\n+  }\n+}\n+\n+case class LocalShuffleReaderExec(\n+    child: QueryStageExec) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def doCanonicalize(): SparkPlan = child.canonicalized\n+\n+  override def outputPartitioning: Partitioning = {"
  }, {
    "author": {
      "login": "JkSelf"
    },
    "body": "Here for local shuffle reader, the partition number of Partitioning is the number of mappers. and the partition number of child.outputPartitioning is the number of reducers.  So how can be `child.outputPartitioning` ?",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-20T01:40:44Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {\n+      case stage: ShuffleQueryStageExec if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+    }\n+\n+    val afterEnsureRequirements = EnsureRequirements(conf).apply(newPlan)\n+    val numExchanges = afterEnsureRequirements.collect {\n+      case e: ShuffleExchangeExec => e\n+    }.length\n+    if (numExchanges > 0) {\n+      logWarning(\"Local shuffle reader optimization is not applied due\" +\n+        \" to additional shuffles will be introduced.\")\n+      revertLocalShuffleReader(newPlan)\n+    } else {\n+      newPlan\n+    }\n+  }\n+}\n+\n+case class LocalShuffleReaderExec(\n+    child: QueryStageExec) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def doCanonicalize(): SparkPlan = child.canonicalized\n+\n+  override def outputPartitioning: Partitioning = {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "sorry, I mean `child.child.outputPartitioning`",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-20T07:58:42Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {\n+      case stage: ShuffleQueryStageExec if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+    }\n+\n+    val afterEnsureRequirements = EnsureRequirements(conf).apply(newPlan)\n+    val numExchanges = afterEnsureRequirements.collect {\n+      case e: ShuffleExchangeExec => e\n+    }.length\n+    if (numExchanges > 0) {\n+      logWarning(\"Local shuffle reader optimization is not applied due\" +\n+        \" to additional shuffles will be introduced.\")\n+      revertLocalShuffleReader(newPlan)\n+    } else {\n+      newPlan\n+    }\n+  }\n+}\n+\n+case class LocalShuffleReaderExec(\n+    child: QueryStageExec) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def doCanonicalize(): SparkPlan = child.canonicalized\n+\n+  override def outputPartitioning: Partitioning = {"
  }, {
    "author": {
      "login": "JkSelf"
    },
    "body": "Here the `child.child.outputPartitioning` is `UnknowPartitioning(0)` and the partiiton number is not equal.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-10-08T02:51:24Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {\n+      case stage: ShuffleQueryStageExec if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+    }\n+\n+    val afterEnsureRequirements = EnsureRequirements(conf).apply(newPlan)\n+    val numExchanges = afterEnsureRequirements.collect {\n+      case e: ShuffleExchangeExec => e\n+    }.length\n+    if (numExchanges > 0) {\n+      logWarning(\"Local shuffle reader optimization is not applied due\" +\n+        \" to additional shuffles will be introduced.\")\n+      revertLocalShuffleReader(newPlan)\n+    } else {\n+      newPlan\n+    }\n+  }\n+}\n+\n+case class LocalShuffleReaderExec(\n+    child: QueryStageExec) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def doCanonicalize(): SparkPlan = child.canonicalized\n+\n+  override def outputPartitioning: Partitioning = {"
  }],
  "prId": 25295
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We can make it a leaf node to hide its `QueryStageExec`. We don't expect any other rules to change the underlying shuffle stage.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-09-18T08:08:10Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {\n+      case stage: ShuffleQueryStageExec if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+    }\n+\n+    val afterEnsureRequirements = EnsureRequirements(conf).apply(newPlan)\n+    val numExchanges = afterEnsureRequirements.collect {\n+      case e: ShuffleExchangeExec => e\n+    }.length\n+    if (numExchanges > 0) {\n+      logWarning(\"Local shuffle reader optimization is not applied due\" +\n+        \" to additional shuffles will be introduced.\")\n+      revertLocalShuffleReader(newPlan)\n+    } else {\n+      newPlan\n+    }\n+  }\n+}\n+\n+case class LocalShuffleReaderExec(\n+    child: QueryStageExec) extends UnaryExecNode {"
  }, {
    "author": {
      "login": "JkSelf"
    },
    "body": "Good suggestions. Updated.",
    "commit": "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "createdAt": "2019-10-08T02:51:57Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{EnsureRequirements, ShuffleExchangeExec}\n+import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec}\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class OptimizedLocalShuffleReader(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private def setIsLocalToFalse(shuffleStage: QueryStageExec): QueryStageExec = {\n+    shuffleStage match {\n+      case stage: ShuffleQueryStageExec =>\n+        stage.isLocalShuffle = false\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+        stage.isLocalShuffle = false\n+    }\n+    shuffleStage\n+  }\n+\n+  private def revertLocalShuffleReader(newPlan: SparkPlan): SparkPlan = {\n+    val revertPlan = newPlan.transformUp {\n+      case localReader: LocalShuffleReaderExec\n+        if (ShuffleQueryStageExec.isShuffleQueryStageExec(localReader.child)) =>\n+        setIsLocalToFalse(localReader.child)\n+    }\n+    revertPlan\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    // Collect the `BroadcastHashJoinExec` nodes and if isEmpty directly return.\n+    val bhjs = plan.collect {\n+      case bhj: BroadcastHashJoinExec => bhj\n+    }\n+\n+    if (!conf.optimizedLocalShuffleReaderEnabled || bhjs.isEmpty) {\n+      return plan\n+    }\n+\n+    // If the streamedPlan is `ShuffleQueryStageExec`, set the value of `isLocalShuffle` to true\n+    bhjs.map {\n+      case bhj: BroadcastHashJoinExec =>\n+        bhj.children map {\n+          case stage: ShuffleQueryStageExec => stage.isLocalShuffle = true\n+          case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) =>\n+            stage.isLocalShuffle = true\n+          case plan: SparkPlan => plan\n+        }\n+    }\n+\n+    // Add the new `LocalShuffleReaderExec` node if the value of `isLocalShuffle` is true\n+    val newPlan = plan.transformUp {\n+      case stage: ShuffleQueryStageExec if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) if (stage.isLocalShuffle) =>\n+        LocalShuffleReaderExec(stage)\n+    }\n+\n+    val afterEnsureRequirements = EnsureRequirements(conf).apply(newPlan)\n+    val numExchanges = afterEnsureRequirements.collect {\n+      case e: ShuffleExchangeExec => e\n+    }.length\n+    if (numExchanges > 0) {\n+      logWarning(\"Local shuffle reader optimization is not applied due\" +\n+        \" to additional shuffles will be introduced.\")\n+      revertLocalShuffleReader(newPlan)\n+    } else {\n+      newPlan\n+    }\n+  }\n+}\n+\n+case class LocalShuffleReaderExec(\n+    child: QueryStageExec) extends UnaryExecNode {"
  }],
  "prId": 25295
}]