[{
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "We could just merge this with `isAggregate(...)`. You could also turn this into an Extractor, if that makes life easier in `EnsureRequirements`.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-21T21:40:04Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "Fixed.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T12:49:17Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {"
  }],
  "prId": 10896
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Should we have a common superclass for the `*AggregateExec` classes?\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-21T21:42:04Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "yea, if they have a common superclass, it is more simple.\nI'll fix this.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T09:57:24Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>"
  }],
  "prId": 10896
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "A lot of duplication here. It would be nice if we have an parent for the `*AggregateExec` nodes.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-21T21:52:47Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+      case agg @ SortAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+    }\n+  }\n+\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n+  }\n+\n+  private[execution] def createPartialAggregate(operator: SparkPlan)"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "How about this change?\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T12:49:09Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+      case agg @ SortAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+    }\n+  }\n+\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n+  }\n+\n+  private[execution] def createPartialAggregate(operator: SparkPlan)"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Much better\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T16:12:25Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+      case agg @ SortAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+    }\n+  }\n+\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n+  }\n+\n+  private[execution] def createPartialAggregate(operator: SparkPlan)"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Could make this public instead of private[execution]? We just opened up a lot of similar APIs.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T18:25:44Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+      case agg @ SortAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+    }\n+  }\n+\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n+  }\n+\n+  private[execution] def createPartialAggregate(operator: SparkPlan)"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Small: The name of the function is also quite misleading. It returns a map side and merge aggregate pair, so `createMapMergeAggregatePair`? Please also add a little bit of documentation.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T18:29:55Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+      case agg @ SortAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+    }\n+  }\n+\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n+  }\n+\n+  private[execution] def createPartialAggregate(operator: SparkPlan)"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "fixed\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-23T08:13:59Z",
    "diffHunk": "@@ -27,26 +27,87 @@ import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateSto\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private[execution] def isAggregate(operator: SparkPlan): Boolean = {\n+    operator.isInstanceOf[HashAggregateExec] || operator.isInstanceOf[SortAggregateExec]\n+  }\n+\n+  private[execution] def supportPartialAggregate(operator: SparkPlan): Boolean = {\n+    assert(isAggregate(operator))\n+    def supportPartial(exprs: Seq[AggregateExpression]) =\n+      exprs.map(_.aggregateFunction).forall(_.supportsPartial)\n+    operator match {\n+      case agg @ HashAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+      case agg @ SortAggregateExec(_, _, aggregateExpressions, _, _, _, _) =>\n+        supportPartial(aggregateExpressions)\n+    }\n+  }\n+\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n+  }\n+\n+  private[execution] def createPartialAggregate(operator: SparkPlan)"
  }],
  "prId": 10896
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Just name this PartialAggregate?\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T15:03:18Z",
    "diffHunk": "@@ -19,34 +19,90 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n+import org.apache.spark.sql.execution.aggregate.{Aggregate => AggregateExec}\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object ExtractPartialAggregate {"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "okay\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-22T15:07:09Z",
    "diffHunk": "@@ -19,34 +19,90 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n+import org.apache.spark.sql.execution.aggregate.{Aggregate => AggregateExec}\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object ExtractPartialAggregate {"
  }],
  "prId": 10896
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Put this in a function. This can be found a few times in the code.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-23T15:50:40Z",
    "diffHunk": "@@ -19,34 +19,94 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object PartialAggregate {\n+\n+  def unapply(plan: SparkPlan): Option[Distribution] = plan match {\n+    case agg: AggregateExec\n+        if agg.aggregateExpressions.map(_.aggregateFunction).forall(_.supportsPartial) =>"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "yea, okay.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-24T01:03:48Z",
    "diffHunk": "@@ -19,34 +19,94 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object PartialAggregate {\n+\n+  def unapply(plan: SparkPlan): Option[Distribution] = plan match {\n+    case agg: AggregateExec\n+        if agg.aggregateExpressions.map(_.aggregateFunction).forall(_.supportsPartial) =>"
  }],
  "prId": 10896
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "mapSideAgg?\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-23T15:51:27Z",
    "diffHunk": "@@ -19,34 +19,94 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object PartialAggregate {\n+\n+  def unapply(plan: SparkPlan): Option[Distribution] = plan match {\n+    case agg: AggregateExec\n+        if agg.aggregateExpressions.map(_.aggregateFunction).forall(_.supportsPartial) =>\n+      Some(agg.requiredChildDistribution.head)\n+    case _ =>\n+      None\n+  }\n+}\n+\n+/**\n  * Utility functions used by the query planner to convert our plan to new aggregation code path.\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n   }\n \n-  private def createAggregate(\n+  /**\n+   * Builds new merge and map-side [[AggregateExec]]s from an input aggregate operator.\n+   * If an aggregation needs a shuffle for satisfying its own distribution and supports partial\n+   * aggregations, a map-side aggregation is appended before the shuffle in\n+   * [[org.apache.spark.sql.execution.exchange.EnsureRequirements]].\n+   */\n+  def createMapMergeAggregatePair(operator: SparkPlan): (SparkPlan, SparkPlan) = operator match {\n+    case agg: AggregateExec =>\n+      val mapSideAgg = createPartialAggregateExec(\n+        agg.groupingExpressions, agg.aggregateExpressions, agg.child)\n+      val mergeAgg = createAggregateExec(\n+        requiredChildDistributionExpressions = agg.requiredChildDistributionExpressions,\n+        groupingExpressions = agg.groupingExpressions.map(_.toAttribute),\n+        aggregateExpressions = updateMergeAggregateMode(agg.aggregateExpressions),\n+        aggregateAttributes = agg.aggregateAttributes,\n+        initialInputBufferOffset = agg.groupingExpressions.length,\n+        resultExpressions = agg.resultExpressions,\n+        child = agg.child"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "In fact, the final plan is [MergeAgg]<-[Shuffle]<-[MapSideAgg]. So, this function just returns the two aggregations separately, and the plan is built in `EnsureRequirements`. Is this a bad idea?\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-24T01:01:49Z",
    "diffHunk": "@@ -19,34 +19,94 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object PartialAggregate {\n+\n+  def unapply(plan: SparkPlan): Option[Distribution] = plan match {\n+    case agg: AggregateExec\n+        if agg.aggregateExpressions.map(_.aggregateFunction).forall(_.supportsPartial) =>\n+      Some(agg.requiredChildDistribution.head)\n+    case _ =>\n+      None\n+  }\n+}\n+\n+/**\n  * Utility functions used by the query planner to convert our plan to new aggregation code path.\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n   }\n \n-  private def createAggregate(\n+  /**\n+   * Builds new merge and map-side [[AggregateExec]]s from an input aggregate operator.\n+   * If an aggregation needs a shuffle for satisfying its own distribution and supports partial\n+   * aggregations, a map-side aggregation is appended before the shuffle in\n+   * [[org.apache.spark.sql.execution.exchange.EnsureRequirements]].\n+   */\n+  def createMapMergeAggregatePair(operator: SparkPlan): (SparkPlan, SparkPlan) = operator match {\n+    case agg: AggregateExec =>\n+      val mapSideAgg = createPartialAggregateExec(\n+        agg.groupingExpressions, agg.aggregateExpressions, agg.child)\n+      val mergeAgg = createAggregateExec(\n+        requiredChildDistributionExpressions = agg.requiredChildDistributionExpressions,\n+        groupingExpressions = agg.groupingExpressions.map(_.toAttribute),\n+        aggregateExpressions = updateMergeAggregateMode(agg.aggregateExpressions),\n+        aggregateAttributes = agg.aggregateAttributes,\n+        initialInputBufferOffset = agg.groupingExpressions.length,\n+        resultExpressions = agg.resultExpressions,\n+        child = agg.child"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "It violates the principle of least surprise. The `mergeAgg` is not usable without the `mapSideAgg`. This is fine for usage in `EnsureRequirements` because it gets straightened out anyway, but can be very surprising if someone uses it in a different way.\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-24T12:14:49Z",
    "diffHunk": "@@ -19,34 +19,94 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object PartialAggregate {\n+\n+  def unapply(plan: SparkPlan): Option[Distribution] = plan match {\n+    case agg: AggregateExec\n+        if agg.aggregateExpressions.map(_.aggregateFunction).forall(_.supportsPartial) =>\n+      Some(agg.requiredChildDistribution.head)\n+    case _ =>\n+      None\n+  }\n+}\n+\n+/**\n  * Utility functions used by the query planner to convert our plan to new aggregation code path.\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n   }\n \n-  private def createAggregate(\n+  /**\n+   * Builds new merge and map-side [[AggregateExec]]s from an input aggregate operator.\n+   * If an aggregation needs a shuffle for satisfying its own distribution and supports partial\n+   * aggregations, a map-side aggregation is appended before the shuffle in\n+   * [[org.apache.spark.sql.execution.exchange.EnsureRequirements]].\n+   */\n+  def createMapMergeAggregatePair(operator: SparkPlan): (SparkPlan, SparkPlan) = operator match {\n+    case agg: AggregateExec =>\n+      val mapSideAgg = createPartialAggregateExec(\n+        agg.groupingExpressions, agg.aggregateExpressions, agg.child)\n+      val mergeAgg = createAggregateExec(\n+        requiredChildDistributionExpressions = agg.requiredChildDistributionExpressions,\n+        groupingExpressions = agg.groupingExpressions.map(_.toAttribute),\n+        aggregateExpressions = updateMergeAggregateMode(agg.aggregateExpressions),\n+        aggregateAttributes = agg.aggregateAttributes,\n+        initialInputBufferOffset = agg.groupingExpressions.length,\n+        resultExpressions = agg.resultExpressions,\n+        child = agg.child"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "I fixed. Is this fix okay?\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-25T01:33:51Z",
    "diffHunk": "@@ -19,34 +19,94 @@ package org.apache.spark.sql.execution.aggregate\n \n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.physical.Distribution\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.execution.streaming.{StateStoreRestoreExec, StateStoreSaveExec}\n \n /**\n+ * A pattern that finds aggregate operators to support partial aggregations.\n+ */\n+object PartialAggregate {\n+\n+  def unapply(plan: SparkPlan): Option[Distribution] = plan match {\n+    case agg: AggregateExec\n+        if agg.aggregateExpressions.map(_.aggregateFunction).forall(_.supportsPartial) =>\n+      Some(agg.requiredChildDistribution.head)\n+    case _ =>\n+      None\n+  }\n+}\n+\n+/**\n  * Utility functions used by the query planner to convert our plan to new aggregation code path.\n  */\n object AggUtils {\n \n-  def planAggregateWithoutPartial(\n+  private def createPartialAggregateExec(\n       groupingExpressions: Seq[NamedExpression],\n       aggregateExpressions: Seq[AggregateExpression],\n-      resultExpressions: Seq[NamedExpression],\n-      child: SparkPlan): Seq[SparkPlan] = {\n+      child: SparkPlan): SparkPlan = {\n+    val groupingAttributes = groupingExpressions.map(_.toAttribute)\n+    val functionsWithDistinct = aggregateExpressions.filter(_.isDistinct)\n+    val partialAggregateExpressions = aggregateExpressions.map {\n+      case agg @ AggregateExpression(_, _, false, _) if functionsWithDistinct.length > 0 =>\n+        agg.copy(mode = PartialMerge)\n+      case agg =>\n+        agg.copy(mode = Partial)\n+    }\n+    val partialAggregateAttributes =\n+      partialAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n+    val partialResultExpressions =\n+      groupingAttributes ++\n+        partialAggregateExpressions.flatMap(_.aggregateFunction.inputAggBufferAttributes)\n \n-    val completeAggregateExpressions = aggregateExpressions.map(_.copy(mode = Complete))\n-    val completeAggregateAttributes = completeAggregateExpressions.map(_.resultAttribute)\n-    SortAggregateExec(\n-      requiredChildDistributionExpressions = Some(groupingExpressions),\n+    createAggregateExec(\n+      requiredChildDistributionExpressions = None,\n       groupingExpressions = groupingExpressions,\n-      aggregateExpressions = completeAggregateExpressions,\n-      aggregateAttributes = completeAggregateAttributes,\n-      initialInputBufferOffset = 0,\n-      resultExpressions = resultExpressions,\n-      child = child\n-    ) :: Nil\n+      aggregateExpressions = partialAggregateExpressions,\n+      aggregateAttributes = partialAggregateAttributes,\n+      initialInputBufferOffset = if (functionsWithDistinct.length > 0) {\n+        groupingExpressions.length + functionsWithDistinct.head.aggregateFunction.children.length\n+      } else {\n+        0\n+      },\n+      resultExpressions = partialResultExpressions,\n+      child = child)\n+  }\n+\n+  private def updateMergeAggregateMode(aggregateExpressions: Seq[AggregateExpression]) = {\n+    def updateMode(mode: AggregateMode) = mode match {\n+      case Partial => PartialMerge\n+      case Complete => Final\n+      case mode => mode\n+    }\n+    aggregateExpressions.map(e => e.copy(mode = updateMode(e.mode)))\n   }\n \n-  private def createAggregate(\n+  /**\n+   * Builds new merge and map-side [[AggregateExec]]s from an input aggregate operator.\n+   * If an aggregation needs a shuffle for satisfying its own distribution and supports partial\n+   * aggregations, a map-side aggregation is appended before the shuffle in\n+   * [[org.apache.spark.sql.execution.exchange.EnsureRequirements]].\n+   */\n+  def createMapMergeAggregatePair(operator: SparkPlan): (SparkPlan, SparkPlan) = operator match {\n+    case agg: AggregateExec =>\n+      val mapSideAgg = createPartialAggregateExec(\n+        agg.groupingExpressions, agg.aggregateExpressions, agg.child)\n+      val mergeAgg = createAggregateExec(\n+        requiredChildDistributionExpressions = agg.requiredChildDistributionExpressions,\n+        groupingExpressions = agg.groupingExpressions.map(_.toAttribute),\n+        aggregateExpressions = updateMergeAggregateMode(agg.aggregateExpressions),\n+        aggregateAttributes = agg.aggregateAttributes,\n+        initialInputBufferOffset = agg.groupingExpressions.length,\n+        resultExpressions = agg.resultExpressions,\n+        child = agg.child"
  }],
  "prId": 10896
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "have we tested the streaming aggregation with the optimization?\n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-30T08:34:53Z",
    "diffHunk": "@@ -225,37 +219,38 @@ object AggUtils {\n         rewrittenDistinctFunctions.zipWithIndex.map { case (func, i) =>\n           // We rewrite the aggregate function to a non-distinct aggregation because\n           // its input will have distinct arguments.\n-          // We just keep the isDistinct setting to true, so when users look at the query plan,\n-          // they still can see distinct aggregations.\n-          val expr = AggregateExpression(func, Final, isDistinct = true)\n+          // We keep the isDistinct setting to true because this flag is used to generate partial\n+          // aggregations and it is easy to see aggregation types in the query plan.\n+          val expr = AggregateExpression(func, Complete, isDistinct = true)\n           // Use original AggregationFunction to lookup attributes, which is used to build\n           // aggregateFunctionToAttribute\n           val attr = functionsWithDistinct(i).resultAttribute\n           (expr, attr)\n-      }.unzip\n+        }.unzip\n \n-      createAggregate(\n+      createAggregateExec(\n         requiredChildDistributionExpressions = Some(groupingAttributes),\n         groupingExpressions = groupingAttributes,\n         aggregateExpressions = finalAggregateExpressions ++ distinctAggregateExpressions,\n         aggregateAttributes = finalAggregateAttributes ++ distinctAggregateAttributes,\n         initialInputBufferOffset = groupingAttributes.length,\n         resultExpressions = resultExpressions,\n-        child = partialDistinctAggregate)\n+        child = partialAggregate)\n     }\n \n     finalAndCompleteAggregate :: Nil\n   }\n \n   /**\n    * Plans a streaming aggregation using the following progression:\n-   *  - Partial Aggregation\n-   *  - Shuffle\n-   *  - Partial Merge (now there is at most 1 tuple per group)\n+   *  - Partial Aggregation (now there is at most 1 tuple per group)\n    *  - StateStoreRestore (now there is 1 tuple from this batch + optionally one from the previous)\n    *  - PartialMerge (now there is at most 1 tuple per group)\n    *  - StateStoreSave (saves the tuple for the next batch)\n    *  - Complete (output the current result of the aggregation)\n+   *\n+   *  If the first aggregation needs a shuffle to satisfy its distribution, a map-side partial\n+   *  an aggregation and a shuffle are added in `EnsureRequirements`.\n    */\n   def planStreamingAggregation(",
    "line": 309
  }, {
    "author": {
      "login": "clockfly"
    },
    "body": "Yes, it is a bit risky to touch this part. \n",
    "commit": "ac6814514e091ffc377fde75d5c79b583c878626",
    "createdAt": "2016-08-30T08:59:44Z",
    "diffHunk": "@@ -225,37 +219,38 @@ object AggUtils {\n         rewrittenDistinctFunctions.zipWithIndex.map { case (func, i) =>\n           // We rewrite the aggregate function to a non-distinct aggregation because\n           // its input will have distinct arguments.\n-          // We just keep the isDistinct setting to true, so when users look at the query plan,\n-          // they still can see distinct aggregations.\n-          val expr = AggregateExpression(func, Final, isDistinct = true)\n+          // We keep the isDistinct setting to true because this flag is used to generate partial\n+          // aggregations and it is easy to see aggregation types in the query plan.\n+          val expr = AggregateExpression(func, Complete, isDistinct = true)\n           // Use original AggregationFunction to lookup attributes, which is used to build\n           // aggregateFunctionToAttribute\n           val attr = functionsWithDistinct(i).resultAttribute\n           (expr, attr)\n-      }.unzip\n+        }.unzip\n \n-      createAggregate(\n+      createAggregateExec(\n         requiredChildDistributionExpressions = Some(groupingAttributes),\n         groupingExpressions = groupingAttributes,\n         aggregateExpressions = finalAggregateExpressions ++ distinctAggregateExpressions,\n         aggregateAttributes = finalAggregateAttributes ++ distinctAggregateAttributes,\n         initialInputBufferOffset = groupingAttributes.length,\n         resultExpressions = resultExpressions,\n-        child = partialDistinctAggregate)\n+        child = partialAggregate)\n     }\n \n     finalAndCompleteAggregate :: Nil\n   }\n \n   /**\n    * Plans a streaming aggregation using the following progression:\n-   *  - Partial Aggregation\n-   *  - Shuffle\n-   *  - Partial Merge (now there is at most 1 tuple per group)\n+   *  - Partial Aggregation (now there is at most 1 tuple per group)\n    *  - StateStoreRestore (now there is 1 tuple from this batch + optionally one from the previous)\n    *  - PartialMerge (now there is at most 1 tuple per group)\n    *  - StateStoreSave (saves the tuple for the next batch)\n    *  - Complete (output the current result of the aggregation)\n+   *\n+   *  If the first aggregation needs a shuffle to satisfy its distribution, a map-side partial\n+   *  an aggregation and a shuffle are added in `EnsureRequirements`.\n    */\n   def planStreamingAggregation(",
    "line": 309
  }],
  "prId": 10896
}]