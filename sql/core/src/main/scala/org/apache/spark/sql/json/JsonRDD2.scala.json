[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Does it mean that we get an empty string? If so, can we keep the `StringType`? Otherwise, I feel we are destroying information.\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T04:02:43Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType"
  }, {
    "author": {
      "login": "NathanHowell"
    },
    "body": "Yes, an empty string gets inferred as a NullType. After inference is\ncomplete any remaining NullType fields get converted back to a StringType.\nThe old code does this and has test coverage for it, but it does seem a bit\nodd.\nOn May 3, 2015 9:03 PM, \"Yin Huai\" notifications@github.com wrote:\n\n> In sql/core/src/main/scala/org/apache/spark/sql/json/JsonRDD2.scala\n> https://github.com/apache/spark/pull/5801#discussion_r29565168:\n> \n> > -    }\n> > -  }\n> >   +\n> > -  /**\n> > -   \\* Infer the type of a json document from the parser's token stream\n> > -   */\n> > -  private def inferField(parser: JsonParser): DataType = {\n> > -    import com.fasterxml.jackson.core.JsonToken._\n> > -    parser.getCurrentToken match {\n> > -      case null | VALUE_NULL => NullType\n> >   +\n> > -      case FIELD_NAME =>\n> > -        parser.nextToken()\n> > -        inferField(parser)\n> >   +\n> > -      case VALUE_STRING if parser.getTextLength < 1 => NullType\n> \n> Does it mean that we get an empty string? If so, can we keep the\n> StringType? Otherwise, I feel we are destroying information.\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/5801/files#r29565168.\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T04:09:52Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType"
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "OK, I see. It is for those datasets that use a `\"\"` as a null. Since it is for inferring the type, it is same to use `NullType` because we can always get the `StringType` from other records. Actually, can we add a comment at there to explain its purpose?\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T04:25:21Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType"
  }, {
    "author": {
      "login": "NathanHowell"
    },
    "body": "Done.\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T05:30:30Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType"
  }],
  "prId": 5801
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Let's add a `TODO` for handling `DecimalType` with the fixed precision and scale.\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T04:42:26Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType\n+      case VALUE_STRING => StringType\n+      case START_OBJECT =>\n+        val builder = Seq.newBuilder[StructField]\n+        while (nextUntil(parser, END_OBJECT)) {\n+          builder += StructField(parser.getCurrentName, inferField(parser), nullable = true)\n+        }\n+\n+        StructType(builder.result().sortBy(_.name))\n+\n+      case START_ARRAY =>\n+        // If this JSON array is empty, we use NullType as a placeholder.\n+        // If this array is not empty in other JSON objects, we can resolve\n+        // the type as we pass through all JSON objects.\n+        var elementType: DataType = NullType\n+        while (nextUntil(parser, END_ARRAY)) {\n+          elementType = compatibleType(elementType, inferField(parser))\n+        }\n+\n+        ArrayType(elementType)\n+\n+      case VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT =>\n+        import JsonParser.NumberType._\n+        parser.getNumberType match {\n+          // For Integer values, use LongType by default.\n+          case INT | LONG => LongType\n+          // Since we do not have a data type backed by BigInteger,\n+          // when we see a Java BigInteger, we use DecimalType.\n+          case BIG_INTEGER | BIG_DECIMAL => DecimalType.Unlimited\n+          case FLOAT | DOUBLE => DoubleType\n+        }\n+\n+      case VALUE_TRUE | VALUE_FALSE => BooleanType\n+    }\n+  }\n+\n+  def nullTypeToStringType(struct: StructType): StructType = {\n+    val fields = struct.fields.map {\n+      case StructField(fieldName, dataType, nullable, _) =>\n+        val newType = dataType match {\n+          case NullType => StringType\n+          case ArrayType(NullType, containsNull) => ArrayType(StringType, containsNull)\n+          case ArrayType(struct: StructType, containsNull) =>\n+            ArrayType(nullTypeToStringType(struct), containsNull)\n+          case struct: StructType =>nullTypeToStringType(struct)\n+          case other: DataType => other\n+        }\n+\n+        StructField(fieldName, newType, nullable)\n+    }\n+\n+    StructType(fields)\n+  }\n+\n+  /**\n+   * Advance the parser until a null or a specific token is found\n+   */\n+  private def nextUntil(parser: JsonParser, stopOn: JsonToken): Boolean = {\n+    parser.nextToken() match {\n+      case null => false\n+      case x => x != stopOn\n+    }\n+  }\n+\n+  /**\n+   * Remove top-level ArrayType wrappers and merge the remaining schemas\n+   */\n+  private def compatibleRootType: (DataType, DataType) => DataType = {\n+    case (ArrayType(ty1, _), ty2) => compatibleRootType(ty1, ty2)\n+    case (ty1, ArrayType(ty2, _)) => compatibleRootType(ty1, ty2)\n+    case (ty1, ty2) => compatibleType(ty1, ty2)\n+  }\n+\n+  /**\n+   * Returns the most general data type for two given data types.\n+   */\n+  private[json] def compatibleType(t1: DataType, t2: DataType): DataType = {\n+    HiveTypeCoercion.findTightestCommonType(t1, t2).getOrElse {\n+      // t1 or t2 is a StructType, ArrayType, or an unexpected type.\n+      (t1, t2) match {\n+        case (other: DataType, NullType) => other\n+        case (NullType, other: DataType) => other\n+        case (StructType(fields1), StructType(fields2)) =>\n+          val newFields = (fields1 ++ fields2).groupBy(field => field.name).map {\n+            case (name, fieldTypes) =>\n+              val dataType = fieldTypes.view.map(_.dataType).reduce(compatibleType)\n+              StructField(name, dataType, nullable = true)\n+          }\n+          StructType(newFields.toSeq.sortBy(_.name))\n+\n+        case (ArrayType(elementType1, containsNull1), ArrayType(elementType2, containsNull2)) =>\n+          ArrayType(compatibleType(elementType1, elementType2), containsNull1 || containsNull2)\n+\n+        // strings and every string is a Json object.\n+        case (_, _) => StringType\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Parse the current token (and related children) according to a desired schema\n+   */\n+  private[sql] def convertField(\n+      factory: JsonFactory,\n+      parser: JsonParser,\n+      schema: DataType): Any = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    (parser.getCurrentToken, schema) match {\n+      case (null | VALUE_NULL, _) =>\n+        null\n+\n+      case (FIELD_NAME, _) =>\n+        parser.nextToken()\n+        convertField(factory, parser, schema)\n+\n+      case (VALUE_STRING, StringType) =>\n+        UTF8String(parser.getText)\n+\n+      case (VALUE_STRING, _) if parser.getTextLength < 1 =>\n+        // guard the non string type\n+        null\n+\n+      case (VALUE_STRING, DateType) =>\n+        DateUtils.millisToDays(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_STRING, TimestampType) =>\n+        new Timestamp(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_NUMBER_INT, TimestampType) =>\n+        new Timestamp(parser.getLongValue)\n+\n+      case (_, StringType) =>\n+        val writer = new ByteArrayOutputStream()\n+        val generator = factory.createGenerator(writer, JsonEncoding.UTF8)\n+        generator.copyCurrentStructure(parser)\n+        generator.close()\n+        UTF8String(writer.toByteArray)\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, FloatType) =>\n+        parser.getFloatValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DoubleType) =>\n+        parser.getDoubleValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DecimalType()) =>"
  }, {
    "author": {
      "login": "NathanHowell"
    },
    "body": "Done.\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T05:45:33Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType\n+      case VALUE_STRING => StringType\n+      case START_OBJECT =>\n+        val builder = Seq.newBuilder[StructField]\n+        while (nextUntil(parser, END_OBJECT)) {\n+          builder += StructField(parser.getCurrentName, inferField(parser), nullable = true)\n+        }\n+\n+        StructType(builder.result().sortBy(_.name))\n+\n+      case START_ARRAY =>\n+        // If this JSON array is empty, we use NullType as a placeholder.\n+        // If this array is not empty in other JSON objects, we can resolve\n+        // the type as we pass through all JSON objects.\n+        var elementType: DataType = NullType\n+        while (nextUntil(parser, END_ARRAY)) {\n+          elementType = compatibleType(elementType, inferField(parser))\n+        }\n+\n+        ArrayType(elementType)\n+\n+      case VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT =>\n+        import JsonParser.NumberType._\n+        parser.getNumberType match {\n+          // For Integer values, use LongType by default.\n+          case INT | LONG => LongType\n+          // Since we do not have a data type backed by BigInteger,\n+          // when we see a Java BigInteger, we use DecimalType.\n+          case BIG_INTEGER | BIG_DECIMAL => DecimalType.Unlimited\n+          case FLOAT | DOUBLE => DoubleType\n+        }\n+\n+      case VALUE_TRUE | VALUE_FALSE => BooleanType\n+    }\n+  }\n+\n+  def nullTypeToStringType(struct: StructType): StructType = {\n+    val fields = struct.fields.map {\n+      case StructField(fieldName, dataType, nullable, _) =>\n+        val newType = dataType match {\n+          case NullType => StringType\n+          case ArrayType(NullType, containsNull) => ArrayType(StringType, containsNull)\n+          case ArrayType(struct: StructType, containsNull) =>\n+            ArrayType(nullTypeToStringType(struct), containsNull)\n+          case struct: StructType =>nullTypeToStringType(struct)\n+          case other: DataType => other\n+        }\n+\n+        StructField(fieldName, newType, nullable)\n+    }\n+\n+    StructType(fields)\n+  }\n+\n+  /**\n+   * Advance the parser until a null or a specific token is found\n+   */\n+  private def nextUntil(parser: JsonParser, stopOn: JsonToken): Boolean = {\n+    parser.nextToken() match {\n+      case null => false\n+      case x => x != stopOn\n+    }\n+  }\n+\n+  /**\n+   * Remove top-level ArrayType wrappers and merge the remaining schemas\n+   */\n+  private def compatibleRootType: (DataType, DataType) => DataType = {\n+    case (ArrayType(ty1, _), ty2) => compatibleRootType(ty1, ty2)\n+    case (ty1, ArrayType(ty2, _)) => compatibleRootType(ty1, ty2)\n+    case (ty1, ty2) => compatibleType(ty1, ty2)\n+  }\n+\n+  /**\n+   * Returns the most general data type for two given data types.\n+   */\n+  private[json] def compatibleType(t1: DataType, t2: DataType): DataType = {\n+    HiveTypeCoercion.findTightestCommonType(t1, t2).getOrElse {\n+      // t1 or t2 is a StructType, ArrayType, or an unexpected type.\n+      (t1, t2) match {\n+        case (other: DataType, NullType) => other\n+        case (NullType, other: DataType) => other\n+        case (StructType(fields1), StructType(fields2)) =>\n+          val newFields = (fields1 ++ fields2).groupBy(field => field.name).map {\n+            case (name, fieldTypes) =>\n+              val dataType = fieldTypes.view.map(_.dataType).reduce(compatibleType)\n+              StructField(name, dataType, nullable = true)\n+          }\n+          StructType(newFields.toSeq.sortBy(_.name))\n+\n+        case (ArrayType(elementType1, containsNull1), ArrayType(elementType2, containsNull2)) =>\n+          ArrayType(compatibleType(elementType1, elementType2), containsNull1 || containsNull2)\n+\n+        // strings and every string is a Json object.\n+        case (_, _) => StringType\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Parse the current token (and related children) according to a desired schema\n+   */\n+  private[sql] def convertField(\n+      factory: JsonFactory,\n+      parser: JsonParser,\n+      schema: DataType): Any = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    (parser.getCurrentToken, schema) match {\n+      case (null | VALUE_NULL, _) =>\n+        null\n+\n+      case (FIELD_NAME, _) =>\n+        parser.nextToken()\n+        convertField(factory, parser, schema)\n+\n+      case (VALUE_STRING, StringType) =>\n+        UTF8String(parser.getText)\n+\n+      case (VALUE_STRING, _) if parser.getTextLength < 1 =>\n+        // guard the non string type\n+        null\n+\n+      case (VALUE_STRING, DateType) =>\n+        DateUtils.millisToDays(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_STRING, TimestampType) =>\n+        new Timestamp(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_NUMBER_INT, TimestampType) =>\n+        new Timestamp(parser.getLongValue)\n+\n+      case (_, StringType) =>\n+        val writer = new ByteArrayOutputStream()\n+        val generator = factory.createGenerator(writer, JsonEncoding.UTF8)\n+        generator.copyCurrentStructure(parser)\n+        generator.close()\n+        UTF8String(writer.toByteArray)\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, FloatType) =>\n+        parser.getFloatValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DoubleType) =>\n+        parser.getDoubleValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DecimalType()) =>"
  }],
  "prId": 5801
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "When will we reach here?\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T04:47:10Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType\n+      case VALUE_STRING => StringType\n+      case START_OBJECT =>\n+        val builder = Seq.newBuilder[StructField]\n+        while (nextUntil(parser, END_OBJECT)) {\n+          builder += StructField(parser.getCurrentName, inferField(parser), nullable = true)\n+        }\n+\n+        StructType(builder.result().sortBy(_.name))\n+\n+      case START_ARRAY =>\n+        // If this JSON array is empty, we use NullType as a placeholder.\n+        // If this array is not empty in other JSON objects, we can resolve\n+        // the type as we pass through all JSON objects.\n+        var elementType: DataType = NullType\n+        while (nextUntil(parser, END_ARRAY)) {\n+          elementType = compatibleType(elementType, inferField(parser))\n+        }\n+\n+        ArrayType(elementType)\n+\n+      case VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT =>\n+        import JsonParser.NumberType._\n+        parser.getNumberType match {\n+          // For Integer values, use LongType by default.\n+          case INT | LONG => LongType\n+          // Since we do not have a data type backed by BigInteger,\n+          // when we see a Java BigInteger, we use DecimalType.\n+          case BIG_INTEGER | BIG_DECIMAL => DecimalType.Unlimited\n+          case FLOAT | DOUBLE => DoubleType\n+        }\n+\n+      case VALUE_TRUE | VALUE_FALSE => BooleanType\n+    }\n+  }\n+\n+  def nullTypeToStringType(struct: StructType): StructType = {\n+    val fields = struct.fields.map {\n+      case StructField(fieldName, dataType, nullable, _) =>\n+        val newType = dataType match {\n+          case NullType => StringType\n+          case ArrayType(NullType, containsNull) => ArrayType(StringType, containsNull)\n+          case ArrayType(struct: StructType, containsNull) =>\n+            ArrayType(nullTypeToStringType(struct), containsNull)\n+          case struct: StructType =>nullTypeToStringType(struct)\n+          case other: DataType => other\n+        }\n+\n+        StructField(fieldName, newType, nullable)\n+    }\n+\n+    StructType(fields)\n+  }\n+\n+  /**\n+   * Advance the parser until a null or a specific token is found\n+   */\n+  private def nextUntil(parser: JsonParser, stopOn: JsonToken): Boolean = {\n+    parser.nextToken() match {\n+      case null => false\n+      case x => x != stopOn\n+    }\n+  }\n+\n+  /**\n+   * Remove top-level ArrayType wrappers and merge the remaining schemas\n+   */\n+  private def compatibleRootType: (DataType, DataType) => DataType = {\n+    case (ArrayType(ty1, _), ty2) => compatibleRootType(ty1, ty2)\n+    case (ty1, ArrayType(ty2, _)) => compatibleRootType(ty1, ty2)\n+    case (ty1, ty2) => compatibleType(ty1, ty2)\n+  }\n+\n+  /**\n+   * Returns the most general data type for two given data types.\n+   */\n+  private[json] def compatibleType(t1: DataType, t2: DataType): DataType = {\n+    HiveTypeCoercion.findTightestCommonType(t1, t2).getOrElse {\n+      // t1 or t2 is a StructType, ArrayType, or an unexpected type.\n+      (t1, t2) match {\n+        case (other: DataType, NullType) => other\n+        case (NullType, other: DataType) => other\n+        case (StructType(fields1), StructType(fields2)) =>\n+          val newFields = (fields1 ++ fields2).groupBy(field => field.name).map {\n+            case (name, fieldTypes) =>\n+              val dataType = fieldTypes.view.map(_.dataType).reduce(compatibleType)\n+              StructField(name, dataType, nullable = true)\n+          }\n+          StructType(newFields.toSeq.sortBy(_.name))\n+\n+        case (ArrayType(elementType1, containsNull1), ArrayType(elementType2, containsNull2)) =>\n+          ArrayType(compatibleType(elementType1, elementType2), containsNull1 || containsNull2)\n+\n+        // strings and every string is a Json object.\n+        case (_, _) => StringType\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Parse the current token (and related children) according to a desired schema\n+   */\n+  private[sql] def convertField(\n+      factory: JsonFactory,\n+      parser: JsonParser,\n+      schema: DataType): Any = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    (parser.getCurrentToken, schema) match {\n+      case (null | VALUE_NULL, _) =>\n+        null\n+\n+      case (FIELD_NAME, _) =>\n+        parser.nextToken()\n+        convertField(factory, parser, schema)\n+\n+      case (VALUE_STRING, StringType) =>\n+        UTF8String(parser.getText)\n+\n+      case (VALUE_STRING, _) if parser.getTextLength < 1 =>\n+        // guard the non string type\n+        null\n+\n+      case (VALUE_STRING, DateType) =>\n+        DateUtils.millisToDays(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_STRING, TimestampType) =>\n+        new Timestamp(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_NUMBER_INT, TimestampType) =>\n+        new Timestamp(parser.getLongValue)\n+\n+      case (_, StringType) =>\n+        val writer = new ByteArrayOutputStream()\n+        val generator = factory.createGenerator(writer, JsonEncoding.UTF8)\n+        generator.copyCurrentStructure(parser)\n+        generator.close()\n+        UTF8String(writer.toByteArray)\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, FloatType) =>\n+        parser.getFloatValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DoubleType) =>\n+        parser.getDoubleValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DecimalType()) =>\n+        Decimal(parser.getDecimalValue)\n+\n+      case (VALUE_NUMBER_INT, ByteType) =>\n+        parser.getByteValue\n+\n+      case (VALUE_NUMBER_INT, ShortType) =>\n+        parser.getShortValue\n+\n+      case (VALUE_NUMBER_INT, IntegerType) =>\n+        parser.getIntValue\n+\n+      case (VALUE_NUMBER_INT, LongType) =>\n+        parser.getLongValue\n+\n+      case (VALUE_TRUE, BooleanType) =>\n+        true\n+\n+      case (VALUE_FALSE, BooleanType) =>\n+        false\n+\n+      case (START_OBJECT, st: StructType) =>\n+        convertObject(factory, parser, st)\n+\n+      case (START_ARRAY, ArrayType(st, _)) =>\n+        convertList(factory, parser, st)\n+\n+      case (START_OBJECT, ArrayType(st, _)) =>\n+        // the business end of SPARK-3308:\n+        // when an object is found but an array is requested just wrap it in a list\n+        convertField(factory, parser, st) :: Nil\n+\n+      case (START_OBJECT, MapType(StringType, kt, _)) =>\n+        convertMap(factory, parser, kt)\n+\n+      case (_, udt: UserDefinedType[_]) =>\n+        udt.deserialize(convertField(factory, parser, udt.sqlType))\n+\n+      case _ =>\n+        parser.skipChildren()\n+        null"
  }, {
    "author": {
      "login": "NathanHowell"
    },
    "body": "Hopefully never (at least the compiler doesn't think so), probably leftover crud from development. I've removed it.\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T05:30:21Z",
    "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {\n+  def jsonStringToRow(\n+      json: RDD[String],\n+      schema: StructType,\n+      columnNameOfCorruptRecords: String): RDD[Row] = {\n+    parseJson(json, schema, columnNameOfCorruptRecords)\n+  }\n+\n+  def inferSchema(\n+      json: RDD[String],\n+      samplingRatio: Double = 1.0,\n+      columnNameOfCorruptRecords: String): StructType = {\n+    require(samplingRatio > 0, s\"samplingRatio ($samplingRatio) should be greater than 0\")\n+    val schemaData = if (samplingRatio > 0.99) {\n+      json\n+    } else {\n+      json.sample(withReplacement = false, samplingRatio, 1)\n+    }\n+\n+    // perform schema inference on each row and merge afterwards\n+    schemaData.mapPartitions { iter =>\n+      val factory = new JsonFactory()\n+      iter.map { row =>\n+        try {\n+          val parser = factory.createParser(row)\n+          parser.nextToken()\n+          inferField(parser)\n+        } catch {\n+          case _: JsonParseException =>\n+            StructType(Seq(StructField(columnNameOfCorruptRecords, StringType)))\n+        }\n+      }\n+    }.treeAggregate[DataType](StructType(Seq()))(compatibleRootType, compatibleRootType) match {\n+      case st: StructType => nullTypeToStringType(st)\n+    }\n+  }\n+\n+  /**\n+   * Infer the type of a json document from the parser's token stream\n+   */\n+  private def inferField(parser: JsonParser): DataType = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    parser.getCurrentToken match {\n+      case null | VALUE_NULL => NullType\n+\n+      case FIELD_NAME =>\n+        parser.nextToken()\n+        inferField(parser)\n+\n+      case VALUE_STRING if parser.getTextLength < 1 => NullType\n+      case VALUE_STRING => StringType\n+      case START_OBJECT =>\n+        val builder = Seq.newBuilder[StructField]\n+        while (nextUntil(parser, END_OBJECT)) {\n+          builder += StructField(parser.getCurrentName, inferField(parser), nullable = true)\n+        }\n+\n+        StructType(builder.result().sortBy(_.name))\n+\n+      case START_ARRAY =>\n+        // If this JSON array is empty, we use NullType as a placeholder.\n+        // If this array is not empty in other JSON objects, we can resolve\n+        // the type as we pass through all JSON objects.\n+        var elementType: DataType = NullType\n+        while (nextUntil(parser, END_ARRAY)) {\n+          elementType = compatibleType(elementType, inferField(parser))\n+        }\n+\n+        ArrayType(elementType)\n+\n+      case VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT =>\n+        import JsonParser.NumberType._\n+        parser.getNumberType match {\n+          // For Integer values, use LongType by default.\n+          case INT | LONG => LongType\n+          // Since we do not have a data type backed by BigInteger,\n+          // when we see a Java BigInteger, we use DecimalType.\n+          case BIG_INTEGER | BIG_DECIMAL => DecimalType.Unlimited\n+          case FLOAT | DOUBLE => DoubleType\n+        }\n+\n+      case VALUE_TRUE | VALUE_FALSE => BooleanType\n+    }\n+  }\n+\n+  def nullTypeToStringType(struct: StructType): StructType = {\n+    val fields = struct.fields.map {\n+      case StructField(fieldName, dataType, nullable, _) =>\n+        val newType = dataType match {\n+          case NullType => StringType\n+          case ArrayType(NullType, containsNull) => ArrayType(StringType, containsNull)\n+          case ArrayType(struct: StructType, containsNull) =>\n+            ArrayType(nullTypeToStringType(struct), containsNull)\n+          case struct: StructType =>nullTypeToStringType(struct)\n+          case other: DataType => other\n+        }\n+\n+        StructField(fieldName, newType, nullable)\n+    }\n+\n+    StructType(fields)\n+  }\n+\n+  /**\n+   * Advance the parser until a null or a specific token is found\n+   */\n+  private def nextUntil(parser: JsonParser, stopOn: JsonToken): Boolean = {\n+    parser.nextToken() match {\n+      case null => false\n+      case x => x != stopOn\n+    }\n+  }\n+\n+  /**\n+   * Remove top-level ArrayType wrappers and merge the remaining schemas\n+   */\n+  private def compatibleRootType: (DataType, DataType) => DataType = {\n+    case (ArrayType(ty1, _), ty2) => compatibleRootType(ty1, ty2)\n+    case (ty1, ArrayType(ty2, _)) => compatibleRootType(ty1, ty2)\n+    case (ty1, ty2) => compatibleType(ty1, ty2)\n+  }\n+\n+  /**\n+   * Returns the most general data type for two given data types.\n+   */\n+  private[json] def compatibleType(t1: DataType, t2: DataType): DataType = {\n+    HiveTypeCoercion.findTightestCommonType(t1, t2).getOrElse {\n+      // t1 or t2 is a StructType, ArrayType, or an unexpected type.\n+      (t1, t2) match {\n+        case (other: DataType, NullType) => other\n+        case (NullType, other: DataType) => other\n+        case (StructType(fields1), StructType(fields2)) =>\n+          val newFields = (fields1 ++ fields2).groupBy(field => field.name).map {\n+            case (name, fieldTypes) =>\n+              val dataType = fieldTypes.view.map(_.dataType).reduce(compatibleType)\n+              StructField(name, dataType, nullable = true)\n+          }\n+          StructType(newFields.toSeq.sortBy(_.name))\n+\n+        case (ArrayType(elementType1, containsNull1), ArrayType(elementType2, containsNull2)) =>\n+          ArrayType(compatibleType(elementType1, elementType2), containsNull1 || containsNull2)\n+\n+        // strings and every string is a Json object.\n+        case (_, _) => StringType\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Parse the current token (and related children) according to a desired schema\n+   */\n+  private[sql] def convertField(\n+      factory: JsonFactory,\n+      parser: JsonParser,\n+      schema: DataType): Any = {\n+    import com.fasterxml.jackson.core.JsonToken._\n+    (parser.getCurrentToken, schema) match {\n+      case (null | VALUE_NULL, _) =>\n+        null\n+\n+      case (FIELD_NAME, _) =>\n+        parser.nextToken()\n+        convertField(factory, parser, schema)\n+\n+      case (VALUE_STRING, StringType) =>\n+        UTF8String(parser.getText)\n+\n+      case (VALUE_STRING, _) if parser.getTextLength < 1 =>\n+        // guard the non string type\n+        null\n+\n+      case (VALUE_STRING, DateType) =>\n+        DateUtils.millisToDays(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_STRING, TimestampType) =>\n+        new Timestamp(DateUtils.stringToTime(parser.getText).getTime)\n+\n+      case (VALUE_NUMBER_INT, TimestampType) =>\n+        new Timestamp(parser.getLongValue)\n+\n+      case (_, StringType) =>\n+        val writer = new ByteArrayOutputStream()\n+        val generator = factory.createGenerator(writer, JsonEncoding.UTF8)\n+        generator.copyCurrentStructure(parser)\n+        generator.close()\n+        UTF8String(writer.toByteArray)\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, FloatType) =>\n+        parser.getFloatValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DoubleType) =>\n+        parser.getDoubleValue\n+\n+      case (VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT, DecimalType()) =>\n+        Decimal(parser.getDecimalValue)\n+\n+      case (VALUE_NUMBER_INT, ByteType) =>\n+        parser.getByteValue\n+\n+      case (VALUE_NUMBER_INT, ShortType) =>\n+        parser.getShortValue\n+\n+      case (VALUE_NUMBER_INT, IntegerType) =>\n+        parser.getIntValue\n+\n+      case (VALUE_NUMBER_INT, LongType) =>\n+        parser.getLongValue\n+\n+      case (VALUE_TRUE, BooleanType) =>\n+        true\n+\n+      case (VALUE_FALSE, BooleanType) =>\n+        false\n+\n+      case (START_OBJECT, st: StructType) =>\n+        convertObject(factory, parser, st)\n+\n+      case (START_ARRAY, ArrayType(st, _)) =>\n+        convertList(factory, parser, st)\n+\n+      case (START_OBJECT, ArrayType(st, _)) =>\n+        // the business end of SPARK-3308:\n+        // when an object is found but an array is requested just wrap it in a list\n+        convertField(factory, parser, st) :: Nil\n+\n+      case (START_OBJECT, MapType(StringType, kt, _)) =>\n+        convertMap(factory, parser, kt)\n+\n+      case (_, udt: UserDefinedType[_]) =>\n+        udt.deserialize(convertField(factory, parser, udt.sqlType))\n+\n+      case _ =>\n+        parser.skipChildren()\n+        null"
  }],
  "prId": 5801
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "would it make sense to name this JacksonStreaming?\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T05:37:18Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "and rename the other one JacksonDataBind\n\nBasically I don't see any connection of this one to \"RDD\".\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T05:41:36Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {"
  }, {
    "author": {
      "login": "NathanHowell"
    },
    "body": "Based on earlier feedback, it seems that this class should replace `JsonRDD` after 1.4.0 is branched. I don't really care that much about the name, since it's internal class (unlike the configuration, which is exposed), but it is more consistent with the work going on in `ParquetRelation2`.\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T05:44:19Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Yup - but it is really confusing to have this named RDD when it is not an RDD at all. When I was reviewing at the code quickly, I was grepping to find class JsonRDD2, and just couldn't find it ...\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T05:57:23Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {"
  }, {
    "author": {
      "login": "NathanHowell"
    },
    "body": "Do you want `JSONRelation` renamed to `JacksonDataBind`?\n",
    "commit": "26fea314ab12657df6c6397f832d35e681d2e27a",
    "createdAt": "2015-05-04T18:07:10Z",
    "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.json\n+\n+import java.io.ByteArrayOutputStream\n+import java.sql.Timestamp\n+\n+import scala.collection.Map\n+\n+import com.fasterxml.jackson.core._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.Logging\n+\n+private[sql] object JsonRDD2 extends Logging {"
  }],
  "prId": 5801
}]