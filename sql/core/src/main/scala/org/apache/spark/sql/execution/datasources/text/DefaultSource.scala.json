[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Do we still need this copy here? We could copy the bytes to BufferHolder directly, I think.\n",
    "commit": "5ef0fc794e55ac8e681d06be1e06e267c254beb0",
    "createdAt": "2015-10-29T17:32:25Z",
    "diffHunk": "@@ -92,17 +100,25 @@ private[sql] class TextRelation(\n     sqlContext.sparkContext.hadoopRDD(\n       conf.asInstanceOf[JobConf], classOf[TextInputFormat], classOf[LongWritable], classOf[Text])\n       .mapPartitions { iter =>\n+        val bufferHolder = new BufferHolder\n+        val unsafeRowWriter = new UnsafeRowWriter\n+        val unsafeRow = new UnsafeRow\n+\n         var buffer = new Array[Byte](1024)\n-        val row = new GenericMutableRow(1)\n         iter.map { case (_, line) =>\n           if (line.getLength > buffer.length) {\n             buffer = new Array[Byte](line.getLength)\n           }\n           System.arraycopy(line.getBytes, 0, buffer, 0, line.getLength)"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Good catch!\n",
    "commit": "5ef0fc794e55ac8e681d06be1e06e267c254beb0",
    "createdAt": "2015-10-30T16:31:49Z",
    "diffHunk": "@@ -92,17 +100,25 @@ private[sql] class TextRelation(\n     sqlContext.sparkContext.hadoopRDD(\n       conf.asInstanceOf[JobConf], classOf[TextInputFormat], classOf[LongWritable], classOf[Text])\n       .mapPartitions { iter =>\n+        val bufferHolder = new BufferHolder\n+        val unsafeRowWriter = new UnsafeRowWriter\n+        val unsafeRow = new UnsafeRow\n+\n         var buffer = new Array[Byte](1024)\n-        val row = new GenericMutableRow(1)\n         iter.map { case (_, line) =>\n           if (line.getLength > buffer.length) {\n             buffer = new Array[Byte](line.getLength)\n           }\n           System.arraycopy(line.getBytes, 0, buffer, 0, line.getLength)"
  }],
  "prId": 9305
}]