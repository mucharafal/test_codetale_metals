[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @mgaido91 .\r\nAlthough Apache Spark `branch-2.4` doesn't support JDK11 officially, some down-streams support JDK11 on `branch-2.4`. Could you include #25738 to match `master` branch?",
    "commit": "1b145e2158679dc27fce07a8ddf17f6341175afe",
    "createdAt": "2019-09-10T03:33:31Z",
    "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution\n+\n+import java.util.Properties\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD(\n+    var sqlRDD: RDD[InternalRow], conf: SQLConf) extends RDD[InternalRow](sqlRDD) {\n+  private val sqlConfigs = conf.getAllConfs\n+  private lazy val sqlConfExecutorSide = {\n+    val props = new Properties()\n+    props.putAll(sqlConfigs.asJava)"
  }],
  "prId": 25734
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Sorry for being late to the party here, why can't we use the `conf` in the constructor? Perhaps that one was supposed to be marked as transient?",
    "commit": "1b145e2158679dc27fce07a8ddf17f6341175afe",
    "createdAt": "2019-09-12T08:27:48Z",
    "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD(\n+    var sqlRDD: RDD[InternalRow], conf: SQLConf) extends RDD[InternalRow](sqlRDD) {\n+  private val sqlConfigs = conf.getAllConfs\n+  private lazy val sqlConfExecutorSide = {",
    "line": 36
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`SQLConf` has marked all its member variables with `@transient`, so practically `SQLConf` is not serializable. This is from day 1: https://github.com/apache/spark/pull/956/files#diff-41ef65b9ef5b518f77e2a03559893f4dR35\r\n\r\nWe can also revisit it. ",
    "commit": "1b145e2158679dc27fce07a8ddf17f6341175afe",
    "createdAt": "2019-09-12T08:52:47Z",
    "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD(\n+    var sqlRDD: RDD[InternalRow], conf: SQLConf) extends RDD[InternalRow](sqlRDD) {\n+  private val sqlConfigs = conf.getAllConfs\n+  private lazy val sqlConfExecutorSide = {",
    "line": 36
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Ok, then it makes sense. Sorry for the fuss.",
    "commit": "1b145e2158679dc27fce07a8ddf17f6341175afe",
    "createdAt": "2019-09-12T08:56:28Z",
    "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * It is just a wrapper over `sqlRDD`, which sets and makes effective all the configs from the\n+ * captured `SQLConf`.\n+ * Please notice that this means we may miss configurations set after the creation of this RDD and\n+ * before its execution.\n+ *\n+ * @param sqlRDD the `RDD` generated by the SQL plan\n+ * @param conf the `SQLConf` to apply to the execution of the SQL plan\n+ */\n+class SQLExecutionRDD(\n+    var sqlRDD: RDD[InternalRow], conf: SQLConf) extends RDD[InternalRow](sqlRDD) {\n+  private val sqlConfigs = conf.getAllConfs\n+  private lazy val sqlConfExecutorSide = {",
    "line": 36
  }],
  "prId": 25734
}]