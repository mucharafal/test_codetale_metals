[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "What's going on here?\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:50:42Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.hadoop.hive.conf.HiveConf\r\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, LeafNode}\r\n+import org.apache.spark.sql.catalyst.analysis.{UnresolvedException, MultiInstanceRelation}\r\n+import org.apache.spark.sql.catalyst.expressions.Attribute\r\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\r\n+import org.apache.spark.sql.catalyst.types._\r\n+import org.apache.hadoop.fs.{FileSystem, Path}\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.fs.permission.FsAction\r\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector\r\n+import org.apache.hadoop.hive.ql.io.orc._\r\n+import org.apache.hadoop.mapred.{FileInputFormat => NewFileInputFormat, JobConf}\r\n+import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind\r\n+import org.apache.hadoop.mapreduce.Job\r\n+import parquet.hadoop.util.ContextUtil\r\n+import java.util.Properties\r\n+import java.io.IOException\r\n+import scala.collection.mutable\r\n+import org.apache.spark.sql.SQLContext\r\n+\r\n+\r\n+private[sql] case class OrcRelation(\r\n+    path: String,\r\n+    @transient conf: Option[Configuration],\r\n+    @transient sqlContext: SQLContext,\r\n+    partitioningAttributes: Seq[Attribute] = Nil)\r\n+  extends LeafNode with MultiInstanceRelation {\r\n+  self: Product =>\r\n+\r\n+  val prop: Properties = new Properties\r\n+\r\n+  var rowClass: Class[_] = null\r\n+\r\n+  val fieldIdCache: mutable.Map[String, Int] = new mutable.HashMap[String, Int]\r\n+\r\n+  val fieldNameTypeCache: mutable.Map[String, String] = new mutable.HashMap[String, String]\r\n+\r\n+  override val output = orcSchema\r\n+\r\n+  def orcSchema: Seq[Attribute] = {\r\n+    val origPath = new Path(path)\r\n+    val reader = OrcFileOperator.readMetaData(origPath)\r\n+\r\n+    if (null != reader) {\r\n+      val inspector = reader.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+      val fields = inspector.getAllStructFieldRefs\r\n+\r\n+      if (fields.size() == 0) {\r\n+        return Seq.empty\r\n+      }\r\n+\r\n+      val totalType = reader.getTypes.get(0)\r\n+      val keys = totalType.getFieldNamesList\r\n+      val types = totalType.getSubtypesList\r\n+      log.info(\"field name is {}\", keys)\r\n+      log.info(\"types is {}\", types)\r\n+\r\n+      val colBuff = new StringBuilder\r\n+      val typeBuff = new StringBuilder\r\n+      for (i <- 0 until fields.size()) {\r\n+        val fieldName = fields.get(i).getFieldName\r\n+        val typeName = fields.get(i).getFieldObjectInspector.getTypeName\r\n+        colBuff.append(fieldName)\r\n+        fieldNameTypeCache.put(fieldName, typeName)\r\n+        fieldIdCache.put(fieldName, i)\r\n+        colBuff.append(\",\")\r\n+        typeBuff.append(typeName)\r\n+        typeBuff.append(\":\")\r\n+      }\r\n+      colBuff.setLength(colBuff.length - 1)\r\n+      typeBuff.setLength(typeBuff.length - 1)\r\n+      prop.setProperty(\"columns\", colBuff.toString())\r\n+      prop.setProperty(\"columns.types\", typeBuff.toString())\r\n+      val attributes = convertToAttributes(reader, keys, types)\r\n+      attributes\r\n+    } else {\r\n+      Seq.empty\r\n+    }\r\n+  }\r\n+\r\n+  def convertToAttributes(\r\n+     reader: Reader,\r\n+     keys: java.util.List[String],\r\n+     types: java.util.List[Integer]): Seq[Attribute] = {\r\n+    val range = 0.until(keys.size())\r\n+    range.map {\r\n+      i => reader.getTypes.get(types.get(i)).getKind match {\r\n+        case Kind.BOOLEAN =>\r\n+          new AttributeReference(keys.get(i), BooleanType, false)()\r\n+        case Kind.STRING =>\r\n+          new AttributeReference(keys.get(i), StringType, true)()\r\n+        case Kind.BYTE =>\r\n+          new AttributeReference(keys.get(i), ByteType, true)()\r\n+        case Kind.SHORT =>\r\n+          new AttributeReference(keys.get(i), ShortType, true)()\r\n+        case Kind.INT =>\r\n+          new AttributeReference(keys.get(i), IntegerType, true)()\r\n+        case Kind.LONG =>\r\n+          new AttributeReference(keys.get(i), LongType, false)()\r\n+        case Kind.FLOAT =>\r\n+          new AttributeReference(keys.get(i), FloatType, false)()\r\n+        case Kind.DOUBLE =>\r\n+          new AttributeReference(keys.get(i), DoubleType, false)()\r\n+        case _ => {\r\n+          log.info(\"unsupported datatype\")\r\n+          null\r\n+        }\r\n+      }\r\n+    }\r\n+  }\r\n+\r\n+  override def newInstance() = OrcRelation(path, conf, sqlContext).asInstanceOf[this.type]\r\n+}\r\n+\r\n+private[sql] object OrcRelation {\r\n+\r\n+\r\n+  // The orc compression short names\r\n+  val shortOrcCompressionCodecNames = Map(\r\n+    \"NONE\"         -> CompressionKind.NONE,\r\n+    \"UNCOMPRESSED\" -> CompressionKind.NONE,\r\n+    \"SNAPPY\"       -> CompressionKind.SNAPPY,\r\n+    \"ZLIB\"         -> CompressionKind.ZLIB,\r\n+    \"LZO\"          -> CompressionKind.LZO)\r\n+\r\n+  /**\r\n+   * Creates a new OrcRelation and underlying Orcfile for the given LogicalPlan. Note that\r\n+   * this is used inside [[org.apache.spark.sql.execution.SparkStrategies]] to\r\n+   * create a resolved relation as a data sink for writing to a Orcfile.\r\n+   *\r\n+   * @param pathString The directory the ORCfile will be stored in.\r\n+   * @param child The child node that will be used for extracting the schema.\r\n+   * @param conf A configuration to be used.\r\n+   * @return An empty OrcRelation with inferred metadata.\r\n+   */\r\n+  def create(pathString: String,\r\n+             child: LogicalPlan,\r\n+             conf: Configuration,\r\n+             sqlContext: SQLContext): OrcRelation = {\r\n+    if (!child.resolved) {\r\n+      throw new UnresolvedException[LogicalPlan](\r\n+        child,\r\n+        \"Attempt to create Orc table from unresolved child\")\r\n+    }\r\n+    createEmpty(pathString, child.output, false, conf, sqlContext)\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an empty OrcRelation and underlying Orcfile that only\r\n+   * consists of the Metadata for the given schema.\r\n+   *\r\n+   * @param pathString The directory the Orcfile will be stored in.\r\n+   * @param attributes The schema of the relation.\r\n+   * @param conf A configuration to be used.\r\n+   * @return An empty OrcRelation.\r\n+   */\r\n+  def createEmpty(pathString: String,\r\n+                  attributes: Seq[Attribute],\r\n+                  allowExisting: Boolean,\r\n+                  conf: Configuration,\r\n+                  sqlContext: SQLContext): OrcRelation = {\r\n+    val path = checkPath(pathString, allowExisting, conf)\r\n+\r\n+   /** set compression kind in hive 0.13.1\r\n+    * conf.set(\r\n+    *   HiveConf.ConfVars.OHIVE_ORC_DEFAULT_COMPRESS.varname,\r\n+    *   shortOrcCompressionCodecNames.getOrElse(\r\n+    *    sqlContext.orcCompressionCodec.toUpperCase, CompressionKind.NONE).name)\r\n+    */\r\n+    val orcRelation = new OrcRelation(path.toString, Some(conf), sqlContext)\r\n+\r\n+    orcRelation\r\n+  }\r\n+\r\n+  private def checkPath(pathStr: String, allowExisting: Boolean, conf: Configuration): Path = {\r\n+    if (pathStr == null) {\r\n+      throw new IllegalArgumentException(\"Unable to create OrcRelation: path is null\")\r\n+    }\r\n+    val origPath = new Path(pathStr)\r\n+    val fs = origPath.getFileSystem(conf)\r\n+    if (fs == null) {\r\n+      throw new IllegalArgumentException(\r\n+        s\"Unable to create OrcRelation: incorrectly formatted path $pathStr\")\r\n+    }\r\n+    val path = origPath.makeQualified(fs)\r\n+    if (!allowExisting && fs.exists(path)) {\r\n+      sys.error(s\"File $pathStr already exists.\")\r\n+    }\r\n+\r\n+    if (fs.exists(path) &&\r\n+      !fs.getFileStatus(path)\r\n+        .getPermission\r\n+        .getUserAction\r\n+        .implies(FsAction.READ_WRITE)) {\r\n+      throw new IOException(\r\n+        s\"Unable to create OrcRelation: path $path not read-writable\")\r\n+    }\r\n+    path\r\n+  }\r\n+}\r\n+\r\n+private[sql] object OrcFileOperator {\r\n+  final val COMPRESSION: String = \"orcfiles.compression\"\r\n+\r\n+  /**\r\n+   *\r\n+   * @param origPath\r\n+   * @return\r\n+   */\r\n+  def readMetaData(origPath: Path): Reader = {\r\n+    val job = new Job()\r\n+    val conf = ContextUtil.getConfiguration(job).asInstanceOf[JobConf]\r\n+    val fs: FileSystem = origPath.getFileSystem(conf)\r\n+    val orcFiles = FileSystemHelper.listFiles(origPath, conf, \".orc\")\r\n+    if (orcFiles != Seq.empty) {\r\n+//      NewFileInputFormat.setInputPaths(conf, orcFiles(0))// why set inputpath here\r"
  }],
  "prId": 2576
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Arguments should be indented 4 spaces from the `def`.\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:51:20Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.hadoop.hive.conf.HiveConf\r\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, LeafNode}\r\n+import org.apache.spark.sql.catalyst.analysis.{UnresolvedException, MultiInstanceRelation}\r\n+import org.apache.spark.sql.catalyst.expressions.Attribute\r\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\r\n+import org.apache.spark.sql.catalyst.types._\r\n+import org.apache.hadoop.fs.{FileSystem, Path}\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.fs.permission.FsAction\r\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector\r\n+import org.apache.hadoop.hive.ql.io.orc._\r\n+import org.apache.hadoop.mapred.{FileInputFormat => NewFileInputFormat, JobConf}\r\n+import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind\r\n+import org.apache.hadoop.mapreduce.Job\r\n+import parquet.hadoop.util.ContextUtil\r\n+import java.util.Properties\r\n+import java.io.IOException\r\n+import scala.collection.mutable\r\n+import org.apache.spark.sql.SQLContext\r\n+\r\n+\r\n+private[sql] case class OrcRelation(\r\n+    path: String,\r\n+    @transient conf: Option[Configuration],\r\n+    @transient sqlContext: SQLContext,\r\n+    partitioningAttributes: Seq[Attribute] = Nil)\r\n+  extends LeafNode with MultiInstanceRelation {\r\n+  self: Product =>\r\n+\r\n+  val prop: Properties = new Properties\r\n+\r\n+  var rowClass: Class[_] = null\r\n+\r\n+  val fieldIdCache: mutable.Map[String, Int] = new mutable.HashMap[String, Int]\r\n+\r\n+  val fieldNameTypeCache: mutable.Map[String, String] = new mutable.HashMap[String, String]\r\n+\r\n+  override val output = orcSchema\r\n+\r\n+  def orcSchema: Seq[Attribute] = {\r\n+    val origPath = new Path(path)\r\n+    val reader = OrcFileOperator.readMetaData(origPath)\r\n+\r\n+    if (null != reader) {\r\n+      val inspector = reader.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+      val fields = inspector.getAllStructFieldRefs\r\n+\r\n+      if (fields.size() == 0) {\r\n+        return Seq.empty\r\n+      }\r\n+\r\n+      val totalType = reader.getTypes.get(0)\r\n+      val keys = totalType.getFieldNamesList\r\n+      val types = totalType.getSubtypesList\r\n+      log.info(\"field name is {}\", keys)\r\n+      log.info(\"types is {}\", types)\r\n+\r\n+      val colBuff = new StringBuilder\r\n+      val typeBuff = new StringBuilder\r\n+      for (i <- 0 until fields.size()) {\r\n+        val fieldName = fields.get(i).getFieldName\r\n+        val typeName = fields.get(i).getFieldObjectInspector.getTypeName\r\n+        colBuff.append(fieldName)\r\n+        fieldNameTypeCache.put(fieldName, typeName)\r\n+        fieldIdCache.put(fieldName, i)\r\n+        colBuff.append(\",\")\r\n+        typeBuff.append(typeName)\r\n+        typeBuff.append(\":\")\r\n+      }\r\n+      colBuff.setLength(colBuff.length - 1)\r\n+      typeBuff.setLength(typeBuff.length - 1)\r\n+      prop.setProperty(\"columns\", colBuff.toString())\r\n+      prop.setProperty(\"columns.types\", typeBuff.toString())\r\n+      val attributes = convertToAttributes(reader, keys, types)\r\n+      attributes\r\n+    } else {\r\n+      Seq.empty\r\n+    }\r\n+  }\r\n+\r\n+  def convertToAttributes(\r\n+     reader: Reader,\r\n+     keys: java.util.List[String],\r\n+     types: java.util.List[Integer]): Seq[Attribute] = {\r\n+    val range = 0.until(keys.size())\r\n+    range.map {\r\n+      i => reader.getTypes.get(types.get(i)).getKind match {\r\n+        case Kind.BOOLEAN =>\r\n+          new AttributeReference(keys.get(i), BooleanType, false)()\r\n+        case Kind.STRING =>\r\n+          new AttributeReference(keys.get(i), StringType, true)()\r\n+        case Kind.BYTE =>\r\n+          new AttributeReference(keys.get(i), ByteType, true)()\r\n+        case Kind.SHORT =>\r\n+          new AttributeReference(keys.get(i), ShortType, true)()\r\n+        case Kind.INT =>\r\n+          new AttributeReference(keys.get(i), IntegerType, true)()\r\n+        case Kind.LONG =>\r\n+          new AttributeReference(keys.get(i), LongType, false)()\r\n+        case Kind.FLOAT =>\r\n+          new AttributeReference(keys.get(i), FloatType, false)()\r\n+        case Kind.DOUBLE =>\r\n+          new AttributeReference(keys.get(i), DoubleType, false)()\r\n+        case _ => {\r\n+          log.info(\"unsupported datatype\")\r\n+          null\r\n+        }\r\n+      }\r\n+    }\r\n+  }\r\n+\r\n+  override def newInstance() = OrcRelation(path, conf, sqlContext).asInstanceOf[this.type]\r\n+}\r\n+\r\n+private[sql] object OrcRelation {\r\n+\r\n+\r\n+  // The orc compression short names\r\n+  val shortOrcCompressionCodecNames = Map(\r\n+    \"NONE\"         -> CompressionKind.NONE,\r\n+    \"UNCOMPRESSED\" -> CompressionKind.NONE,\r\n+    \"SNAPPY\"       -> CompressionKind.SNAPPY,\r\n+    \"ZLIB\"         -> CompressionKind.ZLIB,\r\n+    \"LZO\"          -> CompressionKind.LZO)\r\n+\r\n+  /**\r\n+   * Creates a new OrcRelation and underlying Orcfile for the given LogicalPlan. Note that\r\n+   * this is used inside [[org.apache.spark.sql.execution.SparkStrategies]] to\r\n+   * create a resolved relation as a data sink for writing to a Orcfile.\r\n+   *\r\n+   * @param pathString The directory the ORCfile will be stored in.\r\n+   * @param child The child node that will be used for extracting the schema.\r\n+   * @param conf A configuration to be used.\r\n+   * @return An empty OrcRelation with inferred metadata.\r\n+   */\r\n+  def create(pathString: String,\r"
  }],
  "prId": 2576
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Remove extra space.\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:51:35Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.hadoop.hive.conf.HiveConf\r\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, LeafNode}\r\n+import org.apache.spark.sql.catalyst.analysis.{UnresolvedException, MultiInstanceRelation}\r\n+import org.apache.spark.sql.catalyst.expressions.Attribute\r\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\r\n+import org.apache.spark.sql.catalyst.types._\r\n+import org.apache.hadoop.fs.{FileSystem, Path}\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.fs.permission.FsAction\r\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector\r\n+import org.apache.hadoop.hive.ql.io.orc._\r\n+import org.apache.hadoop.mapred.{FileInputFormat => NewFileInputFormat, JobConf}\r\n+import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind\r\n+import org.apache.hadoop.mapreduce.Job\r\n+import parquet.hadoop.util.ContextUtil\r\n+import java.util.Properties\r\n+import java.io.IOException\r\n+import scala.collection.mutable\r\n+import org.apache.spark.sql.SQLContext\r\n+\r\n+\r\n+private[sql] case class OrcRelation(\r\n+    path: String,\r\n+    @transient conf: Option[Configuration],\r\n+    @transient sqlContext: SQLContext,\r\n+    partitioningAttributes: Seq[Attribute] = Nil)\r\n+  extends LeafNode with MultiInstanceRelation {\r\n+  self: Product =>\r\n+\r\n+  val prop: Properties = new Properties\r\n+\r\n+  var rowClass: Class[_] = null\r\n+\r\n+  val fieldIdCache: mutable.Map[String, Int] = new mutable.HashMap[String, Int]\r\n+\r\n+  val fieldNameTypeCache: mutable.Map[String, String] = new mutable.HashMap[String, String]\r\n+\r\n+  override val output = orcSchema\r\n+\r\n+  def orcSchema: Seq[Attribute] = {\r\n+    val origPath = new Path(path)\r\n+    val reader = OrcFileOperator.readMetaData(origPath)\r\n+\r\n+    if (null != reader) {\r\n+      val inspector = reader.getObjectInspector.asInstanceOf[StructObjectInspector]\r\n+      val fields = inspector.getAllStructFieldRefs\r\n+\r\n+      if (fields.size() == 0) {\r\n+        return Seq.empty\r\n+      }\r\n+\r\n+      val totalType = reader.getTypes.get(0)\r\n+      val keys = totalType.getFieldNamesList\r\n+      val types = totalType.getSubtypesList\r\n+      log.info(\"field name is {}\", keys)\r\n+      log.info(\"types is {}\", types)\r\n+\r\n+      val colBuff = new StringBuilder\r\n+      val typeBuff = new StringBuilder\r\n+      for (i <- 0 until fields.size()) {\r\n+        val fieldName = fields.get(i).getFieldName\r\n+        val typeName = fields.get(i).getFieldObjectInspector.getTypeName\r\n+        colBuff.append(fieldName)\r\n+        fieldNameTypeCache.put(fieldName, typeName)\r\n+        fieldIdCache.put(fieldName, i)\r\n+        colBuff.append(\",\")\r\n+        typeBuff.append(typeName)\r\n+        typeBuff.append(\":\")\r\n+      }\r\n+      colBuff.setLength(colBuff.length - 1)\r\n+      typeBuff.setLength(typeBuff.length - 1)\r\n+      prop.setProperty(\"columns\", colBuff.toString())\r\n+      prop.setProperty(\"columns.types\", typeBuff.toString())\r\n+      val attributes = convertToAttributes(reader, keys, types)\r\n+      attributes\r\n+    } else {\r\n+      Seq.empty\r\n+    }\r\n+  }\r\n+\r\n+  def convertToAttributes(\r\n+     reader: Reader,\r\n+     keys: java.util.List[String],\r\n+     types: java.util.List[Integer]): Seq[Attribute] = {\r\n+    val range = 0.until(keys.size())\r\n+    range.map {\r\n+      i => reader.getTypes.get(types.get(i)).getKind match {\r\n+        case Kind.BOOLEAN =>\r\n+          new AttributeReference(keys.get(i), BooleanType, false)()\r\n+        case Kind.STRING =>\r\n+          new AttributeReference(keys.get(i), StringType, true)()\r\n+        case Kind.BYTE =>\r\n+          new AttributeReference(keys.get(i), ByteType, true)()\r\n+        case Kind.SHORT =>\r\n+          new AttributeReference(keys.get(i), ShortType, true)()\r\n+        case Kind.INT =>\r\n+          new AttributeReference(keys.get(i), IntegerType, true)()\r\n+        case Kind.LONG =>\r\n+          new AttributeReference(keys.get(i), LongType, false)()\r\n+        case Kind.FLOAT =>\r\n+          new AttributeReference(keys.get(i), FloatType, false)()\r\n+        case Kind.DOUBLE =>\r\n+          new AttributeReference(keys.get(i), DoubleType, false)()\r\n+        case _ => {\r\n+          log.info(\"unsupported datatype\")\r\n+          null\r\n+        }\r\n+      }\r\n+    }\r\n+  }\r\n+\r\n+  override def newInstance() = OrcRelation(path, conf, sqlContext).asInstanceOf[this.type]\r\n+}\r\n+\r\n+private[sql] object OrcRelation {\r\n+\r\n+\r"
  }],
  "prId": 2576
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Alphabetize these.  And break them up as described here: https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Imports\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T19:00:45Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.hadoop.hive.conf.HiveConf\r\n+import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, LeafNode}\r\n+import org.apache.spark.sql.catalyst.analysis.{UnresolvedException, MultiInstanceRelation}\r\n+import org.apache.spark.sql.catalyst.expressions.Attribute\r\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\r\n+import org.apache.spark.sql.catalyst.types._\r\n+import org.apache.hadoop.fs.{FileSystem, Path}\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.fs.permission.FsAction\r\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector\r\n+import org.apache.hadoop.hive.ql.io.orc._\r\n+import org.apache.hadoop.mapred.{FileInputFormat => NewFileInputFormat, JobConf}\r\n+import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind\r\n+import org.apache.hadoop.mapreduce.Job\r\n+import parquet.hadoop.util.ContextUtil\r\n+import java.util.Properties\r\n+import java.io.IOException\r\n+import scala.collection.mutable\r\n+import org.apache.spark.sql.SQLContext\r"
  }],
  "prId": 2576
}]