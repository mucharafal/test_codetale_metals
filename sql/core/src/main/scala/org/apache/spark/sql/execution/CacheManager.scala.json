[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`columnNames` -> `columns` because it becomes `Seq[Attribute]` instead of `Seq[String]`.",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-12T04:56:25Z",
    "diffHunk": "@@ -181,6 +182,38 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  private[sql] def analyzeColumnCacheQuery(\n+      query: Dataset[_],\n+      columnNames: Seq[Attribute]): Unit = writeLock {"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```scala\r\n-        val oldStats = cachedData.cachedRepresentation.statsOfPlanToCache\r\n+        val oldStats = relation.statsOfPlanToCache\r\n```",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-12T05:04:04Z",
    "diffHunk": "@@ -181,6 +182,38 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  private[sql] def analyzeColumnCacheQuery(\n+      query: Dataset[_],\n+      columnNames: Seq[Attribute]): Unit = writeLock {\n+    val cachedData = lookupCachedData(query)\n+    if (cachedData.isEmpty) {\n+      logWarning(\"The cached data not found, so you need to cache the query first.\")\n+    } else {\n+      cachedData.foreach { cachedData =>\n+        val relation = cachedData.cachedRepresentation\n+        val (rowCount, newColStats) =\n+          CommandUtils.computeColumnStats(query.sparkSession, relation, columnNames)\n+        val oldStats = cachedData.cachedRepresentation.statsOfPlanToCache"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "ditto.\r\n```scala\r\n-        cachedData.cachedRepresentation.statsOfPlanToCache = newStats\r\n+        relation.statsOfPlanToCache = newStats\r\n```",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-12T05:04:27Z",
    "diffHunk": "@@ -181,6 +182,38 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  private[sql] def analyzeColumnCacheQuery(\n+      query: Dataset[_],\n+      columnNames: Seq[Attribute]): Unit = writeLock {\n+    val cachedData = lookupCachedData(query)\n+    if (cachedData.isEmpty) {\n+      logWarning(\"The cached data not found, so you need to cache the query first.\")\n+    } else {\n+      cachedData.foreach { cachedData =>\n+        val relation = cachedData.cachedRepresentation\n+        val (rowCount, newColStats) =\n+          CommandUtils.computeColumnStats(query.sparkSession, relation, columnNames)\n+        val oldStats = cachedData.cachedRepresentation.statsOfPlanToCache\n+        val newStats = oldStats.copy(\n+          rowCount = Some(rowCount),\n+          attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+        )\n+        cachedData.cachedRepresentation.statsOfPlanToCache = newStats"
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`identifier` -> `name`?",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-13T19:39:04Z",
    "diffHunk": "@@ -181,6 +182,38 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  private[sql] def analyzeColumnCacheQuery(\n+      query: Dataset[_],\n+      column: Seq[Attribute]): Unit = writeLock {\n+    val cachedData = lookupCachedData(query)\n+    if (cachedData.isEmpty) {\n+      logWarning(\"The cached data not found, so you need to cache the query first.\")\n+    } else {\n+      cachedData.foreach { cachedData =>\n+        val relation = cachedData.cachedRepresentation\n+        val (rowCount, newColStats) =\n+          CommandUtils.computeColumnStats(query.sparkSession, relation, column)\n+        val oldStats = relation.statsOfPlanToCache\n+        val newStats = oldStats.copy(\n+          rowCount = Some(rowCount),\n+          attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+        )\n+        relation.statsOfPlanToCache = newStats\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Analyzes column statistics in an already-cached table.\n+   *\n+   * @param spark        The Spark session.\n+   * @param tableName    The identifier of a cached table."
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "fixed",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-14T01:58:32Z",
    "diffHunk": "@@ -181,6 +182,38 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  private[sql] def analyzeColumnCacheQuery(\n+      query: Dataset[_],\n+      column: Seq[Attribute]): Unit = writeLock {\n+    val cachedData = lookupCachedData(query)\n+    if (cachedData.isEmpty) {\n+      logWarning(\"The cached data not found, so you need to cache the query first.\")\n+    } else {\n+      cachedData.foreach { cachedData =>\n+        val relation = cachedData.cachedRepresentation\n+        val (rowCount, newColStats) =\n+          CommandUtils.computeColumnStats(query.sparkSession, relation, column)\n+        val oldStats = relation.statsOfPlanToCache\n+        val newStats = oldStats.copy(\n+          rowCount = Some(rowCount),\n+          attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+        )\n+        relation.statsOfPlanToCache = newStats\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Analyzes column statistics in an already-cached table.\n+   *\n+   * @param spark        The Spark session.\n+   * @param tableName    The identifier of a cached table."
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@maropu . Recently, https://github.com/apache/spark/pull/24028 made an assumption which `cachedData` as an immutable sequence and this is guarded by `this` for all WRITE operations. This is a write operation, isn't it?\r\n\r\ncc @cloud-fan ",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-18T02:06:36Z",
    "diffHunk": "@@ -154,6 +155,22 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  // Analyzes column statistics in the given cache data\n+  private[sql] def analyzeColumnCacheQuery(\n+      sparkSession: SparkSession,\n+      cachedData: CachedData,\n+      column: Seq[Attribute]): Unit = {",
    "line": 20
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "This function is able to ignore that assumption because this only updates `relation.statsOfPlanToCache` instead of `cachedData` itself?\r\n\r\nCan we add some description at the function description?",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-18T02:16:50Z",
    "diffHunk": "@@ -154,6 +155,22 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  // Analyzes column statistics in the given cache data\n+  private[sql] def analyzeColumnCacheQuery(\n+      sparkSession: SparkSession,\n+      cachedData: CachedData,\n+      column: Seq[Attribute]): Unit = {\n+    val relation = cachedData.cachedRepresentation\n+    val (rowCount, newColStats) =\n+      CommandUtils.computeColumnStats(sparkSession, relation, column)\n+    val oldStats = relation.statsOfPlanToCache\n+    val newStats = oldStats.copy(\n+      rowCount = Some(rowCount),\n+      attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+    )\n+    relation.statsOfPlanToCache = newStats",
    "line": 29
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "Yea, I think so. Rather, it seems we need `@volatile` for `statsOfPlanToCache` so that other threads cannot see an incomplete object for `Statistics`?",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-18T02:27:48Z",
    "diffHunk": "@@ -154,6 +155,22 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  // Analyzes column statistics in the given cache data\n+  private[sql] def analyzeColumnCacheQuery(\n+      sparkSession: SparkSession,\n+      cachedData: CachedData,\n+      column: Seq[Attribute]): Unit = {\n+    val relation = cachedData.cachedRepresentation\n+    val (rowCount, newColStats) =\n+      CommandUtils.computeColumnStats(sparkSession, relation, column)\n+    val oldStats = relation.statsOfPlanToCache\n+    val newStats = oldStats.copy(\n+      rowCount = Some(rowCount),\n+      attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+    )\n+    relation.statsOfPlanToCache = newStats",
    "line": 29
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "+1 for adding `@volatile` explicitly.",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-18T02:39:10Z",
    "diffHunk": "@@ -154,6 +155,22 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  // Analyzes column statistics in the given cache data\n+  private[sql] def analyzeColumnCacheQuery(\n+      sparkSession: SparkSession,\n+      cachedData: CachedData,\n+      column: Seq[Attribute]): Unit = {\n+    val relation = cachedData.cachedRepresentation\n+    val (rowCount, newColStats) =\n+      CommandUtils.computeColumnStats(sparkSession, relation, column)\n+    val oldStats = relation.statsOfPlanToCache\n+    val newStats = oldStats.copy(\n+      rowCount = Some(rowCount),\n+      attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+    )\n+    relation.statsOfPlanToCache = newStats",
    "line": 29
  }],
  "prId": 24047
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "@dongjoon-hyun On second thought, we need lock here as you suggested before. Could you check again? https://github.com/apache/spark/pull/24178",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-22T08:17:50Z",
    "diffHunk": "@@ -154,6 +155,22 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  // Analyzes column statistics in the given cache data\n+  private[sql] def analyzeColumnCacheQuery(\n+      sparkSession: SparkSession,\n+      cachedData: CachedData,\n+      column: Seq[Attribute]): Unit = {\n+    val relation = cachedData.cachedRepresentation\n+    val (rowCount, newColStats) =\n+      CommandUtils.computeColumnStats(sparkSession, relation, column)\n+    val oldStats = relation.statsOfPlanToCache\n+    val newStats = oldStats.copy(\n+      rowCount = Some(rowCount),\n+      attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+    )\n+    relation.statsOfPlanToCache = newStats",
    "line": 29
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yes. `Locking` is more conservative and the one I preferred. BTW, what's the change in your mind?",
    "commit": "c6f0fb5e7d0a9b5ede73e1111c058894c65ee238",
    "createdAt": "2019-03-22T16:32:10Z",
    "diffHunk": "@@ -154,6 +155,22 @@ class CacheManager extends Logging {\n     }\n   }\n \n+  // Analyzes column statistics in the given cache data\n+  private[sql] def analyzeColumnCacheQuery(\n+      sparkSession: SparkSession,\n+      cachedData: CachedData,\n+      column: Seq[Attribute]): Unit = {\n+    val relation = cachedData.cachedRepresentation\n+    val (rowCount, newColStats) =\n+      CommandUtils.computeColumnStats(sparkSession, relation, column)\n+    val oldStats = relation.statsOfPlanToCache\n+    val newStats = oldStats.copy(\n+      rowCount = Some(rowCount),\n+      attributeStats = AttributeMap((oldStats.attributeStats ++ newColStats).toSeq)\n+    )\n+    relation.statsOfPlanToCache = newStats",
    "line": 29
  }],
  "prId": 24047
}]