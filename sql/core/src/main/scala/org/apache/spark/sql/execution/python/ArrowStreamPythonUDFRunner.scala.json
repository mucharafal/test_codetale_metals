[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "cc @BryanCutler for arrow side issues.",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-26T16:08:06Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowStreamPythonUDFRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "I think that `ArrowStreamReader.close()` should not close the input stream.  I filed https://issues.apache.org/jira/browse/ARROW-1613 to fix this.",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-26T18:26:03Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowStreamPythonUDFRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave"
  }],
  "prId": 19349
}]