[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "This is expensive, we should avoid that.\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T17:13:27Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]",
    "line": 36
  }, {
    "author": {
      "login": "ilganeli"
    },
    "body": "Is there a preferred way to do this? I could have the HashSet be created once to avoid creating it every time and clear it between calls?\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T17:22:10Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]",
    "line": 36
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Once we make sure that only visit the items once, then the rows will not be outputted twice.\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T17:51:23Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]",
    "line": 36
  }, {
    "author": {
      "login": "ilganeli"
    },
    "body": "The point is that after a sort, everything is reorganized so we may end up traversing some elements that have already been processed, no?\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T19:12:18Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]",
    "line": 36
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Because the iterator can only be consumed once, so we only sort the items that have not been visited.\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T19:17:30Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]",
    "line": 36
  }, {
    "author": {
      "login": "ilganeli"
    },
    "body": "Got it, so just use an `ExternalSorter`` based off that iterator to do the sort to avoid potential memory problems.\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T21:22:01Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]",
    "line": 36
  }],
  "prId": 7514
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "There could be many records, can not be hold in memory, so we should use external sort.\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T17:14:01Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]\n+\n+      // Flag to track whether data has been sorted in which case it's safe to close previously\n+      // used outputWriters\n+      var sorted: Boolean = false\n+\n       // If anything below fails, we should abort the task.\n       try {\n         writerContainer.executorSideSetup(taskContext)\n \n-        // Projects all partition columns and casts them to strings to build partition directories.\n-        val partitionCasts = partitionOutput.map(Cast(_, StringType))\n-        val partitionProj = newProjection(codegenEnabled, partitionCasts, output)\n-        val dataProj = newProjection(codegenEnabled, dataOutput, output)\n+        // Sort the data by partition so that it's possible to use a single outputWriter at a\n+        // time to process the incoming data\n+        def sortRows(iterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+          // Sort by the same key used to look up the outputWriter to allow us to recyle the writer\n+          iterator.toArray.sortBy(writerContainer.computePartitionPath).toIterator",
    "line": 54
  }, {
    "author": {
      "login": "ilganeli"
    },
    "body": "Isn't the iterator object already in memory?\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T17:21:38Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]\n+\n+      // Flag to track whether data has been sorted in which case it's safe to close previously\n+      // used outputWriters\n+      var sorted: Boolean = false\n+\n       // If anything below fails, we should abort the task.\n       try {\n         writerContainer.executorSideSetup(taskContext)\n \n-        // Projects all partition columns and casts them to strings to build partition directories.\n-        val partitionCasts = partitionOutput.map(Cast(_, StringType))\n-        val partitionProj = newProjection(codegenEnabled, partitionCasts, output)\n-        val dataProj = newProjection(codegenEnabled, dataOutput, output)\n+        // Sort the data by partition so that it's possible to use a single outputWriter at a\n+        // time to process the incoming data\n+        def sortRows(iterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+          // Sort by the same key used to look up the outputWriter to allow us to recyle the writer\n+          iterator.toArray.sortBy(writerContainer.computePartitionPath).toIterator",
    "line": 54
  }, {
    "author": {
      "login": "davies"
    },
    "body": "I think the items will be loaded into memory lazily by iterator, that's the advantage of Iterator than Array.\n",
    "commit": "9d5a70e4808f911dd985e63836c6cc30a0c804c8",
    "createdAt": "2015-07-31T17:50:27Z",
    "diffHunk": "@@ -244,34 +244,65 @@ private[sql] case class InsertIntoHadoopFsRelation(\n     }\n \n     def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n+      // Track which rows have been output to disk so that if a data sort is necessary mid-write,\n+      // we don't end up outputting the same data twice\n+      val writtenRows: HashSet[InternalRow] = new HashSet[InternalRow]\n+\n+      // Flag to track whether data has been sorted in which case it's safe to close previously\n+      // used outputWriters\n+      var sorted: Boolean = false\n+\n       // If anything below fails, we should abort the task.\n       try {\n         writerContainer.executorSideSetup(taskContext)\n \n-        // Projects all partition columns and casts them to strings to build partition directories.\n-        val partitionCasts = partitionOutput.map(Cast(_, StringType))\n-        val partitionProj = newProjection(codegenEnabled, partitionCasts, output)\n-        val dataProj = newProjection(codegenEnabled, dataOutput, output)\n+        // Sort the data by partition so that it's possible to use a single outputWriter at a\n+        // time to process the incoming data\n+        def sortRows(iterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+          // Sort by the same key used to look up the outputWriter to allow us to recyle the writer\n+          iterator.toArray.sortBy(writerContainer.computePartitionPath).toIterator",
    "line": 54
  }],
  "prId": 7514
}]