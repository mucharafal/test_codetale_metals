[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "split this into a separate file.",
    "commit": "63ed0b4ebc9d32fce2b5075c045867ea01fd4c27",
    "createdAt": "2018-03-07T03:04:02Z",
    "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.Optional\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.io.IOUtils\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.util.JavaUtils\n+import org.apache.spark.sql.{AnalysisException, Row, SparkSession}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming._\n+import org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousReader\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.{ContinuousReadSupport, DataSourceOptions, DataSourceV2, MicroBatchReadSupport}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.reader.streaming.{ContinuousReader, MicroBatchReader, Offset}\n+import org.apache.spark.sql.types.{LongType, StructField, StructType, TimestampType}\n+import org.apache.spark.util.{ManualClock, SystemClock}\n+\n+object RateSourceProvider {\n+  val SCHEMA =\n+    StructType(StructField(\"timestamp\", TimestampType) :: StructField(\"value\", LongType) :: Nil)\n+\n+  val VERSION = 1\n+\n+  val NUM_PARTITIONS = \"numPartitions\"\n+  val ROWS_PER_SECOND = \"rowsPerSecond\"\n+  val RAMP_UP_TIME = \"rampUpTime\"\n+\n+  /** Calculate the end value we will emit at the time `seconds`. */\n+  def valueAtSecond(seconds: Long, rowsPerSecond: Long, rampUpTimeSeconds: Long): Long = {\n+    // E.g., rampUpTimeSeconds = 4, rowsPerSecond = 10\n+    // Then speedDeltaPerSecond = 2\n+    //\n+    // seconds   = 0 1 2  3  4  5  6\n+    // speed     = 0 2 4  6  8 10 10 (speedDeltaPerSecond * seconds)\n+    // end value = 0 2 6 12 20 30 40 (0 + speedDeltaPerSecond * seconds) * (seconds + 1) / 2\n+    val speedDeltaPerSecond = rowsPerSecond / (rampUpTimeSeconds + 1)\n+    if (seconds <= rampUpTimeSeconds) {\n+      // Calculate \"(0 + speedDeltaPerSecond * seconds) * (seconds + 1) / 2\" in a special way to\n+      // avoid overflow\n+      if (seconds % 2 == 1) {\n+        (seconds + 1) / 2 * speedDeltaPerSecond * seconds\n+      } else {\n+        seconds / 2 * speedDeltaPerSecond * (seconds + 1)\n+      }\n+    } else {\n+      // rampUpPart is just a special case of the above formula: rampUpTimeSeconds == seconds\n+      val rampUpPart = valueAtSecond(rampUpTimeSeconds, rowsPerSecond, rampUpTimeSeconds)\n+      rampUpPart + (seconds - rampUpTimeSeconds) * rowsPerSecond\n+    }\n+  }\n+}\n+\n+class RateSourceProvider extends DataSourceV2\n+  with MicroBatchReadSupport with ContinuousReadSupport with DataSourceRegister {\n+  import RateSourceProvider._\n+\n+  private def checkParameters(options: DataSourceOptions): Unit = {\n+    if (options.get(ROWS_PER_SECOND).isPresent) {\n+      val rowsPerSecond = options.get(ROWS_PER_SECOND).get().toLong\n+      if (rowsPerSecond <= 0) {\n+        throw new IllegalArgumentException(\n+          s\"Invalid value '$rowsPerSecond'. The option 'rowsPerSecond' must be positive\")\n+      }\n+    }\n+\n+    if (options.get(RAMP_UP_TIME).isPresent) {\n+      val rampUpTimeSeconds =\n+        JavaUtils.timeStringAsSec(options.get(RAMP_UP_TIME).get())\n+      if (rampUpTimeSeconds < 0) {\n+        throw new IllegalArgumentException(\n+          s\"Invalid value '$rampUpTimeSeconds'. The option 'rampUpTime' must not be negative\")\n+      }\n+    }\n+\n+    if (options.get(NUM_PARTITIONS).isPresent) {\n+      val numPartitions = options.get(NUM_PARTITIONS).get().toInt\n+      if (numPartitions <= 0) {\n+        throw new IllegalArgumentException(\n+          s\"Invalid value '$numPartitions'. The option 'numPartitions' must be positive\")\n+      }\n+    }\n+  }\n+\n+  override def createMicroBatchReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceOptions): MicroBatchReader = {\n+    checkParameters(options)\n+    if (schema.isPresent) {\n+      throw new AnalysisException(\"The rate source does not support a user-specified schema.\")\n+    }\n+\n+    new RateStreamMicroBatchReader(options, checkpointLocation)\n+  }\n+\n+  override def createContinuousReader(\n+      schema: Optional[StructType],\n+      checkpointLocation: String,\n+      options: DataSourceOptions): ContinuousReader = new RateStreamContinuousReader(options)\n+\n+  override def shortName(): String = \"rate\"\n+}\n+\n+class RateStreamMicroBatchReader(options: DataSourceOptions, checkpointLocation: String)"
  }],
  "prId": 20688
}]