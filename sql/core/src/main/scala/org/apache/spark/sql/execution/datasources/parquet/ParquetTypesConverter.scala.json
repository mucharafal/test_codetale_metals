[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "This file had once been where the old Parquet-Catalyst type conversion code stayed. But that part of code had already been removed when we migrated to `CatalystSchemaConverter`. The rest of this file was used by the old Parquet write path. And now we can finally get rid of it.\n",
    "commit": "fb6ee9fc2d39688dcbdc55398013324ede708848",
    "createdAt": "2015-10-07T19:08:21Z",
    "diffHunk": "@@ -1,160 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.spark.sql.execution.datasources.parquet\n-\n-import java.io.IOException\n-import java.util.{Collections, Arrays}\n-\n-import scala.util.Try\n-\n-import org.apache.hadoop.conf.Configuration\n-import org.apache.hadoop.fs.{FileSystem, Path}\n-import org.apache.hadoop.mapreduce.Job\n-import org.apache.parquet.format.converter.ParquetMetadataConverter\n-import org.apache.parquet.hadoop.metadata.{FileMetaData, ParquetMetadata}\n-import org.apache.parquet.hadoop.util.ContextUtil\n-import org.apache.parquet.hadoop.{Footer, ParquetFileReader, ParquetFileWriter}\n-import org.apache.parquet.schema.MessageType\n-\n-import org.apache.spark.Logging\n-import org.apache.spark.sql.catalyst.expressions.Attribute\n-import org.apache.spark.sql.types._\n-\n-\n-private[parquet] object ParquetTypesConverter extends Logging {\n-  def isPrimitiveType(ctype: DataType): Boolean = ctype match {\n-    case _: NumericType | BooleanType | DateType | TimestampType | StringType | BinaryType => true\n-    case _ => false\n-  }\n-\n-  /**\n-   * Compute the FIXED_LEN_BYTE_ARRAY length needed to represent a given DECIMAL precision.\n-   */\n-  private[parquet] val BYTES_FOR_PRECISION = Array.tabulate[Int](38) { precision =>\n-    var length = 1\n-    while (math.pow(2.0, 8 * length - 1) < math.pow(10.0, precision)) {\n-      length += 1\n-    }\n-    length\n-  }\n-\n-  def convertFromAttributes(attributes: Seq[Attribute]): MessageType = {\n-    val converter = new CatalystSchemaConverter()\n-    converter.convert(StructType.fromAttributes(attributes))\n-  }\n-\n-  def convertFromString(string: String): Seq[Attribute] = {\n-    Try(DataType.fromJson(string)).getOrElse(DataType.fromCaseClassString(string)) match {\n-      case s: StructType => s.toAttributes\n-      case other => sys.error(s\"Can convert $string to row\")\n-    }\n-  }\n-\n-  def convertToString(schema: Seq[Attribute]): String = {\n-    schema.map(_.name).foreach(CatalystSchemaConverter.checkFieldName)\n-    StructType.fromAttributes(schema).json\n-  }\n-\n-  def writeMetaData(attributes: Seq[Attribute], origPath: Path, conf: Configuration): Unit = {\n-    if (origPath == null) {\n-      throw new IllegalArgumentException(\"Unable to write Parquet metadata: path is null\")\n-    }\n-    val fs = origPath.getFileSystem(conf)\n-    if (fs == null) {\n-      throw new IllegalArgumentException(\n-        s\"Unable to write Parquet metadata: path $origPath is incorrectly formatted\")\n-    }\n-    val path = origPath.makeQualified(fs)\n-    if (fs.exists(path) && !fs.getFileStatus(path).isDir) {\n-      throw new IllegalArgumentException(s\"Expected to write to directory $path but found file\")\n-    }\n-    val metadataPath = new Path(path, ParquetFileWriter.PARQUET_METADATA_FILE)\n-    if (fs.exists(metadataPath)) {\n-      try {\n-        fs.delete(metadataPath, true)\n-      } catch {\n-        case e: IOException =>\n-          throw new IOException(s\"Unable to delete previous PARQUET_METADATA_FILE at $metadataPath\")\n-      }\n-    }\n-    val extraMetadata = new java.util.HashMap[String, String]()\n-    extraMetadata.put(\n-      CatalystReadSupport.SPARK_METADATA_KEY,\n-      ParquetTypesConverter.convertToString(attributes))\n-    // TODO: add extra data, e.g., table name, date, etc.?\n-\n-    val parquetSchema: MessageType = ParquetTypesConverter.convertFromAttributes(attributes)\n-    val metaData: FileMetaData = new FileMetaData(\n-      parquetSchema,\n-      extraMetadata,\n-      \"Spark\")\n-\n-    ParquetFileWriter.writeMetadataFile(\n-      conf,\n-      path,\n-      Arrays.asList(new Footer(path, new ParquetMetadata(metaData, Collections.emptyList()))))\n-  }\n-\n-  /**\n-   * Try to read Parquet metadata at the given Path. We first see if there is a summary file\n-   * in the parent directory. If so, this is used. Else we read the actual footer at the given\n-   * location.\n-   * @param origPath The path at which we expect one (or more) Parquet files.\n-   * @param configuration The Hadoop configuration to use.\n-   * @return The `ParquetMetadata` containing among other things the schema.\n-   */\n-  def readMetaData(origPath: Path, configuration: Option[Configuration]): ParquetMetadata = {\n-    if (origPath == null) {\n-      throw new IllegalArgumentException(\"Unable to read Parquet metadata: path is null\")\n-    }\n-    val job = new Job()\n-    val conf = {\n-      // scalastyle:off jobcontext\n-      configuration.getOrElse(ContextUtil.getConfiguration(job))\n-      // scalastyle:on jobcontext\n-    }\n-    val fs: FileSystem = origPath.getFileSystem(conf)\n-    if (fs == null) {\n-      throw new IllegalArgumentException(s\"Incorrectly formatted Parquet metadata path $origPath\")\n-    }\n-    val path = origPath.makeQualified(fs)\n-\n-    val children =\n-      fs\n-        .globStatus(path)\n-        .flatMap { status => if (status.isDir) fs.listStatus(status.getPath) else List(status) }\n-        .filterNot { status =>\n-          val name = status.getPath.getName\n-          (name(0) == '.' || name(0) == '_') && name != ParquetFileWriter.PARQUET_METADATA_FILE\n-        }\n-\n-    // NOTE (lian): Parquet \"_metadata\" file can be very slow if the file consists of lots of row\n-    // groups. Since Parquet schema is replicated among all row groups, we only need to touch a\n-    // single row group to read schema related metadata. Notice that we are making assumptions that\n-    // all data in a single Parquet file have the same schema, which is normally true.\n-    children\n-      // Try any non-\"_metadata\" file first...\n-      .find(_.getPath.getName != ParquetFileWriter.PARQUET_METADATA_FILE)\n-      // ... and fallback to \"_metadata\" if no such file exists (which implies the Parquet file is\n-      // empty, thus normally the \"_metadata\" file is expected to be fairly small).\n-      .orElse(children.find(_.getPath.getName == ParquetFileWriter.PARQUET_METADATA_FILE))\n-      .map(ParquetFileReader.readFooter(conf, _, ParquetMetadataConverter.NO_FILTER))\n-      .getOrElse(\n-        throw new IllegalArgumentException(s\"Could not find Parquet metadata at path $path\"))\n-  }\n-}",
    "line": 160
  }],
  "prId": 8988
}]