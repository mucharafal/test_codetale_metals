[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Do you need these type parameters?",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-06-29T20:58:45Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateInfo,\n+      keySchema,\n+      valueSchema,\n+      indexOrdinal = None,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+      val key = UnsafeProjection.create(keySchema)(new GenericInternalRow(Array[Any](null)))\n+      val numOutputRows = longMetric(\"numOutputRows\")\n+      val numUpdatedStateRows = longMetric(\"numUpdatedStateRows\")\n+      val allUpdatesTimeMs = longMetric(\"allUpdatesTimeMs\")\n+      val commitTimeMs = longMetric(\"commitTimeMs\")\n+      val updatesStartTimeNs = System.nanoTime\n+\n+      val startCount: Long = Option(store.get(key)).map(_.getLong(0)).getOrElse(0L)\n+      var rowCount = startCount\n+\n+      val result = iter.filter { r =>\n+        val x = rowCount < streamLimit\n+        if (x) {\n+          rowCount += 1\n+        }\n+        x\n+      }\n+\n+      CompletionIterator[InternalRow, Iterator[InternalRow]](result, {"
  }, {
    "author": {
      "login": "mukulmurthy"
    },
    "body": "yup :(",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-07-02T18:23:34Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateInfo,\n+      keySchema,\n+      valueSchema,\n+      indexOrdinal = None,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+      val key = UnsafeProjection.create(keySchema)(new GenericInternalRow(Array[Any](null)))\n+      val numOutputRows = longMetric(\"numOutputRows\")\n+      val numUpdatedStateRows = longMetric(\"numUpdatedStateRows\")\n+      val allUpdatesTimeMs = longMetric(\"allUpdatesTimeMs\")\n+      val commitTimeMs = longMetric(\"commitTimeMs\")\n+      val updatesStartTimeNs = System.nanoTime\n+\n+      val startCount: Long = Option(store.get(key)).map(_.getLong(0)).getOrElse(0L)\n+      var rowCount = startCount\n+\n+      val result = iter.filter { r =>\n+        val x = rowCount < streamLimit\n+        if (x) {\n+          rowCount += 1\n+        }\n+        x\n+      }\n+\n+      CompletionIterator[InternalRow, Iterator[InternalRow]](result, {"
  }],
  "prId": 21662
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Existing: Do we really do this in every operator?  Why isn't this the responsibility of the parent class?",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-06-29T20:59:44Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver"
  }, {
    "author": {
      "login": "mukulmurthy"
    },
    "body": "Can chat more about this one offline but from talking to TD it doesn't sound like there's a simple fix for this ",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-07-02T20:09:43Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver"
  }],
  "prId": 21662
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Nit: I'd indent 4 above to distinguish these two blocks visually.",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-06-29T21:00:33Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateInfo,\n+      keySchema,\n+      valueSchema,\n+      indexOrdinal = None,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>"
  }],
  "prId": 21662
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "Isn't this going to result in `streamLimit` records in each partition? I would expect we'd need something like the Global/LocalLimit split.",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-06-29T21:27:36Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateInfo,\n+      keySchema,\n+      valueSchema,\n+      indexOrdinal = None,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+      val key = UnsafeProjection.create(keySchema)(new GenericInternalRow(Array[Any](null)))\n+      val numOutputRows = longMetric(\"numOutputRows\")\n+      val numUpdatedStateRows = longMetric(\"numUpdatedStateRows\")\n+      val allUpdatesTimeMs = longMetric(\"allUpdatesTimeMs\")\n+      val commitTimeMs = longMetric(\"commitTimeMs\")\n+      val updatesStartTimeNs = System.nanoTime\n+\n+      val startCount: Long = Option(store.get(key)).map(_.getLong(0)).getOrElse(0L)\n+      var rowCount = startCount\n+\n+      val result = iter.filter { r =>\n+        val x = rowCount < streamLimit"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "I think its okay due to `override def requiredChildDistribution: Seq[Distribution] = AllTuples :: Nil`.\r\n\r\n+1 to making sure there are tests with more than one partition though.\r\n",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-06-29T21:39:09Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateInfo,\n+      keySchema,\n+      valueSchema,\n+      indexOrdinal = None,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+      val key = UnsafeProjection.create(keySchema)(new GenericInternalRow(Array[Any](null)))\n+      val numOutputRows = longMetric(\"numOutputRows\")\n+      val numUpdatedStateRows = longMetric(\"numUpdatedStateRows\")\n+      val allUpdatesTimeMs = longMetric(\"allUpdatesTimeMs\")\n+      val commitTimeMs = longMetric(\"commitTimeMs\")\n+      val updatesStartTimeNs = System.nanoTime\n+\n+      val startCount: Long = Option(store.get(key)).map(_.getLong(0)).getOrElse(0L)\n+      var rowCount = startCount\n+\n+      val result = iter.filter { r =>\n+        val x = rowCount < streamLimit"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Oh and we should be planning a `LocalLimit` before this and perhaps `GlobalStreamingLimitExec` would be a better name to make the functionality obvious.",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-06-29T21:40:16Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateInfo,\n+      keySchema,\n+      valueSchema,\n+      indexOrdinal = None,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+      val key = UnsafeProjection.create(keySchema)(new GenericInternalRow(Array[Any](null)))\n+      val numOutputRows = longMetric(\"numOutputRows\")\n+      val numUpdatedStateRows = longMetric(\"numUpdatedStateRows\")\n+      val allUpdatesTimeMs = longMetric(\"allUpdatesTimeMs\")\n+      val commitTimeMs = longMetric(\"commitTimeMs\")\n+      val updatesStartTimeNs = System.nanoTime\n+\n+      val startCount: Long = Option(store.get(key)).map(_.getLong(0)).getOrElse(0L)\n+      var rowCount = startCount\n+\n+      val result = iter.filter { r =>\n+        val x = rowCount < streamLimit"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Oh, I missed that distribution. Makes sense then.",
    "commit": "06ceaf940598ad158d156b41c8357b83ebeec9d0",
    "createdAt": "2018-06-29T21:49:40Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit.NANOSECONDS\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.plans.physical.{AllTuples, Distribution, Partitioning}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.execution.streaming.state.StateStoreOps\n+import org.apache.spark.sql.types.{LongType, NullType, StructField, StructType}\n+import org.apache.spark.util.CompletionIterator\n+\n+/**\n+ * A physical operator for executing a streaming limit, which makes sure no more than streamLimit\n+ * rows are returned.\n+ */\n+case class StreamingLimitExec(\n+    streamLimit: Long,\n+    child: SparkPlan,\n+    stateInfo: Option[StatefulOperatorStateInfo] = None)\n+  extends UnaryExecNode with StateStoreWriter {\n+\n+  private val keySchema = StructType(Array(StructField(\"key\", NullType)))\n+  private val valueSchema = StructType(Array(StructField(\"value\", LongType)))\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    metrics // force lazy init at driver\n+\n+    child.execute().mapPartitionsWithStateStore(\n+      getStateInfo,\n+      keySchema,\n+      valueSchema,\n+      indexOrdinal = None,\n+      sqlContext.sessionState,\n+      Some(sqlContext.streams.stateStoreCoordinator)) { (store, iter) =>\n+      val key = UnsafeProjection.create(keySchema)(new GenericInternalRow(Array[Any](null)))\n+      val numOutputRows = longMetric(\"numOutputRows\")\n+      val numUpdatedStateRows = longMetric(\"numUpdatedStateRows\")\n+      val allUpdatesTimeMs = longMetric(\"allUpdatesTimeMs\")\n+      val commitTimeMs = longMetric(\"commitTimeMs\")\n+      val updatesStartTimeNs = System.nanoTime\n+\n+      val startCount: Long = Option(store.get(key)).map(_.getLong(0)).getOrElse(0L)\n+      var rowCount = startCount\n+\n+      val result = iter.filter { r =>\n+        val x = rowCount < streamLimit"
  }],
  "prId": 21662
}]