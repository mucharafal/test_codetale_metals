[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Users doesn't need to know the package name. Just `\"binaryFile\"` data source format.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:49:15Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please also document how to control the input partition size. cc: @cloud-fan ",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:52:58Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Do not expose this variable. We can just inline it. The schema is not that complex.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:53:12Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)\n+ *  - length: `LongType` (the file length)\n+ */\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  val fileStatusSchema = StructType("
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "make it private instead of inline it. So that code is more clear.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T01:59:32Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)\n+ *  - length: `LongType` (the file length)\n+ */\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  val fileStatusSchema = StructType("
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`modificationTime`",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:54:31Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)\n+ *  - length: `LongType` (the file length)\n+ */\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, true) ::\n+      StructField(\"modification_time\", TimestampType, true) ::"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Also need to mention available options.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:54:58Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)\n+ *  - length: `LongType` (the file length)\n+ */"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Need doc.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:55:13Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)\n+ *  - length: `LongType` (the file length)\n+ */\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, true) ::\n+      StructField(\"modification_time\", TimestampType, true) ::\n+      StructField(\"length\", LongType, true) :: Nil)\n+\n+  val binaryFileSchema = StructType("
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "since version and experimental tag?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:55:38Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)\n+ *  - length: `LongType` (the file length)\n+ */\n+class BinaryFileDataSource private() {}"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "maybe just call it `len` to match `FileStatus`.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:56:53Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)\n+ *  - length: `LongType` (the file length)"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It might be useful to mention on some FS implementation, this might not available and fallback to some default value.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T01:03:19Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * `binaryfile` package implements Spark SQL data source API for loading binary file data\n+ * as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modification_time: `TimestampType` (last modification time of the file)"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think it's guaranteed that `modificationTime` and `len` are not null",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T08:34:38Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  private val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, true) ::\n+    StructField(\"modificationTime\", TimestampType, true) ::"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I would name it `length` since other two have not abbreviated names.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T08:35:02Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  private val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, true) ::\n+    StructField(\"modificationTime\", TimestampType, true) ::\n+    StructField(\"len\", LongType, true) :: Nil)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "This is to be consistent with `FileStatus`.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-15T17:31:25Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  private val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, true) ::\n+    StructField(\"modificationTime\", TimestampType, true) ::\n+    StructField(\"len\", LongType, true) :: Nil)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "In string representation, it's `length`:\r\n\r\nhttps://github.com/apache/hadoop/blob/fb8932a727f757b2e9c1c61a18145878d0eb77bd/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileStatus.java#L450\r\n\r\nIf either way makes sense, I think it's better to keep looking consistent in Spark side too.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T03:34:18Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  private val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, true) ::\n+    StructField(\"modificationTime\", TimestampType, true) ::\n+    StructField(\"len\", LongType, true) :: Nil)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "I don't have a strong preference here.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T04:26:30Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  private val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, true) ::\n+    StructField(\"modificationTime\", TimestampType, true) ::\n+    StructField(\"len\", LongType, true) :: Nil)"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`execution` package is supposed to be internal package. So IIRC, this isn't documented in API docs. Can you check?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T08:36:03Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Could you explain more on this ?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-15T20:14:33Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "@HyukjinKwon I still feel the API doc is actually the best location to document data sources, though we need to whitelist them. How about we keeping them here and adding user guide to https://spark.apache.org/docs/latest/sql-data-sources.html after this PR?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-15T20:32:12Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I mean `execution` package is excluded in API doc generation via Unidoc:\r\n\r\nhttps://github.com/apache/spark/blob/8bc304f97ee693b57f33fa6708eb63e2d641c609/project/SparkBuild.scala#L744\r\n\r\nSo, IIRC, this isn't documented at all in the API doc..\r\n",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T03:22:01Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Yes, I know that. But it doesn't hurt to leave the docs here and have another PR to add it to the guide, right?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T04:25:29Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Looks this was added by following Hadoop's FileStatus. Can we add `getAccessTime` too at least? Looks a similar instance to add to me.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T09:09:27Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "We shouldn't add it because it might change often without file content change and the ETL jobs might require updates.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-15T17:34:07Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "typo: keep -> keeps\r\n\r\nhadoop -> Hadoop",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T09:17:27Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - len: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keep the same behavior with hadoop API"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "what's default value?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T09:17:58Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "It should depend on specific FS if that FS do not support record file modificationTime.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-15T18:51:55Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - status: `StructType` (the file status information)\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "* space before `::`\r\n* just a note: we might keep this column nullable in case to handle potential I/O failures",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T05:27:03Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *  - status: `StructType` (the file status information)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - length: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keeps the same behavior with Hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * In order to control the partition size, we can set spark sql configuration\n+ * `spark.sql.files.maxPartitionBytes` and `spark.sql.files.openCostInBytes`.\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\")\n+ *\n+ *   // Java\n+ *   Dataset<Row> df = spark.read().format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")\n+ *     .load(\"path/to/fileDir\");\n+ * }}}\n+ *\n+ * @note This binary file data source does not support saving dataframe to binary files.\n+ * @note This class is public for documentation purpose. Please don't use this class directly.\n+ * Rather, use the data source API as illustrated above.\n+ */\n+@Experimental\n+@Since(\"3.0.0\")\n+class BinaryFileDataSource private() {}\n+\n+object BinaryFileDataSource {\n+\n+  private val fileStatusSchema = StructType(\n+    StructField(\"path\", StringType, false) ::\n+    StructField(\"modificationTime\", TimestampType, false) ::\n+    StructField(\"length\", LongType, false) :: Nil)\n+\n+  /**\n+   * The schema of the dataframe returned by binaryFile data source.\n+   * See doc in `BinaryFileDataSource`\n+   */\n+  val binaryFileSchema = StructType(\n+    StructField(\"content\", BinaryType, false)::"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "Nit: how about changing the extension name \"*.txt\" in the example, e.g. `*.png` or `*.jpg`",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T16:39:58Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * This \"binaryFile\" data source format implements Spark SQL data source API for loading binary\n+ * file data as `DataFrame`.\n+ *\n+ * The loaded `DataFrame` has two columns, the schema is:\n+ *  - content: `BinaryType` (binary data of the file content)\n+ *  - status: `StructType` (the file status information)\n+ *\n+ * The schema of \"status\" column described above is:\n+ *  - path: `StringType` (the file path)\n+ *  - modificationTime: `TimestampType` (last modification time of the file, on some FS\n+ *                                       implementation, this might be not available\n+ *                                       and fallback to some default value.)\n+ *  - length: `LongType` (the file length)\n+ *\n+ * To use binary file data source, you need to set \"binaryFile\" as the format in `DataFrameReader`\n+ * and optionally specify the data source options, available options include:\n+ *  - pathGlobFilter: Only include files with path matching the glob pattern.\n+ *                    The glob pattern keeps the same behavior with Hadoop API\n+ *                    `org.apache.hadoop.fs.FileSystem.globStatus(pathPattern)`\n+ *\n+ * In order to control the partition size, we can set spark sql configuration\n+ * `spark.sql.files.maxPartitionBytes` and `spark.sql.files.openCostInBytes`.\n+ *\n+ * Example:\n+ * {{{\n+ *   // Scala\n+ *   val df = spark.read.format(\"binaryFile\")\n+ *     .option(\"pathGlobFilter\", \"*.txt\")"
  }],
  "prId": 24354
}]