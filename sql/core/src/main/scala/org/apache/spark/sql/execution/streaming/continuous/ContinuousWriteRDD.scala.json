[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "add docs ",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-02T00:50:16Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark.{Partition, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask.{logError, logInfo}\n+import org.apache.spark.sql.sources.v2.writer.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.util.Utils\n+\n+class ContinuousWriteRDD(var prev: RDD[InternalRow], writeTask: DataWriterFactory[InternalRow])"
  }],
  "prId": 21200
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I am having trouble tracking how the currentEpoch is updated and used. Is this field `currentEpoch` used anywhere outside this class? The `ContinuousQueuedDataReader` also has currentEpoch being incremented. I am confused on what is used where.\r\n\r\nCant we converge the different flags to a common thread-local variable that is initialized using the local property, incremented at one place (say, by this writer class) and used everywhere?",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-02T00:59:06Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark.{Partition, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask.{logError, logInfo}\n+import org.apache.spark.sql.sources.v2.writer.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.util.Utils\n+\n+class ContinuousWriteRDD(var prev: RDD[InternalRow], writeTask: DataWriterFactory[InternalRow])\n+    extends RDD[Unit](prev) {\n+\n+  override val partitioner = prev.partitioner\n+\n+  override def getPartitions: Array[Partition] = prev.partitions\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[Unit] = {\n+    val epochCoordinator = EpochCoordinatorRef.get(\n+      context.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n+      SparkEnv.get)\n+    var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+    do {\n+      var dataWriter: DataWriter[InternalRow] = null\n+      // write the data and commit this writer.\n+      Utils.tryWithSafeFinallyAndFailureCallbacks(block = {\n+        try {\n+          val dataIterator = prev.compute(split, context)\n+          dataWriter = writeTask.createDataWriter(\n+            context.partitionId(), context.attemptNumber(), currentEpoch)\n+          while (dataIterator.hasNext) {\n+            dataWriter.write(dataIterator.next())\n+          }\n+          logInfo(s\"Writer for partition ${context.partitionId()} \" +\n+            s\"in epoch $currentEpoch is committing.\")\n+          val msg = dataWriter.commit()\n+          epochCoordinator.send(\n+            CommitPartitionEpoch(context.partitionId(), currentEpoch, msg)\n+          )\n+          logInfo(s\"Writer for partition ${context.partitionId()} \" +\n+            s\"in epoch $currentEpoch committed.\")\n+          currentEpoch += 1",
    "line": 69
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Both nodes do their own independent tracking of currentEpoch. This is required; eventually they won't always be on the same machine.",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-02T04:39:42Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark.{Partition, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask.{logError, logInfo}\n+import org.apache.spark.sql.sources.v2.writer.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.util.Utils\n+\n+class ContinuousWriteRDD(var prev: RDD[InternalRow], writeTask: DataWriterFactory[InternalRow])\n+    extends RDD[Unit](prev) {\n+\n+  override val partitioner = prev.partitioner\n+\n+  override def getPartitions: Array[Partition] = prev.partitions\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[Unit] = {\n+    val epochCoordinator = EpochCoordinatorRef.get(\n+      context.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n+      SparkEnv.get)\n+    var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+    do {\n+      var dataWriter: DataWriter[InternalRow] = null\n+      // write the data and commit this writer.\n+      Utils.tryWithSafeFinallyAndFailureCallbacks(block = {\n+        try {\n+          val dataIterator = prev.compute(split, context)\n+          dataWriter = writeTask.createDataWriter(\n+            context.partitionId(), context.attemptNumber(), currentEpoch)\n+          while (dataIterator.hasNext) {\n+            dataWriter.write(dataIterator.next())\n+          }\n+          logInfo(s\"Writer for partition ${context.partitionId()} \" +\n+            s\"in epoch $currentEpoch is committing.\")\n+          val msg = dataWriter.commit()\n+          epochCoordinator.send(\n+            CommitPartitionEpoch(context.partitionId(), currentEpoch, msg)\n+          )\n+          logInfo(s\"Writer for partition ${context.partitionId()} \" +\n+            s\"in epoch $currentEpoch committed.\")\n+          currentEpoch += 1",
    "line": 69
  }],
  "prId": 21200
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: move to prev line.",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-02T23:25:52Z",
    "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark.{Partition, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask.{logError, logInfo}\n+import org.apache.spark.sql.sources.v2.writer.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The RDD writing to a sink in continuous processing.\n+ *\n+ * Within each task, we repeatedly call prev.compute(). Each resulting iterator contains the data\n+ * to be written for one epoch, which we commit and forward to the driver.\n+ *\n+ * We keep repeating prev.compute() and writing new epochs until the query is shut down.\n+ */\n+class ContinuousWriteRDD(var prev: RDD[InternalRow], writeTask: DataWriterFactory[InternalRow])\n+    extends RDD[Unit](prev) {\n+\n+  override val partitioner = prev.partitioner\n+\n+  override def getPartitions: Array[Partition] = prev.partitions\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[Unit] = {\n+    val epochCoordinator = EpochCoordinatorRef.get(\n+      context.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n+      SparkEnv.get)\n+    var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+    do {\n+      var dataWriter: DataWriter[InternalRow] = null\n+      // write the data and commit this writer.\n+      Utils.tryWithSafeFinallyAndFailureCallbacks(block = {\n+        try {\n+          val dataIterator = prev.compute(split, context)\n+          dataWriter = writeTask.createDataWriter(\n+            context.partitionId(), context.attemptNumber(), currentEpoch)\n+          while (dataIterator.hasNext) {\n+            dataWriter.write(dataIterator.next())\n+          }\n+          logInfo(s\"Writer for partition ${context.partitionId()} \" +\n+            s\"in epoch $currentEpoch is committing.\")\n+          val msg = dataWriter.commit()\n+          epochCoordinator.send(\n+            CommitPartitionEpoch(context.partitionId(), currentEpoch, msg)\n+          )",
    "line": 66
  }],
  "prId": 21200
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why is this `do...while` instead of `while`?",
    "commit": "75c0b78f924d9c2f70b737c105e6f3cbc85d3b6e",
    "createdAt": "2018-05-02T23:27:20Z",
    "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import org.apache.spark.{Partition, SparkEnv, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask.{logError, logInfo}\n+import org.apache.spark.sql.sources.v2.writer.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * The RDD writing to a sink in continuous processing.\n+ *\n+ * Within each task, we repeatedly call prev.compute(). Each resulting iterator contains the data\n+ * to be written for one epoch, which we commit and forward to the driver.\n+ *\n+ * We keep repeating prev.compute() and writing new epochs until the query is shut down.\n+ */\n+class ContinuousWriteRDD(var prev: RDD[InternalRow], writeTask: DataWriterFactory[InternalRow])\n+    extends RDD[Unit](prev) {\n+\n+  override val partitioner = prev.partitioner\n+\n+  override def getPartitions: Array[Partition] = prev.partitions\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[Unit] = {\n+    val epochCoordinator = EpochCoordinatorRef.get(\n+      context.getLocalProperty(ContinuousExecution.EPOCH_COORDINATOR_ID_KEY),\n+      SparkEnv.get)\n+    var currentEpoch = context.getLocalProperty(ContinuousExecution.START_EPOCH_KEY).toLong\n+\n+    do {\n+      var dataWriter: DataWriter[InternalRow] = null\n+      // write the data and commit this writer.\n+      Utils.tryWithSafeFinallyAndFailureCallbacks(block = {\n+        try {\n+          val dataIterator = prev.compute(split, context)\n+          dataWriter = writeTask.createDataWriter(\n+            context.partitionId(), context.attemptNumber(), currentEpoch)\n+          while (dataIterator.hasNext) {\n+            dataWriter.write(dataIterator.next())\n+          }\n+          logInfo(s\"Writer for partition ${context.partitionId()} \" +\n+            s\"in epoch $currentEpoch is committing.\")\n+          val msg = dataWriter.commit()\n+          epochCoordinator.send(\n+            CommitPartitionEpoch(context.partitionId(), currentEpoch, msg)\n+          )\n+          logInfo(s\"Writer for partition ${context.partitionId()} \" +\n+            s\"in epoch $currentEpoch committed.\")\n+          currentEpoch += 1\n+        } catch {\n+          case _: InterruptedException =>\n+          // Continuous shutdown always involves an interrupt. Just finish the task.\n+        }\n+      })(catchBlock = {\n+        // If there is an error, abort this writer. We enter this callback in the middle of\n+        // rethrowing an exception, so compute() will stop executing at this point.\n+        logError(s\"Writer for partition ${context.partitionId()} is aborting.\")\n+        if (dataWriter != null) dataWriter.abort()\n+        logError(s\"Writer for partition ${context.partitionId()} aborted.\")\n+      })\n+    } while (!context.isInterrupted() && !context.isCompleted())"
  }],
  "prId": 21200
}]