[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "So if I have a large input file, say 2GB, with `spark.sql.small.file.split.size` set to 256M, I'll end up with 8 tasks instead of 1?\n",
    "commit": "3ae34b46edc482065ba94471d73fb649b5b72785",
    "createdAt": "2015-08-12T11:04:27Z",
    "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+\n+object CombineSmallFile {\n+  def combineWithFiles[T](rdd: RDD[T], sqlContext: SQLContext, inputFiles: Array[FileStatus])\n+      : RDD[T] = {\n+    if (sqlContext.conf.combineSmallFile) {\n+      val totalLen = inputFiles.map { file =>\n+        if (file.isDir) 0L else file.getLen\n+      }.sum\n+      val numPartitions = (totalLen / sqlContext.conf.splitSize + 1).toInt\n+      rdd.coalesce(numPartitions)",
    "line": 32
  }, {
    "author": {
      "login": "watermen"
    },
    "body": "@liancheng 2GB will split into 16 takes(if hadoop block size is 128M) with `spark.sql.small.file.combine=false`, instead of 1 task. So turn 16 to 8 is also combine.\n",
    "commit": "3ae34b46edc482065ba94471d73fb649b5b72785",
    "createdAt": "2015-08-13T01:47:38Z",
    "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+\n+object CombineSmallFile {\n+  def combineWithFiles[T](rdd: RDD[T], sqlContext: SQLContext, inputFiles: Array[FileStatus])\n+      : RDD[T] = {\n+    if (sqlContext.conf.combineSmallFile) {\n+      val totalLen = inputFiles.map { file =>\n+        if (file.isDir) 0L else file.getLen\n+      }.sum\n+      val numPartitions = (totalLen / sqlContext.conf.splitSize + 1).toInt\n+      rdd.coalesce(numPartitions)",
    "line": 32
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "What if Hadoop block size is configured larger (as many users do)?\n",
    "commit": "3ae34b46edc482065ba94471d73fb649b5b72785",
    "createdAt": "2015-08-13T07:26:12Z",
    "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+\n+object CombineSmallFile {\n+  def combineWithFiles[T](rdd: RDD[T], sqlContext: SQLContext, inputFiles: Array[FileStatus])\n+      : RDD[T] = {\n+    if (sqlContext.conf.combineSmallFile) {\n+      val totalLen = inputFiles.map { file =>\n+        if (file.isDir) 0L else file.getLen\n+      }.sum\n+      val numPartitions = (totalLen / sqlContext.conf.splitSize + 1).toInt\n+      rdd.coalesce(numPartitions)",
    "line": 32
  }, {
    "author": {
      "login": "watermen"
    },
    "body": "The size of split(`spark.sql.small.file.split.size`) can be configured, so if hadoop block size is configured larger, also `spark.sql.small.file.split.size` can be configured larger.\n",
    "commit": "3ae34b46edc482065ba94471d73fb649b5b72785",
    "createdAt": "2015-08-17T03:21:18Z",
    "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+\n+object CombineSmallFile {\n+  def combineWithFiles[T](rdd: RDD[T], sqlContext: SQLContext, inputFiles: Array[FileStatus])\n+      : RDD[T] = {\n+    if (sqlContext.conf.combineSmallFile) {\n+      val totalLen = inputFiles.map { file =>\n+        if (file.isDir) 0L else file.getLen\n+      }.sum\n+      val numPartitions = (totalLen / sqlContext.conf.splitSize + 1).toInt\n+      rdd.coalesce(numPartitions)",
    "line": 32
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "I think this is a very hack way to solve this problem. As we can not tell how the the data source to be split, even for Hadoop, the split size just a hint, use that for computing the partition number probably too risky for a generic data process framework.\n\nAnd the `RDD.coalesce` actually will combine the splits in an arbitrary way, it's probably causes the data skew, as we most likely combine the large partitions into a a single task.\n\nIMO, I'd like to deep investigate how Hive combines the small partitions, by using the `CombineHiveInputFormat` or `HiveInputFormat`, it seems have a strategy to select the partitions according to both input format, and also keep the balance.\n",
    "commit": "3ae34b46edc482065ba94471d73fb649b5b72785",
    "createdAt": "2015-08-28T02:38:39Z",
    "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+\n+object CombineSmallFile {\n+  def combineWithFiles[T](rdd: RDD[T], sqlContext: SQLContext, inputFiles: Array[FileStatus])\n+      : RDD[T] = {\n+    if (sqlContext.conf.combineSmallFile) {\n+      val totalLen = inputFiles.map { file =>\n+        if (file.isDir) 0L else file.getLen\n+      }.sum\n+      val numPartitions = (totalLen / sqlContext.conf.splitSize + 1).toInt\n+      rdd.coalesce(numPartitions)",
    "line": 32
  }],
  "prId": 8125
}]