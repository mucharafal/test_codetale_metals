[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`ParquetScanBuilder` already did most thing. Here, I guess we need `.reduceOption(FilterApi.and)` only. Please correct me if I'm wrong~\r\n\r\nAlso, cc @cloud-fan",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-31T05:35:00Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters of the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)",
    "line": 146
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "We still need to do the actual push down here.\r\nSee my comments above: https://github.com/apache/spark/pull/24327#discussion_r289266681",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-31T06:27:52Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters of the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)",
    "line": 146
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`actual` pushdown implies the other pushdowns in DSv2 is `not-actual`. Also, this seems to make `pushedFilters` in the explained plan inconsistent with the one Parquet library received. I believe this will cause lots of confusion.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-31T16:41:04Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters of the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)",
    "line": 146
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "We need the filter push down here as per the discussion in https://github.com/apache/spark/pull/21696#pullrequestreview-133842990 .",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-05-31T17:21:34Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters of the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)",
    "line": 146
  }],
  "prId": 24327
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "`buildReaderBase` is parameterized, but the result is still casted. Why not parameterize so that ti retursn `ParquetRecordReader[UnsafeRow]` to avoid the cast? I think these casts should be removed.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-06T19:35:45Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters of the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)\n+    } else {\n+      None\n+    }\n+    // PARQUET_INT96_TIMESTAMP_CONVERSION says to apply timezone conversions to int96 timestamps'\n+    // *only* if the file was created by something other than \"parquet-mr\", so check the actual\n+    // writer here for this file.  We have to do this per-file, as each file in the table may\n+    // have different writers.\n+    // Define isCreatedByParquetMr as function to avoid unnecessary parquet footer reads.\n+    def isCreatedByParquetMr: Boolean =\n+      footerFileMetaData.getCreatedBy().startsWith(\"parquet-mr\")\n+\n+    val convertTz =\n+      if (timestampConversion && !isCreatedByParquetMr) {\n+        Some(DateTimeUtils.getTimeZone(conf.get(SQLConf.SESSION_LOCAL_TIMEZONE.key)))\n+      } else {\n+        None\n+      }\n+\n+    val attemptId = new TaskAttemptID(new TaskID(new JobID(), TaskType.MAP, 0), 0)\n+    val hadoopAttemptContext = new TaskAttemptContextImpl(conf, attemptId)\n+\n+    // Try to push down filters when filter push-down is enabled.\n+    // Notice: This push-down is RowGroups level, not individual records.\n+    if (pushed.isDefined) {\n+      ParquetInputFormat.setFilterPredicate(hadoopAttemptContext.getConfiguration, pushed.get)\n+    }\n+    val reader =\n+      buildReaderFunc(split, file.partitionValues, hadoopAttemptContext, pushed, convertTz)\n+    reader.initialize(split, hadoopAttemptContext)\n+    reader\n+  }\n+\n+  private def createRowBaseReader(file: PartitionedFile): ParquetRecordReader[UnsafeRow] = {\n+    buildReaderBase(file, createRowBaseReader0).asInstanceOf[ParquetRecordReader[UnsafeRow]]"
  }],
  "prId": 24327
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "@gengliangwang, why is this cast here? I expected it to be removed when the one in `createRowBaseReader` was removed.",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-12T00:07:59Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters to be pushed down in the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)\n+    } else {\n+      None\n+    }\n+    // PARQUET_INT96_TIMESTAMP_CONVERSION says to apply timezone conversions to int96 timestamps'\n+    // *only* if the file was created by something other than \"parquet-mr\", so check the actual\n+    // writer here for this file.  We have to do this per-file, as each file in the table may\n+    // have different writers.\n+    // Define isCreatedByParquetMr as function to avoid unnecessary parquet footer reads.\n+    def isCreatedByParquetMr: Boolean =\n+      footerFileMetaData.getCreatedBy().startsWith(\"parquet-mr\")\n+\n+    val convertTz =\n+      if (timestampConversion && !isCreatedByParquetMr) {\n+        Some(DateTimeUtils.getTimeZone(conf.get(SQLConf.SESSION_LOCAL_TIMEZONE.key)))\n+      } else {\n+        None\n+      }\n+\n+    val attemptId = new TaskAttemptID(new TaskID(new JobID(), TaskType.MAP, 0), 0)\n+    val hadoopAttemptContext = new TaskAttemptContextImpl(conf, attemptId)\n+\n+    // Try to push down filters when filter push-down is enabled.\n+    // Notice: This push-down is RowGroups level, not individual records.\n+    if (pushed.isDefined) {\n+      ParquetInputFormat.setFilterPredicate(hadoopAttemptContext.getConfiguration, pushed.get)\n+    }\n+    val reader =\n+      buildReaderFunc(split, file.partitionValues, hadoopAttemptContext, pushed, convertTz)\n+    reader.initialize(split, hadoopAttemptContext)\n+    reader\n+  }\n+\n+  private def createRowBaseReader(file: PartitionedFile): RecordReader[Void, UnsafeRow] = {\n+    buildReaderBase(file, createRowBaseReader0)\n+  }\n+\n+  private def createRowBaseReader0(\n+      split: ParquetInputSplit,\n+      partitionValues: InternalRow,\n+      hadoopAttemptContext: TaskAttemptContextImpl,\n+      pushed: Option[FilterPredicate],\n+      convertTz: Option[TimeZone]): RecordReader[Void, UnsafeRow] = {\n+    logDebug(s\"Falling back to parquet-mr\")\n+    val taskContext = Option(TaskContext.get())\n+    // ParquetRecordReader returns UnsafeRow\n+    val readSupport = new ParquetReadSupport(convertTz, enableVectorizedReader = false)\n+    val reader = if (pushed.isDefined && enableRecordFilter) {\n+      val parquetFilter = FilterCompat.get(pushed.get, null)\n+      new ParquetRecordReader[UnsafeRow](readSupport, parquetFilter)\n+    } else {\n+      new ParquetRecordReader[UnsafeRow](readSupport)\n+    }\n+    val iter = new RecordReaderIterator(reader)\n+    // SPARK-23457 Register a task completion lister before `initialization`.\n+    taskContext.foreach(_.addTaskCompletionListener[Unit](_ => iter.close()))\n+    reader\n+  }\n+\n+  private def createVectorizedReader(file: PartitionedFile): VectorizedParquetRecordReader = {\n+    val vectorizedReader =\n+      buildReaderBase(file, createVectorizedReader0).asInstanceOf[VectorizedParquetRecordReader]"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "This is because here we need to call the method `initBatch` and `enableReturningBatches` of VectorizedParquetRecordReader. We can't just change the returned type as `RecordReader[Void, Object]` here. ",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-12T00:41:45Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters to be pushed down in the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)\n+    } else {\n+      None\n+    }\n+    // PARQUET_INT96_TIMESTAMP_CONVERSION says to apply timezone conversions to int96 timestamps'\n+    // *only* if the file was created by something other than \"parquet-mr\", so check the actual\n+    // writer here for this file.  We have to do this per-file, as each file in the table may\n+    // have different writers.\n+    // Define isCreatedByParquetMr as function to avoid unnecessary parquet footer reads.\n+    def isCreatedByParquetMr: Boolean =\n+      footerFileMetaData.getCreatedBy().startsWith(\"parquet-mr\")\n+\n+    val convertTz =\n+      if (timestampConversion && !isCreatedByParquetMr) {\n+        Some(DateTimeUtils.getTimeZone(conf.get(SQLConf.SESSION_LOCAL_TIMEZONE.key)))\n+      } else {\n+        None\n+      }\n+\n+    val attemptId = new TaskAttemptID(new TaskID(new JobID(), TaskType.MAP, 0), 0)\n+    val hadoopAttemptContext = new TaskAttemptContextImpl(conf, attemptId)\n+\n+    // Try to push down filters when filter push-down is enabled.\n+    // Notice: This push-down is RowGroups level, not individual records.\n+    if (pushed.isDefined) {\n+      ParquetInputFormat.setFilterPredicate(hadoopAttemptContext.getConfiguration, pushed.get)\n+    }\n+    val reader =\n+      buildReaderFunc(split, file.partitionValues, hadoopAttemptContext, pushed, convertTz)\n+    reader.initialize(split, hadoopAttemptContext)\n+    reader\n+  }\n+\n+  private def createRowBaseReader(file: PartitionedFile): RecordReader[Void, UnsafeRow] = {\n+    buildReaderBase(file, createRowBaseReader0)\n+  }\n+\n+  private def createRowBaseReader0(\n+      split: ParquetInputSplit,\n+      partitionValues: InternalRow,\n+      hadoopAttemptContext: TaskAttemptContextImpl,\n+      pushed: Option[FilterPredicate],\n+      convertTz: Option[TimeZone]): RecordReader[Void, UnsafeRow] = {\n+    logDebug(s\"Falling back to parquet-mr\")\n+    val taskContext = Option(TaskContext.get())\n+    // ParquetRecordReader returns UnsafeRow\n+    val readSupport = new ParquetReadSupport(convertTz, enableVectorizedReader = false)\n+    val reader = if (pushed.isDefined && enableRecordFilter) {\n+      val parquetFilter = FilterCompat.get(pushed.get, null)\n+      new ParquetRecordReader[UnsafeRow](readSupport, parquetFilter)\n+    } else {\n+      new ParquetRecordReader[UnsafeRow](readSupport)\n+    }\n+    val iter = new RecordReaderIterator(reader)\n+    // SPARK-23457 Register a task completion lister before `initialization`.\n+    taskContext.foreach(_.addTaskCompletionListener[Unit](_ => iter.close()))\n+    reader\n+  }\n+\n+  private def createVectorizedReader(file: PartitionedFile): VectorizedParquetRecordReader = {\n+    val vectorizedReader =\n+      buildReaderBase(file, createVectorizedReader0).asInstanceOf[VectorizedParquetRecordReader]"
  }],
  "prId": 24327
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "no biggie but I'd name it to `createParquetVectorizedReader`",
    "commit": "f658e9265ba741922fc96eec76038addcb6491a1",
    "createdAt": "2019-06-14T01:03:54Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.parquet\n+\n+import java.net.URI\n+import java.util.TimeZone\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.parquet.filter2.compat.FilterCompat\n+import org.apache.parquet.filter2.predicate.{FilterApi, FilterPredicate}\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.SKIP_ROW_GROUPS\n+import org.apache.parquet.hadoop.{ParquetFileReader, ParquetInputFormat, ParquetInputSplit, ParquetRecordReader}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.datasources.{PartitionedFile, RecordReaderIterator}\n+import org.apache.spark.sql.execution.datasources.parquet._\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.sources.v2.reader.{InputPartition, PartitionReader}\n+import org.apache.spark.sql.types.{AtomicType, StructType}\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create Parquet readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of Parquet files.\n+ * @param readDataSchema Required schema of Parquet files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param filters Filters to be pushed down in the batch scan.\n+ */\n+case class ParquetPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    filters: Array[Filter]) extends FilePartitionReaderFactory with Logging {\n+  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n+  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)\n+  private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled\n+  private val enableVectorizedReader: Boolean = sqlConf.parquetVectorizedReaderEnabled &&\n+    resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled\n+  private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion\n+  private val capacity = sqlConf.parquetVectorizedReaderBatchSize\n+  private val enableParquetFilterPushDown: Boolean = sqlConf.parquetFilterPushDown\n+  private val pushDownDate = sqlConf.parquetFilterPushDownDate\n+  private val pushDownTimestamp = sqlConf.parquetFilterPushDownTimestamp\n+  private val pushDownDecimal = sqlConf.parquetFilterPushDownDecimal\n+  private val pushDownStringStartWith = sqlConf.parquetFilterPushDownStringStartWith\n+  private val pushDownInFilterThreshold = sqlConf.parquetFilterPushDownInFilterThreshold\n+\n+  override def supportColumnarReads(partition: InputPartition): Boolean = {\n+    sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&\n+      resultSchema.length <= sqlConf.wholeStageMaxNumFields &&\n+      resultSchema.forall(_.dataType.isInstanceOf[AtomicType])\n+  }\n+\n+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {\n+    val reader = if (enableVectorizedReader) {\n+      createVectorizedReader(file)\n+    } else {\n+      createRowBaseReader(file)\n+    }\n+\n+    val fileReader = new PartitionReader[InternalRow] {\n+      override def next(): Boolean = reader.nextKeyValue()\n+\n+      override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]\n+\n+      override def close(): Unit = reader.close()\n+    }\n+\n+    new PartitionReaderWithPartitionValues(fileReader, readDataSchema,\n+      partitionSchema, file.partitionValues)\n+  }\n+\n+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {\n+    val vectorizedReader = createVectorizedReader(file)\n+    vectorizedReader.enableReturningBatches()\n+\n+    new PartitionReader[ColumnarBatch] {\n+      override def next(): Boolean = vectorizedReader.nextKeyValue()\n+\n+      override def get(): ColumnarBatch =\n+        vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]\n+\n+      override def close(): Unit = vectorizedReader.close()\n+    }\n+  }\n+\n+  private def buildReaderBase[T](\n+      file: PartitionedFile,\n+      buildReaderFunc: (\n+        ParquetInputSplit, InternalRow, TaskAttemptContextImpl, Option[FilterPredicate],\n+          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {\n+    val conf = broadcastedConf.value.value\n+\n+    val filePath = new Path(new URI(file.filePath))\n+    val split =\n+      new org.apache.parquet.hadoop.ParquetInputSplit(\n+        filePath,\n+        file.start,\n+        file.start + file.length,\n+        file.length,\n+        Array.empty,\n+        null)\n+\n+    lazy val footerFileMetaData =\n+      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData\n+    // Try to push down filters when filter push-down is enabled.\n+    val pushed = if (enableParquetFilterPushDown) {\n+      val parquetSchema = footerFileMetaData.getSchema\n+      val parquetFilters = new ParquetFilters(parquetSchema, pushDownDate, pushDownTimestamp,\n+        pushDownDecimal, pushDownStringStartWith, pushDownInFilterThreshold, isCaseSensitive)\n+      filters\n+        // Collects all converted Parquet filter predicates. Notice that not all predicates can be\n+        // converted (`ParquetFilters.createFilter` returns an `Option`). That's why a `flatMap`\n+        // is used here.\n+        .flatMap(parquetFilters.createFilter)\n+        .reduceOption(FilterApi.and)\n+    } else {\n+      None\n+    }\n+    // PARQUET_INT96_TIMESTAMP_CONVERSION says to apply timezone conversions to int96 timestamps'\n+    // *only* if the file was created by something other than \"parquet-mr\", so check the actual\n+    // writer here for this file.  We have to do this per-file, as each file in the table may\n+    // have different writers.\n+    // Define isCreatedByParquetMr as function to avoid unnecessary parquet footer reads.\n+    def isCreatedByParquetMr: Boolean =\n+      footerFileMetaData.getCreatedBy().startsWith(\"parquet-mr\")\n+\n+    val convertTz =\n+      if (timestampConversion && !isCreatedByParquetMr) {\n+        Some(DateTimeUtils.getTimeZone(conf.get(SQLConf.SESSION_LOCAL_TIMEZONE.key)))\n+      } else {\n+        None\n+      }\n+\n+    val attemptId = new TaskAttemptID(new TaskID(new JobID(), TaskType.MAP, 0), 0)\n+    val hadoopAttemptContext = new TaskAttemptContextImpl(conf, attemptId)\n+\n+    // Try to push down filters when filter push-down is enabled.\n+    // Notice: This push-down is RowGroups level, not individual records.\n+    if (pushed.isDefined) {\n+      ParquetInputFormat.setFilterPredicate(hadoopAttemptContext.getConfiguration, pushed.get)\n+    }\n+    val reader =\n+      buildReaderFunc(split, file.partitionValues, hadoopAttemptContext, pushed, convertTz)\n+    reader.initialize(split, hadoopAttemptContext)\n+    reader\n+  }\n+\n+  private def createRowBaseReader(file: PartitionedFile): RecordReader[Void, UnsafeRow] = {\n+    buildReaderBase(file, createRowBaseReader0)\n+  }\n+\n+  private def createRowBaseReader0(\n+      split: ParquetInputSplit,\n+      partitionValues: InternalRow,\n+      hadoopAttemptContext: TaskAttemptContextImpl,\n+      pushed: Option[FilterPredicate],\n+      convertTz: Option[TimeZone]): RecordReader[Void, UnsafeRow] = {\n+    logDebug(s\"Falling back to parquet-mr\")\n+    val taskContext = Option(TaskContext.get())\n+    // ParquetRecordReader returns UnsafeRow\n+    val readSupport = new ParquetReadSupport(convertTz, enableVectorizedReader = false)\n+    val reader = if (pushed.isDefined && enableRecordFilter) {\n+      val parquetFilter = FilterCompat.get(pushed.get, null)\n+      new ParquetRecordReader[UnsafeRow](readSupport, parquetFilter)\n+    } else {\n+      new ParquetRecordReader[UnsafeRow](readSupport)\n+    }\n+    val iter = new RecordReaderIterator(reader)\n+    // SPARK-23457 Register a task completion lister before `initialization`.\n+    taskContext.foreach(_.addTaskCompletionListener[Unit](_ => iter.close()))\n+    reader\n+  }\n+\n+  private def createVectorizedReader(file: PartitionedFile): VectorizedParquetRecordReader = {\n+    val vectorizedReader =\n+      buildReaderBase(file, createVectorizedReader0).asInstanceOf[VectorizedParquetRecordReader]\n+    vectorizedReader.initBatch(partitionSchema, file.partitionValues)\n+    vectorizedReader\n+  }\n+\n+  private def createVectorizedReader0("
  }],
  "prId": 24327
}]