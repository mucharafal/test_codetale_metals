[{
  "comments": [{
    "author": {
      "login": "ivoson"
    },
    "body": "Looks like, the `earliestEpochId ` will be also updated even no data removed from the metrics maps, this may cause some old data never got a chance to be removed.",
    "commit": "8eeb526521ef4b27304b8bc3979999e792996f03",
    "createdAt": "2019-05-27T06:25:40Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils._\n+import org.apache.spark.sql.execution.streaming.{ ProgressReporter, StreamProgress}\n+import org.apache.spark.sql.sources.v2.reader.streaming.SparkDataStream\n+import org.apache.spark.sql.streaming.{SinkProgress, SourceProgress, StreamingQueryProgress}\n+\n+trait ContinuousProgressReporter extends ProgressReporter {\n+\n+  protected def epochEndpoint: RpcEndpointRef\n+\n+  private var earliestEpochId: Long = -1\n+\n+  private val currentDurationsMs =\n+    new mutable.HashMap[Long, (Long, mutable.HashMap[String, Long])]()\n+  private val recordDurationMs = new mutable.HashMap[String, Long]()\n+\n+  private val currentTriggerStartOffsets: mutable.HashMap[Long, Map[SparkDataStream, String]] =\n+    new mutable.HashMap[Long, Map[SparkDataStream, String]]()\n+  private val currentTriggerEndOffsets: mutable.HashMap[Long, Map[SparkDataStream, String]] =\n+    new mutable.HashMap[Long, Map[SparkDataStream, String]]()\n+\n+  // TODO: Restore this from the checkpoint when possible.\n+  private var lastTriggerStartTimestamp = -1L\n+\n+  // Local timestamps and counters.\n+  private var currentTriggerStartTimestamp = -1L\n+\n+  private val noDataProgressEventInterval =\n+    sparkSession.sessionState.conf.streamingNoDataProgressEventInterval\n+\n+  // The timestamp we report an event that has no input data\n+  private var lastNoDataProgressEventTime = Long.MinValue\n+\n+  /** Begins recording statistics about query progress for a given trigger. */\n+  override protected def startTrigger(): Unit = {\n+    logDebug(\"Starting Trigger Calculation\")\n+    if (earliestEpochId == -1) {\n+      earliestEpochId = currentBatchId\n+    }\n+    checkQueueBoundaries()\n+    lastTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    currentDurationsMs.put(currentBatchId,\n+      (currentTriggerStartTimestamp, new mutable.HashMap[String, Long]()))\n+  }\n+\n+  /** Finalizes the query progress and adds it to list of recent status updates. */\n+  protected def finishTrigger(hasNewData: Boolean, epochId: Long, epochStats: EpochStats): Unit = {\n+    assert(currentTriggerStartOffsets.get(epochId).isDefined\n+      && currentTriggerEndOffsets.get(epochId).isDefined\n+      && currentDurationsMs.get(epochId).isDefined)\n+    val currentTriggerStartTimestamp = currentDurationsMs(epochId)._1\n+    val currentTriggerEndTimestamp = triggerClock.getTimeMillis()\n+\n+    val executionStats = extractExecutionStats(hasNewData, Some(epochStats))\n+    val processingTimeSec =\n+      (currentTriggerEndTimestamp - currentTriggerStartTimestamp).toDouble / MILLIS_PER_SECOND\n+\n+    val inputTimeSec = if (lastTriggerStartTimestamp >= 0) {\n+      (currentTriggerStartTimestamp - lastTriggerStartTimestamp).toDouble / MILLIS_PER_SECOND\n+    } else {\n+      Double.NaN\n+    }\n+    logDebug(s\"Execution stats: $executionStats\")\n+\n+    val sourceProgress = sources.distinct.map { source =>\n+      val numRecords = executionStats.inputRows.getOrElse(source, 0L)\n+      new SourceProgress(\n+        description = source.toString,\n+        startOffset = currentTriggerStartOffsets(epochId).get(source).orNull,\n+        endOffset = currentTriggerEndOffsets(epochId).get(source).orNull,\n+        numInputRows = numRecords,\n+        inputRowsPerSecond = numRecords / inputTimeSec,\n+        processedRowsPerSecond = numRecords / processingTimeSec\n+      )\n+    }\n+\n+    val sinkProgress = SinkProgress(\n+      sink.toString,\n+      sinkCommitProgress.map(_.numOutputRows))\n+\n+    val newProgress = new StreamingQueryProgress(\n+      id = id,\n+      runId = runId,\n+      name = name,\n+      timestamp = formatTimestamp(currentTriggerStartTimestamp),\n+      batchId = epochId,\n+      durationMs = new java.util.HashMap(\n+        (currentDurationsMs(epochId)._2 ++ recordDurationMs).toMap.mapValues(long2Long).asJava),\n+      eventTime = new java.util.HashMap(executionStats.eventTimeStats.asJava),\n+      stateOperators = executionStats.stateOperators.toArray,\n+      sources = sourceProgress.toArray,\n+      sink = sinkProgress)\n+\n+    if (hasNewData) {\n+      // Reset noDataEventTimestamp if we processed any data\n+      lastNoDataProgressEventTime = Long.MinValue\n+      updateProgress(newProgress)\n+    } else {\n+      val now = triggerClock.getTimeMillis()\n+      if (now - noDataProgressEventInterval >= lastNoDataProgressEventTime) {\n+        lastNoDataProgressEventTime = now\n+        updateProgress(newProgress)\n+      }\n+    }\n+\n+    currentStatus = currentStatus.copy(isTriggerActive = false)\n+  }\n+\n+  protected def extractSourceToNumInputRows(t: Option[Any] = None)\n+      : Map[SparkDataStream, Long] = {\n+    require(t.isDefined && t.get.isInstanceOf[EpochStats])\n+    Map(sources(0) -> t.get.asInstanceOf[EpochStats].inputRows)\n+  }\n+\n+  /**\n+   * Record the offsets range this trigger will process. Call this before updating\n+   * `committedOffsets` in `StreamExecution` to make sure that the correct range is recorded.\n+   */\n+  override protected def recordTriggerOffsets(\n+      from: StreamProgress,\n+      to: StreamProgress,\n+      epochId: Long): Unit = {\n+    checkQueueBoundaries()\n+    currentTriggerStartOffsets.put(epochId, from.mapValues(_.json))\n+    currentTriggerEndOffsets.put(epochId, to.mapValues(_.json))\n+  }\n+\n+  /** Records the duration of running `body` for the next query progress update. */\n+  def reportTimeTaken[T](triggerDetailKey: String, epochId: Long)(body: => T): T = {\n+    checkQueueBoundaries()\n+    val durations = currentDurationsMs.getOrElseUpdate(epochId,\n+      (triggerClock.getTimeMillis(), new mutable.HashMap[String, Long]()))\n+    addOrUpdateTime(triggerDetailKey, durations._2)(body)\n+  }\n+\n+  /**\n+   * Records the duration of running `body` for once time, which will not be cleared\n+   * when start a new trigger.\n+   */\n+  protected def recordTimeTaken[T](triggerDetailKey: String)(body: => T): T = {\n+    addOrUpdateTime(triggerDetailKey, recordDurationMs)(body)\n+  }\n+\n+  private def checkQueueBoundaries(): Unit = {\n+    if (currentDurationsMs.size > numProgressRetention) {\n+      currentDurationsMs.remove(earliestEpochId)\n+    }\n+\n+    if (currentTriggerStartOffsets.size > numProgressRetention) {\n+      currentTriggerStartOffsets.remove(earliestEpochId)\n+    }\n+\n+    if (currentTriggerEndOffsets.size > numProgressRetention) {\n+      currentTriggerEndOffsets.remove(earliestEpochId)\n+    }\n+\n+    earliestEpochId += 1",
    "line": 180
  }],
  "prId": 24537
}, {
  "comments": [{
    "author": {
      "login": "ivoson"
    },
    "body": "Since the origin `extractExecutionStats` method collects information from the SQL metrics (e.g. state operator metrics and watermark) which is reported only when task ended for now,  is there a plan to handle this part of data?",
    "commit": "8eeb526521ef4b27304b8bc3979999e792996f03",
    "createdAt": "2019-05-27T06:45:51Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils._\n+import org.apache.spark.sql.execution.streaming.{ ProgressReporter, StreamProgress}\n+import org.apache.spark.sql.sources.v2.reader.streaming.SparkDataStream\n+import org.apache.spark.sql.streaming.{SinkProgress, SourceProgress, StreamingQueryProgress}\n+\n+trait ContinuousProgressReporter extends ProgressReporter {\n+\n+  protected def epochEndpoint: RpcEndpointRef\n+\n+  private var earliestEpochId: Long = -1\n+\n+  private val currentDurationsMs =\n+    new mutable.HashMap[Long, (Long, mutable.HashMap[String, Long])]()\n+  private val recordDurationMs = new mutable.HashMap[String, Long]()\n+\n+  private val currentTriggerStartOffsets: mutable.HashMap[Long, Map[SparkDataStream, String]] =\n+    new mutable.HashMap[Long, Map[SparkDataStream, String]]()\n+  private val currentTriggerEndOffsets: mutable.HashMap[Long, Map[SparkDataStream, String]] =\n+    new mutable.HashMap[Long, Map[SparkDataStream, String]]()\n+\n+  // TODO: Restore this from the checkpoint when possible.\n+  private var lastTriggerStartTimestamp = -1L\n+\n+  // Local timestamps and counters.\n+  private var currentTriggerStartTimestamp = -1L\n+\n+  private val noDataProgressEventInterval =\n+    sparkSession.sessionState.conf.streamingNoDataProgressEventInterval\n+\n+  // The timestamp we report an event that has no input data\n+  private var lastNoDataProgressEventTime = Long.MinValue\n+\n+  /** Begins recording statistics about query progress for a given trigger. */\n+  override protected def startTrigger(): Unit = {\n+    logDebug(\"Starting Trigger Calculation\")\n+    if (earliestEpochId == -1) {\n+      earliestEpochId = currentBatchId\n+    }\n+    checkQueueBoundaries()\n+    lastTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    currentDurationsMs.put(currentBatchId,\n+      (currentTriggerStartTimestamp, new mutable.HashMap[String, Long]()))\n+  }\n+\n+  /** Finalizes the query progress and adds it to list of recent status updates. */\n+  protected def finishTrigger(hasNewData: Boolean, epochId: Long, epochStats: EpochStats): Unit = {\n+    assert(currentTriggerStartOffsets.get(epochId).isDefined\n+      && currentTriggerEndOffsets.get(epochId).isDefined\n+      && currentDurationsMs.get(epochId).isDefined)\n+    val currentTriggerStartTimestamp = currentDurationsMs(epochId)._1\n+    val currentTriggerEndTimestamp = triggerClock.getTimeMillis()\n+\n+    val executionStats = extractExecutionStats(hasNewData, Some(epochStats))",
    "line": 77
  }],
  "prId": 24537
}]