[{
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "Some uncertaintyï¼šshould we still keep special handling for hive tables?",
    "commit": "148483bd3fee3ae3af12bbd4a60cd416abc204af",
    "createdAt": "2019-04-25T12:19:19Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.concurrent.Callable\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.QualifiedTableName\n+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, CatalogUtils, UnresolvedCatalogRelation}\n+import org.apache.spark.sql.catalyst.plans.logical.{InsertIntoTable, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.sources.v2.TableProvider\n+\n+/**\n+ * Replaces v2 source [[UnresolvedCatalogRelation]] with concrete relation logical plans.\n+ */\n+class FindDataSourceV2Table(sparkSession: SparkSession) extends Rule[LogicalPlan] {\n+  private def readDataSourceTable(\n+      table: CatalogTable): LogicalPlan = {\n+    val qualifiedTableName = QualifiedTableName(table.database, table.identifier.table)\n+    val catalog = sparkSession.sessionState.catalog\n+    catalog.getCachedPlan(qualifiedTableName, new Callable[LogicalPlan]() {\n+      override def call(): LogicalPlan = {\n+        val cls = DataSourceV2Utils.isV2Source(sparkSession, table.provider.get)\n+        val provider = cls.get.getConstructor().newInstance().asInstanceOf[TableProvider]\n+        val pathOption = table.storage.locationUri.map(\"path\" -> CatalogUtils.URIToString(_))\n+        val dsOptions = DataSourceV2Utils.\n+          extractSessionConfigs(provider, sparkSession.sessionState.conf, pathOption.toMap)\n+        val readTable = DataSourceV2Utils.\n+          getBatchReadTable(sparkSession, Option(table.schema), cls.get, dsOptions)\n+        DataSourceV2Relation.create(readTable.get, dsOptions)\n+      }\n+    })\n+  }\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case i @ InsertIntoTable(UnresolvedCatalogRelation(tableMeta), _, _, _, _)\n+        if DDLUtils.isDatasourceTable(tableMeta) =>\n+      val pathOption = tableMeta.storage.locationUri.map(\"path\" -> CatalogUtils.URIToString(_))\n+      val shouldReadInV2 = DataSourceV2Utils.shouldReadWithV2(\n+        sparkSession,\n+        Option(tableMeta.schema),\n+        tableMeta.provider.get,\n+        pathOption.toMap)\n+\n+      if (shouldReadInV2) {\n+        i.copy(table = readDataSourceTable(tableMeta))\n+      } else {\n+        i\n+      }\n+\n+    case unresolved @ UnresolvedCatalogRelation(tableMeta) if\n+        DDLUtils.isDatasourceTable(tableMeta) =>\n+      val pathOption = tableMeta.storage.locationUri.map(\"path\" -> CatalogUtils.URIToString(_))\n+      val shouldReadInV2 = DataSourceV2Utils.shouldReadWithV2(\n+        sparkSession,\n+        Option(tableMeta.schema),\n+        tableMeta.provider.get,\n+        pathOption.toMap)\n+\n+      if (shouldReadInV2) {\n+        readDataSourceTable(tableMeta)\n+      } else {\n+        unresolved\n+      }\n+  }",
    "line": 82
  }, {
    "author": {
      "login": "ConeyLiu"
    },
    "body": "Hive table will be handled in the `FindDataSourceTable`.",
    "commit": "148483bd3fee3ae3af12bbd4a60cd416abc204af",
    "createdAt": "2019-05-06T08:25:26Z",
    "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import java.util.concurrent.Callable\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.QualifiedTableName\n+import org.apache.spark.sql.catalyst.catalog.{CatalogTable, CatalogUtils, UnresolvedCatalogRelation}\n+import org.apache.spark.sql.catalyst.plans.logical.{InsertIntoTable, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.sources.v2.TableProvider\n+\n+/**\n+ * Replaces v2 source [[UnresolvedCatalogRelation]] with concrete relation logical plans.\n+ */\n+class FindDataSourceV2Table(sparkSession: SparkSession) extends Rule[LogicalPlan] {\n+  private def readDataSourceTable(\n+      table: CatalogTable): LogicalPlan = {\n+    val qualifiedTableName = QualifiedTableName(table.database, table.identifier.table)\n+    val catalog = sparkSession.sessionState.catalog\n+    catalog.getCachedPlan(qualifiedTableName, new Callable[LogicalPlan]() {\n+      override def call(): LogicalPlan = {\n+        val cls = DataSourceV2Utils.isV2Source(sparkSession, table.provider.get)\n+        val provider = cls.get.getConstructor().newInstance().asInstanceOf[TableProvider]\n+        val pathOption = table.storage.locationUri.map(\"path\" -> CatalogUtils.URIToString(_))\n+        val dsOptions = DataSourceV2Utils.\n+          extractSessionConfigs(provider, sparkSession.sessionState.conf, pathOption.toMap)\n+        val readTable = DataSourceV2Utils.\n+          getBatchReadTable(sparkSession, Option(table.schema), cls.get, dsOptions)\n+        DataSourceV2Relation.create(readTable.get, dsOptions)\n+      }\n+    })\n+  }\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case i @ InsertIntoTable(UnresolvedCatalogRelation(tableMeta), _, _, _, _)\n+        if DDLUtils.isDatasourceTable(tableMeta) =>\n+      val pathOption = tableMeta.storage.locationUri.map(\"path\" -> CatalogUtils.URIToString(_))\n+      val shouldReadInV2 = DataSourceV2Utils.shouldReadWithV2(\n+        sparkSession,\n+        Option(tableMeta.schema),\n+        tableMeta.provider.get,\n+        pathOption.toMap)\n+\n+      if (shouldReadInV2) {\n+        i.copy(table = readDataSourceTable(tableMeta))\n+      } else {\n+        i\n+      }\n+\n+    case unresolved @ UnresolvedCatalogRelation(tableMeta) if\n+        DDLUtils.isDatasourceTable(tableMeta) =>\n+      val pathOption = tableMeta.storage.locationUri.map(\"path\" -> CatalogUtils.URIToString(_))\n+      val shouldReadInV2 = DataSourceV2Utils.shouldReadWithV2(\n+        sparkSession,\n+        Option(tableMeta.schema),\n+        tableMeta.provider.get,\n+        pathOption.toMap)\n+\n+      if (shouldReadInV2) {\n+        readDataSourceTable(tableMeta)\n+      } else {\n+        unresolved\n+      }\n+  }",
    "line": 82
  }],
  "prId": 24278
}]