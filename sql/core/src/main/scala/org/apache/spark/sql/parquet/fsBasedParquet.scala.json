[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Seems this comment is outdated?\n",
    "commit": "6063f87a6297f4b2e1b2a0468c1e2f9ff5843228",
    "createdAt": "2015-05-13T16:38:12Z",
    "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.parquet\n+\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.Try\n+\n+import com.google.common.base.Objects\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import parquet.filter2.predicate.FilterApi\n+import parquet.format.converter.ParquetMetadataConverter\n+import parquet.hadoop._\n+import parquet.hadoop.metadata.CompressionCodecName\n+import parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.rdd.RDD._\n+import org.apache.spark.rdd.{NewHadoopPartition, NewHadoopRDD, RDD}\n+import org.apache.spark.sql.sources._\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.sql.{Row, SQLConf, SQLContext}\n+import org.apache.spark.{Logging, Partition => SparkPartition, SparkException}\n+\n+private[sql] class DefaultSource extends FSBasedRelationProvider {\n+  override def createRelation(\n+      sqlContext: SQLContext,\n+      paths: Array[String],\n+      schema: Option[StructType],\n+      partitionColumns: Option[StructType],\n+      parameters: Map[String, String]): FSBasedRelation = {\n+    val partitionSpec = partitionColumns.map(PartitionSpec(_, Seq.empty))\n+    new FSBasedParquetRelation(paths, schema, partitionSpec, parameters)(sqlContext)\n+  }\n+}\n+\n+// NOTE: This class is instantiated and used on executor side only, no need to be serializable.\n+private[sql] class ParquetOutputWriter extends OutputWriter {\n+  private var recordWriter: RecordWriter[Void, Row] = _\n+  private var taskAttemptContext: TaskAttemptContext = _\n+\n+  override def init(\n+      path: String,\n+      dataSchema: StructType,\n+      context: TaskAttemptContext): Unit = {\n+    val conf = context.getConfiguration\n+    val outputFormat = {\n+      // When appending new Parquet files to an existing Parquet file directory, to avoid\n+      // overwriting existing data files, we need to find out the max task ID encoded in these data\n+      // file names.\n+      // TODO Make this snippet a utility function for other data source developers\n+      val maxExistingTaskId = {\n+        // Note that `path` may point to a temporary location.  Here we retrieve the real\n+        // destination path from the configuration\n+        val outputPath = new Path(conf.get(\"spark.sql.sources.output.path\"))\n+        val fs = outputPath.getFileSystem(conf)\n+\n+        if (fs.exists(outputPath)) {\n+          // Pattern used to match task ID in part file names, e.g.:\n+          //\n+          //   part-r-00001.gz.part\n+          //          ^~~~~\n+          val partFilePattern = \"\"\"part-.-(\\d{1,}).*\"\"\".r\n+\n+          fs.listStatus(outputPath).map(_.getPath.getName).map {\n+            case partFilePattern(id) => id.toInt\n+            case name if name.startsWith(\"_\") => 0\n+            case name if name.startsWith(\".\") => 0\n+            case name => sys.error(\n+              s\"\"\"Trying to write Parquet files to directory $outputPath,\n+                 |but found items with illegal name \"$name\"\n+               \"\"\".stripMargin.replace('\\n', ' ').trim)\n+          }.reduceOption(_ max _).getOrElse(0)\n+        } else {\n+          0\n+        }\n+      }\n+\n+      new ParquetOutputFormat[Row]() {\n+        // Here we override `getDefaultWorkFile` for two reasons:\n+        //\n+        //  1. To allow appending.  We need to generate output file name based on the max available\n+        //     task ID computed above.\n+        //\n+        //  2. To allow dynamic partitioning.  Default `getDefaultWorkFile` uses\n+        //     `FileOutputCommitter.getWorkPath()`, which points to the base directory of all",
    "line": 105
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "No. When `FileOutputCommitter` is used, we still use `FileOutputCommitter.getWorkPath()` internally inside `InsertIntoFSBasedRelation`.\n",
    "commit": "6063f87a6297f4b2e1b2a0468c1e2f9ff5843228",
    "createdAt": "2015-05-13T16:40:36Z",
    "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.parquet\n+\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.util.Try\n+\n+import com.google.common.base.Objects\n+import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapreduce._\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import parquet.filter2.predicate.FilterApi\n+import parquet.format.converter.ParquetMetadataConverter\n+import parquet.hadoop._\n+import parquet.hadoop.metadata.CompressionCodecName\n+import parquet.hadoop.util.ContextUtil\n+\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.rdd.RDD._\n+import org.apache.spark.rdd.{NewHadoopPartition, NewHadoopRDD, RDD}\n+import org.apache.spark.sql.sources._\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.sql.{Row, SQLConf, SQLContext}\n+import org.apache.spark.{Logging, Partition => SparkPartition, SparkException}\n+\n+private[sql] class DefaultSource extends FSBasedRelationProvider {\n+  override def createRelation(\n+      sqlContext: SQLContext,\n+      paths: Array[String],\n+      schema: Option[StructType],\n+      partitionColumns: Option[StructType],\n+      parameters: Map[String, String]): FSBasedRelation = {\n+    val partitionSpec = partitionColumns.map(PartitionSpec(_, Seq.empty))\n+    new FSBasedParquetRelation(paths, schema, partitionSpec, parameters)(sqlContext)\n+  }\n+}\n+\n+// NOTE: This class is instantiated and used on executor side only, no need to be serializable.\n+private[sql] class ParquetOutputWriter extends OutputWriter {\n+  private var recordWriter: RecordWriter[Void, Row] = _\n+  private var taskAttemptContext: TaskAttemptContext = _\n+\n+  override def init(\n+      path: String,\n+      dataSchema: StructType,\n+      context: TaskAttemptContext): Unit = {\n+    val conf = context.getConfiguration\n+    val outputFormat = {\n+      // When appending new Parquet files to an existing Parquet file directory, to avoid\n+      // overwriting existing data files, we need to find out the max task ID encoded in these data\n+      // file names.\n+      // TODO Make this snippet a utility function for other data source developers\n+      val maxExistingTaskId = {\n+        // Note that `path` may point to a temporary location.  Here we retrieve the real\n+        // destination path from the configuration\n+        val outputPath = new Path(conf.get(\"spark.sql.sources.output.path\"))\n+        val fs = outputPath.getFileSystem(conf)\n+\n+        if (fs.exists(outputPath)) {\n+          // Pattern used to match task ID in part file names, e.g.:\n+          //\n+          //   part-r-00001.gz.part\n+          //          ^~~~~\n+          val partFilePattern = \"\"\"part-.-(\\d{1,}).*\"\"\".r\n+\n+          fs.listStatus(outputPath).map(_.getPath.getName).map {\n+            case partFilePattern(id) => id.toInt\n+            case name if name.startsWith(\"_\") => 0\n+            case name if name.startsWith(\".\") => 0\n+            case name => sys.error(\n+              s\"\"\"Trying to write Parquet files to directory $outputPath,\n+                 |but found items with illegal name \"$name\"\n+               \"\"\".stripMargin.replace('\\n', ' ').trim)\n+          }.reduceOption(_ max _).getOrElse(0)\n+        } else {\n+          0\n+        }\n+      }\n+\n+      new ParquetOutputFormat[Row]() {\n+        // Here we override `getDefaultWorkFile` for two reasons:\n+        //\n+        //  1. To allow appending.  We need to generate output file name based on the max available\n+        //     task ID computed above.\n+        //\n+        //  2. To allow dynamic partitioning.  Default `getDefaultWorkFile` uses\n+        //     `FileOutputCommitter.getWorkPath()`, which points to the base directory of all",
    "line": 105
  }],
  "prId": 6090
}]