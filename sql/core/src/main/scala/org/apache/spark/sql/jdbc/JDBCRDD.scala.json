[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this is reachable if the connection failed, isn't it?\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-01T20:53:36Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")",
    "line": 125
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "ah nvm you didn't catch the exception\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-01T20:53:50Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")",
    "line": 125
  }, {
    "author": {
      "login": "tmyklebu"
    },
    "body": "@rxin: Yeah.  Is there a \"please emit a compiler warning if you can't prove this line is unreachable\" in Scala?  Or a more idiomatic way to do this?\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-01T21:03:49Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")",
    "line": 125
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "You can just remove the `return`. In this way, types of both `try` expressions are `StructType`, and you don't need the last `RuntimeException`.\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-03T03:39:28Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")",
    "line": 125
  }],
  "prId": 4261
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "var -> val. also can remove the trailing semicolon\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-01T23:14:23Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);",
    "line": 103
  }],
  "prId": 4261
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "just a quick note - the spark coding style does 4 space indent for function parameters in definitions. e.g.\n\n``` scala\ndef scanTable(\n    sc: SparkContext,\n    schema: StructType,\n    ...): RDD[Row => {\n```\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-02T00:06:24Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")\n+  }\n+\n+  /**\n+   * Prune all but the specified columns from the specified Catalyst schema.\n+   *\n+   * @param schema - The Catalyst schema of the master table\n+   * @param columns - The list of desired columns\n+   *\n+   * @return A Catalyst schema corresponding to columns in the given order.\n+   */\n+  private def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields map { x => x.metadata.getString(\"name\") -> x }: _*)\n+    new StructType(columns map { name => fieldMap(name) })\n+  }\n+\n+  /**\n+   * Given a driver string and an url, return a function that loads the\n+   * specified driver string then returns a connection to the JDBC url.\n+   * getConnector is run on the driver code, while the function it returns\n+   * is run on the executor.\n+   *\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   *\n+   * @return A function that loads the driver and connects to the url.\n+   */\n+  def getConnector(driver: String, url: String): () => Connection = {\n+    () => {\n+      try {\n+        if (driver != null) Class.forName(driver)\n+      } catch {\n+        case e: ClassNotFoundException => {\n+          logWarning(s\"Couldn't find class $driver\", e);\n+        }\n+      }\n+      DriverManager.getConnection(url)\n+    }\n+  }\n+  /**\n+   * Build and return JDBCRDD from the given information.\n+   *\n+   * @param sc - Your SparkContext.\n+   * @param schema - The Catalyst schema of the underlying database table.\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   * @param fqTable - The fully-qualified table name (or paren'd SQL query) to use.\n+   * @param requiredColumns - The names of the columns to SELECT.\n+   * @param filters - The filters to include in all WHERE clauses.\n+   * @param parts - An array of JDBCPartitions specifying partition ids and\n+   *    per-partition WHERE clauses.\n+   *\n+   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\".\n+   */\n+  def scanTable(sc: SparkContext,",
    "line": 179
  }],
  "prId": 4261
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "and 2 space indent when calling them\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-02T00:06:37Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")\n+  }\n+\n+  /**\n+   * Prune all but the specified columns from the specified Catalyst schema.\n+   *\n+   * @param schema - The Catalyst schema of the master table\n+   * @param columns - The list of desired columns\n+   *\n+   * @return A Catalyst schema corresponding to columns in the given order.\n+   */\n+  private def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields map { x => x.metadata.getString(\"name\") -> x }: _*)\n+    new StructType(columns map { name => fieldMap(name) })\n+  }\n+\n+  /**\n+   * Given a driver string and an url, return a function that loads the\n+   * specified driver string then returns a connection to the JDBC url.\n+   * getConnector is run on the driver code, while the function it returns\n+   * is run on the executor.\n+   *\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   *\n+   * @return A function that loads the driver and connects to the url.\n+   */\n+  def getConnector(driver: String, url: String): () => Connection = {\n+    () => {\n+      try {\n+        if (driver != null) Class.forName(driver)\n+      } catch {\n+        case e: ClassNotFoundException => {\n+          logWarning(s\"Couldn't find class $driver\", e);\n+        }\n+      }\n+      DriverManager.getConnection(url)\n+    }\n+  }\n+  /**\n+   * Build and return JDBCRDD from the given information.\n+   *\n+   * @param sc - Your SparkContext.\n+   * @param schema - The Catalyst schema of the underlying database table.\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   * @param fqTable - The fully-qualified table name (or paren'd SQL query) to use.\n+   * @param requiredColumns - The names of the columns to SELECT.\n+   * @param filters - The filters to include in all WHERE clauses.\n+   * @param parts - An array of JDBCPartitions specifying partition ids and\n+   *    per-partition WHERE clauses.\n+   *\n+   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\".\n+   */\n+  def scanTable(sc: SparkContext,\n+                schema: StructType,\n+                driver: String,\n+                url: String,\n+                fqTable: String,\n+                requiredColumns: Array[String],\n+                filters: Array[Filter],\n+                parts: Array[Partition]): RDD[Row] = {\n+    val prunedSchema = pruneSchema(schema, requiredColumns)\n+\n+    return new JDBCRDD(sc,\n+        getConnector(driver, url),",
    "line": 190
  }],
  "prId": 4261
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "how about just creating a bunch of conversion functions, and then put those into an array and invoke them?\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-02T01:25:23Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")\n+  }\n+\n+  /**\n+   * Prune all but the specified columns from the specified Catalyst schema.\n+   *\n+   * @param schema - The Catalyst schema of the master table\n+   * @param columns - The list of desired columns\n+   *\n+   * @return A Catalyst schema corresponding to columns in the given order.\n+   */\n+  private def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields map { x => x.metadata.getString(\"name\") -> x }: _*)\n+    new StructType(columns map { name => fieldMap(name) })\n+  }\n+\n+  /**\n+   * Given a driver string and an url, return a function that loads the\n+   * specified driver string then returns a connection to the JDBC url.\n+   * getConnector is run on the driver code, while the function it returns\n+   * is run on the executor.\n+   *\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   *\n+   * @return A function that loads the driver and connects to the url.\n+   */\n+  def getConnector(driver: String, url: String): () => Connection = {\n+    () => {\n+      try {\n+        if (driver != null) Class.forName(driver)\n+      } catch {\n+        case e: ClassNotFoundException => {\n+          logWarning(s\"Couldn't find class $driver\", e);\n+        }\n+      }\n+      DriverManager.getConnection(url)\n+    }\n+  }\n+  /**\n+   * Build and return JDBCRDD from the given information.\n+   *\n+   * @param sc - Your SparkContext.\n+   * @param schema - The Catalyst schema of the underlying database table.\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   * @param fqTable - The fully-qualified table name (or paren'd SQL query) to use.\n+   * @param requiredColumns - The names of the columns to SELECT.\n+   * @param filters - The filters to include in all WHERE clauses.\n+   * @param parts - An array of JDBCPartitions specifying partition ids and\n+   *    per-partition WHERE clauses.\n+   *\n+   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\".\n+   */\n+  def scanTable(sc: SparkContext,\n+                schema: StructType,\n+                driver: String,\n+                url: String,\n+                fqTable: String,\n+                requiredColumns: Array[String],\n+                filters: Array[Filter],\n+                parts: Array[Partition]): RDD[Row] = {\n+    val prunedSchema = pruneSchema(schema, requiredColumns)\n+\n+    return new JDBCRDD(sc,\n+        getConnector(driver, url),\n+        prunedSchema,\n+        fqTable,\n+        requiredColumns,\n+        filters,\n+        parts)\n+  }\n+}\n+\n+/**\n+ * An RDD representing a table in a database accessed via JDBC.  Both the\n+ * driver code and the workers must be able to access the database; the driver\n+ * needs to fetch the schema while the workers need to fetch the data.\n+ */\n+private[sql] class JDBCRDD(\n+    sc: SparkContext,\n+    getConnection: () => Connection,\n+    schema: StructType,\n+    fqTable: String,\n+    columns: Array[String],\n+    filters: Array[Filter],\n+    partitions: Array[Partition])\n+  extends RDD[Row](sc, Nil) {\n+\n+  /**\n+   * Retrieve the list of partitions corresponding to this RDD.\n+   */\n+  override def getPartitions: Array[Partition] = partitions\n+\n+  /**\n+   * `columns`, but as a String suitable for injection into a SQL query.\n+   */\n+  private val columnList: String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.length == 0) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+   * Turns a single Filter into a String representing a SQL expression.\n+   * Returns null for an unhandled filter.\n+   */\n+  private def compileFilter(f: Filter): String = f match {\n+    case EqualTo(attr, value) => s\"$attr = $value\"\n+    case LessThan(attr, value) => s\"$attr < $value\"\n+    case GreaterThan(attr, value) => s\"$attr > $value\"\n+    case LessThanOrEqual(attr, value) => s\"$attr <= $value\"\n+    case GreaterThanOrEqual(attr, value) => s\"$attr >= $value\"\n+    case _ => null\n+  }\n+\n+  /**\n+   * `filters`, but as a WHERE clause suitable for injection into a SQL query.\n+   */\n+  private val filterWhereClause: String = {\n+    val filterStrings = filters map compileFilter filter (_ != null)\n+    if (filterStrings.size > 0) {\n+      val sb = new StringBuilder(\"WHERE \")\n+      filterStrings.foreach(x => sb.append(x).append(\" AND \"))\n+      sb.substring(0, sb.length - 5)\n+    } else \"\"\n+  }\n+\n+  /**\n+   * A WHERE clause representing both `filters`, if any, and the current partition.\n+   */\n+  private def getWhereClause(part: JDBCPartition): String = {\n+    if (part.whereClause != null && filterWhereClause.length > 0) {\n+      filterWhereClause + \" AND \" + part.whereClause\n+    } else if (part.whereClause != null) {\n+      \"WHERE \" + part.whereClause\n+    } else {\n+      filterWhereClause\n+    }\n+  }\n+\n+  // Each JDBC-to-Catalyst conversion corresponds to a tag defined here so that\n+  // we don't have to potentially poke around in the Metadata once for every\n+  // row.  \n+  // Is there a better way to do this?  I'd rather be using a type that\n+  // contains only the tags I define.\n+  abstract class JDBCConversion",
    "line": 271
  }, {
    "author": {
      "login": "tmyklebu"
    },
    "body": "@rxin: That's a straightforward change.  This conversion code is hot, though; I'd want to benchmark that change and see whether it impairs performance.\n\nTo the JVM, iterating over the array and calling one function per field retrieved would look like a single highly-polymorphic call site.  Maybe Hotspot can deal with that now, but it couldn't inline through such call sites 5 years ago.\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-02T01:42:05Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")\n+  }\n+\n+  /**\n+   * Prune all but the specified columns from the specified Catalyst schema.\n+   *\n+   * @param schema - The Catalyst schema of the master table\n+   * @param columns - The list of desired columns\n+   *\n+   * @return A Catalyst schema corresponding to columns in the given order.\n+   */\n+  private def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields map { x => x.metadata.getString(\"name\") -> x }: _*)\n+    new StructType(columns map { name => fieldMap(name) })\n+  }\n+\n+  /**\n+   * Given a driver string and an url, return a function that loads the\n+   * specified driver string then returns a connection to the JDBC url.\n+   * getConnector is run on the driver code, while the function it returns\n+   * is run on the executor.\n+   *\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   *\n+   * @return A function that loads the driver and connects to the url.\n+   */\n+  def getConnector(driver: String, url: String): () => Connection = {\n+    () => {\n+      try {\n+        if (driver != null) Class.forName(driver)\n+      } catch {\n+        case e: ClassNotFoundException => {\n+          logWarning(s\"Couldn't find class $driver\", e);\n+        }\n+      }\n+      DriverManager.getConnection(url)\n+    }\n+  }\n+  /**\n+   * Build and return JDBCRDD from the given information.\n+   *\n+   * @param sc - Your SparkContext.\n+   * @param schema - The Catalyst schema of the underlying database table.\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   * @param fqTable - The fully-qualified table name (or paren'd SQL query) to use.\n+   * @param requiredColumns - The names of the columns to SELECT.\n+   * @param filters - The filters to include in all WHERE clauses.\n+   * @param parts - An array of JDBCPartitions specifying partition ids and\n+   *    per-partition WHERE clauses.\n+   *\n+   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\".\n+   */\n+  def scanTable(sc: SparkContext,\n+                schema: StructType,\n+                driver: String,\n+                url: String,\n+                fqTable: String,\n+                requiredColumns: Array[String],\n+                filters: Array[Filter],\n+                parts: Array[Partition]): RDD[Row] = {\n+    val prunedSchema = pruneSchema(schema, requiredColumns)\n+\n+    return new JDBCRDD(sc,\n+        getConnector(driver, url),\n+        prunedSchema,\n+        fqTable,\n+        requiredColumns,\n+        filters,\n+        parts)\n+  }\n+}\n+\n+/**\n+ * An RDD representing a table in a database accessed via JDBC.  Both the\n+ * driver code and the workers must be able to access the database; the driver\n+ * needs to fetch the schema while the workers need to fetch the data.\n+ */\n+private[sql] class JDBCRDD(\n+    sc: SparkContext,\n+    getConnection: () => Connection,\n+    schema: StructType,\n+    fqTable: String,\n+    columns: Array[String],\n+    filters: Array[Filter],\n+    partitions: Array[Partition])\n+  extends RDD[Row](sc, Nil) {\n+\n+  /**\n+   * Retrieve the list of partitions corresponding to this RDD.\n+   */\n+  override def getPartitions: Array[Partition] = partitions\n+\n+  /**\n+   * `columns`, but as a String suitable for injection into a SQL query.\n+   */\n+  private val columnList: String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.length == 0) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+   * Turns a single Filter into a String representing a SQL expression.\n+   * Returns null for an unhandled filter.\n+   */\n+  private def compileFilter(f: Filter): String = f match {\n+    case EqualTo(attr, value) => s\"$attr = $value\"\n+    case LessThan(attr, value) => s\"$attr < $value\"\n+    case GreaterThan(attr, value) => s\"$attr > $value\"\n+    case LessThanOrEqual(attr, value) => s\"$attr <= $value\"\n+    case GreaterThanOrEqual(attr, value) => s\"$attr >= $value\"\n+    case _ => null\n+  }\n+\n+  /**\n+   * `filters`, but as a WHERE clause suitable for injection into a SQL query.\n+   */\n+  private val filterWhereClause: String = {\n+    val filterStrings = filters map compileFilter filter (_ != null)\n+    if (filterStrings.size > 0) {\n+      val sb = new StringBuilder(\"WHERE \")\n+      filterStrings.foreach(x => sb.append(x).append(\" AND \"))\n+      sb.substring(0, sb.length - 5)\n+    } else \"\"\n+  }\n+\n+  /**\n+   * A WHERE clause representing both `filters`, if any, and the current partition.\n+   */\n+  private def getWhereClause(part: JDBCPartition): String = {\n+    if (part.whereClause != null && filterWhereClause.length > 0) {\n+      filterWhereClause + \" AND \" + part.whereClause\n+    } else if (part.whereClause != null) {\n+      \"WHERE \" + part.whereClause\n+    } else {\n+      filterWhereClause\n+    }\n+  }\n+\n+  // Each JDBC-to-Catalyst conversion corresponds to a tag defined here so that\n+  // we don't have to potentially poke around in the Metadata once for every\n+  // row.  \n+  // Is there a better way to do this?  I'd rather be using a type that\n+  // contains only the tags I define.\n+  abstract class JDBCConversion",
    "line": 271
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "It wouldn't be able to JIT that now either, but it might be more efficient than 20 branches with equality comparisons?\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-02T01:53:05Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")\n+  }\n+\n+  /**\n+   * Prune all but the specified columns from the specified Catalyst schema.\n+   *\n+   * @param schema - The Catalyst schema of the master table\n+   * @param columns - The list of desired columns\n+   *\n+   * @return A Catalyst schema corresponding to columns in the given order.\n+   */\n+  private def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields map { x => x.metadata.getString(\"name\") -> x }: _*)\n+    new StructType(columns map { name => fieldMap(name) })\n+  }\n+\n+  /**\n+   * Given a driver string and an url, return a function that loads the\n+   * specified driver string then returns a connection to the JDBC url.\n+   * getConnector is run on the driver code, while the function it returns\n+   * is run on the executor.\n+   *\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   *\n+   * @return A function that loads the driver and connects to the url.\n+   */\n+  def getConnector(driver: String, url: String): () => Connection = {\n+    () => {\n+      try {\n+        if (driver != null) Class.forName(driver)\n+      } catch {\n+        case e: ClassNotFoundException => {\n+          logWarning(s\"Couldn't find class $driver\", e);\n+        }\n+      }\n+      DriverManager.getConnection(url)\n+    }\n+  }\n+  /**\n+   * Build and return JDBCRDD from the given information.\n+   *\n+   * @param sc - Your SparkContext.\n+   * @param schema - The Catalyst schema of the underlying database table.\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   * @param fqTable - The fully-qualified table name (or paren'd SQL query) to use.\n+   * @param requiredColumns - The names of the columns to SELECT.\n+   * @param filters - The filters to include in all WHERE clauses.\n+   * @param parts - An array of JDBCPartitions specifying partition ids and\n+   *    per-partition WHERE clauses.\n+   *\n+   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\".\n+   */\n+  def scanTable(sc: SparkContext,\n+                schema: StructType,\n+                driver: String,\n+                url: String,\n+                fqTable: String,\n+                requiredColumns: Array[String],\n+                filters: Array[Filter],\n+                parts: Array[Partition]): RDD[Row] = {\n+    val prunedSchema = pruneSchema(schema, requiredColumns)\n+\n+    return new JDBCRDD(sc,\n+        getConnector(driver, url),\n+        prunedSchema,\n+        fqTable,\n+        requiredColumns,\n+        filters,\n+        parts)\n+  }\n+}\n+\n+/**\n+ * An RDD representing a table in a database accessed via JDBC.  Both the\n+ * driver code and the workers must be able to access the database; the driver\n+ * needs to fetch the schema while the workers need to fetch the data.\n+ */\n+private[sql] class JDBCRDD(\n+    sc: SparkContext,\n+    getConnection: () => Connection,\n+    schema: StructType,\n+    fqTable: String,\n+    columns: Array[String],\n+    filters: Array[Filter],\n+    partitions: Array[Partition])\n+  extends RDD[Row](sc, Nil) {\n+\n+  /**\n+   * Retrieve the list of partitions corresponding to this RDD.\n+   */\n+  override def getPartitions: Array[Partition] = partitions\n+\n+  /**\n+   * `columns`, but as a String suitable for injection into a SQL query.\n+   */\n+  private val columnList: String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.length == 0) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+   * Turns a single Filter into a String representing a SQL expression.\n+   * Returns null for an unhandled filter.\n+   */\n+  private def compileFilter(f: Filter): String = f match {\n+    case EqualTo(attr, value) => s\"$attr = $value\"\n+    case LessThan(attr, value) => s\"$attr < $value\"\n+    case GreaterThan(attr, value) => s\"$attr > $value\"\n+    case LessThanOrEqual(attr, value) => s\"$attr <= $value\"\n+    case GreaterThanOrEqual(attr, value) => s\"$attr >= $value\"\n+    case _ => null\n+  }\n+\n+  /**\n+   * `filters`, but as a WHERE clause suitable for injection into a SQL query.\n+   */\n+  private val filterWhereClause: String = {\n+    val filterStrings = filters map compileFilter filter (_ != null)\n+    if (filterStrings.size > 0) {\n+      val sb = new StringBuilder(\"WHERE \")\n+      filterStrings.foreach(x => sb.append(x).append(\" AND \"))\n+      sb.substring(0, sb.length - 5)\n+    } else \"\"\n+  }\n+\n+  /**\n+   * A WHERE clause representing both `filters`, if any, and the current partition.\n+   */\n+  private def getWhereClause(part: JDBCPartition): String = {\n+    if (part.whereClause != null && filterWhereClause.length > 0) {\n+      filterWhereClause + \" AND \" + part.whereClause\n+    } else if (part.whereClause != null) {\n+      \"WHERE \" + part.whereClause\n+    } else {\n+      filterWhereClause\n+    }\n+  }\n+\n+  // Each JDBC-to-Catalyst conversion corresponds to a tag defined here so that\n+  // we don't have to potentially poke around in the Metadata once for every\n+  // row.  \n+  // Is there a better way to do this?  I'd rather be using a type that\n+  // contains only the tags I define.\n+  abstract class JDBCConversion",
    "line": 271
  }, {
    "author": {
      "login": "tmyklebu"
    },
    "body": "Hmm.  I switched from integer tags to these Conversion case classes and didn't see a performance drop.  (One could argue that I could have looked harder.)  If that's really what this code compiles to, I may as well make the change you suggest and benchmark it.\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-02T02:12:41Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")\n+  }\n+\n+  /**\n+   * Prune all but the specified columns from the specified Catalyst schema.\n+   *\n+   * @param schema - The Catalyst schema of the master table\n+   * @param columns - The list of desired columns\n+   *\n+   * @return A Catalyst schema corresponding to columns in the given order.\n+   */\n+  private def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields map { x => x.metadata.getString(\"name\") -> x }: _*)\n+    new StructType(columns map { name => fieldMap(name) })\n+  }\n+\n+  /**\n+   * Given a driver string and an url, return a function that loads the\n+   * specified driver string then returns a connection to the JDBC url.\n+   * getConnector is run on the driver code, while the function it returns\n+   * is run on the executor.\n+   *\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   *\n+   * @return A function that loads the driver and connects to the url.\n+   */\n+  def getConnector(driver: String, url: String): () => Connection = {\n+    () => {\n+      try {\n+        if (driver != null) Class.forName(driver)\n+      } catch {\n+        case e: ClassNotFoundException => {\n+          logWarning(s\"Couldn't find class $driver\", e);\n+        }\n+      }\n+      DriverManager.getConnection(url)\n+    }\n+  }\n+  /**\n+   * Build and return JDBCRDD from the given information.\n+   *\n+   * @param sc - Your SparkContext.\n+   * @param schema - The Catalyst schema of the underlying database table.\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   * @param fqTable - The fully-qualified table name (or paren'd SQL query) to use.\n+   * @param requiredColumns - The names of the columns to SELECT.\n+   * @param filters - The filters to include in all WHERE clauses.\n+   * @param parts - An array of JDBCPartitions specifying partition ids and\n+   *    per-partition WHERE clauses.\n+   *\n+   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\".\n+   */\n+  def scanTable(sc: SparkContext,\n+                schema: StructType,\n+                driver: String,\n+                url: String,\n+                fqTable: String,\n+                requiredColumns: Array[String],\n+                filters: Array[Filter],\n+                parts: Array[Partition]): RDD[Row] = {\n+    val prunedSchema = pruneSchema(schema, requiredColumns)\n+\n+    return new JDBCRDD(sc,\n+        getConnector(driver, url),\n+        prunedSchema,\n+        fqTable,\n+        requiredColumns,\n+        filters,\n+        parts)\n+  }\n+}\n+\n+/**\n+ * An RDD representing a table in a database accessed via JDBC.  Both the\n+ * driver code and the workers must be able to access the database; the driver\n+ * needs to fetch the schema while the workers need to fetch the data.\n+ */\n+private[sql] class JDBCRDD(\n+    sc: SparkContext,\n+    getConnection: () => Connection,\n+    schema: StructType,\n+    fqTable: String,\n+    columns: Array[String],\n+    filters: Array[Filter],\n+    partitions: Array[Partition])\n+  extends RDD[Row](sc, Nil) {\n+\n+  /**\n+   * Retrieve the list of partitions corresponding to this RDD.\n+   */\n+  override def getPartitions: Array[Partition] = partitions\n+\n+  /**\n+   * `columns`, but as a String suitable for injection into a SQL query.\n+   */\n+  private val columnList: String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.length == 0) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+   * Turns a single Filter into a String representing a SQL expression.\n+   * Returns null for an unhandled filter.\n+   */\n+  private def compileFilter(f: Filter): String = f match {\n+    case EqualTo(attr, value) => s\"$attr = $value\"\n+    case LessThan(attr, value) => s\"$attr < $value\"\n+    case GreaterThan(attr, value) => s\"$attr > $value\"\n+    case LessThanOrEqual(attr, value) => s\"$attr <= $value\"\n+    case GreaterThanOrEqual(attr, value) => s\"$attr >= $value\"\n+    case _ => null\n+  }\n+\n+  /**\n+   * `filters`, but as a WHERE clause suitable for injection into a SQL query.\n+   */\n+  private val filterWhereClause: String = {\n+    val filterStrings = filters map compileFilter filter (_ != null)\n+    if (filterStrings.size > 0) {\n+      val sb = new StringBuilder(\"WHERE \")\n+      filterStrings.foreach(x => sb.append(x).append(\" AND \"))\n+      sb.substring(0, sb.length - 5)\n+    } else \"\"\n+  }\n+\n+  /**\n+   * A WHERE clause representing both `filters`, if any, and the current partition.\n+   */\n+  private def getWhereClause(part: JDBCPartition): String = {\n+    if (part.whereClause != null && filterWhereClause.length > 0) {\n+      filterWhereClause + \" AND \" + part.whereClause\n+    } else if (part.whereClause != null) {\n+      \"WHERE \" + part.whereClause\n+    } else {\n+      filterWhereClause\n+    }\n+  }\n+\n+  // Each JDBC-to-Catalyst conversion corresponds to a tag defined here so that\n+  // we don't have to potentially poke around in the Metadata once for every\n+  // row.  \n+  // Is there a better way to do this?  I'd rather be using a type that\n+  // contains only the tags I define.\n+  abstract class JDBCConversion",
    "line": 271
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "btw do we care as much about performance here? seems like jdbc itself can be very slow anyway ....\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-02T02:16:20Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)\n+      } finally {\n+        rs.close()\n+      }\n+    } finally {\n+      conn.close()\n+    }\n+\n+    throw new RuntimeException(\"This line is unreachable.\")\n+  }\n+\n+  /**\n+   * Prune all but the specified columns from the specified Catalyst schema.\n+   *\n+   * @param schema - The Catalyst schema of the master table\n+   * @param columns - The list of desired columns\n+   *\n+   * @return A Catalyst schema corresponding to columns in the given order.\n+   */\n+  private def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields map { x => x.metadata.getString(\"name\") -> x }: _*)\n+    new StructType(columns map { name => fieldMap(name) })\n+  }\n+\n+  /**\n+   * Given a driver string and an url, return a function that loads the\n+   * specified driver string then returns a connection to the JDBC url.\n+   * getConnector is run on the driver code, while the function it returns\n+   * is run on the executor.\n+   *\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   *\n+   * @return A function that loads the driver and connects to the url.\n+   */\n+  def getConnector(driver: String, url: String): () => Connection = {\n+    () => {\n+      try {\n+        if (driver != null) Class.forName(driver)\n+      } catch {\n+        case e: ClassNotFoundException => {\n+          logWarning(s\"Couldn't find class $driver\", e);\n+        }\n+      }\n+      DriverManager.getConnection(url)\n+    }\n+  }\n+  /**\n+   * Build and return JDBCRDD from the given information.\n+   *\n+   * @param sc - Your SparkContext.\n+   * @param schema - The Catalyst schema of the underlying database table.\n+   * @param driver - The class name of the JDBC driver for the given url.\n+   * @param url - The JDBC url to connect to.\n+   * @param fqTable - The fully-qualified table name (or paren'd SQL query) to use.\n+   * @param requiredColumns - The names of the columns to SELECT.\n+   * @param filters - The filters to include in all WHERE clauses.\n+   * @param parts - An array of JDBCPartitions specifying partition ids and\n+   *    per-partition WHERE clauses.\n+   *\n+   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\".\n+   */\n+  def scanTable(sc: SparkContext,\n+                schema: StructType,\n+                driver: String,\n+                url: String,\n+                fqTable: String,\n+                requiredColumns: Array[String],\n+                filters: Array[Filter],\n+                parts: Array[Partition]): RDD[Row] = {\n+    val prunedSchema = pruneSchema(schema, requiredColumns)\n+\n+    return new JDBCRDD(sc,\n+        getConnector(driver, url),\n+        prunedSchema,\n+        fqTable,\n+        requiredColumns,\n+        filters,\n+        parts)\n+  }\n+}\n+\n+/**\n+ * An RDD representing a table in a database accessed via JDBC.  Both the\n+ * driver code and the workers must be able to access the database; the driver\n+ * needs to fetch the schema while the workers need to fetch the data.\n+ */\n+private[sql] class JDBCRDD(\n+    sc: SparkContext,\n+    getConnection: () => Connection,\n+    schema: StructType,\n+    fqTable: String,\n+    columns: Array[String],\n+    filters: Array[Filter],\n+    partitions: Array[Partition])\n+  extends RDD[Row](sc, Nil) {\n+\n+  /**\n+   * Retrieve the list of partitions corresponding to this RDD.\n+   */\n+  override def getPartitions: Array[Partition] = partitions\n+\n+  /**\n+   * `columns`, but as a String suitable for injection into a SQL query.\n+   */\n+  private val columnList: String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.length == 0) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+   * Turns a single Filter into a String representing a SQL expression.\n+   * Returns null for an unhandled filter.\n+   */\n+  private def compileFilter(f: Filter): String = f match {\n+    case EqualTo(attr, value) => s\"$attr = $value\"\n+    case LessThan(attr, value) => s\"$attr < $value\"\n+    case GreaterThan(attr, value) => s\"$attr > $value\"\n+    case LessThanOrEqual(attr, value) => s\"$attr <= $value\"\n+    case GreaterThanOrEqual(attr, value) => s\"$attr >= $value\"\n+    case _ => null\n+  }\n+\n+  /**\n+   * `filters`, but as a WHERE clause suitable for injection into a SQL query.\n+   */\n+  private val filterWhereClause: String = {\n+    val filterStrings = filters map compileFilter filter (_ != null)\n+    if (filterStrings.size > 0) {\n+      val sb = new StringBuilder(\"WHERE \")\n+      filterStrings.foreach(x => sb.append(x).append(\" AND \"))\n+      sb.substring(0, sb.length - 5)\n+    } else \"\"\n+  }\n+\n+  /**\n+   * A WHERE clause representing both `filters`, if any, and the current partition.\n+   */\n+  private def getWhereClause(part: JDBCPartition): String = {\n+    if (part.whereClause != null && filterWhereClause.length > 0) {\n+      filterWhereClause + \" AND \" + part.whereClause\n+    } else if (part.whereClause != null) {\n+      \"WHERE \" + part.whereClause\n+    } else {\n+      filterWhereClause\n+    }\n+  }\n+\n+  // Each JDBC-to-Catalyst conversion corresponds to a tag defined here so that\n+  // we don't have to potentially poke around in the Metadata once for every\n+  // row.  \n+  // Is there a better way to do this?  I'd rather be using a type that\n+  // contains only the tags I define.\n+  abstract class JDBCConversion",
    "line": 271
  }],
  "prId": 4261
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Also, you don't need `new` since `StructType` is a case class.\n",
    "commit": "cf167cea9457e933b1b8ed5f0eff708e6535ef99",
    "createdAt": "2015-02-03T03:39:54Z",
    "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.{Connection, DatabaseMetaData, DriverManager, ResultSet, ResultSetMetaData, SQLException}\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion\n+import org.apache.spark.sql.catalyst.expressions.{Row, SpecificMutableRow}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.sources._\n+\n+private[sql] object JDBCRDD extends Logging {\n+  /**\n+   * Maps a JDBC type to a Catalyst type.  This function is called only when\n+   * the DriverQuirks class corresponding to your database driver returns null.\n+   *\n+   * @param sqlType - A field of java.sql.Types\n+   * @return The Catalyst type corresponding to sqlType.\n+   */\n+  private def getCatalystType(sqlType: Int): DataType = {\n+    val answer = sqlType match {\n+      case java.sql.Types.ARRAY         => null\n+      case java.sql.Types.BIGINT        => LongType\n+      case java.sql.Types.BINARY        => BinaryType\n+      case java.sql.Types.BIT           => BooleanType // Per JDBC; Quirks handles quirky drivers.\n+      case java.sql.Types.BLOB          => BinaryType\n+      case java.sql.Types.BOOLEAN       => BooleanType\n+      case java.sql.Types.CHAR          => StringType\n+      case java.sql.Types.CLOB          => StringType\n+      case java.sql.Types.DATALINK      => null\n+      case java.sql.Types.DATE          => DateType\n+      case java.sql.Types.DECIMAL       => DecimalType.Unlimited\n+      case java.sql.Types.DISTINCT      => null\n+      case java.sql.Types.DOUBLE        => DoubleType\n+      case java.sql.Types.FLOAT         => FloatType\n+      case java.sql.Types.INTEGER       => IntegerType\n+      case java.sql.Types.JAVA_OBJECT   => null\n+      case java.sql.Types.LONGNVARCHAR  => StringType\n+      case java.sql.Types.LONGVARBINARY => BinaryType\n+      case java.sql.Types.LONGVARCHAR   => StringType\n+      case java.sql.Types.NCHAR         => StringType\n+      case java.sql.Types.NCLOB         => StringType\n+      case java.sql.Types.NULL          => null\n+      case java.sql.Types.NUMERIC       => DecimalType.Unlimited\n+      case java.sql.Types.OTHER         => null\n+      case java.sql.Types.REAL          => DoubleType\n+      case java.sql.Types.REF           => StringType\n+      case java.sql.Types.ROWID         => LongType\n+      case java.sql.Types.SMALLINT      => IntegerType\n+      case java.sql.Types.SQLXML        => StringType\n+      case java.sql.Types.STRUCT        => StringType\n+      case java.sql.Types.TIME          => TimestampType\n+      case java.sql.Types.TIMESTAMP     => TimestampType\n+      case java.sql.Types.TINYINT       => IntegerType\n+      case java.sql.Types.VARBINARY     => BinaryType\n+      case java.sql.Types.VARCHAR       => StringType\n+      case _ => null\n+    }\n+\n+    if (answer == null) throw new SQLException(\"Unsupported type \" + sqlType)\n+    answer\n+  }\n+\n+  /**\n+   * Takes a (schema, table) specification and returns the table's Catalyst\n+   * schema.\n+   *\n+   * @param url - The JDBC url to fetch information from.\n+   * @param table - The table name of the desired table.  This may also be a\n+   *   SQL query wrapped in parentheses.\n+   *\n+   * @return A StructType giving the table's Catalyst schema.\n+   * @throws SQLException if the table specification is garbage.\n+   * @throws SQLException if the table contains an unsupported type.\n+   */\n+  def resolveTable(url: String, table: String): StructType = {\n+    val quirks = DriverQuirks.get(url)\n+    val conn: Connection = DriverManager.getConnection(url)\n+    try {\n+      val rs = conn.prepareStatement(s\"SELECT * FROM $table WHERE 1=0\").executeQuery()\n+      try {\n+        val rsmd = rs.getMetaData\n+        val ncols = rsmd.getColumnCount\n+        var fields = new Array[StructField](ncols);\n+        var i = 0\n+        while (i < ncols) {\n+          val columnName = rsmd.getColumnName(i + 1)\n+          val dataType = rsmd.getColumnType(i + 1)\n+          val typeName = rsmd.getColumnTypeName(i + 1)\n+          val fieldSize = rsmd.getPrecision(i + 1)\n+          val nullable = rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls\n+          val metadata = new MetadataBuilder().putString(\"name\", columnName)\n+          var columnType = quirks.getCatalystType(dataType, typeName, fieldSize, metadata)\n+          if (columnType == null) columnType = getCatalystType(dataType)\n+          fields(i) = StructField(columnName, columnType, nullable, metadata.build())\n+          i = i + 1\n+        }\n+        return new StructType(fields)",
    "line": 117
  }],
  "prId": 4261
}]