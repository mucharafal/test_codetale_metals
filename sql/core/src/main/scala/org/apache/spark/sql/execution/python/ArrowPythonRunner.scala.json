[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Is `loadNextBatch` a blocking action and returning false only no batch anymore? But looks like we call `read()` again if no batch is loaded, so `loadNextBatch` is an async action and can return false if the batch is not ready? If it takes too long for the batch to be ready, can recursive `read` be an issue?",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T04:02:08Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          if (root != null) {\n+            root.close()\n+          }\n+          allocator.close()\n+        }\n+      }\n+\n+      override def hasNext: Boolean = super.hasNext || {\n+        if (root != null) {\n+          root.close()\n+        }\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(schema, vectors, root.getRowCount)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              read()"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "I also think whether recursive `read` may cause `StackOverflowException`. \r\nCan we implement this as a loop? Or, can we ensure it does not cause `StackOverflowException`. exception?",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T05:24:32Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          if (root != null) {\n+            root.close()\n+          }\n+          allocator.close()\n+        }\n+      }\n+\n+      override def hasNext: Boolean = super.hasNext || {\n+        if (root != null) {\n+          root.close()\n+        }\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(schema, vectors, root.getRowCount)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              read()"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "I believe `loadNextBatch` is a blocking action. Here's a single-line comment from a source code of the method:\r\n\r\n> Returns true if a batch was read, false on EOS\r\n\r\ncc @BryanCutler Could you confirm this?",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T06:01:38Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          if (root != null) {\n+            root.close()\n+          }\n+          allocator.close()\n+        }\n+      }\n+\n+      override def hasNext: Boolean = super.hasNext || {\n+        if (root != null) {\n+          root.close()\n+        }\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(schema, vectors, root.getRowCount)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              read()"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Oh, it may not incurring `StackOverflowException` as `batchLoaded` is false now and we won't enter the `if` at 153.",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T06:47:55Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          if (root != null) {\n+            root.close()\n+          }\n+          allocator.close()\n+        }\n+      }\n+\n+      override def hasNext: Boolean = super.hasNext || {\n+        if (root != null) {\n+          root.close()\n+        }\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(schema, vectors, root.getRowCount)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              read()"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "@kiszk I might miss something, but I don't think `StackOverflowException` happens because of the protocol to communicate with Python worker.",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T06:50:03Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          if (root != null) {\n+            root.close()\n+          }\n+          allocator.close()\n+        }\n+      }\n+\n+      override def hasNext: Boolean = super.hasNext || {\n+        if (root != null) {\n+          root.close()\n+        }\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(schema, vectors, root.getRowCount)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              read()"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "@ueshin Do you mind to add a comment like:\r\n\r\n    } else {\r\n      // Reach end of stream. Call `read()` again to read control data.\r\n      read()\r\n    }",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T06:54:22Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          if (root != null) {\n+            root.close()\n+          }\n+          allocator.close()\n+        }\n+      }\n+\n+      override def hasNext: Boolean = super.hasNext || {\n+        if (root != null) {\n+          root.close()\n+        }\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(schema, vectors, root.getRowCount)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              read()"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "@viirya Sure, I'll add the comment. Thanks!",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T06:58:02Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }\n+      }\n+    }\n+  }\n+\n+  protected override def newReaderIterator(\n+      stream: DataInputStream,\n+      writerThread: WriterThread,\n+      startTime: Long,\n+      env: SparkEnv,\n+      worker: Socket,\n+      released: AtomicBoolean,\n+      context: TaskContext): Iterator[ColumnarBatch] = {\n+    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {\n+\n+      private val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+        s\"stdin reader for $pythonExec\", 0, Long.MaxValue)\n+\n+      private var reader: ArrowStreamReader = _\n+      private var root: VectorSchemaRoot = _\n+      private var schema: StructType = _\n+      private var vectors: Array[ColumnVector] = _\n+\n+      private var closed = false\n+\n+      context.addTaskCompletionListener { _ =>\n+        // todo: we need something like `read.end()`, which release all the resources, but leave\n+        // the input stream open. `reader.close` will close the socket and we can't reuse worker.\n+        // So here we simply not close the reader, which is problematic.\n+        if (!closed) {\n+          if (root != null) {\n+            root.close()\n+          }\n+          allocator.close()\n+        }\n+      }\n+\n+      override def hasNext: Boolean = super.hasNext || {\n+        if (root != null) {\n+          root.close()\n+        }\n+        allocator.close()\n+        closed = true\n+        false\n+      }\n+\n+      private var batchLoaded = true\n+\n+      protected override def read(): ColumnarBatch = {\n+        if (writerThread.exception.isDefined) {\n+          throw writerThread.exception.get\n+        }\n+        try {\n+          if (reader != null && batchLoaded) {\n+            batchLoaded = reader.loadNextBatch()\n+            if (batchLoaded) {\n+              val batch = new ColumnarBatch(schema, vectors, root.getRowCount)\n+              batch.setNumRows(root.getRowCount)\n+              batch\n+            } else {\n+              read()"
  }],
  "prId": 19349
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "I think we need to write out `END_OF_DATA_SECTION` after all data are written out?",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T04:35:50Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }",
    "line": 100
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "nvm. `ArrowStreamPandasSerializer` is not a `FramedSerializer`.",
    "commit": "7cd78b2aa44e830e0b8b466d1ca80e54359a3c3c",
    "createdAt": "2017-09-27T04:49:39Z",
    "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+import java.net._\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.arrow.vector.VectorSchemaRoot\n+import org.apache.arrow.vector.stream.{ArrowStreamReader, ArrowStreamWriter}\n+\n+import org.apache.spark._\n+import org.apache.spark.api.python._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.arrow.{ArrowUtils, ArrowWriter}\n+import org.apache.spark.sql.execution.vectorized.{ArrowColumnVector, ColumnarBatch, ColumnVector}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Similar to `PythonUDFRunner`, but exchange data with Python worker via Arrow stream.\n+ */\n+class ArrowPythonRunner(\n+    funcs: Seq[ChainedPythonFunctions],\n+    batchSize: Int,\n+    bufferSize: Int,\n+    reuseWorker: Boolean,\n+    evalType: Int,\n+    argOffsets: Array[Array[Int]],\n+    schema: StructType)\n+  extends BasePythonRunner[InternalRow, ColumnarBatch](\n+    funcs, bufferSize, reuseWorker, evalType, argOffsets) {\n+\n+  protected override def newWriterThread(\n+      env: SparkEnv,\n+      worker: Socket,\n+      inputIterator: Iterator[InternalRow],\n+      partitionIndex: Int,\n+      context: TaskContext): WriterThread = {\n+    new WriterThread(env, worker, inputIterator, partitionIndex, context) {\n+\n+      override def writeCommand(dataOut: DataOutputStream): Unit = {\n+        dataOut.writeInt(funcs.length)\n+        funcs.zip(argOffsets).foreach { case (chained, offsets) =>\n+          dataOut.writeInt(offsets.length)\n+          offsets.foreach { offset =>\n+            dataOut.writeInt(offset)\n+          }\n+          dataOut.writeInt(chained.funcs.length)\n+          chained.funcs.foreach { f =>\n+            dataOut.writeInt(f.command.length)\n+            dataOut.write(f.command)\n+          }\n+        }\n+      }\n+\n+      override def writeIteratorToStream(dataOut: DataOutputStream): Unit = {\n+        val arrowSchema = ArrowUtils.toArrowSchema(schema)\n+        val allocator = ArrowUtils.rootAllocator.newChildAllocator(\n+          s\"stdout writer for $pythonExec\", 0, Long.MaxValue)\n+\n+        val root = VectorSchemaRoot.create(arrowSchema, allocator)\n+        val arrowWriter = ArrowWriter.create(root)\n+\n+        var closed = false\n+\n+        context.addTaskCompletionListener { _ =>\n+          if (!closed) {\n+            root.close()\n+            allocator.close()\n+          }\n+        }\n+\n+        val writer = new ArrowStreamWriter(root, null, dataOut)\n+        writer.start()\n+\n+        Utils.tryWithSafeFinally {\n+          while (inputIterator.hasNext) {\n+            var rowCount = 0\n+            while (inputIterator.hasNext && (batchSize <= 0 || rowCount < batchSize)) {\n+              val row = inputIterator.next()\n+              arrowWriter.write(row)\n+              rowCount += 1\n+            }\n+            arrowWriter.finish()\n+            writer.writeBatch()\n+            arrowWriter.reset()\n+          }\n+        } {\n+          writer.end()\n+          root.close()\n+          allocator.close()\n+          closed = true\n+        }",
    "line": 100
  }],
  "prId": 19349
}]