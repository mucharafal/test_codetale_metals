[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "@carsonwang maybe we should also set spark local properties here.",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-01-31T06:05:22Z",
    "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, ExecutionContextExecutorService}\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{CollapseCodegenStages, SparkPlan}\n+import org.apache.spark.sql.execution.adaptive.rule.{AssertChildStagesMaterialized, ReduceNumShufflePartitions}\n+import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, Exchange, ShuffleExchangeExec}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{EventLoop, ThreadUtils}\n+\n+/**\n+ * This class dynamically creates [[QueryStage]] bottom-up, optimize the query plan of query stages\n+ * and materialize them. It creates as many query stages as possible at the same time, and\n+ * materialize a query stage when all its child stages are materialized.\n+ *\n+ * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, and\n+ * all the child query stages of this exchange node are materialized, we try to create a new query\n+ * stage for this exchange node.\n+ *\n+ * To create a new query stage, we first optimize the sub-tree of the exchange. After optimization,\n+ * we check the output partitioning of the optimized sub-tree, and see if the exchange node is still\n+ * necessary.\n+ *\n+ * If the exchange node becomes unnecessary, remove it and give up this query stage creation, and\n+ * continue to traverse the query plan tree until we hit the next exchange node.\n+ *\n+ * If the exchange node is still needed, create the query stage and optimize its sub-tree again.\n+ * It's necessary to have both the pre-creation optimization and post-creation optimization, because\n+ * these 2 optimization have different assumptions. For pre-creation optimization, the shuffle node\n+ * may be removed later on and the current sub-tree may be only a part of a query stage, so we don't\n+ * have the big picture of the query stage yet. For post-creation optimization, the query stage is\n+ * created and we have the big picture of the query stage.\n+ *\n+ * After the query stage is optimized, we materialize it asynchronously, and continue to traverse\n+ * the query plan tree to create more query stages.\n+ *\n+ * When a query stage completes materialization, we trigger the process of query stages creation and\n+ * traverse the query plan tree again.\n+ */\n+class QueryStageCreator(\n+    initialPlan: SparkPlan,\n+    session: SparkSession,\n+    callback: QueryStageTriggerCallback)\n+  extends EventLoop[QueryStageCreatorEvent](\"QueryStageCreator\") {\n+\n+  private def conf = session.sessionState.conf\n+\n+  private val readyStages = mutable.HashSet.empty[Int]\n+\n+  private var currentStageId = 0\n+\n+  private val stageCache = mutable.HashMap.empty[StructType, mutable.Buffer[(Exchange, QueryStage)]]\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan before the stage is\n+  // created. Note that we may end up not creating the query stage, so the rules here should not\n+  // assume the given sub-plan-tree is the entire query plan of the query stage. For example, if a\n+  // rule want to collect all the child query stages, it should not be put here.\n+  private val preStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    AssertChildStagesMaterialized\n+  )\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan after the stage is\n+  // created. Note that once the stage is created, we will not remove it anymore. If a rule changes\n+  // the output partitioning of the sub-plan-tree, which may help to remove the exchange node, it's\n+  // better to put it in `preStageCreationOptimizerRules`, so that we may create less query stages.\n+  private val postStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    ReduceNumShufflePartitions(conf),\n+    CollapseCodegenStages(conf))\n+\n+  private var currentPlan = initialPlan\n+\n+  private implicit def executionContext: ExecutionContextExecutorService = {\n+    QueryStageCreator.executionContext\n+  }\n+\n+  override protected def onReceive(event: QueryStageCreatorEvent): Unit = event match {\n+    case StartCreation =>\n+      // set active session for the event loop thread."
  }, {
    "author": {
      "login": "carsonwang"
    },
    "body": "yes, done",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-02-01T09:37:29Z",
    "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, ExecutionContextExecutorService}\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{CollapseCodegenStages, SparkPlan}\n+import org.apache.spark.sql.execution.adaptive.rule.{AssertChildStagesMaterialized, ReduceNumShufflePartitions}\n+import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, Exchange, ShuffleExchangeExec}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{EventLoop, ThreadUtils}\n+\n+/**\n+ * This class dynamically creates [[QueryStage]] bottom-up, optimize the query plan of query stages\n+ * and materialize them. It creates as many query stages as possible at the same time, and\n+ * materialize a query stage when all its child stages are materialized.\n+ *\n+ * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, and\n+ * all the child query stages of this exchange node are materialized, we try to create a new query\n+ * stage for this exchange node.\n+ *\n+ * To create a new query stage, we first optimize the sub-tree of the exchange. After optimization,\n+ * we check the output partitioning of the optimized sub-tree, and see if the exchange node is still\n+ * necessary.\n+ *\n+ * If the exchange node becomes unnecessary, remove it and give up this query stage creation, and\n+ * continue to traverse the query plan tree until we hit the next exchange node.\n+ *\n+ * If the exchange node is still needed, create the query stage and optimize its sub-tree again.\n+ * It's necessary to have both the pre-creation optimization and post-creation optimization, because\n+ * these 2 optimization have different assumptions. For pre-creation optimization, the shuffle node\n+ * may be removed later on and the current sub-tree may be only a part of a query stage, so we don't\n+ * have the big picture of the query stage yet. For post-creation optimization, the query stage is\n+ * created and we have the big picture of the query stage.\n+ *\n+ * After the query stage is optimized, we materialize it asynchronously, and continue to traverse\n+ * the query plan tree to create more query stages.\n+ *\n+ * When a query stage completes materialization, we trigger the process of query stages creation and\n+ * traverse the query plan tree again.\n+ */\n+class QueryStageCreator(\n+    initialPlan: SparkPlan,\n+    session: SparkSession,\n+    callback: QueryStageTriggerCallback)\n+  extends EventLoop[QueryStageCreatorEvent](\"QueryStageCreator\") {\n+\n+  private def conf = session.sessionState.conf\n+\n+  private val readyStages = mutable.HashSet.empty[Int]\n+\n+  private var currentStageId = 0\n+\n+  private val stageCache = mutable.HashMap.empty[StructType, mutable.Buffer[(Exchange, QueryStage)]]\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan before the stage is\n+  // created. Note that we may end up not creating the query stage, so the rules here should not\n+  // assume the given sub-plan-tree is the entire query plan of the query stage. For example, if a\n+  // rule want to collect all the child query stages, it should not be put here.\n+  private val preStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    AssertChildStagesMaterialized\n+  )\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan after the stage is\n+  // created. Note that once the stage is created, we will not remove it anymore. If a rule changes\n+  // the output partitioning of the sub-plan-tree, which may help to remove the exchange node, it's\n+  // better to put it in `preStageCreationOptimizerRules`, so that we may create less query stages.\n+  private val postStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    ReduceNumShufflePartitions(conf),\n+    CollapseCodegenStages(conf))\n+\n+  private var currentPlan = initialPlan\n+\n+  private implicit def executionContext: ExecutionContextExecutorService = {\n+    QueryStageCreator.executionContext\n+  }\n+\n+  override protected def onReceive(event: QueryStageCreatorEvent): Unit = event match {\n+    case StartCreation =>\n+      // set active session for the event loop thread."
  }],
  "prId": 20303
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "QueryStageCreatorCallback",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-01-31T06:06:54Z",
    "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, ExecutionContextExecutorService}\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{CollapseCodegenStages, SparkPlan}\n+import org.apache.spark.sql.execution.adaptive.rule.{AssertChildStagesMaterialized, ReduceNumShufflePartitions}\n+import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, Exchange, ShuffleExchangeExec}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{EventLoop, ThreadUtils}\n+\n+/**\n+ * This class dynamically creates [[QueryStage]] bottom-up, optimize the query plan of query stages\n+ * and materialize them. It creates as many query stages as possible at the same time, and\n+ * materialize a query stage when all its child stages are materialized.\n+ *\n+ * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, and\n+ * all the child query stages of this exchange node are materialized, we try to create a new query\n+ * stage for this exchange node.\n+ *\n+ * To create a new query stage, we first optimize the sub-tree of the exchange. After optimization,\n+ * we check the output partitioning of the optimized sub-tree, and see if the exchange node is still\n+ * necessary.\n+ *\n+ * If the exchange node becomes unnecessary, remove it and give up this query stage creation, and\n+ * continue to traverse the query plan tree until we hit the next exchange node.\n+ *\n+ * If the exchange node is still needed, create the query stage and optimize its sub-tree again.\n+ * It's necessary to have both the pre-creation optimization and post-creation optimization, because\n+ * these 2 optimization have different assumptions. For pre-creation optimization, the shuffle node\n+ * may be removed later on and the current sub-tree may be only a part of a query stage, so we don't\n+ * have the big picture of the query stage yet. For post-creation optimization, the query stage is\n+ * created and we have the big picture of the query stage.\n+ *\n+ * After the query stage is optimized, we materialize it asynchronously, and continue to traverse\n+ * the query plan tree to create more query stages.\n+ *\n+ * When a query stage completes materialization, we trigger the process of query stages creation and\n+ * traverse the query plan tree again.\n+ */\n+class QueryStageCreator(\n+    initialPlan: SparkPlan,\n+    session: SparkSession,\n+    callback: QueryStageTriggerCallback)\n+  extends EventLoop[QueryStageCreatorEvent](\"QueryStageCreator\") {\n+\n+  private def conf = session.sessionState.conf\n+\n+  private val readyStages = mutable.HashSet.empty[Int]\n+\n+  private var currentStageId = 0\n+\n+  private val stageCache = mutable.HashMap.empty[StructType, mutable.Buffer[(Exchange, QueryStage)]]\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan before the stage is\n+  // created. Note that we may end up not creating the query stage, so the rules here should not\n+  // assume the given sub-plan-tree is the entire query plan of the query stage. For example, if a\n+  // rule want to collect all the child query stages, it should not be put here.\n+  private val preStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    AssertChildStagesMaterialized\n+  )\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan after the stage is\n+  // created. Note that once the stage is created, we will not remove it anymore. If a rule changes\n+  // the output partitioning of the sub-plan-tree, which may help to remove the exchange node, it's\n+  // better to put it in `preStageCreationOptimizerRules`, so that we may create less query stages.\n+  private val postStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    ReduceNumShufflePartitions(conf),\n+    CollapseCodegenStages(conf))\n+\n+  private var currentPlan = initialPlan\n+\n+  private implicit def executionContext: ExecutionContextExecutorService = {\n+    QueryStageCreator.executionContext\n+  }\n+\n+  override protected def onReceive(event: QueryStageCreatorEvent): Unit = event match {\n+    case StartCreation =>\n+      // set active session for the event loop thread.\n+      SparkSession.setActiveSession(session)\n+      currentPlan = createQueryStages(initialPlan)\n+\n+    case MaterializeStage(stage) =>\n+      stage.materialize().onComplete { res =>\n+        if (res.isSuccess) {\n+          post(StageReady(stage))\n+        } else {\n+          callback.onStageMaterializingFailed(stage, res.failed.get)\n+          stop()\n+        }\n+      }\n+\n+    case StageReady(stage) =>\n+      if (stage.isInstanceOf[ResultQueryStage]) {\n+        callback.onPlanUpdate(stage)\n+        stop()\n+      } else {\n+        readyStages += stage.id\n+        currentPlan = createQueryStages(currentPlan)\n+      }\n+  }\n+\n+  override protected def onStart(): Unit = {\n+    post(StartCreation)\n+  }\n+\n+  private def preStageCreationOptimize(plan: SparkPlan): SparkPlan = {\n+    preStageCreationOptimizerRules.foldLeft(plan) {\n+      case (current, rule) => rule(current)\n+    }\n+  }\n+\n+  private def postStageCreationOptimize(plan: SparkPlan): SparkPlan = {\n+    postStageCreationOptimizerRules.foldLeft(plan) {\n+      case (current, rule) => rule(current)\n+    }\n+  }\n+\n+  /**\n+   * Traverse the query plan bottom-up, and creates query stages as many as possible.\n+   */\n+  private def createQueryStages(plan: SparkPlan): SparkPlan = {\n+    val result = createQueryStages0(plan)\n+    if (result.allChildStagesReady) {\n+      val finalPlan = postStageCreationOptimize(preStageCreationOptimize(result.newPlan))\n+      post(StageReady(ResultQueryStage(currentStageId, finalPlan)))\n+      finalPlan\n+    } else {\n+      callback.onPlanUpdate(result.newPlan)\n+      result.newPlan\n+    }\n+  }\n+\n+  /**\n+   * This method is called recursively to traverse the plan tree bottom-up. This method returns two\n+   * information: 1) the new plan after we insert query stages. 2) whether or not the child query\n+   * stages of the new plan are all ready.\n+   *\n+   * if the current plan is an exchange node, and all its child query stages are ready, we try to\n+   * create a new query stage.\n+   */\n+  private def createQueryStages0(plan: SparkPlan): CreateStageResult = plan match {\n+    case e: Exchange =>\n+      val similarStages = stageCache.getOrElseUpdate(e.schema, mutable.Buffer.empty)\n+      similarStages.find(_._1.sameResult(e)) match {\n+        case Some((_, existingStage)) if conf.exchangeReuseEnabled =>\n+          CreateStageResult(\n+            newPlan = ReusedQueryStage(existingStage, e.output),\n+            allChildStagesReady = readyStages.contains(existingStage.id))\n+\n+        case _ =>\n+          val result = createQueryStages0(e.child)\n+          // Try to create a query stage only when all the child query stages are ready.\n+          if (result.allChildStagesReady) {\n+            val optimizedPlan = preStageCreationOptimize(result.newPlan)\n+            e match {\n+              case s: ShuffleExchangeExec =>\n+                (s.desiredPartitioning, optimizedPlan.outputPartitioning) match {\n+                  case (desired: HashPartitioning, actual: HashPartitioning)\n+                      if desired.semanticEquals(actual) =>\n+                    // This shuffle exchange is unnecessary now, remove it. The reason maybe:\n+                    //   1. the child plan has changed its output partitioning after optimization,\n+                    //      and makes this exchange node unnecessary.\n+                    //   2. this exchange node is user specified, which turns out to be unnecessary.\n+                    CreateStageResult(newPlan = optimizedPlan, allChildStagesReady = true)\n+                  case _ =>\n+                    val queryStage = createQueryStage(s.copy(child = optimizedPlan))\n+                    similarStages.append(e -> queryStage)\n+                    // We've created a new stage, which is obviously not ready yet.\n+                    CreateStageResult(newPlan = queryStage, allChildStagesReady = false)\n+                }\n+\n+              case b: BroadcastExchangeExec =>\n+                val queryStage = createQueryStage(b.copy(child = optimizedPlan))\n+                similarStages.append(e -> queryStage)\n+                // We've created a new stage, which is obviously not ready yet.\n+                CreateStageResult(newPlan = queryStage, allChildStagesReady = false)\n+            }\n+          } else {\n+            CreateStageResult(\n+              newPlan = e.withNewChildren(Seq(result.newPlan)),\n+              allChildStagesReady = false)\n+          }\n+      }\n+\n+    case q: QueryStage =>\n+      CreateStageResult(newPlan = q, allChildStagesReady = readyStages.contains(q.id))\n+\n+    case _ =>\n+      if (plan.children.isEmpty) {\n+        CreateStageResult(newPlan = plan, allChildStagesReady = true)\n+      } else {\n+        val results = plan.children.map(createQueryStages0)\n+        CreateStageResult(\n+          newPlan = plan.withNewChildren(results.map(_.newPlan)),\n+          allChildStagesReady = results.forall(_.allChildStagesReady))\n+      }\n+  }\n+\n+  private def createQueryStage(e: Exchange): QueryStage = {\n+    val optimizedPlan = postStageCreationOptimize(e.child)\n+    val queryStage = e match {\n+      case s: ShuffleExchangeExec =>\n+        ShuffleQueryStage(currentStageId, s.copy(child = optimizedPlan))\n+      case b: BroadcastExchangeExec =>\n+        BroadcastQueryStage(currentStageId, b.copy(child = optimizedPlan))\n+    }\n+    currentStageId += 1\n+    post(MaterializeStage(queryStage))\n+    queryStage\n+  }\n+\n+  override protected def onError(e: Throwable): Unit = callback.onError(e)\n+}\n+\n+case class CreateStageResult(newPlan: SparkPlan, allChildStagesReady: Boolean)\n+\n+object QueryStageCreator {\n+  private val executionContext = ExecutionContext.fromExecutorService(\n+    ThreadUtils.newDaemonCachedThreadPool(\"QueryStageCreator\", 16))\n+}\n+\n+trait QueryStageTriggerCallback {"
  }, {
    "author": {
      "login": "carsonwang"
    },
    "body": "Done",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-02-01T09:37:39Z",
    "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.{ExecutionContext, ExecutionContextExecutorService}\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.{CollapseCodegenStages, SparkPlan}\n+import org.apache.spark.sql.execution.adaptive.rule.{AssertChildStagesMaterialized, ReduceNumShufflePartitions}\n+import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, Exchange, ShuffleExchangeExec}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.{EventLoop, ThreadUtils}\n+\n+/**\n+ * This class dynamically creates [[QueryStage]] bottom-up, optimize the query plan of query stages\n+ * and materialize them. It creates as many query stages as possible at the same time, and\n+ * materialize a query stage when all its child stages are materialized.\n+ *\n+ * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, and\n+ * all the child query stages of this exchange node are materialized, we try to create a new query\n+ * stage for this exchange node.\n+ *\n+ * To create a new query stage, we first optimize the sub-tree of the exchange. After optimization,\n+ * we check the output partitioning of the optimized sub-tree, and see if the exchange node is still\n+ * necessary.\n+ *\n+ * If the exchange node becomes unnecessary, remove it and give up this query stage creation, and\n+ * continue to traverse the query plan tree until we hit the next exchange node.\n+ *\n+ * If the exchange node is still needed, create the query stage and optimize its sub-tree again.\n+ * It's necessary to have both the pre-creation optimization and post-creation optimization, because\n+ * these 2 optimization have different assumptions. For pre-creation optimization, the shuffle node\n+ * may be removed later on and the current sub-tree may be only a part of a query stage, so we don't\n+ * have the big picture of the query stage yet. For post-creation optimization, the query stage is\n+ * created and we have the big picture of the query stage.\n+ *\n+ * After the query stage is optimized, we materialize it asynchronously, and continue to traverse\n+ * the query plan tree to create more query stages.\n+ *\n+ * When a query stage completes materialization, we trigger the process of query stages creation and\n+ * traverse the query plan tree again.\n+ */\n+class QueryStageCreator(\n+    initialPlan: SparkPlan,\n+    session: SparkSession,\n+    callback: QueryStageTriggerCallback)\n+  extends EventLoop[QueryStageCreatorEvent](\"QueryStageCreator\") {\n+\n+  private def conf = session.sessionState.conf\n+\n+  private val readyStages = mutable.HashSet.empty[Int]\n+\n+  private var currentStageId = 0\n+\n+  private val stageCache = mutable.HashMap.empty[StructType, mutable.Buffer[(Exchange, QueryStage)]]\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan before the stage is\n+  // created. Note that we may end up not creating the query stage, so the rules here should not\n+  // assume the given sub-plan-tree is the entire query plan of the query stage. For example, if a\n+  // rule want to collect all the child query stages, it should not be put here.\n+  private val preStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    AssertChildStagesMaterialized\n+  )\n+\n+  // The optimizer rules that will be applied to a sub-tree of the query plan after the stage is\n+  // created. Note that once the stage is created, we will not remove it anymore. If a rule changes\n+  // the output partitioning of the sub-plan-tree, which may help to remove the exchange node, it's\n+  // better to put it in `preStageCreationOptimizerRules`, so that we may create less query stages.\n+  private val postStageCreationOptimizerRules: Seq[Rule[SparkPlan]] = Seq(\n+    ReduceNumShufflePartitions(conf),\n+    CollapseCodegenStages(conf))\n+\n+  private var currentPlan = initialPlan\n+\n+  private implicit def executionContext: ExecutionContextExecutorService = {\n+    QueryStageCreator.executionContext\n+  }\n+\n+  override protected def onReceive(event: QueryStageCreatorEvent): Unit = event match {\n+    case StartCreation =>\n+      // set active session for the event loop thread.\n+      SparkSession.setActiveSession(session)\n+      currentPlan = createQueryStages(initialPlan)\n+\n+    case MaterializeStage(stage) =>\n+      stage.materialize().onComplete { res =>\n+        if (res.isSuccess) {\n+          post(StageReady(stage))\n+        } else {\n+          callback.onStageMaterializingFailed(stage, res.failed.get)\n+          stop()\n+        }\n+      }\n+\n+    case StageReady(stage) =>\n+      if (stage.isInstanceOf[ResultQueryStage]) {\n+        callback.onPlanUpdate(stage)\n+        stop()\n+      } else {\n+        readyStages += stage.id\n+        currentPlan = createQueryStages(currentPlan)\n+      }\n+  }\n+\n+  override protected def onStart(): Unit = {\n+    post(StartCreation)\n+  }\n+\n+  private def preStageCreationOptimize(plan: SparkPlan): SparkPlan = {\n+    preStageCreationOptimizerRules.foldLeft(plan) {\n+      case (current, rule) => rule(current)\n+    }\n+  }\n+\n+  private def postStageCreationOptimize(plan: SparkPlan): SparkPlan = {\n+    postStageCreationOptimizerRules.foldLeft(plan) {\n+      case (current, rule) => rule(current)\n+    }\n+  }\n+\n+  /**\n+   * Traverse the query plan bottom-up, and creates query stages as many as possible.\n+   */\n+  private def createQueryStages(plan: SparkPlan): SparkPlan = {\n+    val result = createQueryStages0(plan)\n+    if (result.allChildStagesReady) {\n+      val finalPlan = postStageCreationOptimize(preStageCreationOptimize(result.newPlan))\n+      post(StageReady(ResultQueryStage(currentStageId, finalPlan)))\n+      finalPlan\n+    } else {\n+      callback.onPlanUpdate(result.newPlan)\n+      result.newPlan\n+    }\n+  }\n+\n+  /**\n+   * This method is called recursively to traverse the plan tree bottom-up. This method returns two\n+   * information: 1) the new plan after we insert query stages. 2) whether or not the child query\n+   * stages of the new plan are all ready.\n+   *\n+   * if the current plan is an exchange node, and all its child query stages are ready, we try to\n+   * create a new query stage.\n+   */\n+  private def createQueryStages0(plan: SparkPlan): CreateStageResult = plan match {\n+    case e: Exchange =>\n+      val similarStages = stageCache.getOrElseUpdate(e.schema, mutable.Buffer.empty)\n+      similarStages.find(_._1.sameResult(e)) match {\n+        case Some((_, existingStage)) if conf.exchangeReuseEnabled =>\n+          CreateStageResult(\n+            newPlan = ReusedQueryStage(existingStage, e.output),\n+            allChildStagesReady = readyStages.contains(existingStage.id))\n+\n+        case _ =>\n+          val result = createQueryStages0(e.child)\n+          // Try to create a query stage only when all the child query stages are ready.\n+          if (result.allChildStagesReady) {\n+            val optimizedPlan = preStageCreationOptimize(result.newPlan)\n+            e match {\n+              case s: ShuffleExchangeExec =>\n+                (s.desiredPartitioning, optimizedPlan.outputPartitioning) match {\n+                  case (desired: HashPartitioning, actual: HashPartitioning)\n+                      if desired.semanticEquals(actual) =>\n+                    // This shuffle exchange is unnecessary now, remove it. The reason maybe:\n+                    //   1. the child plan has changed its output partitioning after optimization,\n+                    //      and makes this exchange node unnecessary.\n+                    //   2. this exchange node is user specified, which turns out to be unnecessary.\n+                    CreateStageResult(newPlan = optimizedPlan, allChildStagesReady = true)\n+                  case _ =>\n+                    val queryStage = createQueryStage(s.copy(child = optimizedPlan))\n+                    similarStages.append(e -> queryStage)\n+                    // We've created a new stage, which is obviously not ready yet.\n+                    CreateStageResult(newPlan = queryStage, allChildStagesReady = false)\n+                }\n+\n+              case b: BroadcastExchangeExec =>\n+                val queryStage = createQueryStage(b.copy(child = optimizedPlan))\n+                similarStages.append(e -> queryStage)\n+                // We've created a new stage, which is obviously not ready yet.\n+                CreateStageResult(newPlan = queryStage, allChildStagesReady = false)\n+            }\n+          } else {\n+            CreateStageResult(\n+              newPlan = e.withNewChildren(Seq(result.newPlan)),\n+              allChildStagesReady = false)\n+          }\n+      }\n+\n+    case q: QueryStage =>\n+      CreateStageResult(newPlan = q, allChildStagesReady = readyStages.contains(q.id))\n+\n+    case _ =>\n+      if (plan.children.isEmpty) {\n+        CreateStageResult(newPlan = plan, allChildStagesReady = true)\n+      } else {\n+        val results = plan.children.map(createQueryStages0)\n+        CreateStageResult(\n+          newPlan = plan.withNewChildren(results.map(_.newPlan)),\n+          allChildStagesReady = results.forall(_.allChildStagesReady))\n+      }\n+  }\n+\n+  private def createQueryStage(e: Exchange): QueryStage = {\n+    val optimizedPlan = postStageCreationOptimize(e.child)\n+    val queryStage = e match {\n+      case s: ShuffleExchangeExec =>\n+        ShuffleQueryStage(currentStageId, s.copy(child = optimizedPlan))\n+      case b: BroadcastExchangeExec =>\n+        BroadcastQueryStage(currentStageId, b.copy(child = optimizedPlan))\n+    }\n+    currentStageId += 1\n+    post(MaterializeStage(queryStage))\n+    queryStage\n+  }\n+\n+  override protected def onError(e: Throwable): Unit = callback.onError(e)\n+}\n+\n+case class CreateStageResult(newPlan: SparkPlan, allChildStagesReady: Boolean)\n+\n+object QueryStageCreator {\n+  private val executionContext = ExecutionContext.fromExecutorService(\n+    ThreadUtils.newDaemonCachedThreadPool(\"QueryStageCreator\", 16))\n+}\n+\n+trait QueryStageTriggerCallback {"
  }],
  "prId": 20303
}]