[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Do we have the case for commit to behave as no-op?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-01T07:08:36Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Only in transactional DB, we need to commit and close a transaction before closing the DB. So commit is no-op here but handles the transaction in OptimisticTransactionDbInstance. \r\n\r\nClose is used to close the DB. ",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T06:21:08Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Do we need to reimplement Scala `require`?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-02T13:02:38Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null\n+  }\n+\n+  override def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(txn != null, \"Transaction is not set\")\n+    verify(\n+      closeDbOnCompletion == false,\n+      \"Cannot close a DB without aborting/commiting the transactions\")\n+    val readOptions = new ReadOptions()\n+    val itr: RocksIterator = txn.getIterator(readOptions)\n+    Option(itr) match {\n+      case Some(i) =>\n+        logDebug(s\"creating iterator from transaction DB\")\n+        createUnsafeRowPairIterator(i, readOptions, false)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+}\n+\n+object RocksDbInstance {\n+\n+  RocksDB.loadLibrary()\n+\n+  private val destroyOptions: Options = new Options()\n+\n+  val lRUCache = new LRUCache(1024 * 1024 * 1024, 6, false, 0.05)\n+\n+  def destroyDB(path: String): Unit = {\n+    val f: File = new File(path)\n+    if (f.exists()) {\n+      RocksDB.destroyDB(path, destroyOptions)\n+      FileUtils.deleteDirectory(f)\n+    }\n+  }\n+\n+  def restoreFromBackup(backupDir: String, dbDir: String): Unit = {\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(backupDir)\n+      val be = BackupEngine.open(Env.getDefault, backupableDBOptions)\n+      val restoreOptions = new RestoreOptions(false)\n+      be.restoreDbFromLatestBackup(dbDir, dbDir, restoreOptions)\n+      restoreOptions.close()\n+      backupableDBOptions.close()\n+      be.close()\n+    }\n+  }\n+\n+  def verify(condition: => Boolean, msg: String): Unit = {"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Fixed",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T06:29:35Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null\n+  }\n+\n+  override def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(txn != null, \"Transaction is not set\")\n+    verify(\n+      closeDbOnCompletion == false,\n+      \"Cannot close a DB without aborting/commiting the transactions\")\n+    val readOptions = new ReadOptions()\n+    val itr: RocksIterator = txn.getIterator(readOptions)\n+    Option(itr) match {\n+      case Some(i) =>\n+        logDebug(s\"creating iterator from transaction DB\")\n+        createUnsafeRowPairIterator(i, readOptions, false)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+}\n+\n+object RocksDbInstance {\n+\n+  RocksDB.loadLibrary()\n+\n+  private val destroyOptions: Options = new Options()\n+\n+  val lRUCache = new LRUCache(1024 * 1024 * 1024, 6, false, 0.05)\n+\n+  def destroyDB(path: String): Unit = {\n+    val f: File = new File(path)\n+    if (f.exists()) {\n+      RocksDB.destroyDB(path, destroyOptions)\n+      FileUtils.deleteDirectory(f)\n+    }\n+  }\n+\n+  def restoreFromBackup(backupDir: String, dbDir: String): Unit = {\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(backupDir)\n+      val be = BackupEngine.open(Env.getDefault, backupableDBOptions)\n+      val restoreOptions = new RestoreOptions(false)\n+      be.restoreDbFromLatestBackup(dbDir, dbDir, restoreOptions)\n+      restoreOptions.close()\n+      backupableDBOptions.close()\n+      be.close()\n+    }\n+  }\n+\n+  def verify(condition: => Boolean, msg: String): Unit = {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Not yet understand why this needed in 3 different places?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T12:40:30Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null\n+  }\n+\n+  override def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(txn != null, \"Transaction is not set\")\n+    verify(\n+      closeDbOnCompletion == false,\n+      \"Cannot close a DB without aborting/commiting the transactions\")\n+    val readOptions = new ReadOptions()\n+    val itr: RocksIterator = txn.getIterator(readOptions)\n+    Option(itr) match {\n+      case Some(i) =>\n+        logDebug(s\"creating iterator from transaction DB\")\n+        createUnsafeRowPairIterator(i, readOptions, false)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+}\n+\n+object RocksDbInstance {\n+\n+  RocksDB.loadLibrary()",
    "line": 397
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "We need to make sure Rocksdb libraries are loaded before we make any operation on top of it. Calling it twice is a no-op. I have added loadLibrary at all possible entry point. \r\n```\r\n /**\r\n   * Loads the necessary library files.\r\n   * Calling this method twice will have no effect.\r\n   * By default the method extracts the shared library for loading at\r\n   * java.io.tmpdir, however, you can override this temporary location by\r\n   * setting the environment variable ROCKSDB_SHAREDLIB_DIR.\r\n   */\r\n  public static void loadLibrary() {\r\n    if (libraryLoaded.get() == LibraryState.LOADED) {\r\n      return;\r\n    }\r\n```",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T06:29:08Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null\n+  }\n+\n+  override def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(txn != null, \"Transaction is not set\")\n+    verify(\n+      closeDbOnCompletion == false,\n+      \"Cannot close a DB without aborting/commiting the transactions\")\n+    val readOptions = new ReadOptions()\n+    val itr: RocksIterator = txn.getIterator(readOptions)\n+    Option(itr) match {\n+      case Some(i) =>\n+        logDebug(s\"creating iterator from transaction DB\")\n+        createUnsafeRowPairIterator(i, readOptions, false)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+}\n+\n+object RocksDbInstance {\n+\n+  RocksDB.loadLibrary()",
    "line": 397
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Wouldn't it be easier to say\r\n```\r\nif (readOnly) {\r\n...\r\n} else {\r\n...\r\n}\r\n```\r\n",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T15:09:58Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Your suggestion totally makes sense. I went overboard in using match..case construct. Will make the changes.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T06:08:51Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Shouldn't be a possible transaction closed?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T15:12:52Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Semantic is that transaction will be committed/aborted before close is invoked. So I did not worry about `txn` here.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T06:22:45Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I'm fine with this, but then we can add some `require` here (just in case somebody comes later with a change and doesn't know this decision).\r\n",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T12:30:23Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Done",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-09T04:17:25Z",
    "diffHunk": "@@ -0,0 +1,434 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.rocksdb._\n+import org.rocksdb.RocksDB\n+import org.rocksdb.util.SizeUnit\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+class RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String)\n+    extends Logging {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  protected var db: RocksDB = null\n+  protected var dbPath: String = _\n+  protected val readOptions: ReadOptions = new ReadOptions()\n+  protected val writeOptions: WriteOptions = new WriteOptions()\n+  protected val table_options = new BlockBasedTableConfig\n+  protected val options: Options = new Options()\n+\n+  def isOpen(): Boolean = {\n+    db != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(db == null, \"Another rocksDb instance is already actve\")\n+    try {\n+      setOptions(conf)\n+      db = readOnly match {\n+        case true =>\n+          options.setCreateIfMissing(false)\n+          RocksDB.openReadOnly(options, path)\n+        case false =>\n+          options.setCreateIfMissing(true)\n+          RocksDB.open(options, path)\n+      }\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating rocksDb instance ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def get(key: UnsafeRow): UnsafeRow = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None => null\n+    }\n+  }\n+\n+  def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.put(key.getBytes, value.getBytes)\n+  }\n+\n+  def remove(key: UnsafeRow): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    db.delete(key.getBytes)\n+  }\n+\n+  def commit(backupPath: Option[String] = None): Unit = {\n+    backupPath.foreach(f => createCheckpoint(db, f))\n+  }\n+\n+  def abort: Unit = {\n+    // no-op\n+  }\n+\n+  def close(): Unit = {\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the db\")\n+    db.close()\n+    db = null\n+  }\n+\n+  def iterator(closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    Option(db.getSnapshot) match {\n+      case Some(snapshot) =>\n+        logDebug(s\"Inside rockdDB iterator function\")\n+        var snapshotReadOptions: ReadOptions = new ReadOptions().setSnapshot(snapshot)\n+        val itr = db.newIterator(snapshotReadOptions)\n+        createUnsafeRowPairIterator(itr, snapshotReadOptions, closeDbOnCompletion)\n+      case None =>\n+        Iterator.empty\n+    }\n+  }\n+\n+  protected def createUnsafeRowPairIterator(\n+      itr: RocksIterator,\n+      itrReadOptions: ReadOptions,\n+      closeDbOnCompletion: Boolean): Iterator[UnsafeRowPair] = {\n+\n+    itr.seekToFirst()\n+\n+    new Iterator[UnsafeRowPair] {\n+      override def hasNext: Boolean = {\n+        if (itr.isValid) {\n+          true\n+        } else {\n+          itrReadOptions.close()\n+          if (closeDbOnCompletion) {\n+            close()\n+          }\n+          logDebug(s\"read from DB completed\")\n+          false\n+        }\n+      }\n+\n+      override def next(): UnsafeRowPair = {\n+        val keyBytes = itr.key\n+        val key = new UnsafeRow(keySchema.fields.length)\n+        key.pointTo(keyBytes, keyBytes.length)\n+        val valueBytes = itr.value\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueBytes, valueBytes.length)\n+        itr.next()\n+        new UnsafeRowPair(key, value)\n+      }\n+    }\n+  }\n+\n+  def printStats: Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    try {\n+      val stats = db.getProperty(\"rocksdb.stats\")\n+      logInfo(s\"Stats = $stats\")\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Exception while getting stats\")\n+    }\n+  }\n+\n+  def setOptions(conf: Map[String, String]): Unit = {\n+\n+    // Read options\n+    readOptions.setFillCache(false)\n+\n+    // Write options\n+    writeOptions.setSync(false)\n+    writeOptions.setDisableWAL(true)\n+\n+    val dataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.blockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"64\")\n+      .toInt\n+\n+    val metadataBlockSize = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.metadataBlockSizeInKB\".toLowerCase(Locale.ROOT),\n+        \"4\")\n+      .toInt\n+\n+    // Table configs\n+    // https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters\n+    table_options\n+      .setBlockSize(dataBlockSize)\n+      .setBlockSizeDeviation(5)\n+      .setMetadataBlockSize(metadataBlockSize)\n+      .setFilterPolicy(new BloomFilter(10, false))\n+      .setPartitionFilters(true)\n+      .setIndexType(IndexType.kTwoLevelIndexSearch)\n+      .setBlockCache(lRUCache)\n+      .setCacheIndexAndFilterBlocks(true)\n+      .setPinTopLevelIndexAndFilter(true)\n+      .setCacheIndexAndFilterBlocksWithHighPriority(true)\n+      .setPinL0FilterAndIndexBlocksInCache(true)\n+      .setFormatVersion(4) // https://rocksdb.org/blog/2019/03/08/format-version-4.html\n+      .setIndexBlockRestartInterval(16)\n+\n+    var bufferNumber = conf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.bufferNumber\".toLowerCase(Locale.ROOT),\n+        \"5\")\n+      .toInt\n+\n+    bufferNumber = Math.max(bufferNumber, 3)\n+\n+    val bufferNumberToMaintain = Math.max(bufferNumber - 2, 3)\n+\n+    logInfo(\n+      s\"Using Max Buffer Name = $bufferNumber & \" +\n+        s\"max buffer number to maintain = $bufferNumberToMaintain\")\n+\n+    // DB Options\n+    options\n+      .setCreateIfMissing(true)\n+      .setMaxWriteBufferNumber(bufferNumber)\n+      .setMaxWriteBufferNumberToMaintain(bufferNumberToMaintain)\n+      .setMaxBackgroundCompactions(4)\n+      .setMaxBackgroundFlushes(2)\n+      .setMaxOpenFiles(-1)\n+      .setMaxFileOpeningThreads(4)\n+      .setWriteBufferSize(256 * SizeUnit.MB)\n+      .setTargetFileSizeBase(256 * SizeUnit.MB)\n+      .setLevelZeroFileNumCompactionTrigger(10)\n+      .setLevelZeroSlowdownWritesTrigger(20)\n+      .setLevelZeroStopWritesTrigger(40)\n+      .setMaxBytesForLevelBase(2 * SizeUnit.GB)\n+      .setTableFormatConfig(table_options)\n+\n+  }\n+\n+  def createCheckpoint(rocksDb: RocksDB, dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val c = Checkpoint.create(rocksDb)\n+      val f: File = new File(dir)\n+      if (f.exists()) {\n+        FileUtils.deleteDirectory(f)\n+      }\n+      c.createCheckpoint(dir)\n+      c.close()\n+    }\n+    logDebug(s\"Creating createCheckpoint at $dir took $elapsedMs ms.\")\n+  }\n+\n+  def createBackup(dir: String): Unit = {\n+    verify(isOpen(), \"Open rocksDb instance before any operation\")\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      val backupableDBOptions = new BackupableDBOptions(dir)\n+      backupableDBOptions.setDestroyOldData(true)\n+      val env: Env = Env.getDefault\n+      env.setBackgroundThreads(2)\n+      val be = BackupEngine.open(env, backupableDBOptions)\n+      be.createNewBackup(db, true) //\n+      backupableDBOptions.close()\n+      env.close()\n+      be.close()\n+    }\n+    logInfo(s\"Creating backup at $dir takes $elapsedMs ms.\")\n+  }\n+}\n+\n+class OptimisticTransactionDbInstance(\n+    keySchema: StructType,\n+    valueSchema: StructType,\n+    identifier: String)\n+    extends RocksDbInstance(keySchema: StructType, valueSchema: StructType, identifier: String) {\n+\n+  import RocksDbInstance._\n+  RocksDB.loadLibrary()\n+\n+  var otdb: OptimisticTransactionDB = null\n+  var txn: Transaction = null\n+\n+  override def isOpen(): Boolean = {\n+    otdb != null\n+  }\n+\n+  def open(path: String, conf: Map[String, String]): Unit = {\n+    open(path, conf, false)\n+  }\n+\n+  override def open(path: String, conf: Map[String, String], readOnly: Boolean): Unit = {\n+    verify(otdb == null, \"Another OptimisticTransactionDbInstance instance is already actve\")\n+    verify(readOnly == false, \"Cannot open OptimisticTransactionDbInstance in Readonly mode\")\n+    try {\n+      setOptions(conf)\n+      options.setCreateIfMissing(true)\n+      otdb = OptimisticTransactionDB.open(options, path)\n+      dbPath = path\n+    } catch {\n+      case e: Throwable =>\n+        throw new IllegalStateException(\n+          s\"Error while creating OptimisticTransactionDb instance\" +\n+            s\" ${e.getMessage}\",\n+          e)\n+    }\n+  }\n+\n+  def startTransactions(): Unit = {\n+    verify(isOpen(), \"Open OptimisticTransactionDbInstance before performing any operation\")\n+    Option(txn) match {\n+      case None =>\n+        val optimisticTransactionOptions = new OptimisticTransactionOptions()\n+        txn = otdb.beginTransaction(writeOptions, optimisticTransactionOptions)\n+        txn.setSavePoint()\n+      case Some(x) =>\n+        throw new IllegalStateException(s\"Already started a transaction\")\n+    }\n+  }\n+\n+  override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before inserting any key\")\n+    txn.put(key.getBytes, value.getBytes)\n+  }\n+\n+  override def remove(key: UnsafeRow): Unit = {\n+    verify(txn != null, \"Start Transaction before deleting any key\")\n+    txn.delete(key.getBytes)\n+  }\n+\n+  override def get(key: UnsafeRow): UnsafeRow = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    Option(txn.get(readOptions, key.getBytes)) match {\n+      case Some(valueInBytes) =>\n+        val value = new UnsafeRow(valueSchema.fields.length)\n+        value.pointTo(valueInBytes, valueInBytes.length)\n+        value\n+      case None =>\n+        null\n+    }\n+  }\n+\n+  override def commit(backupPath: Option[String] = None): Unit = {\n+    verify(txn != null, \"Start Transaction before fetching any key-value\")\n+    // printTrxStats\n+    try {\n+      val file = new File(dbPath, identifier.toUpperCase(Locale.ROOT))\n+      file.createNewFile()\n+      txn.commit()\n+      txn.close()\n+      txn = null\n+      backupPath.foreach(f => createCheckpoint(otdb.asInstanceOf[RocksDB], f))\n+    } catch {\n+      case e: Exception =>\n+        log.error(s\"Unable to commit the transactions. Error message = ${e.getMessage}\")\n+        throw e\n+    }\n+  }\n+\n+  def printTrxStats(): Unit = {\n+    verify(txn != null, \"No open Transaction\")\n+    logInfo(s\"\"\"\n+         | deletes = ${txn.getNumDeletes}\n+         | numKeys = ${txn.getNumKeys}\n+         | puts =  ${txn.getNumPuts}\n+         | time =  ${txn.getElapsedTime}\n+       \"\"\".stripMargin)\n+  }\n+\n+  override def abort(): Unit = {\n+    verify(txn != null, \"No Transaction to abort\")\n+    txn.rollbackToSavePoint()\n+    txn.close()\n+    txn = null\n+  }\n+\n+  override def close(): Unit = {\n+    verify(isOpen(), \"No DB to close\")\n+    readOptions.close()\n+    writeOptions.close()\n+    logDebug(\"Closing the transaction db\")\n+    otdb.close()\n+    otdb = null"
  }],
  "prId": 24922
}]