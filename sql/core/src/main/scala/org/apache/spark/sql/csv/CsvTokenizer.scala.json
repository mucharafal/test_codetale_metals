[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "same here\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T07:44:16Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+/**\n+ * Tokenizer based on RFC 4180 for comma separated values.\n+ * It implements an iterator that returns each tokenized line as an Array[Any].\n+ */\n+private[sql] class CsvTokenizer(inputIter: Iterator[String],\n+    delimiter: String, quote: String) extends Iterator[Array[Any]] {"
  }],
  "prId": 1351
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "Can we use an existing tokenizer such as http://commons.apache.org/proper/commons-lang/javadocs/api-3.1/org/apache/commons/lang3/text/StrTokenizer.html ?\n\nWe already depend on commons-lang in Spark. \n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T08:05:29Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+/**\n+ * Tokenizer based on RFC 4180 for comma separated values.\n+ * It implements an iterator that returns each tokenized line as an Array[Any].\n+ */\n+private[sql] class CsvTokenizer(inputIter: Iterator[String],"
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "I tried it. I have two issues with it.\n- To correctly parse new lines in quoted fields, I will need to implement some logic around it (which will not be much different from current implementation)\n- It only accepts single character delimiter.\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-10T19:25:57Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.csv\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+/**\n+ * Tokenizer based on RFC 4180 for comma separated values.\n+ * It implements an iterator that returns each tokenized line as an Array[Any].\n+ */\n+private[sql] class CsvTokenizer(inputIter: Iterator[String],"
  }],
  "prId": 1351
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Should we use the open source CSV parser (e.g. https://commons.apache.org/proper/commons-csv/), rather than re-implement it?\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-11T00:46:57Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*",
    "line": 1
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Actually commons-csv supports the following standard per its document:\nDEFAULT\nEXCEL\nMYSQL\nRFC4180\nTDF\nhttps://commons.apache.org/proper/commons-csv/apidocs/org/apache/commons/csv/CSVFormat.html\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-11T00:54:45Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*",
    "line": 1
  }, {
    "author": {
      "login": "falaki"
    },
    "body": "Hey Cheng. Thanks for the suggestion and other comments. I looked into this and another CSV tokenizer. Unfortunately, these tokenizers do not support reading from an Iterator. The support/code needed to make them work with an Iterator is no less than doing it from scratch, especially because a CSV row can potentially span multiple lines of the input iterator. \n\nI applied your other comments.\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-13T00:44:17Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*",
    "line": 1
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Is the intention of supporting reading from an iterator to handle records having newline characters? Seems the issue that we cannot escape newline characters is a hadoop problem. Maybe a better approach is to have a new TextInputFormat with a new LineReader (in another PR)?\n",
    "commit": "11e6422ee25c8e61948b6d35375a6ccd1823dcc7",
    "createdAt": "2014-07-13T01:19:25Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*",
    "line": 1
  }],
  "prId": 1351
}]