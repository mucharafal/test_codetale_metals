[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "document what these parameters mean?\n\ne.g. what schemaLess mean, and what the seq of string tuples mean for inputRowFormat and outputRowFormat?\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-08-25T05:41:45Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema(0)))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+        sb.append(row.get(i, inputSchema(i)))\n+        i += 1\n+      }\n+      sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\"))\n+      sb.toString()\n+    }\n+    outputStream.write(data.getBytes(StandardCharsets.UTF_8))\n+  }\n+\n+  override def run(): Unit = Utils.logUncaughtExceptions {\n+    init()\n+\n+    // We can't use Utils.tryWithSafeFinally here because we also need a `catch` block, so\n+    // let's use a variable to record whether the `finally` block was hit due to an exception\n+    var threwException: Boolean = true\n+    val numColumns = inputSchema.length\n+    try {\n+      iter.map(outputProjection).foreach(row => processRow(row, numColumns))\n+      threwException = false\n+    } catch {\n+      case t: Throwable =>\n+        // An error occurred while writing input, so kill the child process. According to the\n+        // Javadoc this call will not throw an exception:\n+        _exception = t\n+        proc.destroy()\n+        throw t\n+    } finally {\n+      try {\n+        Utils.tryLogNonFatalError(outputStream.close())\n+        if (proc.waitFor() != 0) {\n+          logError(stderrBuffer.toString) // log the stderr circular buffer\n+        }\n+      } catch {\n+        case NonFatal(exceptionFromFinallyBlock) =>\n+          if (!threwException) {\n+            throw exceptionFromFinallyBlock\n+          } else {\n+            log.error(\"Exception in finally block\", exceptionFromFinallyBlock)\n+          }\n+      }\n+    }\n+  }\n+}\n+\n+private[sql]\n+object ScriptTransformIOSchema {\n+  def apply(input: ScriptInputOutputSchema): ScriptTransformIOSchema = {\n+    new ScriptTransformIOSchema(\n+      input.inputRowFormat,\n+      input.outputRowFormat,\n+      input.schemaLess)\n+  }\n+}\n+\n+/**\n+ * The wrapper class of Hive input and output schema properties\n+ */\n+private[sql] class ScriptTransformIOSchema (\n+    inputRowFormat: Seq[(String, String)],\n+    outputRowFormat: Seq[(String, String)],\n+    schemaLess: Boolean) extends Serializable {",
    "line": 326
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "documented the params\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-09-09T00:56:32Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema(0)))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+        sb.append(row.get(i, inputSchema(i)))\n+        i += 1\n+      }\n+      sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\"))\n+      sb.toString()\n+    }\n+    outputStream.write(data.getBytes(StandardCharsets.UTF_8))\n+  }\n+\n+  override def run(): Unit = Utils.logUncaughtExceptions {\n+    init()\n+\n+    // We can't use Utils.tryWithSafeFinally here because we also need a `catch` block, so\n+    // let's use a variable to record whether the `finally` block was hit due to an exception\n+    var threwException: Boolean = true\n+    val numColumns = inputSchema.length\n+    try {\n+      iter.map(outputProjection).foreach(row => processRow(row, numColumns))\n+      threwException = false\n+    } catch {\n+      case t: Throwable =>\n+        // An error occurred while writing input, so kill the child process. According to the\n+        // Javadoc this call will not throw an exception:\n+        _exception = t\n+        proc.destroy()\n+        throw t\n+    } finally {\n+      try {\n+        Utils.tryLogNonFatalError(outputStream.close())\n+        if (proc.waitFor() != 0) {\n+          logError(stderrBuffer.toString) // log the stderr circular buffer\n+        }\n+      } catch {\n+        case NonFatal(exceptionFromFinallyBlock) =>\n+          if (!threwException) {\n+            throw exceptionFromFinallyBlock\n+          } else {\n+            log.error(\"Exception in finally block\", exceptionFromFinallyBlock)\n+          }\n+      }\n+    }\n+  }\n+}\n+\n+private[sql]\n+object ScriptTransformIOSchema {\n+  def apply(input: ScriptInputOutputSchema): ScriptTransformIOSchema = {\n+    new ScriptTransformIOSchema(\n+      input.inputRowFormat,\n+      input.outputRowFormat,\n+      input.schemaLess)\n+  }\n+}\n+\n+/**\n+ * The wrapper class of Hive input and output schema properties\n+ */\n+private[sql] class ScriptTransformIOSchema (\n+    inputRowFormat: Seq[(String, String)],\n+    outputRowFormat: Seq[(String, String)],\n+    schemaLess: Boolean) extends Serializable {",
    "line": 326
  }],
  "prId": 14702
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can we fix the indent (using 4 space indent) for function parameters in this file?\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-08-25T05:42:25Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],"
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "Fixed the ident. Is there any wiki which has instructions on the code style to be used ? For every PR in past I have manually adjusted the formatting as per rest code.. its about time to ask.\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-09-09T00:58:19Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],"
  }],
  "prId": 14702
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "This would call `toString()` on our internal representation of a value. This will lead to unexpected results as soon as you would use a `Date` or a `Timestamp`. See the discussion (and potential solution) in PR https://github.com/apache/spark/pull/14279. \n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-08-25T12:01:24Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema(0)))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+        sb.append(row.get(i, inputSchema(i)))"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "We could also do this using a (code generated) projection. The beauty there would be that we could make it return a single `UTF8String` which can be dumped straight into the outputstream.\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-08-25T12:05:48Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema(0)))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+        sb.append(row.get(i, inputSchema(i)))"
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "@hvanhovell : for now I am not going the code-gen route as it will make the diff bigger and delay things. I am doing what PR #14279 did and adding code-gen as `Future TODOs` in PR description.\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-09-09T00:56:18Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema(0)))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+        sb.append(row.get(i, inputSchema(i)))"
  }],
  "prId": 14702
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Lets put this in a val.\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-08-25T12:02:09Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema(0)))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))"
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "did this change\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-09-09T00:54:09Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema(0)))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))"
  }],
  "prId": 14702
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Lets put this in a val.\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-08-25T12:02:20Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")"
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "did this change\n",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2016-09-09T00:54:05Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificMutableRow(output.map(_.dataType))\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"))\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), 2)\n+              .map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(input: Seq[Expression],\n+           script: String,\n+           child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(writerException: Option[Throwable],\n+                               cause: Throwable = null,\n+                               proc: Process,\n+                               stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")"
  }],
  "prId": 14702
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "We also need to override `outputPartitioning`?\r\n```Scala\r\n  override def outputPartitioning: Partitioning = child.outputPartitioning\r\n```",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2017-01-02T18:49:10Z",
    "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{SQLDate, SQLTimestamp}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, DateType, StructType, TimestampType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+",
    "line": 59
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "did this change",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2017-01-02T22:46:21Z",
    "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{SQLDate, SQLTimestamp}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, DateType, StructType, TimestampType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+",
    "line": 59
  }],
  "prId": 14702
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "```Scala\r\ncase class ScriptInputOutputSchema(\r\n    inputRowFormat: Seq[(String, String)],\r\n    outputRowFormat: Seq[(String, String)],\r\n    inputSerdeClass: Option[String],\r\n    outputSerdeClass: Option[String],\r\n    inputSerdeProps: Seq[(String, String)],\r\n    outputSerdeProps: Seq[(String, String)],\r\n    recordReaderClass: Option[String],\r\n    recordWriterClass: Option[String],\r\n    schemaLess: Boolean)\r\n```\r\n\r\nExcept `inputRowFormat `, `outputRowFormat ` and  `schemaLess `, we ignore all the other fields. I think we should not silently ignore them. For example, we do not respect any user-specified conf values of `hive.script.recordreader` and `hive.script.recordwriter`. Thus, could we issue an exception when users set them?",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2017-02-05T01:22:57Z",
    "diffHunk": "@@ -0,0 +1,337 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{SQLDate, SQLTimestamp}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, DateType, StructType, TimestampType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificInternalRow(output.map(_.dataType))\n+      val fieldDelimiter = ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\")\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(fieldDelimiter).map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(fieldDelimiter, 2).map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(\n+      input: Seq[Expression],\n+      script: String,\n+      child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {\n+    val broadcastedHadoopConf =\n+      new SerializableConfiguration(sqlContext.sessionState.newHadoopConf())\n+\n+    child.execute().mapPartitions { iter =>\n+      if (iter.hasNext) {\n+        val proj = UnsafeProjection.create(schema)\n+        processIterator(iter, broadcastedHadoopConf.value).map(proj)\n+      } else {\n+        // If the input iterator has no rows then do not launch the external script.\n+        Iterator.empty\n+      }\n+    }\n+  }\n+\n+  def checkFailureAndPropagate(\n+      writerException: Option[Throwable],\n+      cause: Throwable = null,\n+      proc: Process,\n+      stderrBuffer: CircularBuffer): Unit = {\n+    if (writerException.isDefined) {\n+      throw writerException.get\n+    }\n+\n+    // Checks if the proc is still alive (incase the command ran was bad)\n+    // The ideal way to do this is to use Java 8's Process#isAlive()\n+    // but it cannot be used because Spark still supports Java 7.\n+    // Following is a workaround used to check if a process is alive in Java 7\n+    // TODO: Once builds are switched to Java 8, this can be changed\n+    try {\n+      val exitCode = proc.exitValue()\n+      if (exitCode != 0) {\n+        logError(stderrBuffer.toString) // log the stderr circular buffer\n+        throw new SparkException(s\"Subprocess exited with status $exitCode. \" +\n+          s\"Error: ${stderrBuffer.toString}\", cause)\n+      }\n+    } catch {\n+      case _: IllegalThreadStateException =>\n+      // This means that the process is still alive. Move ahead\n+    }\n+  }\n+\n+  def createReader(inputStream: InputStream): BufferedReader =\n+    new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))\n+\n+  def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow]\n+}\n+\n+private[sql] class ScriptTransformationWriterThread(\n+    iter: Iterator[InternalRow],\n+    inputSchema: Seq[DataType],\n+    outputProjection: Projection,\n+    ioschema: ScriptTransformIOSchema,\n+    outputStream: OutputStream,\n+    proc: Process,\n+    stderrBuffer: CircularBuffer,\n+    taskContext: TaskContext,\n+    conf: Configuration\n+  ) extends Thread(\"Thread-ScriptTransformation-Feed\") with Logging with Serializable {\n+\n+  setDaemon(true)\n+\n+  @volatile protected var _exception: Throwable = null\n+\n+  protected val lineDelimiter = ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATLINES\")\n+  protected val fieldDelimiter = ioschema.inputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\")\n+\n+  /** Contains the exception thrown while writing the parent iterator to the external process. */\n+  def exception: Option[Throwable] = Option(_exception)\n+\n+  protected def init(): Unit = {\n+    TaskContext.setTaskContext(taskContext)\n+  }\n+\n+  protected def processRow(row: InternalRow, numColumns: Int): Unit = {\n+    val data = if (numColumns == 0) {\n+      lineDelimiter\n+    } else {\n+      val sb = new StringBuilder\n+      sb.append(row.get(0, inputSchema.head))\n+      var i = 1\n+      while (i < numColumns) {\n+        sb.append(fieldDelimiter)\n+        val columnType = inputSchema(i)\n+        val fieldValue = row.get(i, columnType)\n+        val fieldStringValue = columnType match {\n+          case _: DateType =>\n+            DateTimeUtils.dateToString(fieldValue.asInstanceOf[SQLDate])\n+          case _: TimestampType =>\n+            DateTimeUtils.timestampToString(fieldValue.asInstanceOf[SQLTimestamp])\n+          case _ =>\n+            fieldValue.toString\n+        }\n+        sb.append(fieldStringValue)\n+        i += 1\n+      }\n+      sb.append(lineDelimiter)\n+      sb.toString()\n+    }\n+    outputStream.write(data.getBytes(StandardCharsets.UTF_8))\n+  }\n+\n+  override def run(): Unit = Utils.logUncaughtExceptions {\n+    init()\n+\n+    // We can't use Utils.tryWithSafeFinally here because we also need a `catch` block, so\n+    // let's use a variable to record whether the `finally` block was hit due to an exception\n+    var threwException: Boolean = true\n+    val numColumns = inputSchema.length\n+    try {\n+      iter.map(outputProjection).foreach(row => processRow(row, numColumns))\n+      threwException = false\n+    } catch {\n+      case t: Throwable =>\n+        // An error occurred while writing input, so kill the child process. According to the\n+        // Javadoc this call will not throw an exception:\n+        _exception = t\n+        proc.destroy()\n+        throw t\n+    } finally {\n+      try {\n+        Utils.tryLogNonFatalError(outputStream.close())\n+        if (proc.waitFor() != 0) {\n+          logError(stderrBuffer.toString) // log the stderr circular buffer\n+        }\n+      } catch {\n+        case NonFatal(exceptionFromFinallyBlock) =>\n+          if (!threwException) {\n+            throw exceptionFromFinallyBlock\n+          } else {\n+            log.error(\"Exception in finally block\", exceptionFromFinallyBlock)\n+          }\n+      }\n+    }\n+  }\n+}\n+\n+private[sql]\n+object ScriptTransformIOSchema {\n+  def apply(input: ScriptInputOutputSchema): ScriptTransformIOSchema = {\n+    new ScriptTransformIOSchema(",
    "line": 308
  }],
  "prId": 14702
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit: the indent issue: \r\n```Scala\r\n  def execute(\r\n      sqlContext: SQLContext,\r\n      child: SparkPlan,\r\n      schema: StructType): RDD[InternalRow] = {\r\n```",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2017-02-05T04:33:30Z",
    "diffHunk": "@@ -0,0 +1,337 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{SQLDate, SQLTimestamp}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, DateType, StructType, TimestampType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificInternalRow(output.map(_.dataType))\n+      val fieldDelimiter = ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\")\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(fieldDelimiter).map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(fieldDelimiter, 2).map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(\n+      input: Seq[Expression],\n+      script: String,\n+      child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {",
    "line": 166
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Can we replace `sqlContext: SQLContext` by `hadoopConf: Configuration`?",
    "commit": "d9047f0d5728075d8b50c64afbef65a5279a1847",
    "createdAt": "2017-02-05T04:45:28Z",
    "diffHunk": "@@ -0,0 +1,337 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.script\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema\n+import org.apache.spark.sql.catalyst.plans.physical.Partitioning\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{SQLDate, SQLTimestamp}\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+import org.apache.spark.sql.types.{DataType, DateType, StructType, TimestampType}\n+import org.apache.spark.util.{CircularBuffer, RedirectThread, SerializableConfiguration, Utils}\n+\n+/**\n+ * Transforms the input by forking and running the specified script.\n+ *\n+ * @param input the set of expression that should be passed to the script.\n+ * @param script the command that should be executed.\n+ * @param output the attributes that are produced by the script.\n+ */\n+private[sql]\n+case class ScriptTransformationExec(\n+    input: Seq[Expression],\n+    script: String,\n+    output: Seq[Attribute],\n+    child: SparkPlan,\n+    ioschema: ScriptTransformIOSchema)\n+  extends UnaryExecNode with ScriptTransformBase {\n+\n+  override def producedAttributes: AttributeSet = outputSet -- inputSet\n+\n+  override def outputPartitioning: Partitioning = child.outputPartitioning\n+\n+  protected override def doExecute(): RDD[InternalRow] =\n+    execute(sqlContext, child, schema)\n+\n+  override def processIterator(\n+      inputIterator: Iterator[InternalRow],\n+      hadoopConf: Configuration) : Iterator[InternalRow] = {\n+\n+    val (proc, inputStream, outputStream, stderrBuffer, outputProjection) =\n+      init(input, script, child)\n+\n+    // This new thread will consume the ScriptTransformation's input rows and write them to the\n+    // external process. That process's output will be read by this current thread.\n+    val writerThread = new ScriptTransformationWriterThread(\n+      inputIterator,\n+      input.map(_.dataType),\n+      outputProjection,\n+      ioschema,\n+      outputStream,\n+      proc,\n+      stderrBuffer,\n+      TaskContext.get(),\n+      hadoopConf\n+    )\n+\n+    val reader = createReader(inputStream)\n+\n+    val outputIterator: Iterator[InternalRow] = new Iterator[InternalRow] {\n+      var curLine: String = null\n+      val mutableRow = new SpecificInternalRow(output.map(_.dataType))\n+      val fieldDelimiter = ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\")\n+\n+      override def hasNext: Boolean = {\n+        try {\n+          if (curLine == null) {\n+            curLine = reader.readLine()\n+            if (curLine == null) {\n+              checkFailureAndPropagate(writerThread.exception, null, proc, stderrBuffer)\n+              return false\n+            }\n+          }\n+          true\n+        } catch {\n+          case NonFatal(e) =>\n+            // If this exception is due to abrupt / unclean termination of `proc`,\n+            // then detect it and propagate a better exception message for end users\n+            checkFailureAndPropagate(writerThread.exception, e, proc, stderrBuffer)\n+\n+            throw e\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (!hasNext) {\n+          throw new NoSuchElementException\n+        }\n+        val prevLine = curLine\n+        curLine = reader.readLine()\n+        if (!ioschema.isSchemaLess) {\n+          new GenericInternalRow(\n+            prevLine.split(fieldDelimiter).map(CatalystTypeConverters.convertToCatalyst))\n+        } else {\n+          new GenericInternalRow(\n+            prevLine.split(fieldDelimiter, 2).map(CatalystTypeConverters.convertToCatalyst))\n+        }\n+      }\n+    }\n+\n+    writerThread.start()\n+    outputIterator\n+  }\n+}\n+\n+private[sql] trait ScriptTransformBase extends Serializable with Logging {\n+\n+  def init(\n+      input: Seq[Expression],\n+      script: String,\n+      child: SparkPlan\n+    ): (Process, InputStream, OutputStream, CircularBuffer, InterpretedProjection) = {\n+\n+    val cmd = List(\"/bin/bash\", \"-c\", script)\n+    val builder = new ProcessBuilder(cmd.asJava)\n+\n+    val proc = builder.start()\n+    val inputStream = proc.getInputStream\n+    val outputStream = proc.getOutputStream\n+    val errorStream = proc.getErrorStream\n+\n+    // In order to avoid deadlocks, we need to consume the error output of the child process.\n+    // To avoid issues caused by large error output, we use a circular buffer to limit the amount\n+    // of error output that we retain. See SPARK-7862 for more discussion of the deadlock / hang\n+    // that motivates this.\n+    val stderrBuffer = new CircularBuffer(2048)\n+    new RedirectThread(\n+      errorStream,\n+      stderrBuffer,\n+      \"Thread-ScriptTransformation-STDERR-Consumer\").start()\n+\n+    val outputProjection = new InterpretedProjection(input, child.output)\n+    (proc, inputStream, outputStream, stderrBuffer, outputProjection)\n+  }\n+\n+  def execute(sqlContext: SQLContext,\n+              child: SparkPlan,\n+              schema: StructType): RDD[InternalRow] = {",
    "line": 166
  }],
  "prId": 14702
}]