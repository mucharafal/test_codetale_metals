[{
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Can we make `rocksDbPath` configurable as in [here](https://github.com/apache/flink/blob/d8aac6ffb833d1d0348be0f6d280b465213d5df5/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java#L327)?\r\n",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-06-20T10:56:04Z",
    "diffHunk": "@@ -0,0 +1,625 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+\n+   /*\n+    * Additional configurations related to rocksDb. This will capture all configs in\n+    * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+    */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long)\n+    extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+       rocksDbWriteInstance = new OptimisticTransactionDbInstance(\n+          keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state. \" +\n+        s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange( start: Option[UnsafeRow],\n+                           end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state \" +\n+        s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(\n+            s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+          storeMap.remove(version)\n+          close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+          storeMap.remove(version)\n+          close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if ( rocksDbWriteInstance != null ) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+     /*\n+      * Get an iterator of all the store data.\n+      * This can be called only after committing all the updates made in the current thread.\n+      */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use checkpointed db for previous version\n+          logDebug(s\"state = loaded/aborted using checkpointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using checkpointed DB with version $newVersion\")\n+          // use checkpointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap { case (name, value) =>\n+        // just allow searching from list cause the list is small enough\n+        supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0), customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+   /*\n+    * Initialize the provide with more contextual information from the SQL operator.\n+    * This method will be called first after creating an instance of the StateStoreProvider by\n+    * reflection.\n+    *\n+    * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+    * @param keySchema       Schema of keys to be stored\n+    * @param valueSchema     Schema of value to be stored\n+    * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+    *                        which the StateStore implementation could index the data.\n+    * @param storeConfs      Configurations used by the StateStores\n+    * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+    *                        to save state data\n+    */\n+  override def init(stateStoreId: StateStoreId,\n+                    keySchema: StructType,\n+                    valueSchema: StructType,\n+                    keyIndexOrdinal: Option[ Int ], // for sorting the data by their keys\n+                    storeConfs: StateStoreConf,\n+                    hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs.\n+      filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+  }\n+\n+   /*\n+    * Return the id of the StateStores this provider will generate.\n+    * Should be the same as the one passed in init().\n+    */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+   /*\n+    * Optional custom metrics that the implementation may want to report.\n+    *\n+    * @note The StateStore objects created by this provider must report the same custom metrics\n+    *       (specifically, same names) through `StateStore.metrics`.\n+    */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[ StateStoreCustomMetric ] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+     try {\n+       if (checkIfStateExists(version - 1) ) {\n+         found = true\n+         lastAvailableVersion = version - 1\n+       } else {\n+         // TODO check for numberOfVersionsToRetain\n+         // Destroy DB so that we can recontruct it using snapshot and delta files\n+         RocksDbInstance.destroyDB(rocksDbPath)\n+       }\n+\n+       // Check for snapshot files starting from \"version\"\n+       while (!found && lastAvailableVersion > 0) {\n+         found = {\n+           try {\n+             loadSnapshotFile(lastAvailableVersion)\n+           } catch {\n+             case e: Exception =>\n+               logError(s\"$e while reading snapshot file\")\n+               throw e\n+           }\n+         }\n+         if (!found) {\n+           lastAvailableVersion = lastAvailableVersion - 1\n+         }\n+         logInfo(s\"Snapshot for $lastAvailableVersion for \" +\n+           s\"partition ${stateStoreId_.partitionId} found = $found\")\n+       }\n+\n+       rocksDbWriteInstance = new OptimisticTransactionDbInstance(\n+         keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+       // Load all the deltas from the version after the last available\n+       // one up to the target version.\n+       // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+       for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+         val fileToRead = deltaFile(baseDir, deltaVersion)\n+         updateFromDeltaFile(fm, fileToRead, keySchema, valueSchema,\n+           rocksDbWriteInstance, sparkConf)\n+         logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+       }\n+\n+       rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+       rocksDbWriteInstance.close()\n+       rocksDbWriteInstance = null\n+     }\n+     catch {\n+       case e: IllegalStateException =>\n+         logError(s\"Exception while loading state ${e.getMessage}\")\n+         if (rocksDbWriteInstance != null) {\n+           rocksDbWriteInstance.abort()\n+           rocksDbWriteInstance.close()\n+         }\n+         throw e\n+     }\n+   }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))",
    "line": 435
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Sure. Will make the changes.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-06-21T04:25:25Z",
    "diffHunk": "@@ -0,0 +1,625 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+\n+   /*\n+    * Additional configurations related to rocksDb. This will capture all configs in\n+    * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+    */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long)\n+    extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+       rocksDbWriteInstance = new OptimisticTransactionDbInstance(\n+          keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state. \" +\n+        s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange( start: Option[UnsafeRow],\n+                           end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state \" +\n+        s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(\n+            s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+          storeMap.remove(version)\n+          close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+          storeMap.remove(version)\n+          close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if ( rocksDbWriteInstance != null ) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+     /*\n+      * Get an iterator of all the store data.\n+      * This can be called only after committing all the updates made in the current thread.\n+      */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use checkpointed db for previous version\n+          logDebug(s\"state = loaded/aborted using checkpointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using checkpointed DB with version $newVersion\")\n+          // use checkpointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap { case (name, value) =>\n+        // just allow searching from list cause the list is small enough\n+        supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0), customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+   /*\n+    * Initialize the provide with more contextual information from the SQL operator.\n+    * This method will be called first after creating an instance of the StateStoreProvider by\n+    * reflection.\n+    *\n+    * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+    * @param keySchema       Schema of keys to be stored\n+    * @param valueSchema     Schema of value to be stored\n+    * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+    *                        which the StateStore implementation could index the data.\n+    * @param storeConfs      Configurations used by the StateStores\n+    * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+    *                        to save state data\n+    */\n+  override def init(stateStoreId: StateStoreId,\n+                    keySchema: StructType,\n+                    valueSchema: StructType,\n+                    keyIndexOrdinal: Option[ Int ], // for sorting the data by their keys\n+                    storeConfs: StateStoreConf,\n+                    hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs.\n+      filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+  }\n+\n+   /*\n+    * Return the id of the StateStores this provider will generate.\n+    * Should be the same as the one passed in init().\n+    */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+   /*\n+    * Optional custom metrics that the implementation may want to report.\n+    *\n+    * @note The StateStore objects created by this provider must report the same custom metrics\n+    *       (specifically, same names) through `StateStore.metrics`.\n+    */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[ StateStoreCustomMetric ] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+     try {\n+       if (checkIfStateExists(version - 1) ) {\n+         found = true\n+         lastAvailableVersion = version - 1\n+       } else {\n+         // TODO check for numberOfVersionsToRetain\n+         // Destroy DB so that we can recontruct it using snapshot and delta files\n+         RocksDbInstance.destroyDB(rocksDbPath)\n+       }\n+\n+       // Check for snapshot files starting from \"version\"\n+       while (!found && lastAvailableVersion > 0) {\n+         found = {\n+           try {\n+             loadSnapshotFile(lastAvailableVersion)\n+           } catch {\n+             case e: Exception =>\n+               logError(s\"$e while reading snapshot file\")\n+               throw e\n+           }\n+         }\n+         if (!found) {\n+           lastAvailableVersion = lastAvailableVersion - 1\n+         }\n+         logInfo(s\"Snapshot for $lastAvailableVersion for \" +\n+           s\"partition ${stateStoreId_.partitionId} found = $found\")\n+       }\n+\n+       rocksDbWriteInstance = new OptimisticTransactionDbInstance(\n+         keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+       // Load all the deltas from the version after the last available\n+       // one up to the target version.\n+       // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+       for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+         val fileToRead = deltaFile(baseDir, deltaVersion)\n+         updateFromDeltaFile(fm, fileToRead, keySchema, valueSchema,\n+           rocksDbWriteInstance, sparkConf)\n+         logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+       }\n+\n+       rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+       rocksDbWriteInstance.close()\n+       rocksDbWriteInstance = null\n+     }\n+     catch {\n+       case e: IllegalStateException =>\n+         logError(s\"Exception while loading state ${e.getMessage}\")\n+         if (rocksDbWriteInstance != null) {\n+           rocksDbWriteInstance.abort()\n+           rocksDbWriteInstance.close()\n+         }\n+         throw e\n+     }\n+   }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))",
    "line": 435
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Done",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-06-26T09:07:46Z",
    "diffHunk": "@@ -0,0 +1,625 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+\n+   /*\n+    * Additional configurations related to rocksDb. This will capture all configs in\n+    * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+    */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long)\n+    extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+       rocksDbWriteInstance = new OptimisticTransactionDbInstance(\n+          keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state. \" +\n+        s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange( start: Option[UnsafeRow],\n+                           end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state \" +\n+        s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(\n+            s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+          storeMap.remove(version)\n+          close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+          storeMap.remove(version)\n+          close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if ( rocksDbWriteInstance != null ) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+     /*\n+      * Get an iterator of all the store data.\n+      * This can be called only after committing all the updates made in the current thread.\n+      */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use checkpointed db for previous version\n+          logDebug(s\"state = loaded/aborted using checkpointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using checkpointed DB with version $newVersion\")\n+          // use checkpointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap { case (name, value) =>\n+        // just allow searching from list cause the list is small enough\n+        supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0), customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+   /*\n+    * Initialize the provide with more contextual information from the SQL operator.\n+    * This method will be called first after creating an instance of the StateStoreProvider by\n+    * reflection.\n+    *\n+    * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+    * @param keySchema       Schema of keys to be stored\n+    * @param valueSchema     Schema of value to be stored\n+    * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+    *                        which the StateStore implementation could index the data.\n+    * @param storeConfs      Configurations used by the StateStores\n+    * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+    *                        to save state data\n+    */\n+  override def init(stateStoreId: StateStoreId,\n+                    keySchema: StructType,\n+                    valueSchema: StructType,\n+                    keyIndexOrdinal: Option[ Int ], // for sorting the data by their keys\n+                    storeConfs: StateStoreConf,\n+                    hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs.\n+      filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+  }\n+\n+   /*\n+    * Return the id of the StateStores this provider will generate.\n+    * Should be the same as the one passed in init().\n+    */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+   /*\n+    * Optional custom metrics that the implementation may want to report.\n+    *\n+    * @note The StateStore objects created by this provider must report the same custom metrics\n+    *       (specifically, same names) through `StateStore.metrics`.\n+    */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[ StateStoreCustomMetric ] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+     try {\n+       if (checkIfStateExists(version - 1) ) {\n+         found = true\n+         lastAvailableVersion = version - 1\n+       } else {\n+         // TODO check for numberOfVersionsToRetain\n+         // Destroy DB so that we can recontruct it using snapshot and delta files\n+         RocksDbInstance.destroyDB(rocksDbPath)\n+       }\n+\n+       // Check for snapshot files starting from \"version\"\n+       while (!found && lastAvailableVersion > 0) {\n+         found = {\n+           try {\n+             loadSnapshotFile(lastAvailableVersion)\n+           } catch {\n+             case e: Exception =>\n+               logError(s\"$e while reading snapshot file\")\n+               throw e\n+           }\n+         }\n+         if (!found) {\n+           lastAvailableVersion = lastAvailableVersion - 1\n+         }\n+         logInfo(s\"Snapshot for $lastAvailableVersion for \" +\n+           s\"partition ${stateStoreId_.partitionId} found = $found\")\n+       }\n+\n+       rocksDbWriteInstance = new OptimisticTransactionDbInstance(\n+         keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+       // Load all the deltas from the version after the last available\n+       // one up to the target version.\n+       // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+       for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+         val fileToRead = deltaFile(baseDir, deltaVersion)\n+         updateFromDeltaFile(fm, fileToRead, keySchema, valueSchema,\n+           rocksDbWriteInstance, sparkConf)\n+         logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+       }\n+\n+       rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+       rocksDbWriteInstance.close()\n+       rocksDbWriteInstance = null\n+     }\n+     catch {\n+       case e: IllegalStateException =>\n+         logError(s\"Exception while loading state ${e.getMessage}\")\n+         if (rocksDbWriteInstance != null) {\n+           rocksDbWriteInstance.abort()\n+           rocksDbWriteInstance.close()\n+         }\n+         throw e\n+     }\n+   }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))",
    "line": 435
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "merge with line before this line",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-06-25T04:37:26Z",
    "diffHunk": "@@ -0,0 +1,625 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+\n+   /*\n+    * Additional configurations related to rocksDb. This will capture all configs in\n+    * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+    */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long)\n+    extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+       rocksDbWriteInstance = new OptimisticTransactionDbInstance(\n+          keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state. \" +\n+        s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange( start: Option[UnsafeRow],\n+                           end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(state == UPDATING, s\"Current state of the store is $state \" +\n+        s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(\n+            s\"Error committing version $newVersion into $this\", e)"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "`private[sql]` or narrower scope would work. public is too broad only for unit test.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-01T00:34:03Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"backup\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  private def getTempPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"tmp\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  // making it public for unit tests"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "will fix it",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T10:35:33Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"backup\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  private def getTempPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"tmp\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  // making it public for unit tests"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This should be realistic path, like spark local directory or temporary directory. In many cases it will try to create '/media' in root directory and fail.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-01T00:37:02Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"backup\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  private def getTempPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"tmp\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  // making it public for unit tests\n+  def getLocalDirectory: String = localDirectory\n+}\n+\n+object RocksDbStateStoreProvider {\n+\n+  val ROCKS_DB_BASE_PATH: String = \"/media/ephemeral0/spark/rocksdb\""
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "will make the changes.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T10:35:24Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"backup\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  private def getTempPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"tmp\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  // making it public for unit tests\n+  def getLocalDirectory: String = localDirectory\n+}\n+\n+object RocksDbStateStoreProvider {\n+\n+  val ROCKS_DB_BASE_PATH: String = \"/media/ephemeral0/spark/rocksdb\""
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "implementation is nearly same as `getTempPath`. Please deduplicate.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-01T00:41:33Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "will do",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T10:35:01Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "`private[sql]` or narrower scope would work. public is too broad only for unit test.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-01T00:41:50Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Would be better if we could place fields together.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:02:13Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Will make the changes.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T10:34:45Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Could we deduplicate this one too? (Sorry I'm reading the code backward.)",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-01T00:42:46Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Will do",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T10:34:55Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Please address this sooner - either fail the query or do workaround. Let's not leave this as undefined behavior.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-01T00:47:47Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "And same here. HDFS state store provider \"loads\" the version if it's not cached in memory. RocksDB state store provider should guarantee similar one. Let's make it safe.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:38:01Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "import order looks incorrect",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T05:46:38Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "fixed",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:13:23Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: `RocksDB` as it is not used for variable or class name.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T05:49:02Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "fixed",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:14:03Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Maybe better to add `()` as it doesn't only get some value but modifies something. If we are OK with removing `()` I'd rather remove `()` in method definition as well.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:04:56Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "fixed",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:18:36Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Let's make it be consistent - having this as one-liner as same as HDFS state store provider seems to be enough. Do you encounter any issue on this?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:09:47Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify("
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Since I have added more states, I wanted to include the current state in the log message for better debuggability in case of any failures. Can remove that if we dont see any value in logging current state.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T09:41:27Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify("
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "ditto.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:10:11Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Let's not leave TODO - please address this in this PR.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:10:43Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "ditto",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:11:36Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify("
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "ditto",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:11:41Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Just curious, is there any reason to change order of lines? The order of changing state, committing delta file, and updating backend are all different compared to HDFS state store. If the order doesn't matter, please leave them as same order so that it can be refactored easily.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:15:21Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Commit delta files need to be done after committing the rocksdb instance (and taking backup). Changing state can happen anytime. I will change the order so that committing state is same as HDFS state store",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T09:45:18Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "According to RocksDB FAQ, better to comment here it's an \"estimated\" value due to the nature of RocksDB.\r\nhttps://github.com/facebook/rocksdb/wiki/RocksDB-FAQ\r\n",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:18:09Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L",
    "line": 111
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "added",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:17:18Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L",
    "line": 111
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "According to the FAQ, `numEntriesInDb` here is an estimated value, as well as some of entries are tombstone, so this value is also become \"estimated\" and even very roughly. Better to comment here as well.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:24:26Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L",
    "line": 112
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: unnecessary `return`",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:50:10Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics("
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "removed",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:19:54Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics("
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "private",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T06:52:40Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "added",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:22:19Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Do we need to apply uppercase on number string?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:02:14Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Nope. removed it.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:22:14Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "If it deals with local filesystem, we can just change these lines as one-liner, `new File(rocksDbPath, version.toString).exists()`.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:05:47Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "done",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:23:26Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Long lines with multiple `var`s (especially flags) might indicate that this method could be broken down to multiple pieces which may not need `var`.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:10:22Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Doesn't look necessary to place them for each line - two lines would be enough.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:13:51Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile("
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "We may not want to print out this for all delta versions before snapshot. DEBUG log level or only log once when we find nearest snapshot version.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:18:37Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo("
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This while loop is only used when reconstructing RocksDB - then why not putting it to `else` and move `found` flag to here to narrow the scope of found?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:20:09Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\""
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I guess below works exactly same and reduce indentation.\r\n\r\n```\r\ntry {\r\n  found = loadSnapshotFile(lastAvailableVersion)\r\n} catch {\r\n  case e: Exception =>\r\n    logError(s\"$e while reading snapshot file\")\r\n    throw e\r\n}\r\n```",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:23:03Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Changed.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:25:58Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "TODO or no longer valid?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:32:30Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "no longer valid. removed the todo",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:20:53Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Looks like `result` is not used, then let's use `_`.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:33:39Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Done",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T08:23:51Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: `recontruct` -> `reconstruct`",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:33:54Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Looks like we can break down this method to two pieces: 1) load latest snapshot with returning its version, 2) apply delta from snapshot version + 1 to version. Two vars are only needed only one method - they may not be strictly needed to be var.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:39:19Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance ="
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "`concat` doesn't look to differ from using \"+\" and latter is simpler, string interpolation rocks.\r\n\r\nYou can even change two lines to below (one line longer but avoid calling getTempPath twice):\r\n\r\n```\r\nval versionTempPath = getTempPath(version)\r\nval tmpLocDir: File = new File(versionTempPath)\r\nval tmpLocFile: File = new File(s\"${versionTempPath}.tar\")\r\n```\r\n",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:44:35Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This can just be placed earlier (just after `fileToRead`) to do early return. Otherwise let's avoid explicitly using `return`.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:45:56Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "No need to be here, as this is unreachable if we use this pattern in try block:\r\n\r\n```\r\nif (...) {\r\n  true\r\n} else {\r\n  false\r\n}\r\n```\r\n\r\nEven no need to add `return`.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:49:56Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: `Snaphot` -> `Snapshot`",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:51:31Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Might be better to let caller specify where to extract. Better flexibility.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T07:53:02Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This is correct but not exhaustive. No way to indicate the corruption anyway if some sst files are missing.\r\n\r\nIf we're applying checksum correctly, we might be OK to skip checking this. Do you encounter similar issue while testing/using?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:00:19Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "ditto.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:07:10Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Does the provider guarantee `dbPath` always contains the content of the version? HDFS state store provider always \"loads\" the version first and then snapshot it, so it guarantees snapshot is properly created.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:18:09Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)",
    "line": 513
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "if `dbPath` exists, as per the code semantics, it will have backed up version. Every time we commit/close a version, a backed up version is created at a different folder location.\r\n\r\nI will make changes to check if `dbPath` exists before creating TarFile\r\n\r\nCreating backup helped in physical isolation between current active DB and prev versions but it comes with a cost. (Every micro-batch takes some extra time to complete). I will try to find any other alternative to save this cost.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T10:06:28Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)",
    "line": 513
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "It could be placed in same line.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:20:40Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: remove `}`",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:21:03Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "ditto.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:23:43Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: enclosing paren not needed.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:25:57Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Scala is not that smart enough to automatically create decreasing range. \r\n`(1 to (earliestVersionToRetain -1)).reverse` or `((earliestVersionToRetain -1) to 1 by -1)` will do the trick.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:32:57Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Btw, what if version is going to be pretty huge like 1000 or so? I know this is simpler, but you may still need to list the directory and find things to delete.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:34:57Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "not used. can be removed.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:35:40Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'd rather not catch this and propagate exception to caller - test should be failed.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:40:30Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "We can remove `return` here and the last line of method if we don't catch exception.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T08:41:13Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: s/provide/provider",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T12:24:12Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator."
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Would be good to handle false return value (this applies to other places as well).",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T15:22:09Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Have handled it now in FileUtility. Remove the delete call here and will let createTarFile take care of deleting the existing destination filename.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T09:51:07Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "There are small number of windows users where `Path.SEPARATOR` was problem. Not sure this is the case here but worth to mention.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-05T15:24:06Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"backup\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  private def getTempPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"tmp\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version"
  }, {
    "author": {
      "login": "itsvikramagr"
    },
    "body": "Will use File to create the path.",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-08T10:14:26Z",
    "diffHunk": "@@ -0,0 +1,652 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state;\n+\n+import java.io._\n+import java.util\n+import java.util.Locale\n+\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.{SparkConf, SparkEnv}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.io.FileUtility\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.sql.execution.streaming.CheckpointFileManager\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Logging {\n+\n+  /* Internal fields and methods */\n+  @volatile private var stateStoreId_ : StateStoreId = _\n+  @volatile private var keySchema: StructType = _\n+  @volatile private var valueSchema: StructType = _\n+  @volatile private var storeConf: StateStoreConf = _\n+  @volatile private var hadoopConf: Configuration = _\n+  @volatile private var numberOfVersionsToRetain: Int = _\n+  @volatile private var localDirectory: String = _\n+\n+  /*\n+   * Additional configurations related to rocksDb. This will capture all configs in\n+   * SQLConf that start with `spark.sql.streaming.stateStore.rocksDb`\n+   */\n+  @volatile private var rocksDbConf: Map[String, String] = Map.empty[String, String]\n+\n+  private lazy val baseDir: Path = stateStoreId.storeCheckpointLocation()\n+  private lazy val fm = CheckpointFileManager.create(baseDir, hadoopConf)\n+  private lazy val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)\n+  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)\n+\n+  import WALUtils._\n+\n+  /** Implementation of [[StateStore]] API which is backed by RocksDb and HDFS */\n+  class RocksDbStateStore(val version: Long) extends StateStore with Logging {\n+\n+    /** Trait and classes representing the internal state of the store */\n+    trait STATE\n+    case object LOADED extends STATE\n+    case object UPDATING extends STATE\n+    case object COMMITTED extends STATE\n+    case object ABORTED extends STATE\n+\n+    private val newVersion = version + 1\n+    @volatile private var state: STATE = LOADED\n+    private val finalDeltaFile: Path = deltaFile(baseDir, newVersion)\n+    private lazy val deltaFileStream = fm.createAtomic(finalDeltaFile, overwriteIfPossible = true)\n+    private lazy val compressedStream = compressStream(deltaFileStream, sparkConf)\n+\n+    override def id: StateStoreId = RocksDbStateStoreProvider.this.stateStoreId\n+\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var numEntriesInDb: Long = 0L\n+    var bytesUsedByDb: Long = 0L\n+\n+    private def initTransaction(): Unit = {\n+      if (state == LOADED && rocksDbWriteInstance == null) {\n+        logDebug(s\"Creating Transactional DB for batch $version\")\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, newVersion.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        state = UPDATING\n+        rocksDbWriteInstance.startTransactions()\n+      }\n+    }\n+\n+    override def get(key: UnsafeRow): UnsafeRow = {\n+      initTransaction\n+      rocksDbWriteInstance.get(key)\n+    }\n+\n+    override def put(key: UnsafeRow, value: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state. \" +\n+          s\"Cannot put after already committed or aborted\")\n+      val keyCopy = key.copy()\n+      val valueCopy = value.copy()\n+      rocksDbWriteInstance.put(keyCopy, valueCopy)\n+      writeUpdateToDeltaFile(compressedStream, keyCopy, valueCopy)\n+    }\n+\n+    override def remove(key: UnsafeRow): Unit = {\n+      initTransaction\n+      verify(state == UPDATING, \"Cannot remove after already committed or aborted\")\n+      rocksDbWriteInstance.remove(key)\n+      // TODO check if removed value is null\n+      writeRemoveToDeltaFile(compressedStream, key)\n+    }\n+\n+    override def getRange(\n+        start: Option[UnsafeRow],\n+        end: Option[UnsafeRow]): Iterator[UnsafeRowPair] = {\n+      verify(state == UPDATING, \"Cannot getRange after already committed or aborted\")\n+      iterator()\n+    }\n+\n+    /** Commit all the updates that have been made to the store, and return the new version. */\n+    override def commit(): Long = {\n+      initTransaction\n+      verify(\n+        state == UPDATING,\n+        s\"Current state of the store is $state \" +\n+          s\"Cannot commit after already committed or aborted\")\n+      try {\n+        state = COMMITTED\n+        synchronized {\n+          rocksDbWriteInstance.commit(Some(getBackupPath(newVersion)))\n+          finalizeDeltaFile(compressedStream)\n+        }\n+        numEntriesInDb = rocksDbWriteInstance.otdb.getLongProperty(\"rocksdb.estimate-num-keys\")\n+        bytesUsedByDb = numEntriesInDb * (keySchema.defaultSize + valueSchema.defaultSize)\n+        newVersion\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error committing version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    /*\n+     * Abort all the updates made on this store. This store will not be usable any more.\n+     */\n+    override def abort(): Unit = {\n+      // This if statement is to ensure that files are deleted only if there are changes to the\n+      // StateStore. We have two StateStores for each task, one which is used only for reading, and\n+      // the other used for read+write. We don't want the read-only to delete state files.\n+      try {\n+        if (state == UPDATING) {\n+          state = ABORTED\n+          synchronized {\n+            rocksDbWriteInstance.abort()\n+            cancelDeltaFile(compressedStream, deltaFileStream)\n+          }\n+          logInfo(s\"Aborted version $newVersion for $this\")\n+        } else {\n+          state = ABORTED\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          throw new IllegalStateException(s\"Error aborting version $newVersion into $this\", e)\n+      } finally {\n+        storeMap.remove(version)\n+        close()\n+      }\n+    }\n+\n+    def close(): Unit = {\n+      if (rocksDbWriteInstance != null) {\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      }\n+    }\n+\n+    /*\n+     * Get an iterator of all the store data.\n+     * This can be called only after committing all the updates made in the current thread.\n+     */\n+    override def iterator(): Iterator[UnsafeRowPair] = {\n+      state match {\n+        case UPDATING =>\n+          logDebug(\"state = updating using transaction DB\")\n+          // We need to use current db to read uncommitted transactions\n+          rocksDbWriteInstance.iterator(closeDbOnCompletion = false)\n+\n+        case LOADED | ABORTED =>\n+          // use check-pointed db for previous version\n+          logDebug(s\"state = loaded/aborted using check-pointed DB with version $version\")\n+          if (version == 0) {\n+            Iterator.empty\n+          } else {\n+            val path = getBackupPath(version)\n+            val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, version.toString)\n+            r.open(path, rocksDbConf, readOnly = true)\n+            r.iterator(closeDbOnCompletion = true)\n+          }\n+        case COMMITTED =>\n+          logDebug(s\"state = committed using check-pointed DB with version $newVersion\")\n+          // use check-pointed db for current updated version\n+          val path = getBackupPath(newVersion)\n+          val r: RocksDbInstance =\n+            new RocksDbInstance(keySchema, valueSchema, newVersion.toString)\n+          r.open(path, rocksDbConf, readOnly = true)\n+          r.iterator(closeDbOnCompletion = true)\n+\n+        case _ => Iterator.empty\n+      }\n+    }\n+\n+    override def metrics: StateStoreMetrics = {\n+      val metricsFromProvider: Map[String, Long] = getMetricsForProvider()\n+      val customMetrics = metricsFromProvider.flatMap {\n+        case (name, value) =>\n+          // just allow searching from list cause the list is small enough\n+          supportedCustomMetrics.find(_.name == name).map(_ -> value)\n+      }\n+      return StateStoreMetrics(\n+        Math.max(numEntriesInDb, 0),\n+        Math.max(bytesUsedByDb, 0),\n+        customMetrics)\n+    }\n+\n+    /*\n+     * Whether all updates have been committed\n+     */\n+    override def hasCommitted: Boolean = {\n+      state == COMMITTED\n+    }\n+\n+    override def toString(): String = {\n+      s\"RocksDbStateStore[id=(op=${id.operatorId},part=${id.partitionId}),dir=$baseDir]\"\n+    }\n+\n+  }\n+\n+  /*\n+   * Initialize the provide with more contextual information from the SQL operator.\n+   * This method will be called first after creating an instance of the StateStoreProvider by\n+   * reflection.\n+   *\n+   * @param stateStoreId    Id of the versioned StateStores that this provider will generate\n+   * @param keySchema       Schema of keys to be stored\n+   * @param valueSchema     Schema of value to be stored\n+   * @param keyIndexOrdinal Optional column (represent as the ordinal of the field in keySchema) by\n+   *                        which the StateStore implementation could index the data.\n+   * @param storeConfs      Configurations used by the StateStores\n+   * @param hadoopConf      Hadoop configuration that could be used by StateStore\n+   *                        to save state data\n+   */\n+  override def init(\n+      stateStoreId: StateStoreId,\n+      keySchema: StructType,\n+      valueSchema: StructType,\n+      keyIndexOrdinal: Option[Int], // for sorting the data by their keys\n+      storeConfs: StateStoreConf,\n+      hadoopConf: Configuration): Unit = {\n+    this.stateStoreId_ = stateStoreId\n+    this.keySchema = keySchema\n+    this.valueSchema = valueSchema\n+    this.storeConf = storeConfs\n+    this.hadoopConf = hadoopConf\n+    // TODO add new conf for `maxVersionsToRetainInMemory`\n+    this.numberOfVersionsToRetain = storeConfs.maxVersionsToRetainInMemory\n+    fm.mkdirs(baseDir)\n+    this.rocksDbConf = storeConf.confs\n+      .filter(_._1.startsWith(\"spark.sql.streaming.stateStore.rocksDb\"))\n+      .map {\n+        case (k, v) => (k.toLowerCase(Locale.ROOT), v)\n+      }\n+    this.localDirectory = this.rocksDbConf\n+      .getOrElse(\n+        \"spark.sql.streaming.stateStore.rocksDb.localDirectory\".toLowerCase(Locale.ROOT),\n+        RocksDbStateStoreProvider.ROCKS_DB_BASE_PATH)\n+  }\n+\n+  /*\n+   * Return the id of the StateStores this provider will generate.\n+   * Should be the same as the one passed in init().\n+   */\n+  override def stateStoreId: StateStoreId = stateStoreId_\n+\n+  /*\n+   * Called when the provider instance is unloaded from the executor\n+   */\n+  override def close(): Unit = {\n+    storeMap.values.asScala.foreach(_.close)\n+    storeMap.clear()\n+  }\n+\n+  private val storeMap = new util.HashMap[Long, RocksDbStateStore]()\n+\n+  /*\n+   * Optional custom metrics that the implementation may want to report.\n+   *\n+   * @note The StateStore objects created by this provider must report the same custom metrics\n+   *       (specifically, same names) through `StateStore.metrics`.\n+   */\n+  // TODO\n+  override def supportedCustomMetrics: Seq[StateStoreCustomMetric] = {\n+    Nil\n+  }\n+\n+  override def toString(): String = {\n+    s\"RocksDbStateStoreProvider[\" +\n+      s\"id = (op=${stateStoreId.operatorId},part=${stateStoreId.partitionId}),dir = $baseDir]\"\n+  }\n+\n+  def getMetricsForProvider(): Map[String, Long] = synchronized {\n+    Map.empty[String, Long]\n+  }\n+\n+  /*\n+   * Return an instance of [[StateStore]] representing state data of the given version\n+   */\n+  override def getStore(version: Long): StateStore = synchronized {\n+    logInfo(s\"get Store for version $version\")\n+    require(version >= 0, \"Version cannot be less than 0\")\n+    if (storeMap.containsKey(version)) {\n+      storeMap.get(version)\n+    } else {\n+      val store = createStore(version)\n+      storeMap.put(version, store)\n+      store\n+    }\n+  }\n+\n+  def createStore(version: Long): RocksDbStateStore = {\n+    val newStore = new RocksDbStateStore(version)\n+    logInfo(\n+      s\"Creating a new Store for version $version and partition ${stateStoreId_.partitionId}\")\n+    if (version > 0 & !checkIfStateExists(version)) {\n+      // load the data in the rocksDB\n+      logInfo(s\"Loading state for $version and partition ${stateStoreId_.partitionId}\")\n+      loadState(version)\n+    }\n+    newStore\n+  }\n+\n+  def checkIfStateExists(version: Long): Boolean = {\n+    val dbPath: Path = new Path(rocksDbPath, version.toString.toUpperCase(Locale.ROOT))\n+    val f: File = new File(dbPath.toString)\n+    f.exists()\n+  }\n+\n+  def loadState(version: Long): Unit = {\n+    // search for state on snapshot\n+    var rocksDbWriteInstance: OptimisticTransactionDbInstance = null\n+    var lastAvailableVersion = version\n+    var found = false\n+    val (result, elapsedMs) = Utils.timeTakenMs {\n+      try {\n+        if (checkIfStateExists(version - 1)) {\n+          found = true\n+          lastAvailableVersion = version - 1\n+        } else {\n+          // Destroy DB so that we can recontruct it using snapshot and delta files\n+          RocksDbInstance.destroyDB(rocksDbPath)\n+        }\n+\n+        // Check for snapshot files starting from \"version\"\n+        while (!found && lastAvailableVersion > 0) {\n+          found = {\n+            try {\n+              loadSnapshotFile(lastAvailableVersion)\n+            } catch {\n+              case e: Exception =>\n+                logError(s\"$e while reading snapshot file\")\n+                throw e\n+            }\n+          }\n+          if (!found) {\n+            lastAvailableVersion = lastAvailableVersion - 1\n+          }\n+          logInfo(\n+            s\"Snapshot for $lastAvailableVersion for \" +\n+              s\"partition ${stateStoreId_.partitionId} found = $found\")\n+        }\n+\n+        rocksDbWriteInstance =\n+          new OptimisticTransactionDbInstance(keySchema, valueSchema, version.toString)\n+        rocksDbWriteInstance.open(rocksDbPath, rocksDbConf)\n+        rocksDbWriteInstance.startTransactions()\n+\n+        // Load all the deltas from the version after the last available\n+        // one up to the target version.\n+        // The last available version is the one with a full snapshot, so it doesn't need deltas.\n+        for (deltaVersion <- (lastAvailableVersion + 1) to version) {\n+          val fileToRead = deltaFile(baseDir, deltaVersion)\n+          updateFromDeltaFile(\n+            fm,\n+            fileToRead,\n+            keySchema,\n+            valueSchema,\n+            rocksDbWriteInstance,\n+            sparkConf)\n+          logInfo(s\"Read delta file for version $version of $this from $fileToRead\")\n+        }\n+\n+        rocksDbWriteInstance.commit(Some(getBackupPath(version)))\n+        rocksDbWriteInstance.close()\n+        rocksDbWriteInstance = null\n+      } catch {\n+        case e: IllegalStateException =>\n+          logError(s\"Exception while loading state ${e.getMessage}\")\n+          if (rocksDbWriteInstance != null) {\n+            rocksDbWriteInstance.abort()\n+            rocksDbWriteInstance.close()\n+          }\n+          throw e\n+      }\n+    }\n+    logInfo(s\"Loading state for $version takes $elapsedMs ms.\")\n+  }\n+\n+  private def loadSnapshotFile(version: Long): Boolean = {\n+    val fileToRead = snapshotFile(baseDir, version)\n+    val tmpLocDir: File = new File(getTempPath(version))\n+    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    try {\n+      if (!fm.exists(fileToRead)) {\n+        return false\n+      }\n+      logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n+      if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n+        FileUtility.unTar(tmpLocFile)\n+        if (!tmpLocDir.list().exists(_.endsWith(\".sst\"))) {\n+          logWarning(\"Snaphot files are corrupted\")\n+          throw new IOException(\n+            s\"Error reading snapshot file $fileToRead of $this:\" +\n+              s\" No SST files found\")\n+        }\n+        FileUtils.moveDirectory(tmpLocDir, new File(rocksDbPath))\n+        return true\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Exception while loading snapshot file $e\")\n+        throw e\n+    } finally {\n+      if (tmpLocFile.exists()) {\n+        tmpLocFile.delete()\n+      }\n+      FileUtils.deleteDirectory(tmpLocDir)\n+    }\n+    return false\n+  }\n+\n+  /** Optional method for providers to allow for background maintenance (e.g. compactions) */\n+  override def doMaintenance(): Unit = {\n+    try {\n+      val (files: Seq[WALUtils.StoreFile], e1) = Utils.timeTakenMs(fetchFiles(fm, baseDir))\n+      logDebug(s\"fetchFiles() took $e1 ms.\")\n+      doSnapshot(files)\n+      cleanup(files)\n+      cleanRocksDBBackupInstances(files)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error performing snapshot and cleaning up $this\")\n+    }\n+  }\n+\n+  private def doSnapshot(files: Seq[WALUtils.StoreFile]): Unit = {\n+    if (files.nonEmpty) {\n+      val lastVersion = files.last.version\n+      val deltaFilesForLastVersion =\n+        filesForVersion(files, lastVersion).filter(_.isSnapshot == false)\n+      if (deltaFilesForLastVersion.size > storeConf.minDeltasForSnapshot) {\n+        val dbPath = getBackupPath(lastVersion)\n+        val snapShotFileName = getTempPath(lastVersion).concat(\".snapshot\")\n+        val f = new File(snapShotFileName)\n+        f.delete() // delete any existing tarball\n+        try {\n+          val (_, t1) = Utils.timeTakenMs {\n+            FileUtility.createTarFile(dbPath, snapShotFileName)\n+            val targetFile = snapshotFile(baseDir, lastVersion)\n+            uploadFile(fm, new Path(snapShotFileName), targetFile, sparkConf)\n+          }\n+          logInfo(\n+            s\"Creating snapshot file for\" +\n+              s\" ${stateStoreId_.partitionId} took $t1 ms.\")\n+        } catch {\n+          case e: Exception =>\n+            logError(s\"Exception while creating snapshot $e} \")\n+            throw e\n+        } finally {\n+          f.delete() // delete the tarball\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Clean up old snapshots and delta files that are not needed any more. It ensures that last\n+   * few versions of the store can be recovered from the files, so re-executed RDD operations\n+   * can re-apply updates on the past versions of the store.\n+   */\n+  private[state] def cleanup(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head\n+          val filesToDelete = files.filter(_.version < earliestFileToRetain.version)\n+          val (_, e2) = Utils.timeTakenMs {\n+            filesToDelete.foreach { f =>\n+              fm.delete(f.path)\n+              val file = new File(rocksDbPath, f.version.toString.toUpperCase(Locale.ROOT))\n+              if (file.exists()) {\n+                file.delete()\n+              }\n+            }\n+          }\n+          logDebug(s\"deleting files took $e2 ms.\")\n+          logInfo(\n+            s\"Deleted files older than ${earliestFileToRetain.version} for $this: \" +\n+              filesToDelete.mkString(\", \"))\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  private def cleanRocksDBBackupInstances(files: Seq[WALUtils.StoreFile]): Unit = {\n+    try {\n+      if (files.nonEmpty) {\n+        val earliestVersionToRetain = files.last.version - storeConf.minVersionsToRetain\n+        if (earliestVersionToRetain > 0) {\n+          for (v <- (earliestVersionToRetain - 1) to 1) {\n+            // Destroy the backup path\n+            logDebug((s\"Destroying backup version = $v\"))\n+            RocksDbInstance.destroyDB(getBackupPath(v))\n+          }\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) => logWarning(s\"Error cleaning up files for $this\", e)\n+    }\n+  }\n+\n+  // Used only for unit tests\n+  private[sql] def latestIterator(): Iterator[UnsafeRowPair] = synchronized {\n+    val versionsInFiles = fetchFiles(fm, baseDir).map(_.version).toSet\n+    var itr = Iterator.empty\n+    if (versionsInFiles.nonEmpty) {\n+      val maxVersion = versionsInFiles.max\n+      if (maxVersion == 0) {\n+        return Iterator.empty\n+      }\n+      // FIXME assuming maxVersion exists in rocksDB\n+      val path = getBackupPath(maxVersion)\n+      val r: RocksDbInstance = new RocksDbInstance(keySchema, valueSchema, maxVersion.toString)\n+      try {\n+        r.open(path, rocksDbConf, readOnly = true)\n+        return r.iterator(false)\n+      } catch {\n+        case e: Exception =>\n+        // do nothing\n+      }\n+    }\n+    Iterator.empty\n+  }\n+\n+  // making it public for unit tests\n+  lazy val rocksDbPath: String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"db\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+    dir\n+  }\n+\n+  private def getBackupPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"backup\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version\n+  }\n+\n+  private def getTempPath(version: Long): String = {\n+    val checkpointRootLocationPath = new Path(stateStoreId.checkpointRootLocation)\n+\n+    val basePath = new Path(\n+      localDirectory,\n+      new Path(\n+        \"tmp\",\n+        checkpointRootLocationPath.getName + \"_\" + checkpointRootLocationPath.hashCode()))\n+\n+    val dir = basePath.toString + Path.SEPARATOR +\n+      stateStoreId_.operatorId + Path.SEPARATOR +\n+      stateStoreId_.partitionId\n+\n+    val f: File = new File(dir)\n+\n+    if (!f.exists()) {\n+      logInfo(s\"creating rocksDb directory at : $dir\")\n+      f.mkdirs()\n+    }\n+\n+    dir + Path.SEPARATOR + version"
  }],
  "prId": 24922
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Maybe we want to pass `tmpLocFile.getAbsolutePath` instead?",
    "commit": "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "createdAt": "2019-07-09T13:01:08Z",
    "diffHunk": "@@ -425,17 +424,18 @@ private[sql] class RocksDbStateStoreProvider extends StateStoreProvider with Log\n \n   private def loadSnapshotFile(version: Long): Boolean = {\n     val fileToRead = snapshotFile(baseDir, version)\n-    val tmpLocDir: File = new File(getTempPath(version))\n-    val tmpLocFile: File = new File(getTempPath(version).concat(\".tar\"))\n+    if (!fm.exists(fileToRead)) {\n+      return false\n+    }\n+    val versionTempPath = getTempPath(version)\n+    val tmpLocDir: File = new File(versionTempPath)\n+    val tmpLocFile: File = new File(s\"${versionTempPath}.tar\")\n     try {\n-      if (!fm.exists(fileToRead)) {\n-        return false\n-      }\n       logInfo(s\"Will download $fileToRead at location ${tmpLocFile.toString()}\")\n       if (downloadFile(fm, fileToRead, new Path(tmpLocFile.getAbsolutePath), sparkConf)) {\n-        FileUtility.unTar(tmpLocFile)\n+        FileUtility.extractTarFile(s\"{versionTempPath}.tar\", versionTempPath)"
  }],
  "prId": 24922
}]