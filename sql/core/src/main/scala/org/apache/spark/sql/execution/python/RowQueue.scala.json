[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think this block is still slightly confusing because it's calling `iter.hasNext()` but never calls `iter.nex()`.\n\nIf `LinkedList`'s iterator implements `Iterator.remove()` then I think it would be preferable to use that.\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-22T20:34:47Z",
    "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ */\n+private[python] trait RowQueue {\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size\n+  // the first location where a new row would be written\n+  private var writeOffset = page.getBaseOffset\n+  // points to the start of the next row to read\n+  private var readOffset = page.getBaseOffset\n+  private val resultRow = new UnsafeRow(numFields)\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    val size = row.getSizeInBytes\n+    if (writeOffset + 4 + size > endOfPage) {\n+      // if there is not enough space in this page to hold the new record\n+      if (writeOffset + 4 <= endOfPage) {\n+        // if there's extra space at the end of the page, store a special \"end-of-page\" length (-1)\n+        Platform.putInt(base, writeOffset, -1)\n+      }\n+      false\n+    } else {\n+      Platform.putInt(base, writeOffset, size)\n+      Platform.copyMemory(row.getBaseObject, row.getBaseOffset, base, writeOffset + 4, size)\n+      writeOffset += 4 + size\n+      true\n+    }\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    if (readOffset + 4 > endOfPage || Platform.getInt(base, readOffset) < 0) {\n+      null\n+    } else {\n+      val size = Platform.getInt(base, readOffset)\n+      resultRow.pointTo(base, readOffset + 4, size)\n+      readOffset += 4 + size\n+      resultRow\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that is backed by a file on disk. This queue will stop accepting new rows once any\n+ * reader has begun reading from the queue.\n+ */\n+private[python] case class DiskRowQueue(file: File, fields: Int) extends RowQueue {\n+  private var fout = new FileOutputStream(file.toString)\n+  private var out = new DataOutputStream(new BufferedOutputStream(fout))\n+  private var unreadBytes = 0L\n+\n+  private var fin: FileInputStream = _\n+  private var in: DataInputStream = _\n+  private val resultRow = new UnsafeRow(fields)\n+\n+  def add(row: UnsafeRow): Boolean = synchronized {\n+    if (out == null) {\n+      // Another thread is reading, stop writing this one\n+      return false\n+    }\n+    out.writeInt(row.getSizeInBytes)\n+    out.write(row.getBytes)\n+    unreadBytes += 4 + row.getSizeInBytes\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = synchronized {\n+    if (out != null) {\n+      out.close()\n+      out = null\n+      fout.close()\n+      fout = null\n+\n+      fin = new FileInputStream(file.toString)\n+      in = new DataInputStream(new BufferedInputStream(fin))\n+    }\n+\n+    if (unreadBytes > 0) {\n+      val size = in.readInt()\n+      val bytes = new Array[Byte](size)\n+      in.readFully(bytes)\n+      unreadBytes -= 4 + size\n+      resultRow.pointTo(bytes, size)\n+      resultRow\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def close(): Unit = synchronized {\n+    Closeables.close(fout, true)\n+    fout = null\n+    Closeables.close(fin, true)\n+    fin = null\n+    if (file.exists()) {\n+      file.delete()\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that has a list of RowQueues, which could be in memory or disk.\n+ *\n+ * HybridRowQueue could be safely appended in one thread, and pulled in another thread in the same\n+ * time.\n+ */\n+private[python] case class HybridRowQueue(\n+    memManager: TaskMemoryManager,\n+    tempDir: File,\n+    numFields: Int)\n+  extends MemoryConsumer(memManager) with RowQueue {\n+\n+  // Each buffer should have at least one row\n+  private var queues = new java.util.LinkedList[RowQueue]()\n+\n+  private var writing: RowQueue = _\n+  private var reading: RowQueue = _\n+\n+  // exposed for testing\n+  private[python] def numQueues(): Int = queues.size()\n+\n+  def spill(size: Long, trigger: MemoryConsumer): Long = {\n+    if (trigger == this) {\n+      // When it's triggered by itself, it should write upcoming rows into disk instead of copying\n+      // the rows already in the queue.\n+      return 0L\n+    }\n+    var released = 0L\n+    synchronized {\n+      // poll out all the buffers and add them back in the same order to make sure that the rows\n+      // are in correct order.\n+      val iter = queues.iterator()\n+      val newQueues = new java.util.LinkedList[RowQueue]()\n+      while (iter.hasNext) {\n+        val queue = queues.remove()"
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Done\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-26T17:53:30Z",
    "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ */\n+private[python] trait RowQueue {\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size\n+  // the first location where a new row would be written\n+  private var writeOffset = page.getBaseOffset\n+  // points to the start of the next row to read\n+  private var readOffset = page.getBaseOffset\n+  private val resultRow = new UnsafeRow(numFields)\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    val size = row.getSizeInBytes\n+    if (writeOffset + 4 + size > endOfPage) {\n+      // if there is not enough space in this page to hold the new record\n+      if (writeOffset + 4 <= endOfPage) {\n+        // if there's extra space at the end of the page, store a special \"end-of-page\" length (-1)\n+        Platform.putInt(base, writeOffset, -1)\n+      }\n+      false\n+    } else {\n+      Platform.putInt(base, writeOffset, size)\n+      Platform.copyMemory(row.getBaseObject, row.getBaseOffset, base, writeOffset + 4, size)\n+      writeOffset += 4 + size\n+      true\n+    }\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    if (readOffset + 4 > endOfPage || Platform.getInt(base, readOffset) < 0) {\n+      null\n+    } else {\n+      val size = Platform.getInt(base, readOffset)\n+      resultRow.pointTo(base, readOffset + 4, size)\n+      readOffset += 4 + size\n+      resultRow\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that is backed by a file on disk. This queue will stop accepting new rows once any\n+ * reader has begun reading from the queue.\n+ */\n+private[python] case class DiskRowQueue(file: File, fields: Int) extends RowQueue {\n+  private var fout = new FileOutputStream(file.toString)\n+  private var out = new DataOutputStream(new BufferedOutputStream(fout))\n+  private var unreadBytes = 0L\n+\n+  private var fin: FileInputStream = _\n+  private var in: DataInputStream = _\n+  private val resultRow = new UnsafeRow(fields)\n+\n+  def add(row: UnsafeRow): Boolean = synchronized {\n+    if (out == null) {\n+      // Another thread is reading, stop writing this one\n+      return false\n+    }\n+    out.writeInt(row.getSizeInBytes)\n+    out.write(row.getBytes)\n+    unreadBytes += 4 + row.getSizeInBytes\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = synchronized {\n+    if (out != null) {\n+      out.close()\n+      out = null\n+      fout.close()\n+      fout = null\n+\n+      fin = new FileInputStream(file.toString)\n+      in = new DataInputStream(new BufferedInputStream(fin))\n+    }\n+\n+    if (unreadBytes > 0) {\n+      val size = in.readInt()\n+      val bytes = new Array[Byte](size)\n+      in.readFully(bytes)\n+      unreadBytes -= 4 + size\n+      resultRow.pointTo(bytes, size)\n+      resultRow\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def close(): Unit = synchronized {\n+    Closeables.close(fout, true)\n+    fout = null\n+    Closeables.close(fin, true)\n+    fin = null\n+    if (file.exists()) {\n+      file.delete()\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that has a list of RowQueues, which could be in memory or disk.\n+ *\n+ * HybridRowQueue could be safely appended in one thread, and pulled in another thread in the same\n+ * time.\n+ */\n+private[python] case class HybridRowQueue(\n+    memManager: TaskMemoryManager,\n+    tempDir: File,\n+    numFields: Int)\n+  extends MemoryConsumer(memManager) with RowQueue {\n+\n+  // Each buffer should have at least one row\n+  private var queues = new java.util.LinkedList[RowQueue]()\n+\n+  private var writing: RowQueue = _\n+  private var reading: RowQueue = _\n+\n+  // exposed for testing\n+  private[python] def numQueues(): Int = queues.size()\n+\n+  def spill(size: Long, trigger: MemoryConsumer): Long = {\n+    if (trigger == this) {\n+      // When it's triggered by itself, it should write upcoming rows into disk instead of copying\n+      // the rows already in the queue.\n+      return 0L\n+    }\n+    var released = 0L\n+    synchronized {\n+      // poll out all the buffers and add them back in the same order to make sure that the rows\n+      // are in correct order.\n+      val iter = queues.iterator()\n+      val newQueues = new java.util.LinkedList[RowQueue]()\n+      while (iter.hasNext) {\n+        val queue = queues.remove()"
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Is it possible `reading` and `writing` are referring to the same queue? If it is, we might close the writing queue which is going to add new row.\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-23T02:54:06Z",
    "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ */\n+private[python] trait RowQueue {\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size\n+  // the first location where a new row would be written\n+  private var writeOffset = page.getBaseOffset\n+  // points to the start of the next row to read\n+  private var readOffset = page.getBaseOffset\n+  private val resultRow = new UnsafeRow(numFields)\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    val size = row.getSizeInBytes\n+    if (writeOffset + 4 + size > endOfPage) {\n+      // if there is not enough space in this page to hold the new record\n+      if (writeOffset + 4 <= endOfPage) {\n+        // if there's extra space at the end of the page, store a special \"end-of-page\" length (-1)\n+        Platform.putInt(base, writeOffset, -1)\n+      }\n+      false\n+    } else {\n+      Platform.putInt(base, writeOffset, size)\n+      Platform.copyMemory(row.getBaseObject, row.getBaseOffset, base, writeOffset + 4, size)\n+      writeOffset += 4 + size\n+      true\n+    }\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    if (readOffset + 4 > endOfPage || Platform.getInt(base, readOffset) < 0) {\n+      null\n+    } else {\n+      val size = Platform.getInt(base, readOffset)\n+      resultRow.pointTo(base, readOffset + 4, size)\n+      readOffset += 4 + size\n+      resultRow\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that is backed by a file on disk. This queue will stop accepting new rows once any\n+ * reader has begun reading from the queue.\n+ */\n+private[python] case class DiskRowQueue(file: File, fields: Int) extends RowQueue {\n+  private var fout = new FileOutputStream(file.toString)\n+  private var out = new DataOutputStream(new BufferedOutputStream(fout))\n+  private var unreadBytes = 0L\n+\n+  private var fin: FileInputStream = _\n+  private var in: DataInputStream = _\n+  private val resultRow = new UnsafeRow(fields)\n+\n+  def add(row: UnsafeRow): Boolean = synchronized {\n+    if (out == null) {\n+      // Another thread is reading, stop writing this one\n+      return false\n+    }\n+    out.writeInt(row.getSizeInBytes)\n+    out.write(row.getBytes)\n+    unreadBytes += 4 + row.getSizeInBytes\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = synchronized {\n+    if (out != null) {\n+      out.close()\n+      out = null\n+      fout.close()\n+      fout = null\n+\n+      fin = new FileInputStream(file.toString)\n+      in = new DataInputStream(new BufferedInputStream(fin))\n+    }\n+\n+    if (unreadBytes > 0) {\n+      val size = in.readInt()\n+      val bytes = new Array[Byte](size)\n+      in.readFully(bytes)\n+      unreadBytes -= 4 + size\n+      resultRow.pointTo(bytes, size)\n+      resultRow\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def close(): Unit = synchronized {\n+    Closeables.close(fout, true)\n+    fout = null\n+    Closeables.close(fin, true)\n+    fin = null\n+    if (file.exists()) {\n+      file.delete()\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that has a list of RowQueues, which could be in memory or disk.\n+ *\n+ * HybridRowQueue could be safely appended in one thread, and pulled in another thread in the same\n+ * time.\n+ */\n+private[python] case class HybridRowQueue(\n+    memManager: TaskMemoryManager,\n+    tempDir: File,\n+    numFields: Int)\n+  extends MemoryConsumer(memManager) with RowQueue {\n+\n+  // Each buffer should have at least one row\n+  private var queues = new java.util.LinkedList[RowQueue]()\n+\n+  private var writing: RowQueue = _\n+  private var reading: RowQueue = _\n+\n+  // exposed for testing\n+  private[python] def numQueues(): Int = queues.size()\n+\n+  def spill(size: Long, trigger: MemoryConsumer): Long = {\n+    if (trigger == this) {\n+      // When it's triggered by itself, it should write upcoming rows into disk instead of copying\n+      // the rows already in the queue.\n+      return 0L\n+    }\n+    var released = 0L\n+    synchronized {\n+      // poll out all the buffers and add them back in the same order to make sure that the rows\n+      // are in correct order.\n+      val iter = queues.iterator()\n+      val newQueues = new java.util.LinkedList[RowQueue]()\n+      while (iter.hasNext) {\n+        val queue = queues.remove()\n+        val newQueue = if (!queues.isEmpty && queue.isInstanceOf[InMemoryRowQueue]) {\n+          val diskQueue = createDiskQueue()\n+          var row = queue.remove()\n+          while (row != null) {\n+            diskQueue.add(row)\n+            row = queue.remove()\n+          }\n+          released += queue.asInstanceOf[InMemoryRowQueue].page.size()\n+          queue.close()\n+          diskQueue\n+        } else {\n+          queue\n+        }\n+        newQueues.add(newQueue)\n+      }\n+      queues = newQueues\n+    }\n+    released\n+  }\n+\n+  private def createDiskQueue(): RowQueue = {\n+    DiskRowQueue(File.createTempFile(\"buffer\", \"\", tempDir), numFields)\n+  }\n+\n+  private def createNewQueue(required: Long): RowQueue = {\n+    val page = try {\n+      allocatePage(required)\n+    } catch {\n+      case _: OutOfMemoryError =>\n+        null\n+    }\n+    val buffer = if (page != null) {\n+      new InMemoryRowQueue(page, numFields) {\n+        override def close(): Unit = {\n+          freePage(page)\n+        }\n+      }\n+    } else {\n+      createDiskQueue()\n+    }\n+\n+    synchronized {\n+      queues.add(buffer)\n+    }\n+    buffer\n+  }\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    if (writing == null || !writing.add(row)) {\n+      writing = createNewQueue(4 + row.getSizeInBytes)\n+      if (!writing.add(row)) {\n+        throw new SparkException(s\"failed to push a row into $writing\")\n+      }\n+    }\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    var row: UnsafeRow = null\n+    if (reading != null) {\n+      row = reading.remove()\n+    }\n+    if (row == null) {\n+      if (reading != null) {\n+        reading.close()",
    "line": 257
  }, {
    "author": {
      "login": "davies"
    },
    "body": "The reader is always behind writer, so when we reach the end of a reading queue, that means the queue is not written any more.\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-23T21:00:25Z",
    "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ */\n+private[python] trait RowQueue {\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size\n+  // the first location where a new row would be written\n+  private var writeOffset = page.getBaseOffset\n+  // points to the start of the next row to read\n+  private var readOffset = page.getBaseOffset\n+  private val resultRow = new UnsafeRow(numFields)\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    val size = row.getSizeInBytes\n+    if (writeOffset + 4 + size > endOfPage) {\n+      // if there is not enough space in this page to hold the new record\n+      if (writeOffset + 4 <= endOfPage) {\n+        // if there's extra space at the end of the page, store a special \"end-of-page\" length (-1)\n+        Platform.putInt(base, writeOffset, -1)\n+      }\n+      false\n+    } else {\n+      Platform.putInt(base, writeOffset, size)\n+      Platform.copyMemory(row.getBaseObject, row.getBaseOffset, base, writeOffset + 4, size)\n+      writeOffset += 4 + size\n+      true\n+    }\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    if (readOffset + 4 > endOfPage || Platform.getInt(base, readOffset) < 0) {\n+      null\n+    } else {\n+      val size = Platform.getInt(base, readOffset)\n+      resultRow.pointTo(base, readOffset + 4, size)\n+      readOffset += 4 + size\n+      resultRow\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that is backed by a file on disk. This queue will stop accepting new rows once any\n+ * reader has begun reading from the queue.\n+ */\n+private[python] case class DiskRowQueue(file: File, fields: Int) extends RowQueue {\n+  private var fout = new FileOutputStream(file.toString)\n+  private var out = new DataOutputStream(new BufferedOutputStream(fout))\n+  private var unreadBytes = 0L\n+\n+  private var fin: FileInputStream = _\n+  private var in: DataInputStream = _\n+  private val resultRow = new UnsafeRow(fields)\n+\n+  def add(row: UnsafeRow): Boolean = synchronized {\n+    if (out == null) {\n+      // Another thread is reading, stop writing this one\n+      return false\n+    }\n+    out.writeInt(row.getSizeInBytes)\n+    out.write(row.getBytes)\n+    unreadBytes += 4 + row.getSizeInBytes\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = synchronized {\n+    if (out != null) {\n+      out.close()\n+      out = null\n+      fout.close()\n+      fout = null\n+\n+      fin = new FileInputStream(file.toString)\n+      in = new DataInputStream(new BufferedInputStream(fin))\n+    }\n+\n+    if (unreadBytes > 0) {\n+      val size = in.readInt()\n+      val bytes = new Array[Byte](size)\n+      in.readFully(bytes)\n+      unreadBytes -= 4 + size\n+      resultRow.pointTo(bytes, size)\n+      resultRow\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def close(): Unit = synchronized {\n+    Closeables.close(fout, true)\n+    fout = null\n+    Closeables.close(fin, true)\n+    fin = null\n+    if (file.exists()) {\n+      file.delete()\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that has a list of RowQueues, which could be in memory or disk.\n+ *\n+ * HybridRowQueue could be safely appended in one thread, and pulled in another thread in the same\n+ * time.\n+ */\n+private[python] case class HybridRowQueue(\n+    memManager: TaskMemoryManager,\n+    tempDir: File,\n+    numFields: Int)\n+  extends MemoryConsumer(memManager) with RowQueue {\n+\n+  // Each buffer should have at least one row\n+  private var queues = new java.util.LinkedList[RowQueue]()\n+\n+  private var writing: RowQueue = _\n+  private var reading: RowQueue = _\n+\n+  // exposed for testing\n+  private[python] def numQueues(): Int = queues.size()\n+\n+  def spill(size: Long, trigger: MemoryConsumer): Long = {\n+    if (trigger == this) {\n+      // When it's triggered by itself, it should write upcoming rows into disk instead of copying\n+      // the rows already in the queue.\n+      return 0L\n+    }\n+    var released = 0L\n+    synchronized {\n+      // poll out all the buffers and add them back in the same order to make sure that the rows\n+      // are in correct order.\n+      val iter = queues.iterator()\n+      val newQueues = new java.util.LinkedList[RowQueue]()\n+      while (iter.hasNext) {\n+        val queue = queues.remove()\n+        val newQueue = if (!queues.isEmpty && queue.isInstanceOf[InMemoryRowQueue]) {\n+          val diskQueue = createDiskQueue()\n+          var row = queue.remove()\n+          while (row != null) {\n+            diskQueue.add(row)\n+            row = queue.remove()\n+          }\n+          released += queue.asInstanceOf[InMemoryRowQueue].page.size()\n+          queue.close()\n+          diskQueue\n+        } else {\n+          queue\n+        }\n+        newQueues.add(newQueue)\n+      }\n+      queues = newQueues\n+    }\n+    released\n+  }\n+\n+  private def createDiskQueue(): RowQueue = {\n+    DiskRowQueue(File.createTempFile(\"buffer\", \"\", tempDir), numFields)\n+  }\n+\n+  private def createNewQueue(required: Long): RowQueue = {\n+    val page = try {\n+      allocatePage(required)\n+    } catch {\n+      case _: OutOfMemoryError =>\n+        null\n+    }\n+    val buffer = if (page != null) {\n+      new InMemoryRowQueue(page, numFields) {\n+        override def close(): Unit = {\n+          freePage(page)\n+        }\n+      }\n+    } else {\n+      createDiskQueue()\n+    }\n+\n+    synchronized {\n+      queues.add(buffer)\n+    }\n+    buffer\n+  }\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    if (writing == null || !writing.add(row)) {\n+      writing = createNewQueue(4 + row.getSizeInBytes)\n+      if (!writing.add(row)) {\n+        throw new SparkException(s\"failed to push a row into $writing\")\n+      }\n+    }\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    var row: UnsafeRow = null\n+    if (reading != null) {\n+      row = reading.remove()\n+    }\n+    if (row == null) {\n+      if (reading != null) {\n+        reading.close()",
    "line": 257
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "lins05"
    },
    "body": "Seems this `spill` method is only used in tests code?\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T19:11:37Z",
    "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size\n+  // the first location where a new row would be written\n+  private var writeOffset = page.getBaseOffset\n+  // points to the start of the next row to read\n+  private var readOffset = page.getBaseOffset\n+  private val resultRow = new UnsafeRow(numFields)\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    val size = row.getSizeInBytes\n+    if (writeOffset + 4 + size > endOfPage) {\n+      // if there is not enough space in this page to hold the new record\n+      if (writeOffset + 4 <= endOfPage) {\n+        // if there's extra space at the end of the page, store a special \"end-of-page\" length (-1)\n+        Platform.putInt(base, writeOffset, -1)\n+      }\n+      false\n+    } else {\n+      Platform.putInt(base, writeOffset, size)\n+      Platform.copyMemory(row.getBaseObject, row.getBaseOffset, base, writeOffset + 4, size)\n+      writeOffset += 4 + size\n+      true\n+    }\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    if (readOffset + 4 > endOfPage || Platform.getInt(base, readOffset) < 0) {\n+      null\n+    } else {\n+      val size = Platform.getInt(base, readOffset)\n+      resultRow.pointTo(base, readOffset + 4, size)\n+      readOffset += 4 + size\n+      resultRow\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that is backed by a file on disk. This queue will stop accepting new rows once any\n+ * reader has begun reading from the queue.\n+ */\n+private[python] case class DiskRowQueue(file: File, fields: Int) extends RowQueue {\n+  private var out = new DataOutputStream(\n+    new BufferedOutputStream(new FileOutputStream(file.toString)))\n+  private var unreadBytes = 0L\n+\n+  private var in: DataInputStream = _\n+  private val resultRow = new UnsafeRow(fields)\n+\n+  def add(row: UnsafeRow): Boolean = synchronized {\n+    if (out == null) {\n+      // Another thread is reading, stop writing this one\n+      return false\n+    }\n+    out.writeInt(row.getSizeInBytes)\n+    out.write(row.getBytes)\n+    unreadBytes += 4 + row.getSizeInBytes\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = synchronized {\n+    if (out != null) {\n+      out.close()\n+      out = null\n+      in = new DataInputStream(new BufferedInputStream(new FileInputStream(file.toString)))\n+    }\n+\n+    if (unreadBytes > 0) {\n+      val size = in.readInt()\n+      val bytes = new Array[Byte](size)\n+      in.readFully(bytes)\n+      unreadBytes -= 4 + size\n+      resultRow.pointTo(bytes, size)\n+      resultRow\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def close(): Unit = synchronized {\n+    Closeables.close(out, true)\n+    out = null\n+    Closeables.close(in, true)\n+    in = null\n+    if (file.exists()) {\n+      file.delete()\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that has a list of RowQueues, which could be in memory or disk.\n+ *\n+ * HybridRowQueue could be safely appended in one thread, and pulled in another thread in the same\n+ * time.\n+ */\n+private[python] case class HybridRowQueue(\n+    memManager: TaskMemoryManager,\n+    tempDir: File,\n+    numFields: Int)\n+  extends MemoryConsumer(memManager) with RowQueue {\n+\n+  // Each buffer should have at least one row\n+  private var queues = new java.util.LinkedList[RowQueue]()\n+\n+  private var writing: RowQueue = _\n+  private var reading: RowQueue = _\n+\n+  // exposed for testing\n+  private[python] def numQueues(): Int = queues.size()\n+\n+  def spill(size: Long, trigger: MemoryConsumer): Long = {",
    "line": 180
  }, {
    "author": {
      "login": "davies"
    },
    "body": "No, it will be used as a MemoryConsumer (called by other memory consumer).\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T20:34:15Z",
    "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size\n+  // the first location where a new row would be written\n+  private var writeOffset = page.getBaseOffset\n+  // points to the start of the next row to read\n+  private var readOffset = page.getBaseOffset\n+  private val resultRow = new UnsafeRow(numFields)\n+\n+  def add(row: UnsafeRow): Boolean = {\n+    val size = row.getSizeInBytes\n+    if (writeOffset + 4 + size > endOfPage) {\n+      // if there is not enough space in this page to hold the new record\n+      if (writeOffset + 4 <= endOfPage) {\n+        // if there's extra space at the end of the page, store a special \"end-of-page\" length (-1)\n+        Platform.putInt(base, writeOffset, -1)\n+      }\n+      false\n+    } else {\n+      Platform.putInt(base, writeOffset, size)\n+      Platform.copyMemory(row.getBaseObject, row.getBaseOffset, base, writeOffset + 4, size)\n+      writeOffset += 4 + size\n+      true\n+    }\n+  }\n+\n+  def remove(): UnsafeRow = {\n+    if (readOffset + 4 > endOfPage || Platform.getInt(base, readOffset) < 0) {\n+      null\n+    } else {\n+      val size = Platform.getInt(base, readOffset)\n+      resultRow.pointTo(base, readOffset + 4, size)\n+      readOffset += 4 + size\n+      resultRow\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that is backed by a file on disk. This queue will stop accepting new rows once any\n+ * reader has begun reading from the queue.\n+ */\n+private[python] case class DiskRowQueue(file: File, fields: Int) extends RowQueue {\n+  private var out = new DataOutputStream(\n+    new BufferedOutputStream(new FileOutputStream(file.toString)))\n+  private var unreadBytes = 0L\n+\n+  private var in: DataInputStream = _\n+  private val resultRow = new UnsafeRow(fields)\n+\n+  def add(row: UnsafeRow): Boolean = synchronized {\n+    if (out == null) {\n+      // Another thread is reading, stop writing this one\n+      return false\n+    }\n+    out.writeInt(row.getSizeInBytes)\n+    out.write(row.getBytes)\n+    unreadBytes += 4 + row.getSizeInBytes\n+    true\n+  }\n+\n+  def remove(): UnsafeRow = synchronized {\n+    if (out != null) {\n+      out.close()\n+      out = null\n+      in = new DataInputStream(new BufferedInputStream(new FileInputStream(file.toString)))\n+    }\n+\n+    if (unreadBytes > 0) {\n+      val size = in.readInt()\n+      val bytes = new Array[Byte](size)\n+      in.readFully(bytes)\n+      unreadBytes -= 4 + size\n+      resultRow.pointTo(bytes, size)\n+      resultRow\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def close(): Unit = synchronized {\n+    Closeables.close(out, true)\n+    out = null\n+    Closeables.close(in, true)\n+    in = null\n+    if (file.exists()) {\n+      file.delete()\n+    }\n+  }\n+}\n+\n+/**\n+ * A RowQueue that has a list of RowQueues, which could be in memory or disk.\n+ *\n+ * HybridRowQueue could be safely appended in one thread, and pulled in another thread in the same\n+ * time.\n+ */\n+private[python] case class HybridRowQueue(\n+    memManager: TaskMemoryManager,\n+    tempDir: File,\n+    numFields: Int)\n+  extends MemoryConsumer(memManager) with RowQueue {\n+\n+  // Each buffer should have at least one row\n+  private var queues = new java.util.LinkedList[RowQueue]()\n+\n+  private var writing: RowQueue = _\n+  private var reading: RowQueue = _\n+\n+  // exposed for testing\n+  private[python] def numQueues(): Int = queues.size()\n+\n+  def spill(size: Long, trigger: MemoryConsumer): Long = {",
    "line": 180
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This comment helps, but I still find it confusing how this guarantee that the reader is always behind the writer is enforced by the order in which rows are queued and sent to Python. I think that it would be helpful to have either an expository paragraph or a diagram like the one that we drew when discussing this offline.\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T19:17:13Z",
    "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one",
    "line": 33
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Added a diagram in BatchEvalPythonExec\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-30T21:13:47Z",
    "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one",
    "line": 33
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "\"iff the row has been added to the queue.\"\n\nCan you comment on when a row would be failed to add?\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T21:02:16Z",
    "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer. See the doc of class [[BatchEvalPythonExec]]\n+ * on how it works.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it."
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "OK i understand this now after reading more code. Regardless it would be great to explain some of the possible causes when this can return false, e.g. when we run out of buffer space.\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T21:08:17Z",
    "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer. See the doc of class [[BatchEvalPythonExec]]\n+ * on how it works.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it."
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "This sentence is not obvious to me what it means, or why this matters from an end-user's perspective. Can you explain?\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T21:03:04Z",
    "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer. See the doc of class [[BatchEvalPythonExec]]\n+ * on how it works.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called."
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "I would document -1 length means end of a page here too.\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T21:05:13Z",
    "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer. See the doc of class [[BatchEvalPythonExec]]\n+ * on how it works.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]",
    "line": 62
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can we add more asserts everywhere to guard the invariant readOffset <= writeOffset?\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T21:06:25Z",
    "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer. See the doc of class [[BatchEvalPythonExec]]\n+ * on how it works.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)"
  }],
  "prId": 15089
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "My biggest question here is how the java memory model actually works with respect to two threads reading/writing using unsafe without any synchronization primitives. I wouldn't be surprised if this could lead to stale reads. We should verify this wouldn't be an issue; otherwise we should add explicit synchronization.\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-29T21:25:11Z",
    "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer. See the doc of class [[BatchEvalPythonExec]]\n+ * on how it works.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size",
    "line": 69
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Added. It's still unclear the Unsafe read/write is synchronized by `synchronized` or not. Should we add `fence` here?\n",
    "commit": "ca607e038259cff7d50b732801cfc5144dedb6d1",
    "createdAt": "2016-09-30T21:20:52Z",
    "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql.execution.python\n+\n+import java.io._\n+\n+import com.google.common.io.Closeables\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.memory.{MemoryConsumer, TaskMemoryManager}\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.memory.MemoryBlock\n+\n+/**\n+ * A RowQueue is an FIFO queue for UnsafeRow.\n+ *\n+ * This RowQueue is ONLY designed and used for Python UDF, which has only one writer and only one\n+ * reader, the reader ALWAYS ran behind the writer. See the doc of class [[BatchEvalPythonExec]]\n+ * on how it works.\n+ */\n+private[python] trait RowQueue {\n+\n+  /**\n+   * Add a row to the end of it, returns true iff the row has added into it.\n+   */\n+  def add(row: UnsafeRow): Boolean\n+\n+  /**\n+   * Retrieve and remove the first row, returns null if it's empty.\n+   *\n+   * It can only be called after add is called.\n+   */\n+  def remove(): UnsafeRow\n+\n+  /**\n+   * Cleanup all the resources.\n+   */\n+  def close(): Unit\n+}\n+\n+/**\n+ * A RowQueue that is based on in-memory page. UnsafeRows are appended into it until it's full.\n+ * Another thread could read from it at the same time (behind the writer).\n+ *\n+ * The format of UnsafeRow in page:\n+ * [4 bytes to hold length of record (N)] [N bytes to hold record] [...]\n+ */\n+private[python] abstract class InMemoryRowQueue(val page: MemoryBlock, numFields: Int)\n+  extends RowQueue {\n+  private val base: AnyRef = page.getBaseObject\n+  private val endOfPage: Long = page.getBaseOffset + page.size",
    "line": 69
  }],
  "prId": 15089
}]