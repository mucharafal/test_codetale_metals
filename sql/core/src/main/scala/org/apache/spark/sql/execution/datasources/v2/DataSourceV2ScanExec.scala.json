[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Can't Spark choose to use InternalRow reads instead? Why can't the source support both?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T20:58:39Z",
    "diffHunk": "@@ -39,52 +36,43 @@ case class DataSourceV2ScanExec(\n     @transient source: DataSourceV2,\n     @transient options: Map[String, String],\n     @transient pushedFilters: Seq[Expression],\n-    @transient reader: DataSourceReader)\n+    @transient readSupport: ReadSupport,\n+    @transient scanConfig: ScanConfig)\n   extends LeafExecNode with DataSourceV2StringFormat with ColumnarBatchScan {\n \n   override def simpleString: String = \"ScanV2 \" + metadataString\n \n   // TODO: unify the equal/hashCode implementation for all data source v2 query plans.\n   override def equals(other: Any): Boolean = other match {\n     case other: DataSourceV2ScanExec =>\n-      output == other.output && reader.getClass == other.reader.getClass && options == other.options\n+      output == other.output && readSupport.getClass == other.readSupport.getClass &&\n+        options == other.options\n     case _ => false\n   }\n \n   override def hashCode(): Int = {\n     Seq(output, source, options).hashCode()\n   }\n \n-  override def outputPartitioning: physical.Partitioning = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() && batchPartitions.size == 1 =>\n-      SinglePartition\n-\n-    case r: SupportsScanColumnarBatch if !r.enableBatchRead() && partitions.size == 1 =>\n-      SinglePartition\n-\n-    case r if !r.isInstanceOf[SupportsScanColumnarBatch] && partitions.size == 1 =>\n+  override def outputPartitioning: physical.Partitioning = readSupport match {\n+    case _ if partitions.length == 1 =>\n       SinglePartition\n \n     case s: SupportsReportPartitioning =>\n       new DataSourcePartitioning(\n-        s.outputPartitioning(), AttributeMap(output.map(a => a -> a.name)))\n+        s.outputPartitioning(scanConfig), AttributeMap(output.map(a => a -> a.name)))\n \n     case _ => super.outputPartitioning\n   }\n \n-  private lazy val partitions: Seq[InputPartition[InternalRow]] = {\n-    reader.planInputPartitions().asScala\n-  }\n+  private lazy val partitions: Seq[InputPartition] = readSupport.planInputPartitions(scanConfig)\n \n-  private lazy val batchPartitions: Seq[InputPartition[ColumnarBatch]] = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      assert(!reader.isInstanceOf[ContinuousReader],\n-        \"continuous stream reader does not support columnar read yet.\")\n-      r.planBatchInputPartitions().asScala\n-  }\n+  private lazy val partitionReaderFactory = readSupport.createReaderFactory(scanConfig)\n \n-  private lazy val inputRDD: RDD[InternalRow] = reader match {\n-    case _: ContinuousReader =>\n+  private lazy val inputRDD: RDD[InternalRow] = readSupport match {\n+    case _: ContinuousReadSupport =>\n+      assert(!partitionReaderFactory.supportColumnarReads(),"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "maybe we should rename `supportColumnarReads` to `doColumnarReads`? A source can support both, and it should tell Spark which mode it wants to use.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T02:31:36Z",
    "diffHunk": "@@ -39,52 +36,43 @@ case class DataSourceV2ScanExec(\n     @transient source: DataSourceV2,\n     @transient options: Map[String, String],\n     @transient pushedFilters: Seq[Expression],\n-    @transient reader: DataSourceReader)\n+    @transient readSupport: ReadSupport,\n+    @transient scanConfig: ScanConfig)\n   extends LeafExecNode with DataSourceV2StringFormat with ColumnarBatchScan {\n \n   override def simpleString: String = \"ScanV2 \" + metadataString\n \n   // TODO: unify the equal/hashCode implementation for all data source v2 query plans.\n   override def equals(other: Any): Boolean = other match {\n     case other: DataSourceV2ScanExec =>\n-      output == other.output && reader.getClass == other.reader.getClass && options == other.options\n+      output == other.output && readSupport.getClass == other.readSupport.getClass &&\n+        options == other.options\n     case _ => false\n   }\n \n   override def hashCode(): Int = {\n     Seq(output, source, options).hashCode()\n   }\n \n-  override def outputPartitioning: physical.Partitioning = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() && batchPartitions.size == 1 =>\n-      SinglePartition\n-\n-    case r: SupportsScanColumnarBatch if !r.enableBatchRead() && partitions.size == 1 =>\n-      SinglePartition\n-\n-    case r if !r.isInstanceOf[SupportsScanColumnarBatch] && partitions.size == 1 =>\n+  override def outputPartitioning: physical.Partitioning = readSupport match {\n+    case _ if partitions.length == 1 =>\n       SinglePartition\n \n     case s: SupportsReportPartitioning =>\n       new DataSourcePartitioning(\n-        s.outputPartitioning(), AttributeMap(output.map(a => a -> a.name)))\n+        s.outputPartitioning(scanConfig), AttributeMap(output.map(a => a -> a.name)))\n \n     case _ => super.outputPartitioning\n   }\n \n-  private lazy val partitions: Seq[InputPartition[InternalRow]] = {\n-    reader.planInputPartitions().asScala\n-  }\n+  private lazy val partitions: Seq[InputPartition] = readSupport.planInputPartitions(scanConfig)\n \n-  private lazy val batchPartitions: Seq[InputPartition[ColumnarBatch]] = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      assert(!reader.isInstanceOf[ContinuousReader],\n-        \"continuous stream reader does not support columnar read yet.\")\n-      r.planBatchInputPartitions().asScala\n-  }\n+  private lazy val partitionReaderFactory = readSupport.createReaderFactory(scanConfig)\n \n-  private lazy val inputRDD: RDD[InternalRow] = reader match {\n-    case _: ContinuousReader =>\n+  private lazy val inputRDD: RDD[InternalRow] = readSupport match {\n+    case _: ContinuousReadSupport =>\n+      assert(!partitionReaderFactory.supportColumnarReads(),"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "This is a slightly different case. Can Spark choose not to use columnar reads if the source returns true for `supportsColumnarReads`? If so, then this isn't a problem.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T15:57:15Z",
    "diffHunk": "@@ -39,52 +36,43 @@ case class DataSourceV2ScanExec(\n     @transient source: DataSourceV2,\n     @transient options: Map[String, String],\n     @transient pushedFilters: Seq[Expression],\n-    @transient reader: DataSourceReader)\n+    @transient readSupport: ReadSupport,\n+    @transient scanConfig: ScanConfig)\n   extends LeafExecNode with DataSourceV2StringFormat with ColumnarBatchScan {\n \n   override def simpleString: String = \"ScanV2 \" + metadataString\n \n   // TODO: unify the equal/hashCode implementation for all data source v2 query plans.\n   override def equals(other: Any): Boolean = other match {\n     case other: DataSourceV2ScanExec =>\n-      output == other.output && reader.getClass == other.reader.getClass && options == other.options\n+      output == other.output && readSupport.getClass == other.readSupport.getClass &&\n+        options == other.options\n     case _ => false\n   }\n \n   override def hashCode(): Int = {\n     Seq(output, source, options).hashCode()\n   }\n \n-  override def outputPartitioning: physical.Partitioning = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() && batchPartitions.size == 1 =>\n-      SinglePartition\n-\n-    case r: SupportsScanColumnarBatch if !r.enableBatchRead() && partitions.size == 1 =>\n-      SinglePartition\n-\n-    case r if !r.isInstanceOf[SupportsScanColumnarBatch] && partitions.size == 1 =>\n+  override def outputPartitioning: physical.Partitioning = readSupport match {\n+    case _ if partitions.length == 1 =>\n       SinglePartition\n \n     case s: SupportsReportPartitioning =>\n       new DataSourcePartitioning(\n-        s.outputPartitioning(), AttributeMap(output.map(a => a -> a.name)))\n+        s.outputPartitioning(scanConfig), AttributeMap(output.map(a => a -> a.name)))\n \n     case _ => super.outputPartitioning\n   }\n \n-  private lazy val partitions: Seq[InputPartition[InternalRow]] = {\n-    reader.planInputPartitions().asScala\n-  }\n+  private lazy val partitions: Seq[InputPartition] = readSupport.planInputPartitions(scanConfig)\n \n-  private lazy val batchPartitions: Seq[InputPartition[ColumnarBatch]] = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      assert(!reader.isInstanceOf[ContinuousReader],\n-        \"continuous stream reader does not support columnar read yet.\")\n-      r.planBatchInputPartitions().asScala\n-  }\n+  private lazy val partitionReaderFactory = readSupport.createReaderFactory(scanConfig)\n \n-  private lazy val inputRDD: RDD[InternalRow] = reader match {\n-    case _: ContinuousReader =>\n+  private lazy val inputRDD: RDD[InternalRow] = readSupport match {\n+    case _: ContinuousReadSupport =>\n+      assert(!partitionReaderFactory.supportColumnarReads(),"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think the answer is no. We may have columnar only data source, so I don't think it's a good idea to let Spark decide the scan mode.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T17:04:43Z",
    "diffHunk": "@@ -39,52 +36,43 @@ case class DataSourceV2ScanExec(\n     @transient source: DataSourceV2,\n     @transient options: Map[String, String],\n     @transient pushedFilters: Seq[Expression],\n-    @transient reader: DataSourceReader)\n+    @transient readSupport: ReadSupport,\n+    @transient scanConfig: ScanConfig)\n   extends LeafExecNode with DataSourceV2StringFormat with ColumnarBatchScan {\n \n   override def simpleString: String = \"ScanV2 \" + metadataString\n \n   // TODO: unify the equal/hashCode implementation for all data source v2 query plans.\n   override def equals(other: Any): Boolean = other match {\n     case other: DataSourceV2ScanExec =>\n-      output == other.output && reader.getClass == other.reader.getClass && options == other.options\n+      output == other.output && readSupport.getClass == other.readSupport.getClass &&\n+        options == other.options\n     case _ => false\n   }\n \n   override def hashCode(): Int = {\n     Seq(output, source, options).hashCode()\n   }\n \n-  override def outputPartitioning: physical.Partitioning = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() && batchPartitions.size == 1 =>\n-      SinglePartition\n-\n-    case r: SupportsScanColumnarBatch if !r.enableBatchRead() && partitions.size == 1 =>\n-      SinglePartition\n-\n-    case r if !r.isInstanceOf[SupportsScanColumnarBatch] && partitions.size == 1 =>\n+  override def outputPartitioning: physical.Partitioning = readSupport match {\n+    case _ if partitions.length == 1 =>\n       SinglePartition\n \n     case s: SupportsReportPartitioning =>\n       new DataSourcePartitioning(\n-        s.outputPartitioning(), AttributeMap(output.map(a => a -> a.name)))\n+        s.outputPartitioning(scanConfig), AttributeMap(output.map(a => a -> a.name)))\n \n     case _ => super.outputPartitioning\n   }\n \n-  private lazy val partitions: Seq[InputPartition[InternalRow]] = {\n-    reader.planInputPartitions().asScala\n-  }\n+  private lazy val partitions: Seq[InputPartition] = readSupport.planInputPartitions(scanConfig)\n \n-  private lazy val batchPartitions: Seq[InputPartition[ColumnarBatch]] = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      assert(!reader.isInstanceOf[ContinuousReader],\n-        \"continuous stream reader does not support columnar read yet.\")\n-      r.planBatchInputPartitions().asScala\n-  }\n+  private lazy val partitionReaderFactory = readSupport.createReaderFactory(scanConfig)\n \n-  private lazy val inputRDD: RDD[InternalRow] = reader match {\n-    case _: ContinuousReader =>\n+  private lazy val inputRDD: RDD[InternalRow] = readSupport match {\n+    case _: ContinuousReadSupport =>\n+      assert(!partitionReaderFactory.supportColumnarReads(),"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Works for me. What is the issue to add batch support?",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-09T18:57:01Z",
    "diffHunk": "@@ -39,52 +36,43 @@ case class DataSourceV2ScanExec(\n     @transient source: DataSourceV2,\n     @transient options: Map[String, String],\n     @transient pushedFilters: Seq[Expression],\n-    @transient reader: DataSourceReader)\n+    @transient readSupport: ReadSupport,\n+    @transient scanConfig: ScanConfig)\n   extends LeafExecNode with DataSourceV2StringFormat with ColumnarBatchScan {\n \n   override def simpleString: String = \"ScanV2 \" + metadataString\n \n   // TODO: unify the equal/hashCode implementation for all data source v2 query plans.\n   override def equals(other: Any): Boolean = other match {\n     case other: DataSourceV2ScanExec =>\n-      output == other.output && reader.getClass == other.reader.getClass && options == other.options\n+      output == other.output && readSupport.getClass == other.readSupport.getClass &&\n+        options == other.options\n     case _ => false\n   }\n \n   override def hashCode(): Int = {\n     Seq(output, source, options).hashCode()\n   }\n \n-  override def outputPartitioning: physical.Partitioning = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() && batchPartitions.size == 1 =>\n-      SinglePartition\n-\n-    case r: SupportsScanColumnarBatch if !r.enableBatchRead() && partitions.size == 1 =>\n-      SinglePartition\n-\n-    case r if !r.isInstanceOf[SupportsScanColumnarBatch] && partitions.size == 1 =>\n+  override def outputPartitioning: physical.Partitioning = readSupport match {\n+    case _ if partitions.length == 1 =>\n       SinglePartition\n \n     case s: SupportsReportPartitioning =>\n       new DataSourcePartitioning(\n-        s.outputPartitioning(), AttributeMap(output.map(a => a -> a.name)))\n+        s.outputPartitioning(scanConfig), AttributeMap(output.map(a => a -> a.name)))\n \n     case _ => super.outputPartitioning\n   }\n \n-  private lazy val partitions: Seq[InputPartition[InternalRow]] = {\n-    reader.planInputPartitions().asScala\n-  }\n+  private lazy val partitions: Seq[InputPartition] = readSupport.planInputPartitions(scanConfig)\n \n-  private lazy val batchPartitions: Seq[InputPartition[ColumnarBatch]] = reader match {\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      assert(!reader.isInstanceOf[ContinuousReader],\n-        \"continuous stream reader does not support columnar read yet.\")\n-      r.planBatchInputPartitions().asScala\n-  }\n+  private lazy val partitionReaderFactory = readSupport.createReaderFactory(scanConfig)\n \n-  private lazy val inputRDD: RDD[InternalRow] = reader match {\n-    case _: ContinuousReader =>\n+  private lazy val inputRDD: RDD[InternalRow] = readSupport match {\n+    case _: ContinuousReadSupport =>\n+      assert(!partitionReaderFactory.supportColumnarReads(),"
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "This should not cast. Just call `readSupport.createContinuousReaderFactory(...)` here or use a variable that maintains the reader factory's type.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-07T21:00:00Z",
    "diffHunk": "@@ -93,21 +81,17 @@ case class DataSourceV2ScanExec(\n         sparkContext,\n         sqlContext.conf.continuousStreamingExecutorQueueSize,\n         sqlContext.conf.continuousStreamingExecutorPollIntervalMs,\n-        partitions).asInstanceOf[RDD[InternalRow]]\n-\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      new DataSourceRDD(sparkContext, batchPartitions).asInstanceOf[RDD[InternalRow]]\n+        partitions,\n+        schema,\n+        partitionReaderFactory.asInstanceOf[ContinuousPartitionReaderFactory])"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`DataSourceV2ScanExec` is shared between batch and streaming, so the `partitionReaderFactory` here is a general type instead of the concrete `ContinuousPartitionReaderFactory`. I think we can avoid this cast in the future refactoring, when we have a dedicated scan plan for continuous streaming.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T02:33:43Z",
    "diffHunk": "@@ -93,21 +81,17 @@ case class DataSourceV2ScanExec(\n         sparkContext,\n         sqlContext.conf.continuousStreamingExecutorQueueSize,\n         sqlContext.conf.continuousStreamingExecutorPollIntervalMs,\n-        partitions).asInstanceOf[RDD[InternalRow]]\n-\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      new DataSourceRDD(sparkContext, batchPartitions).asInstanceOf[RDD[InternalRow]]\n+        partitions,\n+        schema,\n+        partitionReaderFactory.asInstanceOf[ContinuousPartitionReaderFactory])"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "However you want to do it is fine with me, but I've seen excessive casting in the SQL back-end so I'm against adding it when it isn't necessary, like this case.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T15:58:14Z",
    "diffHunk": "@@ -93,21 +81,17 @@ case class DataSourceV2ScanExec(\n         sparkContext,\n         sqlContext.conf.continuousStreamingExecutorQueueSize,\n         sqlContext.conf.continuousStreamingExecutorPollIntervalMs,\n-        partitions).asInstanceOf[RDD[InternalRow]]\n-\n-    case r: SupportsScanColumnarBatch if r.enableBatchRead() =>\n-      new DataSourceRDD(sparkContext, batchPartitions).asInstanceOf[RDD[InternalRow]]\n+        partitions,\n+        schema,\n+        partitionReaderFactory.asInstanceOf[ContinuousPartitionReaderFactory])"
  }],
  "prId": 22009
}]