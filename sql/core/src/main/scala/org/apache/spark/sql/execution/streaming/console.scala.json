[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: you can use\r\n```\r\ncase class ConsoleRelation(override val sqlContext: SQLContext, data: DataFrame) extends BaseRelation {\r\n  override def schema: StructType = data.schema\r\n}\r\n```",
    "commit": "942433e410fa885151828f06471c2d9801703d0c",
    "createdAt": "2017-06-21T17:58:01Z",
    "diffHunk": "@@ -51,7 +53,15 @@ class ConsoleSink(options: Map[String, String]) extends Sink with Logging {\n   }\n }\n \n-class ConsoleSinkProvider extends StreamSinkProvider with DataSourceRegister {\n+case class ConsoleRelation(Context: SQLContext, data: DataFrame) extends BaseRelation {"
  }],
  "prId": 18347
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "You can just call `data.showInternal(numRowsToShow, isTruncated)`. This is a hack in ConsoleSink to avoid using a wrong planner. That's not a problem in the batch DataFrames.",
    "commit": "942433e410fa885151828f06471c2d9801703d0c",
    "createdAt": "2017-06-21T17:59:43Z",
    "diffHunk": "@@ -60,5 +70,23 @@ class ConsoleSinkProvider extends StreamSinkProvider with DataSourceRegister {\n     new ConsoleSink(parameters)\n   }\n \n+  def createRelation(\n+      sqlContext: SQLContext,\n+      mode: SaveMode,\n+      parameters: Map[String, String],\n+      data: DataFrame): BaseRelation = {\n+    // Number of rows to display, by default 20 rows\n+    val numRowsToShow = parameters.get(\"numRows\").map(_.toInt).getOrElse(20)\n+\n+    // Truncate the displayed data if it is too long, by default it is true\n+    val isTruncated = parameters.get(\"truncate\").map(_.toBoolean).getOrElse(true)\n+\n+    data.sparkSession.createDataFrame("
  }],
  "prId": 18347
}]