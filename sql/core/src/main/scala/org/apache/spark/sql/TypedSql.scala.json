[{
  "comments": [{
    "author": {
      "login": "gzm0"
    },
    "body": "Consider adding arrays here\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T18:06:13Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)\n+      c.Expr(tree)\n+    }\n+\n+    // TODO: Handle nullable fields\n+    def genRecord(row: Tree, fields: Seq[(String, DataType)]) = {\n+      case class ImplSchema(name: String, tpe: Type, impl: Tree)\n+\n+      val implSchemas = for {\n+        ((name, dataType),i) <- fields.zipWithIndex\n+      } yield {\n+        val tpe = c.typeCheck(genGetField(q\"null: $rowTpe\", i, dataType)).tpe\n+        val tree = genGetField(row, i, dataType)\n+\n+        ImplSchema(name, tpe, tree)\n+      }\n+\n+      val schema = implSchemas.map(f => (f.name, f.tpe))\n+\n+      val (spFlds, objFields) = implSchemas.partition(s =>\n+        rMacros.specializedTypes.contains(s.tpe))\n+\n+      val spImplsByTpe = {\n+        val grouped = spFlds.groupBy(_.tpe)\n+        grouped.mapValues { _.map(s => s.name -> s.impl).toMap }\n+      }\n+\n+      val dataObjImpl = {\n+        val impls = objFields.map(s => s.name -> s.impl).toMap\n+        val lookupTree = rMacros.genLookup(q\"fieldName\", impls, mayCache = false)\n+        q\"($lookupTree).asInstanceOf[T]\"\n+      }\n+\n+      rMacros.specializedRecord(schema)(tq\"Serializable\")()(dataObjImpl) {\n+        case tpe if spImplsByTpe.contains(tpe) =>\n+          rMacros.genLookup(q\"fieldName\", spImplsByTpe(tpe), mayCache = false)\n+      }\n+    }\n+\n+    /**\n+     * Generate a tree that retrieves a given field for a given type.\n+     * Constructs a nested record if necessary\n+     */\n+    def genGetField(row: Tree, index: Int, t: DataType): Tree = t match {\n+      case t: PrimitiveType =>\n+        val methodName = newTermName(\"get\" + primitiveForType(t))\n+        q\"$row.$methodName($index)\"\n+      case StructType(structFields) =>\n+        val fields = structFields.map(f => (f.name, f.dataType))\n+        genRecord(q\"$row($index).asInstanceOf[$rowTpe]\", fields)\n+      case _ =>"
  }],
  "prId": 1759
}, {
  "comments": [{
    "author": {
      "login": "gzm0"
    },
    "body": "Lingering debug code\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T18:06:55Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)"
  }],
  "prId": 1759
}, {
  "comments": [{
    "author": {
      "login": "gzm0"
    },
    "body": "Lingering debug code\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T18:07:08Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Is there a clean way to do logging from macro code that isn't printlns?\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T18:07:38Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)"
  }, {
    "author": {
      "login": "gzm0"
    },
    "body": "What do you mean by logging? Do you mean some kind of compile-time-log? Not that I heard of.\n\nMacro debugging is yet an open issue, usual techniques include:\n- println\n- -Xprint:typer flag for scalac\n- showTree in REPL\n\nBut of course none of these should remain in production code IMHO.\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T22:10:24Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)"
  }, {
    "author": {
      "login": "vjovanov"
    },
    "body": "You can look what I made in Yin-Yang. It works for us quite well. Here are the links:\n\nMethod:\nhttps://github.com/vjovanov/yin-yang/blob/master/components/core/src/Utils.scala#L59\nConfig: \nhttps://github.com/vjovanov/yin-yang/blob/master/components/yin-yang/src/YYTransformer.scala#L16\nUsage:\nhttps://github.com/vjovanov/yin-yang/blob/master/components/yin-yang/src/YYTransformer.scala#L72\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-05T10:39:42Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)"
  }],
  "prId": 1759
}, {
  "comments": [{
    "author": {
      "login": "gzm0"
    },
    "body": "IIUC, a null value in a primitive field will cause a NPE. We might want to use the java types for all the primitive fields and rely on implicit conversion on the usage site. This way, a NPE can be avoided at the usage site. It comes at the cost of boxing everything though. \n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T18:11:26Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)\n+      c.Expr(tree)\n+    }\n+\n+    // TODO: Handle nullable fields"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Yeah, this is a good question about our interfaces.  I see a couple of ways we could handle this:\n- Have a separate isNull method and make calling the primitive accessor invalid when that method returns true.  If a user fails to check we throw an exception.\n- Box everything\n- Use nullability information as follows:  Return `Option[Type]` when the attribute is nullable, return the primitive when it is not nullable.  Right now we don't do a great job in the optimizer of propagating nullability information, but overtime this should get better.  That way we could avoid the cost of `Option` on any attribute that was involved in a predicate that would prevent it from being null.\n\nPersonally I like the last option the best.  It makes it very explicit to users when things could be null, and still gives them a way to get high-performance access when a primitive value cannot be null.  It however does introduce some possible confusion (i.e. changing the query in subtle ways, such as adding a predicate, could change the return types). This approach also requires the most work to be done improving catalyst's analysis.\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T18:25:26Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)\n+      c.Expr(tree)\n+    }\n+\n+    // TODO: Handle nullable fields"
  }, {
    "author": {
      "login": "gzm0"
    },
    "body": "After having given this some thought, I first liked the 3rd approach best, I'd like to advocate for the 1st:\n\nMapping a nullable type to an option makes sense if it comes directly from a database layout. However, most of the nullability in the uses cases for SQL will probably come from joins and are therefore potentially ruled out by further conditions. \n\nIMHO changing the return type of a query based on complex static analysis of the SQL query is a _very_ bad idea, especially since these types can't be ascribed.\n\nTherefore, it seems better to leave it up to the user to check this (and consider it a limitation of SQL's type system) and provide a `isNull` and a `toOption` method. \n\nIt is unclear to me how this is implementable. A value class with these members and an implicit conversion to its contained type might be a possibility. \n\nWDYT?\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-04T22:22:41Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)\n+      c.Expr(tree)\n+    }\n+\n+    // TODO: Handle nullable fields"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Your point about changing return type based on complex static analysis is well taken and that is my hesitation as well.  That said...\n\nMy though was that you could do something that is similar to a type ascription by adding a `WHERE a IS NOT NULL` or similar to the query whenever you don't want to deal with the option type.  This forces the programmer to explicitly denote a handling for null values (filter them out).\n\nRegarding joins, for inner joins you won't change the nullability of any output attributes so it'll still relate to the database schema.  For outer joins I think we _should_ be forcing the programmer to explicitly deal with the fact that they are introducing nullability though their choice of join.\n\nAnother possibility here would be to have two interpolators, one with boxing costs but simple semantics and one with explicit Options or primitives based on the SQL analysis.\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-05T00:59:00Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)\n+      c.Expr(tree)\n+    }\n+\n+    // TODO: Handle nullable fields"
  }, {
    "author": {
      "login": "gzm0"
    },
    "body": "I was more talking about things like (all schema values non-nullable)\n\n``` sql\nSELECT p1.name, AVG(p2.height) as average\nFROM players AS p1 LEFT JOIN players AS p2 ON p2.score > p1.score\nGROUP BY p1.id, p1.name\nHAVING average - p1.height > 0 -- implicitly filters average for null\n```\n\nversus\n\n``` sql\nSELECT p1.name, AVG(p2.height) as average\nFROM players AS p1 INNER JOIN players AS p2 ON p2.score > p1.score\nGROUP BY p1.id, p1.name\nHAVING average - p1.height > 0 -- average is NOT NULL since we inner join\n```\n\nwhat should the return type of `average` be in the first example? Would we require the programmer to explicitly add `average IS NOT NULL`? \n\nI don't have a more flagrant example at hand, but I'm just concerned that these kind of things could lead to tons of \"useless\" `IS NOT NULL` clauses...\n\nOn the other hand, I see that not using boxing (through `Option`) in the front end only pushes the problem farther to the backend, since it needs nullability to make storage decisions I suppose. (However, this is transparent to the user, so can be changed easily)\n\nLet me know what you think..\n",
    "commit": "677fa3d1cfffa7129cd5c8ab01b8918234cf4ce5",
    "createdAt": "2014-08-07T08:08:02Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.analysis._\n+import org.apache.spark.sql.catalyst.expressions.{Expression, ScalaUdf, AttributeReference}\n+import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n+import org.apache.spark.sql.catalyst.types._\n+\n+import scala.language.experimental.macros\n+import scala.language.existentials\n+\n+import records._\n+import Macros.RecordMacros\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.{SqlParser, ScalaReflection}\n+\n+/**\n+ * A collection of Scala macros for working with SQL in a type-safe way.\n+ */\n+private[sql] object SQLMacros {\n+  import scala.reflect.macros._\n+\n+  def sqlImpl(c: Context)(args: c.Expr[Any]*) =\n+    new Macros[c.type](c).sql(args)\n+\n+  case class Schema(dataType: DataType, nullable: Boolean)\n+\n+  class Macros[C <: Context](val c: C) extends ScalaReflection {\n+    val universe: c.universe.type = c.universe\n+\n+    import c.universe._\n+\n+    val rowTpe = tq\"_root_.org.apache.spark.sql.catalyst.expressions.Row\"\n+\n+    val rMacros = new RecordMacros[c.type](c)\n+\n+    trait InterpolatedItem {\n+      def placeholderName: String\n+      def registerCode: Tree\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry)\n+    }\n+\n+    case class InterpolatedUDF(index: Int, expr: c.Expr[Any], returnType: DataType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"func$index\"\n+\n+      def registerCode = q\"\"\"registerFunction($placeholderName, $expr)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        registry.registerFunction(\n+          placeholderName, (_: Seq[Expression]) => ScalaUdf(null, returnType, Nil))\n+      }\n+    }\n+\n+    case class InterpolatedTable(index: Int, expr: c.Expr[Any], schema: StructType)\n+      extends InterpolatedItem{\n+\n+      val placeholderName = s\"table$index\"\n+\n+      def registerCode = q\"\"\"$expr.registerTempTable($placeholderName)\"\"\"\n+\n+      def localRegister(catalog: Catalog, registry: FunctionRegistry) = {\n+        catalog.registerTable(None, placeholderName, LocalRelation(schema.toAttributes :_*))\n+      }\n+    }\n+\n+    case class RecSchema(name: String, index: Int, cType: DataType, tpe: Type)\n+\n+    def sql(args: Seq[c.Expr[Any]]) = {\n+\n+      val q\"\"\"\n+        $interpName(\n+          scala.StringContext.apply(..$rawParts))\"\"\" = c.prefix.tree\n+\n+      //rawParts.map(_.toString).foreach(println)\n+\n+      val parts =\n+        rawParts.map(\n+          _.toString.stripPrefix(\"\\\"\")\n+           .replaceAll(\"\\\\\\\\\", \"\")\n+           .stripSuffix(\"\\\"\"))\n+\n+      val interpolatedArguments = args.zipWithIndex.map { case (arg, i) =>\n+        // println(arg + \" \" + arg.actualType)\n+        arg.actualType match {\n+          case TypeRef(_, _, Seq(schemaType)) =>\n+            InterpolatedTable(i, arg, schemaFor(schemaType).dataType.asInstanceOf[StructType])\n+          case TypeRef(_, _, Seq(inputType, outputType)) =>\n+            InterpolatedUDF(i, arg, schemaFor(outputType).dataType)\n+        }\n+      }\n+\n+      val query = parts(0) + args.indices.map { i =>\n+        interpolatedArguments(i).placeholderName + parts(i + 1)\n+      }.mkString(\"\")\n+\n+      val parser = new SqlParser()\n+      val logicalPlan = parser(query)\n+      val catalog = new SimpleCatalog(true)\n+      val functionRegistry = new SimpleFunctionRegistry\n+      val analyzer = new Analyzer(catalog, functionRegistry, true)\n+\n+      interpolatedArguments.foreach(_.localRegister(catalog, functionRegistry))\n+      val analyzedPlan = analyzer(logicalPlan)\n+\n+      val fields = analyzedPlan.output.map(attr => (attr.name, attr.dataType))\n+      val record = genRecord(q\"row\", fields)\n+\n+      val tree = q\"\"\"\n+        ..${interpolatedArguments.map(_.registerCode)}\n+        val result = sql($query)\n+        result.map(row => $record)\n+      \"\"\"\n+\n+      // println(tree)\n+      c.Expr(tree)\n+    }\n+\n+    // TODO: Handle nullable fields"
  }],
  "prId": 1759
}]