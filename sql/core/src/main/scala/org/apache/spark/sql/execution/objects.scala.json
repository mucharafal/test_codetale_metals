[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "To avoid copying the bytes, here I create safe rows. However, according to https://github.com/apache/spark/pull/10511, operators should always produce unsafe rows. Actually python UDF operator(`BatchPythonEvaluation`) also produce safe rows, which may also have problems. Should we bring back the `requireUnsafeRow` stuff? In some cases like here, converting to unsafe rows is expensive and may not have much benefit.\n\ncc @davies \n",
    "commit": "1095d7f8c217ec006dd3b538d4d49da2cf5a287d",
    "createdAt": "2016-02-15T07:19:03Z",
    "diffHunk": "@@ -67,6 +74,72 @@ case class MapPartitions(\n   }\n }\n \n+case class PythonMapPartitions(\n+    func: PythonFunction,\n+    output: Seq[Attribute],\n+    child: SparkPlan) extends UnaryNode {\n+\n+  override def expressions: Seq[Expression] = Nil\n+\n+  private def isPickled(schema: StructType): Boolean = {\n+    schema.length == 1 && schema.head.dataType == BinaryType &&\n+      schema.head.metadata.contains(\"pickled\")\n+  }\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute().map(_.copy())\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val childIsPickled = isPickled(child.schema)\n+    val outputIsPickled = isPickled(schema)\n+\n+    inputRDD.mapPartitions { iter =>\n+      val inputIterator = if (childIsPickled) {\n+        iter.map(_.getBinary(0))\n+      } else {\n+        EvaluatePython.registerPicklers()  // register pickler for Row\n+\n+        val pickle = new Pickler\n+\n+        // Input iterator to Python: input rows are grouped so we send them in batches to Python.\n+        // For each row, add it to the queue.\n+        iter.grouped(100).map { inputRows =>\n+          val toBePickled = inputRows.map { row =>\n+            EvaluatePython.toJava(row, child.schema)\n+          }.toArray\n+          pickle.dumps(toBePickled)\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // Output iterator for results from Python.\n+      val outputIterator =\n+        new PythonRunner(\n+          func.command,\n+          func.envVars,\n+          func.pythonIncludes,\n+          func.pythonExec,\n+          func.pythonVer,\n+          func.broadcastVars,\n+          func.accumulator,\n+          bufferSize,\n+          reuseWorker\n+        ).compute(inputIterator, context.partitionId(), context)\n+\n+      if (outputIsPickled) {\n+        outputIterator.map(bytes => InternalRow(bytes))"
  }, {
    "author": {
      "login": "davies"
    },
    "body": "BatchPythonEvaluation will produce UnsafeRow.\n",
    "commit": "1095d7f8c217ec006dd3b538d4d49da2cf5a287d",
    "createdAt": "2016-02-15T08:01:33Z",
    "diffHunk": "@@ -67,6 +74,72 @@ case class MapPartitions(\n   }\n }\n \n+case class PythonMapPartitions(\n+    func: PythonFunction,\n+    output: Seq[Attribute],\n+    child: SparkPlan) extends UnaryNode {\n+\n+  override def expressions: Seq[Expression] = Nil\n+\n+  private def isPickled(schema: StructType): Boolean = {\n+    schema.length == 1 && schema.head.dataType == BinaryType &&\n+      schema.head.metadata.contains(\"pickled\")\n+  }\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute().map(_.copy())\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val childIsPickled = isPickled(child.schema)\n+    val outputIsPickled = isPickled(schema)\n+\n+    inputRDD.mapPartitions { iter =>\n+      val inputIterator = if (childIsPickled) {\n+        iter.map(_.getBinary(0))\n+      } else {\n+        EvaluatePython.registerPicklers()  // register pickler for Row\n+\n+        val pickle = new Pickler\n+\n+        // Input iterator to Python: input rows are grouped so we send them in batches to Python.\n+        // For each row, add it to the queue.\n+        iter.grouped(100).map { inputRows =>\n+          val toBePickled = inputRows.map { row =>\n+            EvaluatePython.toJava(row, child.schema)\n+          }.toArray\n+          pickle.dumps(toBePickled)\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // Output iterator for results from Python.\n+      val outputIterator =\n+        new PythonRunner(\n+          func.command,\n+          func.envVars,\n+          func.pythonIncludes,\n+          func.pythonExec,\n+          func.pythonVer,\n+          func.broadcastVars,\n+          func.accumulator,\n+          bufferSize,\n+          reuseWorker\n+        ).compute(inputIterator, context.partitionId(), context)\n+\n+      if (outputIsPickled) {\n+        outputIterator.map(bytes => InternalRow(bytes))"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Oh sorry, I missed the unsafe projection at the very last. Then we can probably add an unsafe projection here too.\n",
    "commit": "1095d7f8c217ec006dd3b538d4d49da2cf5a287d",
    "createdAt": "2016-02-15T08:17:01Z",
    "diffHunk": "@@ -67,6 +74,72 @@ case class MapPartitions(\n   }\n }\n \n+case class PythonMapPartitions(\n+    func: PythonFunction,\n+    output: Seq[Attribute],\n+    child: SparkPlan) extends UnaryNode {\n+\n+  override def expressions: Seq[Expression] = Nil\n+\n+  private def isPickled(schema: StructType): Boolean = {\n+    schema.length == 1 && schema.head.dataType == BinaryType &&\n+      schema.head.metadata.contains(\"pickled\")\n+  }\n+\n+  override protected def doExecute(): RDD[InternalRow] = {\n+    val inputRDD = child.execute().map(_.copy())\n+    val bufferSize = inputRDD.conf.getInt(\"spark.buffer.size\", 65536)\n+    val reuseWorker = inputRDD.conf.getBoolean(\"spark.python.worker.reuse\", defaultValue = true)\n+    val childIsPickled = isPickled(child.schema)\n+    val outputIsPickled = isPickled(schema)\n+\n+    inputRDD.mapPartitions { iter =>\n+      val inputIterator = if (childIsPickled) {\n+        iter.map(_.getBinary(0))\n+      } else {\n+        EvaluatePython.registerPicklers()  // register pickler for Row\n+\n+        val pickle = new Pickler\n+\n+        // Input iterator to Python: input rows are grouped so we send them in batches to Python.\n+        // For each row, add it to the queue.\n+        iter.grouped(100).map { inputRows =>\n+          val toBePickled = inputRows.map { row =>\n+            EvaluatePython.toJava(row, child.schema)\n+          }.toArray\n+          pickle.dumps(toBePickled)\n+        }\n+      }\n+\n+      val context = TaskContext.get()\n+\n+      // Output iterator for results from Python.\n+      val outputIterator =\n+        new PythonRunner(\n+          func.command,\n+          func.envVars,\n+          func.pythonIncludes,\n+          func.pythonExec,\n+          func.pythonVer,\n+          func.broadcastVars,\n+          func.accumulator,\n+          bufferSize,\n+          reuseWorker\n+        ).compute(inputIterator, context.partitionId(), context)\n+\n+      if (outputIsPickled) {\n+        outputIterator.map(bytes => InternalRow(bytes))"
  }],
  "prId": 11117
}]