[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "How about changing it to\r\n```Scala\r\n      sparkSession.sessionState.executePlan(dataSource.planForWriting(saveMode, query)).toRdd\r\n```",
    "commit": "5221c7cf11dc0accfcd1205177d0332bca042ffc",
    "createdAt": "2017-12-12T19:37:33Z",
    "diffHunk": "@@ -67,8 +67,9 @@ case class InsertIntoDataSourceDirCommand(\n \n     val saveMode = if (overwrite) SaveMode.Overwrite else SaveMode.ErrorIfExists\n     try {\n-      sparkSession.sessionState.executePlan(dataSource.planForWriting(saveMode, query))\n-      dataSource.writeAndRead(saveMode, query)",
    "line": 5
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "I implemented like this at first, but after checking this patch (https://github.com/apache/spark/pull/18064/files), I changed to current implementation, is the wrapping of execution id unnecessary here?",
    "commit": "5221c7cf11dc0accfcd1205177d0332bca042ffc",
    "createdAt": "2017-12-13T13:36:48Z",
    "diffHunk": "@@ -67,8 +67,9 @@ case class InsertIntoDataSourceDirCommand(\n \n     val saveMode = if (overwrite) SaveMode.Overwrite else SaveMode.ErrorIfExists\n     try {\n-      sparkSession.sessionState.executePlan(dataSource.planForWriting(saveMode, query))\n-      dataSource.writeAndRead(saveMode, query)",
    "line": 5
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "I think it is not needed. You can try it. ",
    "commit": "5221c7cf11dc0accfcd1205177d0332bca042ffc",
    "createdAt": "2017-12-13T22:57:48Z",
    "diffHunk": "@@ -67,8 +67,9 @@ case class InsertIntoDataSourceDirCommand(\n \n     val saveMode = if (overwrite) SaveMode.Overwrite else SaveMode.ErrorIfExists\n     try {\n-      sparkSession.sessionState.executePlan(dataSource.planForWriting(saveMode, query))\n-      dataSource.writeAndRead(saveMode, query)",
    "line": 5
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "You're right, test the query \"INSERT OVERWRITE DIRECTORY '/home/liyuanjian/tmp' USING json SELECT 1 AS a, 'c' as b;\".\r\n![image](https://user-images.githubusercontent.com/4833765/33997144-6d159446-e11e-11e7-8bae-f485873d84c3.png)",
    "commit": "5221c7cf11dc0accfcd1205177d0332bca042ffc",
    "createdAt": "2017-12-14T14:33:33Z",
    "diffHunk": "@@ -67,8 +67,9 @@ case class InsertIntoDataSourceDirCommand(\n \n     val saveMode = if (overwrite) SaveMode.Overwrite else SaveMode.ErrorIfExists\n     try {\n-      sparkSession.sessionState.executePlan(dataSource.planForWriting(saveMode, query))\n-      dataSource.writeAndRead(saveMode, query)",
    "line": 5
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Could we revert all the changes except this?",
    "commit": "5221c7cf11dc0accfcd1205177d0332bca042ffc",
    "createdAt": "2017-12-14T17:55:59Z",
    "diffHunk": "@@ -67,8 +67,9 @@ case class InsertIntoDataSourceDirCommand(\n \n     val saveMode = if (overwrite) SaveMode.Overwrite else SaveMode.ErrorIfExists\n     try {\n-      sparkSession.sessionState.executePlan(dataSource.planForWriting(saveMode, query))\n-      dataSource.writeAndRead(saveMode, query)",
    "line": 5
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Revert done.\r\nSorry, maybe I misunderstand your words of 'get rid of dataSource.writeAndRead'. Like you and Wenchen's discussion in https://github.com/apache/spark/pull/16481, shouldn't we make `writeAndRead` just return a BaseRelation without write to the destination? Thank you for your patient reply.",
    "commit": "5221c7cf11dc0accfcd1205177d0332bca042ffc",
    "createdAt": "2017-12-15T03:46:09Z",
    "diffHunk": "@@ -67,8 +67,9 @@ case class InsertIntoDataSourceDirCommand(\n \n     val saveMode = if (overwrite) SaveMode.Overwrite else SaveMode.ErrorIfExists\n     try {\n-      sparkSession.sessionState.executePlan(dataSource.planForWriting(saveMode, query))\n-      dataSource.writeAndRead(saveMode, query)",
    "line": 5
  }],
  "prId": 19941
}]