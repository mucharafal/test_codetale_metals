[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "`numOutputRows += outputRows`\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-09-30T22:48:40Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  registerGauge(\"inputRate.total\", currentInputRate)\n+  registerGauge(\"processingRate.total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate.total\", () => currentOutputRate)\n+  registerGauge(\"latencyMs\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate.${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate.${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "no .. its numOutputRows in the current trigger.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-01T00:27:42Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  registerGauge(\"inputRate.total\", currentInputRate)\n+  registerGauge(\"processingRate.total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate.total\", () => currentOutputRate)\n+  registerGauge(\"latencyMs\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate.${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate.${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows"
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "I'm confused by this comment.  Shouldn't metrics be agnostic as to the type of sink, not just ganglia?  Are hyphens valid in identifier names for all currently used sinks?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-03T16:47:43Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group",
    "line": 65
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Metrics are generally agnostic to sinks, but different sinks probably have different ways of representing data visually. I tested with Ganglia (assuming that its the most common one in production), found that it clusters based on the last period. So multiple metrics names A.B.X, A.B.Y, A.B.Z will be put in a cluster A.B . \n\nRegarding validity, we using Codahales `Metrics.name(partialName1, partialName2, ...)` underneath which is supposed to check formats and all that stuff. So I am assuming that whatever passes through that check will be valid for all sinks. And hyphen seems to be fine with Ganglia sink.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-03T20:05:32Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group",
    "line": 65
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "Is there a reason not to just make RateCalculator monoidal so it sums correctly, even if you later changed to different time intervals per source?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-03T17:37:32Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerInfo(STATUS_MESSAGE, \"\")\n+    reportTriggerInfo(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerInfo(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerInfo(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerInfo(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum",
    "line": 157
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Could you elaborate further with an example?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-03T20:00:06Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerInfo(STATUS_MESSAGE, \"\")\n+    reportTriggerInfo(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerInfo(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerInfo(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerInfo(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum",
    "line": 157
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "If RateCalculator has a zero and an addition method, then you can do\n\n```\ninputRates.map(_._2).foldLeft(RateCalculator.zero)(_+_)\n```\n\nand not have to rely on the comment about using currentRate being safe because time intervals are the same.\nIf you don't want to mess with greatest common divisor or whatever until you have an actual different interval, you can just make the addition method throw unless the intervals in the two rate objects are the same.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-03T21:18:13Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerInfo(STATUS_MESSAGE, \"\")\n+    reportTriggerInfo(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerInfo(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerInfo(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerInfo(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum",
    "line": 157
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "> using the same time interval for all sources\n\nActually, I noticed that this is not true right now. There is a race condition since source infos are updated separately. Then it's possible that when this method is called, some sources have the new info while the others still have the info for last batch. Right?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-04T18:15:06Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerInfo(STATUS_MESSAGE, \"\")\n+    reportTriggerInfo(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerInfo(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerInfo(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerInfo(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum",
    "line": 157
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "How about updating all sources together in one method?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-04T18:17:57Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerInfo(STATUS_MESSAGE, \"\")\n+    reportTriggerInfo(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerInfo(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerInfo(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerInfo(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum",
    "line": 157
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "For the record, resolved. Synchronization has been simplified by the stream execution thread updating a volatile reference to the query status, every time any of components used in the status is updated. StreamingQuery.status only read the current value of the volatile reference. See `updateStatus()` in the PR.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-07T01:28:30Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerInfo = new mutable.HashMap[String, String]\n+  private val sourceTriggerInfo = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerInfo.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerInfo.clear()\n+    sourceTriggerInfo.values.foreach(_.clear())\n+\n+    reportTriggerInfo(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerInfo(s, TRIGGER_ID, triggerId))\n+    reportTriggerInfo(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTimestamp(key: String): Unit = synchronized {\n+    triggerInfo.put(key, triggerClock.getTimeMillis().toString)\n+  }\n+\n+  def reportLatency(key: String, latencyMs: Long): Unit = synchronized {\n+    triggerInfo.put(key, latencyMs.toString)\n+  }\n+\n+  def reportLatency(source: Source, key: String, latencyMs: Long): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, latencyMs.toString)\n+  }\n+\n+  def reportTriggerInfo[T](key: String, value: T): Unit = synchronized {\n+    triggerInfo.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerInfo[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerInfo(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerInfo(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerInfo(STATUS_MESSAGE, \"\")\n+    reportTriggerInfo(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerInfo(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerInfo(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerInfo(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum",
    "line": 157
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "I would avoid inner classes unless there is a reason.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-04T23:09:31Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentOutputRate(): Double = synchronized { outputRate.currentRate }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    outputRate.stop()\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+\n+  class RateCalculator {"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Also, scala doc\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-05T00:39:38Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentOutputRate(): Double = synchronized { outputRate.currentRate }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    outputRate.stop()\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+\n+  class RateCalculator {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I made that inner class because I wanted to wrap the simple state management of the rate calculation, without polluting the namespace with a class that is used only inside StreamMetrics.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-05T01:21:20Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+  private val outputRate = new RateCalculator\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"outputRate\", () => currentOutputRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumRows(inputRows: Map[Source, Long], outputRows: Option[Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+    numOutputRows = outputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update output rate = num rows output to the sink in current trigger duration\n+    outputRate.update(numOutputRows.getOrElse(0), currentTriggerDuration)\n+    logDebug(\"Output rate updated to \" + outputRate.currentRate)\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentOutputRate(): Double = synchronized { outputRate.currentRate }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    outputRate.stop()\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+\n+  class RateCalculator {"
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Scaladoc\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-04T23:10:21Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "added.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-05T01:21:25Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.util.Clock\n+\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)"
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "can't sources be dynamically added/removed, e.g. in Kafka?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T00:58:44Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>",
    "line": 70
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Once a query has started, sources cant be dynamically added. At least not as of now. A KafkaSource internally can dynamically add topics and partitions (in case that is what you are confused about). \n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T18:17:29Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>",
    "line": 70
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "yeah, I'm not sure how the Kafka source works exactly. I thought every topic there was a source. I guess the only place where you'll have multiple sources is if you're unioning multiple file sources or a file source with a kafka stream. I'm not sure if we even support that though.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T21:16:30Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>",
    "line": 70
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "even if there are multiple sources, the set of sources is currently not dynamic. so this is not a concern.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T23:22:06Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>",
    "line": 70
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "won't this blow up Codahale with hundreds of metrics? I believe it also greatly hurts ganglia performance. \n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T01:02:07Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+  /** Simple utility class to calculate rate while avoiding DivideByZero */\n+  class RateCalculator {\n+    @volatile private var rate: Option[Double] = None\n+\n+    def update(numRows: Long, timeGapMs: Long): Unit = {\n+      if (timeGapMs > 0) {\n+        rate = Some(numRows.toDouble * 1000 / timeGapMs)\n+      } else {\n+        rate = None\n+        logDebug(s\"Rate updates cannot with zero or negative time gap $timeGapMs\")\n+      }\n+    }\n+\n+    def currentRate: Double = rate.getOrElse(0.0)\n+\n+    def stop(): Unit = { rate = None }\n+  }\n+\n+\n+  val TRIGGER_ID = \"triggerId\"\n+  val ACTIVE = \"isActive\"\n+  val DATA_AVAILABLE = \"isDataAvailable\"\n+  val STATUS_MESSAGE = \"statusMessage\"\n+\n+  val START_TIMESTAMP = \"timestamp.triggerStart\"\n+  val GET_OFFSET_TIMESTAMP = \"timestamp.afterGetOffset\"\n+  val GET_BATCH_TIMESTAMP = \"timestamp.afterGetBatch\"\n+  val FINISH_TIMESTAMP = \"timestamp.triggerFinish\"\n+\n+  val GET_OFFSET_LATENCY = \"latency.getOffset\"\n+  val OFFSET_WAL_WRITE_LATENCY = \"latency.offsetLogWrite\"\n+  val GET_BATCH_LATENCY = \"latency.getBatch\"\n+  val TRIGGER_LATENCY = \"latency.fullTrigger\"\n+  val SOURCE_GET_OFFSET_LATENCY = \"latency.sourceGetOffset\"\n+  val SOURCE_GET_BATCH_LATENCY = \"latency.sourceGetBatch\"\n+\n+  val NUM_INPUT_ROWS = \"numRows.input.total\"\n+  val NUM_OUTPUT_ROWS = \"numRows.output\"\n+  val NUM_SOURCE_INPUT_ROWS = \"numRows.input.source\"\n+  def NUM_TOTAL_STATE_ROWS(aggId: Int): String = s\"numRows.state.aggregation$aggId.total\""
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "ok, I think this is not used as a new metric name in Codahale right?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T01:02:35Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+  /** Simple utility class to calculate rate while avoiding DivideByZero */\n+  class RateCalculator {\n+    @volatile private var rate: Option[Double] = None\n+\n+    def update(numRows: Long, timeGapMs: Long): Unit = {\n+      if (timeGapMs > 0) {\n+        rate = Some(numRows.toDouble * 1000 / timeGapMs)\n+      } else {\n+        rate = None\n+        logDebug(s\"Rate updates cannot with zero or negative time gap $timeGapMs\")\n+      }\n+    }\n+\n+    def currentRate: Double = rate.getOrElse(0.0)\n+\n+    def stop(): Unit = { rate = None }\n+  }\n+\n+\n+  val TRIGGER_ID = \"triggerId\"\n+  val ACTIVE = \"isActive\"\n+  val DATA_AVAILABLE = \"isDataAvailable\"\n+  val STATUS_MESSAGE = \"statusMessage\"\n+\n+  val START_TIMESTAMP = \"timestamp.triggerStart\"\n+  val GET_OFFSET_TIMESTAMP = \"timestamp.afterGetOffset\"\n+  val GET_BATCH_TIMESTAMP = \"timestamp.afterGetBatch\"\n+  val FINISH_TIMESTAMP = \"timestamp.triggerFinish\"\n+\n+  val GET_OFFSET_LATENCY = \"latency.getOffset\"\n+  val OFFSET_WAL_WRITE_LATENCY = \"latency.offsetLogWrite\"\n+  val GET_BATCH_LATENCY = \"latency.getBatch\"\n+  val TRIGGER_LATENCY = \"latency.fullTrigger\"\n+  val SOURCE_GET_OFFSET_LATENCY = \"latency.sourceGetOffset\"\n+  val SOURCE_GET_BATCH_LATENCY = \"latency.sourceGetBatch\"\n+\n+  val NUM_INPUT_ROWS = \"numRows.input.total\"\n+  val NUM_OUTPUT_ROWS = \"numRows.output\"\n+  val NUM_SOURCE_INPUT_ROWS = \"numRows.input.source\"\n+  def NUM_TOTAL_STATE_ROWS(aggId: Int): String = s\"numRows.state.aggregation$aggId.total\""
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "This is a good point. As of now, every started streaming query would dump stuff to metrics, even there is not need to monitor them. Maybe we should have a query specific, or session-specific flag to enable Dropwizard metric reporting for streaming queries.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T18:19:25Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+  /** Simple utility class to calculate rate while avoiding DivideByZero */\n+  class RateCalculator {\n+    @volatile private var rate: Option[Double] = None\n+\n+    def update(numRows: Long, timeGapMs: Long): Unit = {\n+      if (timeGapMs > 0) {\n+        rate = Some(numRows.toDouble * 1000 / timeGapMs)\n+      } else {\n+        rate = None\n+        logDebug(s\"Rate updates cannot with zero or negative time gap $timeGapMs\")\n+      }\n+    }\n+\n+    def currentRate: Double = rate.getOrElse(0.0)\n+\n+    def stop(): Unit = { rate = None }\n+  }\n+\n+\n+  val TRIGGER_ID = \"triggerId\"\n+  val ACTIVE = \"isActive\"\n+  val DATA_AVAILABLE = \"isDataAvailable\"\n+  val STATUS_MESSAGE = \"statusMessage\"\n+\n+  val START_TIMESTAMP = \"timestamp.triggerStart\"\n+  val GET_OFFSET_TIMESTAMP = \"timestamp.afterGetOffset\"\n+  val GET_BATCH_TIMESTAMP = \"timestamp.afterGetBatch\"\n+  val FINISH_TIMESTAMP = \"timestamp.triggerFinish\"\n+\n+  val GET_OFFSET_LATENCY = \"latency.getOffset\"\n+  val OFFSET_WAL_WRITE_LATENCY = \"latency.offsetLogWrite\"\n+  val GET_BATCH_LATENCY = \"latency.getBatch\"\n+  val TRIGGER_LATENCY = \"latency.fullTrigger\"\n+  val SOURCE_GET_OFFSET_LATENCY = \"latency.sourceGetOffset\"\n+  val SOURCE_GET_BATCH_LATENCY = \"latency.sourceGetBatch\"\n+\n+  val NUM_INPUT_ROWS = \"numRows.input.total\"\n+  val NUM_OUTPUT_ROWS = \"numRows.output\"\n+  val NUM_SOURCE_INPUT_ROWS = \"numRows.input.source\"\n+  def NUM_TOTAL_STATE_ROWS(aggId: Int): String = s\"numRows.state.aggregation$aggId.total\""
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "@marmbrus @zsxwing what do you think?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T18:19:39Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+  /** Simple utility class to calculate rate while avoiding DivideByZero */\n+  class RateCalculator {\n+    @volatile private var rate: Option[Double] = None\n+\n+    def update(numRows: Long, timeGapMs: Long): Unit = {\n+      if (timeGapMs > 0) {\n+        rate = Some(numRows.toDouble * 1000 / timeGapMs)\n+      } else {\n+        rate = None\n+        logDebug(s\"Rate updates cannot with zero or negative time gap $timeGapMs\")\n+      }\n+    }\n+\n+    def currentRate: Double = rate.getOrElse(0.0)\n+\n+    def stop(): Unit = { rate = None }\n+  }\n+\n+\n+  val TRIGGER_ID = \"triggerId\"\n+  val ACTIVE = \"isActive\"\n+  val DATA_AVAILABLE = \"isDataAvailable\"\n+  val STATUS_MESSAGE = \"statusMessage\"\n+\n+  val START_TIMESTAMP = \"timestamp.triggerStart\"\n+  val GET_OFFSET_TIMESTAMP = \"timestamp.afterGetOffset\"\n+  val GET_BATCH_TIMESTAMP = \"timestamp.afterGetBatch\"\n+  val FINISH_TIMESTAMP = \"timestamp.triggerFinish\"\n+\n+  val GET_OFFSET_LATENCY = \"latency.getOffset\"\n+  val OFFSET_WAL_WRITE_LATENCY = \"latency.offsetLogWrite\"\n+  val GET_BATCH_LATENCY = \"latency.getBatch\"\n+  val TRIGGER_LATENCY = \"latency.fullTrigger\"\n+  val SOURCE_GET_OFFSET_LATENCY = \"latency.sourceGetOffset\"\n+  val SOURCE_GET_BATCH_LATENCY = \"latency.sourceGetBatch\"\n+\n+  val NUM_INPUT_ROWS = \"numRows.input.total\"\n+  val NUM_OUTPUT_ROWS = \"numRows.output\"\n+  val NUM_SOURCE_INPUT_ROWS = \"numRows.input.source\"\n+  def NUM_TOTAL_STATE_ROWS(aggId: Int): String = s\"numRows.state.aggregation$aggId.total\""
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "We use our own metrics, I'm sure others do as well.  Having a way to turn them off would be useful.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T18:24:04Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+  /** Simple utility class to calculate rate while avoiding DivideByZero */\n+  class RateCalculator {\n+    @volatile private var rate: Option[Double] = None\n+\n+    def update(numRows: Long, timeGapMs: Long): Unit = {\n+      if (timeGapMs > 0) {\n+        rate = Some(numRows.toDouble * 1000 / timeGapMs)\n+      } else {\n+        rate = None\n+        logDebug(s\"Rate updates cannot with zero or negative time gap $timeGapMs\")\n+      }\n+    }\n+\n+    def currentRate: Double = rate.getOrElse(0.0)\n+\n+    def stop(): Unit = { rate = None }\n+  }\n+\n+\n+  val TRIGGER_ID = \"triggerId\"\n+  val ACTIVE = \"isActive\"\n+  val DATA_AVAILABLE = \"isDataAvailable\"\n+  val STATUS_MESSAGE = \"statusMessage\"\n+\n+  val START_TIMESTAMP = \"timestamp.triggerStart\"\n+  val GET_OFFSET_TIMESTAMP = \"timestamp.afterGetOffset\"\n+  val GET_BATCH_TIMESTAMP = \"timestamp.afterGetBatch\"\n+  val FINISH_TIMESTAMP = \"timestamp.triggerFinish\"\n+\n+  val GET_OFFSET_LATENCY = \"latency.getOffset\"\n+  val OFFSET_WAL_WRITE_LATENCY = \"latency.offsetLogWrite\"\n+  val GET_BATCH_LATENCY = \"latency.getBatch\"\n+  val TRIGGER_LATENCY = \"latency.fullTrigger\"\n+  val SOURCE_GET_OFFSET_LATENCY = \"latency.sourceGetOffset\"\n+  val SOURCE_GET_BATCH_LATENCY = \"latency.sourceGetBatch\"\n+\n+  val NUM_INPUT_ROWS = \"numRows.input.total\"\n+  val NUM_OUTPUT_ROWS = \"numRows.output\"\n+  val NUM_SOURCE_INPUT_ROWS = \"numRows.input.source\"\n+  def NUM_TOTAL_STATE_ROWS(aggId: Int): String = s\"numRows.state.aggregation$aggId.total\""
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I added a sqlconf to disable it by default, can be enabled by setting that metric when needed for production uses. \n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-11T23:21:23Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var numOutputRows: Option[Long] = None\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    numOutputRows = None\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)\n+\n+    // Report number of rows\n+    val totalNumInputRows = numInputRows.values.sum\n+    reportTriggerStatus(NUM_INPUT_ROWS, totalNumInputRows)\n+    reportTriggerStatus(NUM_OUTPUT_ROWS, numOutputRows.getOrElse(0))\n+    numInputRows.foreach { case (s, r) =>\n+      reportSourceTriggerStatus(s, NUM_SOURCE_INPUT_ROWS, r)\n+    }\n+\n+    val currentTriggerDuration = currentTriggerFinishTimestamp - currentTriggerStartTimestamp\n+    val previousInputIntervalOption = if (previousTriggerStartTimestamp >= 0) {\n+      Some(currentTriggerStartTimestamp - previousTriggerStartTimestamp)\n+    } else None\n+\n+    // Update input rate = num rows received by each source during the previous trigger interval\n+    // Interval is measures as interval between start times of previous and current trigger.\n+    //\n+    // TODO: Instead of trigger start, we should use time when getOffset was called on each source\n+    // as this may be different for each source if there are many sources in the query plan\n+    // and getOffset is called serially on them.\n+    if (previousInputIntervalOption.nonEmpty) {\n+      sources.foreach { s =>\n+        inputRates(s).update(numInputRows.getOrElse(s, 0), previousInputIntervalOption.get)\n+      }\n+    }\n+\n+    // Update processing rate = num rows processed for each source in current trigger duration\n+    sources.foreach { s =>\n+      processingRates(s).update(numInputRows.getOrElse(s, 0), currentTriggerDuration)\n+    }\n+\n+    // Update latency = if data present, 0.5 * previous trigger interval + current trigger duration\n+    if (previousInputIntervalOption.nonEmpty && totalNumInputRows > 0) {\n+      latency = Some((previousInputIntervalOption.get.toDouble / 2) + currentTriggerDuration)\n+    } else {\n+      latency = None\n+    }\n+\n+    previousTriggerStartTimestamp = currentTriggerStartTimestamp\n+    currentTriggerStartTimestamp = -1\n+  }\n+\n+  // =========== Getter methods ===========\n+\n+  def currentInputRate(): Double = synchronized {\n+    // Since we are calculating source input rates using the same time interval for all sources\n+    // it is fine to calculate total input rate as the sum of per source input rate.\n+    inputRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceInputRate(source: Source): Double = synchronized {\n+    inputRates(source).currentRate\n+  }\n+\n+  def currentProcessingRate(): Double = synchronized {\n+    // Since we are calculating source processing rates using the same time interval for all sources\n+    // it is fine to calculate total processing rate as the sum of per source processing rate.\n+    processingRates.map(_._2.currentRate).sum\n+  }\n+\n+  def currentSourceProcessingRate(source: Source): Double = synchronized {\n+    processingRates(source).currentRate\n+  }\n+\n+  def currentLatency(): Option[Double] = synchronized { latency }\n+\n+  def currentTriggerStatus(): Map[String, String] = synchronized { triggerStatus.toMap }\n+\n+  def currentSourceTriggerStatus(source: Source): Map[String, String] = synchronized {\n+    sourceTriggerStatus(source).toMap\n+  }\n+\n+  // =========== Other methods ===========\n+\n+  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {\n+    synchronized {\n+      metricRegistry.register(name, new Gauge[T] {\n+        override def getValue: T = f()\n+      })\n+    }\n+  }\n+\n+  def stop(): Unit = synchronized {\n+    inputRates.valuesIterator.foreach { _.stop() }\n+    processingRates.valuesIterator.foreach { _.stop() }\n+    latency = None\n+  }\n+}\n+\n+object StreamMetrics extends Logging {\n+  /** Simple utility class to calculate rate while avoiding DivideByZero */\n+  class RateCalculator {\n+    @volatile private var rate: Option[Double] = None\n+\n+    def update(numRows: Long, timeGapMs: Long): Unit = {\n+      if (timeGapMs > 0) {\n+        rate = Some(numRows.toDouble * 1000 / timeGapMs)\n+      } else {\n+        rate = None\n+        logDebug(s\"Rate updates cannot with zero or negative time gap $timeGapMs\")\n+      }\n+    }\n+\n+    def currentRate: Double = rate.getOrElse(0.0)\n+\n+    def stop(): Unit = { rate = None }\n+  }\n+\n+\n+  val TRIGGER_ID = \"triggerId\"\n+  val ACTIVE = \"isActive\"\n+  val DATA_AVAILABLE = \"isDataAvailable\"\n+  val STATUS_MESSAGE = \"statusMessage\"\n+\n+  val START_TIMESTAMP = \"timestamp.triggerStart\"\n+  val GET_OFFSET_TIMESTAMP = \"timestamp.afterGetOffset\"\n+  val GET_BATCH_TIMESTAMP = \"timestamp.afterGetBatch\"\n+  val FINISH_TIMESTAMP = \"timestamp.triggerFinish\"\n+\n+  val GET_OFFSET_LATENCY = \"latency.getOffset\"\n+  val OFFSET_WAL_WRITE_LATENCY = \"latency.offsetLogWrite\"\n+  val GET_BATCH_LATENCY = \"latency.getBatch\"\n+  val TRIGGER_LATENCY = \"latency.fullTrigger\"\n+  val SOURCE_GET_OFFSET_LATENCY = \"latency.sourceGetOffset\"\n+  val SOURCE_GET_BATCH_LATENCY = \"latency.sourceGetBatch\"\n+\n+  val NUM_INPUT_ROWS = \"numRows.input.total\"\n+  val NUM_OUTPUT_ROWS = \"numRows.output\"\n+  val NUM_SOURCE_INPUT_ROWS = \"numRows.input.source\"\n+  def NUM_TOTAL_STATE_ROWS(aggId: Int): String = s\"numRows.state.aggregation$aggId.total\""
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Is there a reason this isn't just a listener that forwards the information from the Status?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-12T19:12:20Z",
    "diffHunk": "@@ -0,0 +1,240 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics",
    "line": 34
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "It could be built that way. But some class has to collect the trigger details in a map while a trigger is active. There are two options regarding where to keep the map\n1. Inside StreamExecution\n2. Inside another class with the similar life cycle as StreamExecution. \n\nThe first one, I felt, would clutter the StreamExecution internal with non-essential code. Hence I went with the second option and put everything in StreamMetrics, which has the same lifecycle as StreamExecution and updated/read only by the StreamExecution thread.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-12T20:35:15Z",
    "diffHunk": "@@ -0,0 +1,240 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics",
    "line": 34
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Sorry, I think its reasonable to put the metrics in a separate class since there is a lot of code there (though some of the methods are in StreamExecution and I was a little unclear on why).  I'm asking why this also handles the Codahale metrics.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-12T20:56:49Z",
    "diffHunk": "@@ -0,0 +1,240 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics",
    "line": 34
  }],
  "prId": 15307
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "What is the meaning of `ACTIVE`?  It seems it is different than `state == ACTIVE`?\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-12T19:39:25Z",
    "diffHunk": "@@ -0,0 +1,240 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "No not the same. I can renamed this to TRIGGER_ACTIVE to avoid confusion from the query state ACTIVE.\n",
    "commit": "59722126a74a03ca2e0b0e8c2f3e3477a302aad0",
    "createdAt": "2016-10-12T20:26:22Z",
    "diffHunk": "@@ -0,0 +1,240 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.{util => ju}\n+\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Gauge, MetricRegistry}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.metrics.source.{Source => CodahaleSource}\n+import org.apache.spark.util.Clock\n+\n+/**\n+ * Class that manages all the metrics related to a StreamingQuery. It does the following.\n+ * - Calculates metrics (rates, latencies, etc.) based on information reported by StreamExecution.\n+ * - Allows the current metric values to be queried\n+ * - Serves some of the metrics through Codahale/DropWizard metrics\n+ *\n+ * @param sources Unique set of sources in a query\n+ * @param triggerClock Clock used for triggering in StreamExecution\n+ * @param codahaleSourceName Root name for all the Codahale metrics\n+ */\n+class StreamMetrics(sources: Set[Source], triggerClock: Clock, codahaleSourceName: String)\n+  extends CodahaleSource with Logging {\n+\n+  import StreamMetrics._\n+\n+  // Trigger infos\n+  private val triggerStatus = new mutable.HashMap[String, String]\n+  private val sourceTriggerStatus = new mutable.HashMap[Source, mutable.HashMap[String, String]]\n+\n+  // Rate estimators for sources and sinks\n+  private val inputRates = new mutable.HashMap[Source, RateCalculator]\n+  private val processingRates = new mutable.HashMap[Source, RateCalculator]\n+\n+  // Number of input rows in the current trigger\n+  private val numInputRows = new mutable.HashMap[Source, Long]\n+  private var currentTriggerStartTimestamp: Long = -1\n+  private var previousTriggerStartTimestamp: Long = -1\n+  private var latency: Option[Double] = None\n+\n+  override val sourceName: String = codahaleSourceName\n+  override val metricRegistry: MetricRegistry = new MetricRegistry\n+\n+  // =========== Initialization ===========\n+\n+  // Metric names should not have . in them, so that all the metrics of a query are identified\n+  // together in Ganglia as a single metric group\n+  registerGauge(\"inputRate-total\", currentInputRate)\n+  registerGauge(\"processingRate-total\", () => currentProcessingRate)\n+  registerGauge(\"latency\", () => currentLatency().getOrElse(-1.0))\n+\n+  sources.foreach { s =>\n+    inputRates.put(s, new RateCalculator)\n+    processingRates.put(s, new RateCalculator)\n+    sourceTriggerStatus.put(s, new mutable.HashMap[String, String])\n+\n+    registerGauge(s\"inputRate-${s.toString}\", () => currentSourceInputRate(s))\n+    registerGauge(s\"processingRate-${s.toString}\", () => currentSourceProcessingRate(s))\n+  }\n+\n+  // =========== Setter methods ===========\n+\n+  def reportTriggerStarted(triggerId: Long): Unit = synchronized {\n+    numInputRows.clear()\n+    triggerStatus.clear()\n+    sourceTriggerStatus.values.foreach(_.clear())\n+\n+    reportTriggerStatus(TRIGGER_ID, triggerId)\n+    sources.foreach(s => reportSourceTriggerStatus(s, TRIGGER_ID, triggerId))\n+    reportTriggerStatus(ACTIVE, true)\n+    currentTriggerStartTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(START_TIMESTAMP, currentTriggerStartTimestamp)\n+  }\n+\n+  def reportTriggerStatus[T](key: String, value: T): Unit = synchronized {\n+    triggerStatus.put(key, value.toString)\n+  }\n+\n+  def reportSourceTriggerStatus[T](source: Source, key: String, value: T): Unit = synchronized {\n+    sourceTriggerStatus(source).put(key, value.toString)\n+  }\n+\n+  def reportNumInputRows(inputRows: Map[Source, Long]): Unit = synchronized {\n+    numInputRows ++= inputRows\n+  }\n+\n+  def reportTriggerFinished(): Unit = synchronized {\n+    require(currentTriggerStartTimestamp >= 0)\n+    val currentTriggerFinishTimestamp = triggerClock.getTimeMillis()\n+    reportTriggerStatus(FINISH_TIMESTAMP, currentTriggerFinishTimestamp)\n+    reportTriggerStatus(STATUS_MESSAGE, \"\")\n+    reportTriggerStatus(ACTIVE, false)"
  }],
  "prId": 15307
}]