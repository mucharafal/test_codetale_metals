[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "We aren't actually using AppendOnlyMap though right?\n",
    "commit": "188221e41b69741f5060480445945f45e75d0fc5",
    "createdAt": "2014-08-18T02:01:57Z",
    "diffHunk": "@@ -19,16 +19,15 @@ package org.apache.spark.sql.execution\n \n import java.util.{HashMap => JavaHashMap}\n \n-import scala.collection.mutable.{ArrayBuffer, BitSet}\n import scala.concurrent.ExecutionContext.Implicits.global\n import scala.concurrent._\n import scala.concurrent.duration._\n \n import org.apache.spark.annotation.DeveloperApi\n-import org.apache.spark.sql.SQLContext\n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.plans._\n import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.util.collection.{AppendOnlyMap, CompactBuffer}"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "Ah true, I'll remove the import\n",
    "commit": "188221e41b69741f5060480445945f45e75d0fc5",
    "createdAt": "2014-08-18T03:44:07Z",
    "diffHunk": "@@ -19,16 +19,15 @@ package org.apache.spark.sql.execution\n \n import java.util.{HashMap => JavaHashMap}\n \n-import scala.collection.mutable.{ArrayBuffer, BitSet}\n import scala.concurrent.ExecutionContext.Implicits.global\n import scala.concurrent._\n import scala.concurrent.duration._\n \n import org.apache.spark.annotation.DeveloperApi\n-import org.apache.spark.sql.SQLContext\n import org.apache.spark.sql.catalyst.expressions._\n import org.apache.spark.sql.catalyst.plans._\n import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.util.collection.{AppendOnlyMap, CompactBuffer}"
  }],
  "prId": 1993
}]