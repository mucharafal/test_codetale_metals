[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This comment looks out-of-date, probably a result of the splitting of the larger patch.\n",
    "commit": "c02fc3f4179df860ebb8c24614247c016c3603e6",
    "createdAt": "2015-08-03T04:14:45Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.optimizer\n+\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.{Inner, LeftOuter, RightOuter, LeftSemi}\n+import org.apache.spark.sql.catalyst.plans.logical.{Project, Filter, Join, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+/**\n+ * An optimization rule used to insert Filters to filter out rows whose equal join keys\n+ * have at least one null values. For this kind of rows, they will not contribute to\n+ * the join results of equal joins because a null does not equal another null. We can\n+ * filter them out before shuffling join input rows. For example, we have two tables\n+ *\n+ * table1(key String, value Int)\n+ * \"str1\"|1\n+ * null  |2\n+ *\n+ * table2(key String, value Int)\n+ * \"str1\"|3\n+ * null  |4\n+ *\n+ * For a inner equal join, the result will be\n+ * \"str1\"|1|\"str1\"|3\n+ *\n+ * those two rows having null as the value of key will not contribute to the result.\n+ * So, we can filter them out early.\n+ *\n+ * This optimization rule can be disabled by setting spark.sql.advancedOptimization to false.\n+ *\n+ */\n+case class FilterNullsInJoinKey(\n+    sqlContext: SQLContext)\n+  extends Rule[LogicalPlan] {\n+\n+  /**\n+   * Checks if we need to add a Filter operator. We will add a Filter when\n+   * there is any attribute in `keys` whose corresponding attribute of `keys`\n+   * in `plan.output` is still nullable (`nullable` field is `true`).\n+   */\n+  private def needsFilter(keys: Seq[Expression], plan: LogicalPlan): Boolean = {\n+    val keyAttributeSet = AttributeSet(keys.filter(_.isInstanceOf[Attribute]))\n+    plan.output.filter(keyAttributeSet.contains).exists(_.nullable)\n+  }\n+\n+  /**\n+   * Adds a Filter operator to make sure that every attribute in `keys` is non-nullable.\n+   */\n+  private def addFilterIfNecessary(\n+      keys: Seq[Expression],\n+      child: LogicalPlan): LogicalPlan = {\n+    // We get all attributes from keys.\n+    val attributes = keys.filter {\n+      case attr: Attribute => true\n+      case _ => false\n+    }\n+\n+    // Then, we create a Filter to make sure these attributes are non-nullable.\n+    val filter =\n+      if (attributes.nonEmpty) {\n+        Filter(Not(AtLeastNNulls(1, attributes)), child)\n+      } else {\n+        child\n+      }\n+\n+    // We return attributes representing keys (keyAttributes) and the filter."
  }],
  "prId": 7768
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Could just be `keys.filter(_.isInstanceOf[Attribute])`.\n",
    "commit": "c02fc3f4179df860ebb8c24614247c016c3603e6",
    "createdAt": "2015-08-03T04:15:15Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.optimizer\n+\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.{Inner, LeftOuter, RightOuter, LeftSemi}\n+import org.apache.spark.sql.catalyst.plans.logical.{Project, Filter, Join, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+/**\n+ * An optimization rule used to insert Filters to filter out rows whose equal join keys\n+ * have at least one null values. For this kind of rows, they will not contribute to\n+ * the join results of equal joins because a null does not equal another null. We can\n+ * filter them out before shuffling join input rows. For example, we have two tables\n+ *\n+ * table1(key String, value Int)\n+ * \"str1\"|1\n+ * null  |2\n+ *\n+ * table2(key String, value Int)\n+ * \"str1\"|3\n+ * null  |4\n+ *\n+ * For a inner equal join, the result will be\n+ * \"str1\"|1|\"str1\"|3\n+ *\n+ * those two rows having null as the value of key will not contribute to the result.\n+ * So, we can filter them out early.\n+ *\n+ * This optimization rule can be disabled by setting spark.sql.advancedOptimization to false.\n+ *\n+ */\n+case class FilterNullsInJoinKey(\n+    sqlContext: SQLContext)\n+  extends Rule[LogicalPlan] {\n+\n+  /**\n+   * Checks if we need to add a Filter operator. We will add a Filter when\n+   * there is any attribute in `keys` whose corresponding attribute of `keys`\n+   * in `plan.output` is still nullable (`nullable` field is `true`).\n+   */\n+  private def needsFilter(keys: Seq[Expression], plan: LogicalPlan): Boolean = {\n+    val keyAttributeSet = AttributeSet(keys.filter(_.isInstanceOf[Attribute]))\n+    plan.output.filter(keyAttributeSet.contains).exists(_.nullable)\n+  }\n+\n+  /**\n+   * Adds a Filter operator to make sure that every attribute in `keys` is non-nullable.\n+   */\n+  private def addFilterIfNecessary(\n+      keys: Seq[Expression],\n+      child: LogicalPlan): LogicalPlan = {\n+    // We get all attributes from keys.\n+    val attributes = keys.filter {"
  }],
  "prId": 7768
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "These arguments are slightly underindented.\n",
    "commit": "c02fc3f4179df860ebb8c24614247c016c3603e6",
    "createdAt": "2015-08-03T04:16:32Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.optimizer\n+\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\n+import org.apache.spark.sql.catalyst.plans.{Inner, LeftOuter, RightOuter, LeftSemi}\n+import org.apache.spark.sql.catalyst.plans.logical.{Project, Filter, Join, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+\n+/**\n+ * An optimization rule used to insert Filters to filter out rows whose equal join keys\n+ * have at least one null values. For this kind of rows, they will not contribute to\n+ * the join results of equal joins because a null does not equal another null. We can\n+ * filter them out before shuffling join input rows. For example, we have two tables\n+ *\n+ * table1(key String, value Int)\n+ * \"str1\"|1\n+ * null  |2\n+ *\n+ * table2(key String, value Int)\n+ * \"str1\"|3\n+ * null  |4\n+ *\n+ * For a inner equal join, the result will be\n+ * \"str1\"|1|\"str1\"|3\n+ *\n+ * those two rows having null as the value of key will not contribute to the result.\n+ * So, we can filter them out early.\n+ *\n+ * This optimization rule can be disabled by setting spark.sql.advancedOptimization to false.\n+ *\n+ */\n+case class FilterNullsInJoinKey(\n+    sqlContext: SQLContext)\n+  extends Rule[LogicalPlan] {\n+\n+  /**\n+   * Checks if we need to add a Filter operator. We will add a Filter when\n+   * there is any attribute in `keys` whose corresponding attribute of `keys`\n+   * in `plan.output` is still nullable (`nullable` field is `true`).\n+   */\n+  private def needsFilter(keys: Seq[Expression], plan: LogicalPlan): Boolean = {\n+    val keyAttributeSet = AttributeSet(keys.filter(_.isInstanceOf[Attribute]))\n+    plan.output.filter(keyAttributeSet.contains).exists(_.nullable)\n+  }\n+\n+  /**\n+   * Adds a Filter operator to make sure that every attribute in `keys` is non-nullable.\n+   */\n+  private def addFilterIfNecessary(\n+      keys: Seq[Expression],\n+      child: LogicalPlan): LogicalPlan = {\n+    // We get all attributes from keys.\n+    val attributes = keys.filter {\n+      case attr: Attribute => true\n+      case _ => false\n+    }\n+\n+    // Then, we create a Filter to make sure these attributes are non-nullable.\n+    val filter =\n+      if (attributes.nonEmpty) {\n+        Filter(Not(AtLeastNNulls(1, attributes)), child)\n+      } else {\n+        child\n+      }\n+\n+    // We return attributes representing keys (keyAttributes) and the filter.\n+    // keyAttributes will be used to rewrite the join condition.\n+    filter\n+  }\n+\n+  /**\n+   * We reconstruct the join condition.\n+   */\n+  private def reconstructJoinCondition(\n+    leftKeys: Seq[Expression],"
  }],
  "prId": 7768
}]