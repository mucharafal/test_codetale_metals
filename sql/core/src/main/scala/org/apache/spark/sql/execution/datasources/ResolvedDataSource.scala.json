[{
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "An alternative idea is to throw an exception ASAP so that users can easily understand error reasons.\n",
    "commit": "d2a1ecfd61b79b9f598b6b8c6150f95b42a7b107",
    "createdAt": "2016-02-19T11:00:52Z",
    "diffHunk": "@@ -130,7 +141,49 @@ object ResolvedDataSource extends Logging {\n       bucketSpec: Option[BucketSpec],\n       provider: String,\n       options: Map[String, String]): ResolvedDataSource = {\n-    val clazz: Class[_] = lookupDataSource(provider)\n+    // Here, it tries to find out data source by file extensions if the `format()` is not called.\n+    // The auto-detection is based on given paths and it recognizes glob pattern as well but\n+    // it does not recursively check the sub-paths even if the given paths are directories.\n+    // This source detection goes the following steps\n+    //\n+    //   1. Check `provider` and use this if this is not `null`.\n+    //   2. If `provider` is not given, then it tries to detect the source types by extension.\n+    //      at this point, if detects only if all the given paths have the same extension.\n+    //   3. if it fails to detect, use the datasource given to `spark.sql.sources.default`.\n+    //\n+    val paths = {\n+      val caseInsensitiveOptions = new CaseInsensitiveMap(options)\n+      if (caseInsensitiveOptions.contains(\"paths\") &&\n+        caseInsensitiveOptions.contains(\"path\")) {\n+        throw new AnalysisException(s\"Both path and paths options are present.\")\n+      }\n+      caseInsensitiveOptions.get(\"paths\")\n+        .map(_.split(\"(?<!\\\\\\\\),\").map(StringUtils.unEscapeString(_, '\\\\', ',')))\n+        .getOrElse(Array(caseInsensitiveOptions(\"path\")))\n+        .flatMap{ pathString =>\n+        val hdfsPath = new Path(pathString)\n+        val fs = hdfsPath.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+        val qualified = hdfsPath.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        SparkHadoopUtil.get.globPathIfNecessary(qualified).map(_.toString)\n+      }\n+    }\n+    val safeProvider = Option(provider).getOrElse {\n+      val safePaths = paths.filterNot { path =>\n+        FilenameUtils.getBaseName(path)\n+        path.startsWith(\"_\") || path.startsWith(\".\")\n+      }\n+      val extensions = safePaths.map { path =>\n+        FilenameUtils.getExtension(path).toLowerCase\n+      }\n+      val defaultDataSourceName = sqlContext.conf.defaultDataSourceName\n+      if (extensions.exists(extensions.head != _)) {\n+        defaultDataSourceName"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Then original call `read().load()` for files having no extensions would throw an exception which breaks backword compatibility.\n\nAlso, default data source option becomes useless and should be removed.\n\nIf we drop this for Spark 2.0, I think that is a reasonable idea.\n",
    "commit": "d2a1ecfd61b79b9f598b6b8c6150f95b42a7b107",
    "createdAt": "2016-02-19T11:09:05Z",
    "diffHunk": "@@ -130,7 +141,49 @@ object ResolvedDataSource extends Logging {\n       bucketSpec: Option[BucketSpec],\n       provider: String,\n       options: Map[String, String]): ResolvedDataSource = {\n-    val clazz: Class[_] = lookupDataSource(provider)\n+    // Here, it tries to find out data source by file extensions if the `format()` is not called.\n+    // The auto-detection is based on given paths and it recognizes glob pattern as well but\n+    // it does not recursively check the sub-paths even if the given paths are directories.\n+    // This source detection goes the following steps\n+    //\n+    //   1. Check `provider` and use this if this is not `null`.\n+    //   2. If `provider` is not given, then it tries to detect the source types by extension.\n+    //      at this point, if detects only if all the given paths have the same extension.\n+    //   3. if it fails to detect, use the datasource given to `spark.sql.sources.default`.\n+    //\n+    val paths = {\n+      val caseInsensitiveOptions = new CaseInsensitiveMap(options)\n+      if (caseInsensitiveOptions.contains(\"paths\") &&\n+        caseInsensitiveOptions.contains(\"path\")) {\n+        throw new AnalysisException(s\"Both path and paths options are present.\")\n+      }\n+      caseInsensitiveOptions.get(\"paths\")\n+        .map(_.split(\"(?<!\\\\\\\\),\").map(StringUtils.unEscapeString(_, '\\\\', ',')))\n+        .getOrElse(Array(caseInsensitiveOptions(\"path\")))\n+        .flatMap{ pathString =>\n+        val hdfsPath = new Path(pathString)\n+        val fs = hdfsPath.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+        val qualified = hdfsPath.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        SparkHadoopUtil.get.globPathIfNecessary(qualified).map(_.toString)\n+      }\n+    }\n+    val safeProvider = Option(provider).getOrElse {\n+      val safePaths = paths.filterNot { path =>\n+        FilenameUtils.getBaseName(path)\n+        path.startsWith(\"_\") || path.startsWith(\".\")\n+      }\n+      val extensions = safePaths.map { path =>\n+        FilenameUtils.getExtension(path).toLowerCase\n+      }\n+      val defaultDataSourceName = sqlContext.conf.defaultDataSourceName\n+      if (extensions.exists(extensions.head != _)) {\n+        defaultDataSourceName"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "Aha, I see. Either way, it'd be better to add tests in case files have inconsistent extensions for better test coverage.\n",
    "commit": "d2a1ecfd61b79b9f598b6b8c6150f95b42a7b107",
    "createdAt": "2016-02-19T11:12:37Z",
    "diffHunk": "@@ -130,7 +141,49 @@ object ResolvedDataSource extends Logging {\n       bucketSpec: Option[BucketSpec],\n       provider: String,\n       options: Map[String, String]): ResolvedDataSource = {\n-    val clazz: Class[_] = lookupDataSource(provider)\n+    // Here, it tries to find out data source by file extensions if the `format()` is not called.\n+    // The auto-detection is based on given paths and it recognizes glob pattern as well but\n+    // it does not recursively check the sub-paths even if the given paths are directories.\n+    // This source detection goes the following steps\n+    //\n+    //   1. Check `provider` and use this if this is not `null`.\n+    //   2. If `provider` is not given, then it tries to detect the source types by extension.\n+    //      at this point, if detects only if all the given paths have the same extension.\n+    //   3. if it fails to detect, use the datasource given to `spark.sql.sources.default`.\n+    //\n+    val paths = {\n+      val caseInsensitiveOptions = new CaseInsensitiveMap(options)\n+      if (caseInsensitiveOptions.contains(\"paths\") &&\n+        caseInsensitiveOptions.contains(\"path\")) {\n+        throw new AnalysisException(s\"Both path and paths options are present.\")\n+      }\n+      caseInsensitiveOptions.get(\"paths\")\n+        .map(_.split(\"(?<!\\\\\\\\),\").map(StringUtils.unEscapeString(_, '\\\\', ',')))\n+        .getOrElse(Array(caseInsensitiveOptions(\"path\")))\n+        .flatMap{ pathString =>\n+        val hdfsPath = new Path(pathString)\n+        val fs = hdfsPath.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+        val qualified = hdfsPath.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+        SparkHadoopUtil.get.globPathIfNecessary(qualified).map(_.toString)\n+      }\n+    }\n+    val safeProvider = Option(provider).getOrElse {\n+      val safePaths = paths.filterNot { path =>\n+        FilenameUtils.getBaseName(path)\n+        path.startsWith(\"_\") || path.startsWith(\".\")\n+      }\n+      val extensions = safePaths.map { path =>\n+        FilenameUtils.getExtension(path).toLowerCase\n+      }\n+      val defaultDataSourceName = sqlContext.conf.defaultDataSourceName\n+      if (extensions.exists(extensions.head != _)) {\n+        defaultDataSourceName"
  }],
  "prId": 11270
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "note that i'd move this detection code into a separate class, so we can unit test it.\n",
    "commit": "d2a1ecfd61b79b9f598b6b8c6150f95b42a7b107",
    "createdAt": "2016-02-20T06:08:28Z",
    "diffHunk": "@@ -130,7 +141,49 @@ object ResolvedDataSource extends Logging {\n       bucketSpec: Option[BucketSpec],\n       provider: String,\n       options: Map[String, String]): ResolvedDataSource = {\n-    val clazz: Class[_] = lookupDataSource(provider)\n+    // Here, it tries to find out data source by file extensions if the `format()` is not called.\n+    // The auto-detection is based on given paths and it recognizes glob pattern as well but\n+    // it does not recursively check the sub-paths even if the given paths are directories.\n+    // This source detection goes the following steps\n+    //\n+    //   1. Check `provider` and use this if this is not `null`.\n+    //   2. If `provider` is not given, then it tries to detect the source types by extension.\n+    //      at this point, if detects only if all the given paths have the same extension.\n+    //   3. if it fails to detect, use the datasource given to `spark.sql.sources.default`.\n+    //\n+    val paths = {"
  }],
  "prId": 11270
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "indentation is weird here\n",
    "commit": "d2a1ecfd61b79b9f598b6b8c6150f95b42a7b107",
    "createdAt": "2016-02-22T07:40:59Z",
    "diffHunk": "@@ -130,7 +131,28 @@ object ResolvedDataSource extends Logging {\n       bucketSpec: Option[BucketSpec],\n       provider: String,\n       options: Map[String, String]): ResolvedDataSource = {\n-    val clazz: Class[_] = lookupDataSource(provider)\n+    val paths = {\n+      val caseInsensitiveOptions = new CaseInsensitiveMap(options)\n+      if (caseInsensitiveOptions.contains(\"paths\") &&\n+        caseInsensitiveOptions.contains(\"path\")) {\n+        throw new AnalysisException(s\"Both path and paths options are present.\")\n+      }\n+      caseInsensitiveOptions.get(\"paths\")\n+        .map(_.split(\"(?<!\\\\\\\\),\").map(StringUtils.unEscapeString(_, '\\\\', ',')))\n+        .getOrElse(Array(caseInsensitiveOptions.getOrElse(\"path\", {\n+        throw new IllegalArgumentException(\"'path' is not specified\")\n+      })))"
  }],
  "prId": 11270
}]