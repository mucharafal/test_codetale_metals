[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We don't need to do this anymore. Now the plan is immutable, we have to create a new plan when applying push down optimizations, and we can also update `output` at that time.",
    "commit": "11220db7879798967cda85d2aa4e68fefb8ec646",
    "createdAt": "2018-01-31T04:52:52Z",
    "diffHunk": "@@ -19,50 +19,31 @@ package org.apache.spark.sql.execution.datasources.v2\n \n import java.util.Objects\n \n-import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference}\n-import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeSet, Expression}\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n \n /**\n- * A base class for data source reader holder with customized equals/hashCode methods.\n+ * A base class for data source v2 related query plan. It defines the equals/hashCode methods\n+ * according to some common information.\n  */\n-trait DataSourceReaderHolder {\n+trait DataSourceV2QueryPlan {\n \n-  /**\n-   * The full output of the data source reader, without column pruning.\n-   */\n-  def fullOutput: Seq[AttributeReference]\n+  def output: Seq[Attribute]\n+  def sourceClass: Class[_ <: DataSourceV2]\n+  def filters: Set[Expression]\n \n-  /**\n-   * The held data source reader.\n-   */\n-  def reader: DataSourceReader\n-\n-  /**\n-   * The metadata of this data source reader that can be used for equality test.\n-   */\n-  private def metadata: Seq[Any] = {\n-    val filters: Any = reader match {\n-      case s: SupportsPushDownCatalystFilters => s.pushedCatalystFilters().toSet\n-      case s: SupportsPushDownFilters => s.pushedFilters().toSet\n-      case _ => Nil\n-    }\n-    Seq(fullOutput, reader.getClass, reader.readSchema(), filters)\n-  }\n+  // The metadata of this data source relation that can be used for equality test.\n+  private def metadata: Seq[Any] = Seq(output, sourceClass, filters)\n \n   def canEqual(other: Any): Boolean\n \n   override def equals(other: Any): Boolean = other match {\n-    case other: DataSourceReaderHolder =>\n-      canEqual(other) && metadata.length == other.metadata.length &&\n-        metadata.zip(other.metadata).forall { case (l, r) => l == r }\n+    case other: DataSourceV2QueryPlan =>\n+      canEqual(other) && metadata == other.metadata\n     case _ => false\n   }\n \n   override def hashCode(): Int = {\n     metadata.map(Objects.hashCode).foldLeft(0)((a, b) => 31 * a + b)\n   }\n-\n-  lazy val output: Seq[Attribute] = reader.readSchema().map(_.name).map { name =>"
  }],
  "prId": 20448
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "We might need to emphasize this is for both physical and logical plans.",
    "commit": "11220db7879798967cda85d2aa4e68fefb8ec646",
    "createdAt": "2018-01-31T05:03:00Z",
    "diffHunk": "@@ -19,50 +19,31 @@ package org.apache.spark.sql.execution.datasources.v2\n \n import java.util.Objects\n \n-import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference}\n-import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeSet, Expression}\n+import org.apache.spark.sql.sources.v2.DataSourceV2\n \n /**\n- * A base class for data source reader holder with customized equals/hashCode methods.\n+ * A base class for data source v2 related query plan. It defines the equals/hashCode methods\n+ * according to some common information."
  }],
  "prId": 20448
}]