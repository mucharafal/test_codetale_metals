[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "For now, this class just copies ShuffledHashJoin; I'm going to edit it now to take advantage of the fact that the inputs are sorted.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-04T22:54:03Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, RightOuter, LeftOuter, JoinType}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.collection.CompactBuffer\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  // this is to manually construct an ordering that can be used to compare keys from both sides\n+  private val keyOrdering: RowOrdering = RowOrdering.forSchema(leftKeys.map(_.dataType))\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case FullOuter => Nil // when doing Full Outer join, NULL rows from both sides are not ordered.\n+    case _ => requiredOrders(leftKeys)\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n+\n+  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] =\n+    keys.map(SortOrder(_, Ascending))\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    // TODO(josh): why is this copying necessary?\n+    val leftResults = left.execute().map(_.copy())\n+    val rightResults = right.execute().map(_.copy())\n+    val joinedRow = new JoinedRow()\n+    leftResults.zipPartitions(rightResults) { (leftIter, rightIter) =>\n+      joinType match {\n+        case LeftOuter =>\n+          // TODO(josh): for SMJ we would buffer keys here:"
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Current status of this patch: supports left and right outer joins.  I'll finish full outer join tomorrow, which should be similar to the above cases in terms of overall strategy and code re-use.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-05T09:13:10Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, RightOuter, LeftOuter, JoinType}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.collection.CompactBuffer\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  // this is to manually construct an ordering that can be used to compare keys from both sides\n+  private val keyOrdering: RowOrdering = RowOrdering.forSchema(leftKeys.map(_.dataType))\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case FullOuter => Nil // when doing Full Outer join, NULL rows from both sides are not ordered.\n+    case _ => requiredOrders(leftKeys)\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n+\n+  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] =\n+    keys.map(SortOrder(_, Ascending))\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    val joinedRow = new JoinedRow()\n+    left.execute().zipPartitions(right.execute()) { (leftIter, rightIter) =>\n+      joinType match {\n+        case LeftOuter =>\n+          val smjScanner = new SortMergeJoinScanner(\n+            streamedKeyGenerator,\n+            buildKeyGenerator,\n+            keyOrdering,\n+            streamedIter = leftIter,\n+            buildIter = rightIter\n+          )\n+          // TODO(josh): this is a little terse and needs explanation:\n+          Iterator.continually(0).takeWhile(_ => smjScanner.findNextOuterJoinRows()).flatMap { _ =>\n+            leftOuterIterator(\n+              joinedRow.withLeft(smjScanner.getStreamedRow),\n+              smjScanner.getBuildMatches)\n+          }\n+\n+        case RightOuter =>\n+          val smjScanner = new SortMergeJoinScanner(\n+            streamedKeyGenerator,\n+            buildKeyGenerator,\n+            keyOrdering,\n+            streamedIter = rightIter,\n+            buildIter = leftIter\n+          )\n+          // TODO(josh): this is a little terse and needs explanation:\n+          Iterator.continually(0).takeWhile(_ => smjScanner.findNextOuterJoinRows()).flatMap { _ =>\n+            rightOuterIterator(\n+              smjScanner.getBuildMatches,\n+              joinedRow.withRight(smjScanner.getStreamedRow))\n+          }\n+\n+        case FullOuter =>\n+          // TODO(josh): handle this case efficiently in SMJ"
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This is a bit dense and I'd like suggestions on writing something more understandable yet similarly compact.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-05T09:14:09Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, RightOuter, LeftOuter, JoinType}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.collection.CompactBuffer\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  // this is to manually construct an ordering that can be used to compare keys from both sides\n+  private val keyOrdering: RowOrdering = RowOrdering.forSchema(leftKeys.map(_.dataType))\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case FullOuter => Nil // when doing Full Outer join, NULL rows from both sides are not ordered.\n+    case _ => requiredOrders(leftKeys)\n+  }\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n+\n+  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] =\n+    keys.map(SortOrder(_, Ascending))\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    val joinedRow = new JoinedRow()\n+    left.execute().zipPartitions(right.execute()) { (leftIter, rightIter) =>\n+      joinType match {\n+        case LeftOuter =>\n+          val smjScanner = new SortMergeJoinScanner(\n+            streamedKeyGenerator,\n+            buildKeyGenerator,\n+            keyOrdering,\n+            streamedIter = leftIter,\n+            buildIter = rightIter\n+          )\n+          // TODO(josh): this is a little terse and needs explanation:\n+          Iterator.continually(0).takeWhile(_ => smjScanner.findNextOuterJoinRows()).flatMap { _ =>"
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This is carried over from SortMergeJoin but there's actually two bad problems here:\n- This should not be called eagerly the constructor since that can lead to analysis errors because `leftKeys` might be unresolved when the operator is constructed.\n- This should not call `RowOrdering` directly, since that will always create an interpreted ordering.  Instead, we should be calling `newOrdering` inside of `doExecute()` in order to enable use of code generation.\n\nI am going to fix this issue in a few minutes by reviving an old ordering cleanup patch of mine.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-05T19:23:31Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, RightOuter, LeftOuter, JoinType}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.collection.CompactBuffer\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  // this is to manually construct an ordering that can be used to compare keys from both sides\n+  private val keyOrdering: RowOrdering = RowOrdering.forSchema(leftKeys.map(_.dataType))"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Fixes for both issues are in #7973.  I'm going to temporarily rebase / merge on top of this patch in order to unblock testing work on this patch\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-05T20:50:49Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, RightOuter, LeftOuter, JoinType}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+import org.apache.spark.util.collection.CompactBuffer\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  // this is to manually construct an ordering that can be used to compare keys from both sides\n+  private val keyOrdering: RowOrdering = RowOrdering.forSchema(leftKeys.map(_.dataType))"
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "This could only require clustered.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-08T16:20:44Z",
    "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil",
    "line": 76
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think that we need this to ensure that values with the same key are adjacent within a single partition. The child outputs are already clustered (see line 71), but that's not sufficient to ensure that the tuples with the same key appear adjacent to one another within the same partition.  Note that `SortMergeJoin` also declares this same `requiredChildOrdering`.\n\nThe existing documentation on this is confusing, though:\n\n``` scala\n/**\n * Represents data where tuples that share the same values for the `clustering`\n * [[Expression Expressions]] will be co-located. Based on the context, this\n * can mean such tuples are either co-located in the same partition or they will be contiguous\n * within a single partition.\n */\ncase class ClusteredDistribution(clustering: Seq[Expression]) extends Distribution {\n```\n\nHowever, `ClusteredDistribution` is satisfied by `HashPartitioning`, which I think implies that specifying `ClusteredDistribution` alone is not sufficient to ensure that tuples with the same key are adjacent in the output partitions.\n\nFinally, we do need to declare _some_ ordering so that we know whether we've passed the left or right iterator position where matches would be found.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-08T16:29:53Z",
    "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil",
    "line": 76
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Thanks for the explain, what we need is another kind of clustedInSidePartition, we can leave this optimization later (1.6). It's easy to have same prefix for sorting key(it's only generated from first column of sorting key), using hash of joining key as prefix will be better.\n\nIn this patch, we prefer SortMergeJoin than ShuffleHashJoin, not sure it will introduce performance regression or not, we should check that in QA.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-08T20:35:02Z",
    "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil",
    "line": 76
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I suppose that whether we should prefer hash- or sort-based join is going to depend on our data size.  Recalling one of our earlier discussions of aggregation, I wonder if the \"if you could only have one strategy, you would choose sort since it can handle larger inputs\" argument applies here.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-08T20:38:37Z",
    "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil",
    "line": 76
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Yes, it's safe to choose sort. For aggregation, we have to introduce hybrid aggregation to address the performance regression from sort based aggregation. We may also should do this for join in next release.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-08T20:46:17Z",
    "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil",
    "line": 76
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "I don't think we need to worry about sort regression for joins right now at\nall. Just switch to sorting for shuffle join.\n\nFor aggregation the problem is that aggregation leads to data reduction,\nand hash based aggregation exploits that fact, whereas sort based\naggregation doesn't and that's why it is really important to start with\nhash and fallback to sort.\n\nIn join however it is very unclear whether sort vs hash is better. There\nhave been a lot of papers coming out about this topic and it has never been\nconclusive.\n\nOn Saturday, August 8, 2015, Davies Liu notifications@github.com wrote:\n\n> In\n> sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeOuterJoin.scala\n> https://github.com/apache/spark/pull/7904#discussion_r36581693:\n> \n> > -      throw new IllegalArgumentException(\n> > -        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n> > -  }\n> >   +\n> > -  override def outputOrdering: Seq[SortOrder] = joinType match {\n> > -    case LeftOuter => requiredOrders(leftKeys)\n> > -    case RightOuter => requiredOrders(rightKeys)\n> > -    case x => throw new IllegalArgumentException(\n> > -      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n> > -  }\n> >   +\n> > -  override def requiredChildDistribution: Seq[Distribution] =\n> > -    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n> >   +\n> > -  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n> > -    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n> \n> Yes, it's safe to choose sort. For aggregation, we have to introduce\n> hybrid aggregation to address the performance regression from sort based\n> aggregation. We may also should do this for join in next release.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/7904/files#r36581693.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-08T21:25:26Z",
    "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil",
    "line": 76
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Quick question about this, actually: if the join keys contain nulls then a left or right outer join may output rows with null join keys. Does this have any impact on the `outputOrdering` (e.g. is it safe to say that it's still ordered by the left keys if those columns are nullable in the output)?  Presumably this is safe, since those nulls were also ordered in the input, but I just want to confirm.  @yhuai?\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-10T20:33:24Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, RowIterator, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    // For left and right outer joins, the output is partitioned by the streamed input's join keys.\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    // For left and right outer joins, the output is ordered by the streamed input's join keys.\n+    case LeftOuter => requiredOrders(leftKeys)",
    "line": 66
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Yeah, we can say output rows are order by left keys (as long as our sort operator groups them together). Rows with null right keys will not be grouped.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-10T21:30:28Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, RowIterator, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    // For left and right outer joins, the output is partitioned by the streamed input's join keys.\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    // For left and right outer joins, the output is ordered by the streamed input's join keys.\n+    case LeftOuter => requiredOrders(leftKeys)",
    "line": 66
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Looks like we need to use `while` loop at here?\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-10T23:27:36Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, RowIterator, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    // For left and right outer joins, the output is partitioned by the streamed input's join keys.\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    // For left and right outer joins, the output is ordered by the streamed input's join keys.\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n+\n+  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] = {\n+    // This must be ascending in order to agree with the `keyOrdering` defined in `doExecute()`.\n+    keys.map(SortOrder(_, Ascending))\n+  }\n+\n+  private def isUnsafeMode: Boolean = {\n+    (codegenEnabled && unsafeEnabled\n+      && UnsafeProjection.canSupport(leftKeys)\n+      && UnsafeProjection.canSupport(rightKeys)\n+      && UnsafeProjection.canSupport(schema))\n+  }\n+\n+  override def outputsUnsafeRows: Boolean = isUnsafeMode\n+  override def canProcessUnsafeRows: Boolean = isUnsafeMode\n+  override def canProcessSafeRows: Boolean = !isUnsafeMode\n+\n+  private def createLeftKeyGenerator(): Projection = {\n+    if (isUnsafeMode) {\n+      UnsafeProjection.create(leftKeys, left.output)\n+    } else {\n+      newProjection(leftKeys, left.output)\n+    }\n+  }\n+\n+  private def createRightKeyGenerator(): Projection = {\n+    if (isUnsafeMode) {\n+      UnsafeProjection.create(rightKeys, right.output)\n+    } else {\n+      newProjection(rightKeys, right.output)\n+    }\n+  }\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    left.execute().zipPartitions(right.execute()) { (leftIter, rightIter) =>\n+      // An ordering that can be used to compare keys from both sides.\n+      val keyOrdering = newNaturalAscendingOrdering(leftKeys.map(_.dataType))\n+      val boundCondition: (InternalRow) => Boolean = {\n+        condition.map { cond =>\n+          newPredicate(cond, left.output ++ right.output)\n+        }.getOrElse {\n+          (r: InternalRow) => true\n+        }\n+      }\n+      val resultProj: InternalRow => InternalRow = {\n+        if (isUnsafeMode) {\n+          UnsafeProjection.create(schema)\n+        } else {\n+          identity[InternalRow]\n+        }\n+      }\n+\n+      joinType match {\n+        case LeftOuter =>\n+          val smjScanner = new SortMergeJoinScanner(\n+            streamedKeyGenerator = createLeftKeyGenerator(),\n+            bufferedKeyGenerator = createRightKeyGenerator(),\n+            keyOrdering,\n+            streamedIter = RowIterator.fromScala(leftIter),\n+            bufferedIter = RowIterator.fromScala(rightIter)\n+          )\n+          val rightNullRow = new GenericInternalRow(right.output.length)\n+          new LeftOuterIterator(smjScanner, rightNullRow, boundCondition, resultProj).toScala\n+\n+        case RightOuter =>\n+          val smjScanner = new SortMergeJoinScanner(\n+            streamedKeyGenerator = createRightKeyGenerator(),\n+            bufferedKeyGenerator = createLeftKeyGenerator(),\n+            keyOrdering,\n+            streamedIter = RowIterator.fromScala(rightIter),\n+            bufferedIter = RowIterator.fromScala(leftIter)\n+          )\n+          val leftNullRow = new GenericInternalRow(left.output.length)\n+          new RightOuterIterator(smjScanner, leftNullRow, boundCondition, resultProj).toScala\n+\n+        case x =>\n+          throw new IllegalArgumentException(\n+            s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+      }\n+    }\n+  }\n+}\n+\n+\n+private class LeftOuterIterator(\n+    smjScanner: SortMergeJoinScanner,\n+    rightNullRow: InternalRow,\n+    boundCondition: InternalRow => Boolean,\n+    resultProj: InternalRow => InternalRow\n+  ) extends RowIterator {\n+  private[this] val joinedRow: JoinedRow = new JoinedRow()\n+  private[this] var rightIdx: Int = 0\n+  assert(smjScanner.getBufferedMatches.length == 0)\n+\n+  private def advanceLeft(): Boolean = {\n+    rightIdx = 0\n+    if (smjScanner.findNextOuterJoinRows()) {\n+      joinedRow.withLeft(smjScanner.getStreamedRow)\n+      if (smjScanner.getBufferedMatches.isEmpty) {\n+        // There are no matching right rows, so return nulls for the right row\n+        joinedRow.withRight(rightNullRow)\n+      } else {\n+        // Find the next row from the right input that satisfied the bound condition\n+        if (!advanceRightUntilBoundConditionSatisfied()) {\n+          joinedRow.withRight(rightNullRow)\n+        }\n+      }\n+      true\n+    } else {\n+      // Left input has been exhausted\n+      false\n+    }\n+  }\n+\n+  private def advanceRightUntilBoundConditionSatisfied(): Boolean = {\n+    var foundMatch: Boolean = false\n+    if (!foundMatch && rightIdx < smjScanner.getBufferedMatches.length) {"
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Use `while` loop at here?\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-10T23:27:48Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, RowIterator, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    // For left and right outer joins, the output is partitioned by the streamed input's join keys.\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    // For left and right outer joins, the output is ordered by the streamed input's join keys.\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n+\n+  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] = {\n+    // This must be ascending in order to agree with the `keyOrdering` defined in `doExecute()`.\n+    keys.map(SortOrder(_, Ascending))\n+  }\n+\n+  private def isUnsafeMode: Boolean = {\n+    (codegenEnabled && unsafeEnabled\n+      && UnsafeProjection.canSupport(leftKeys)\n+      && UnsafeProjection.canSupport(rightKeys)\n+      && UnsafeProjection.canSupport(schema))\n+  }\n+\n+  override def outputsUnsafeRows: Boolean = isUnsafeMode\n+  override def canProcessUnsafeRows: Boolean = isUnsafeMode\n+  override def canProcessSafeRows: Boolean = !isUnsafeMode\n+\n+  private def createLeftKeyGenerator(): Projection = {\n+    if (isUnsafeMode) {\n+      UnsafeProjection.create(leftKeys, left.output)\n+    } else {\n+      newProjection(leftKeys, left.output)\n+    }\n+  }\n+\n+  private def createRightKeyGenerator(): Projection = {\n+    if (isUnsafeMode) {\n+      UnsafeProjection.create(rightKeys, right.output)\n+    } else {\n+      newProjection(rightKeys, right.output)\n+    }\n+  }\n+\n+  override def doExecute(): RDD[InternalRow] = {\n+    left.execute().zipPartitions(right.execute()) { (leftIter, rightIter) =>\n+      // An ordering that can be used to compare keys from both sides.\n+      val keyOrdering = newNaturalAscendingOrdering(leftKeys.map(_.dataType))\n+      val boundCondition: (InternalRow) => Boolean = {\n+        condition.map { cond =>\n+          newPredicate(cond, left.output ++ right.output)\n+        }.getOrElse {\n+          (r: InternalRow) => true\n+        }\n+      }\n+      val resultProj: InternalRow => InternalRow = {\n+        if (isUnsafeMode) {\n+          UnsafeProjection.create(schema)\n+        } else {\n+          identity[InternalRow]\n+        }\n+      }\n+\n+      joinType match {\n+        case LeftOuter =>\n+          val smjScanner = new SortMergeJoinScanner(\n+            streamedKeyGenerator = createLeftKeyGenerator(),\n+            bufferedKeyGenerator = createRightKeyGenerator(),\n+            keyOrdering,\n+            streamedIter = RowIterator.fromScala(leftIter),\n+            bufferedIter = RowIterator.fromScala(rightIter)\n+          )\n+          val rightNullRow = new GenericInternalRow(right.output.length)\n+          new LeftOuterIterator(smjScanner, rightNullRow, boundCondition, resultProj).toScala\n+\n+        case RightOuter =>\n+          val smjScanner = new SortMergeJoinScanner(\n+            streamedKeyGenerator = createRightKeyGenerator(),\n+            bufferedKeyGenerator = createLeftKeyGenerator(),\n+            keyOrdering,\n+            streamedIter = RowIterator.fromScala(rightIter),\n+            bufferedIter = RowIterator.fromScala(leftIter)\n+          )\n+          val leftNullRow = new GenericInternalRow(left.output.length)\n+          new RightOuterIterator(smjScanner, leftNullRow, boundCondition, resultProj).toScala\n+\n+        case x =>\n+          throw new IllegalArgumentException(\n+            s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+      }\n+    }\n+  }\n+}\n+\n+\n+private class LeftOuterIterator(\n+    smjScanner: SortMergeJoinScanner,\n+    rightNullRow: InternalRow,\n+    boundCondition: InternalRow => Boolean,\n+    resultProj: InternalRow => InternalRow\n+  ) extends RowIterator {\n+  private[this] val joinedRow: JoinedRow = new JoinedRow()\n+  private[this] var rightIdx: Int = 0\n+  assert(smjScanner.getBufferedMatches.length == 0)\n+\n+  private def advanceLeft(): Boolean = {\n+    rightIdx = 0\n+    if (smjScanner.findNextOuterJoinRows()) {\n+      joinedRow.withLeft(smjScanner.getStreamedRow)\n+      if (smjScanner.getBufferedMatches.isEmpty) {\n+        // There are no matching right rows, so return nulls for the right row\n+        joinedRow.withRight(rightNullRow)\n+      } else {\n+        // Find the next row from the right input that satisfied the bound condition\n+        if (!advanceRightUntilBoundConditionSatisfied()) {\n+          joinedRow.withRight(rightNullRow)\n+        }\n+      }\n+      true\n+    } else {\n+      // Left input has been exhausted\n+      false\n+    }\n+  }\n+\n+  private def advanceRightUntilBoundConditionSatisfied(): Boolean = {\n+    var foundMatch: Boolean = false\n+    if (!foundMatch && rightIdx < smjScanner.getBufferedMatches.length) {\n+      foundMatch = boundCondition(joinedRow.withRight(smjScanner.getBufferedMatches(rightIdx)))\n+      rightIdx += 1\n+    }\n+    foundMatch\n+  }\n+\n+  override def advanceNext(): Boolean = {\n+    advanceRightUntilBoundConditionSatisfied() || advanceLeft()\n+  }\n+\n+  override def getRow: InternalRow = resultProj(joinedRow)\n+}\n+\n+private class RightOuterIterator(\n+    smjScanner: SortMergeJoinScanner,\n+    leftNullRow: InternalRow,\n+    boundCondition: InternalRow => Boolean,\n+    resultProj: InternalRow => InternalRow\n+  ) extends RowIterator {\n+  private[this] val joinedRow: JoinedRow = new JoinedRow()\n+  private[this] var leftIdx: Int = 0\n+  assert(smjScanner.getBufferedMatches.length == 0)\n+\n+  private def advanceRight(): Boolean = {\n+    leftIdx = 0\n+    if (smjScanner.findNextOuterJoinRows()) {\n+      joinedRow.withRight(smjScanner.getStreamedRow)\n+      if (smjScanner.getBufferedMatches.isEmpty) {\n+        // There are no matching left rows, so return nulls for the left row\n+        joinedRow.withLeft(leftNullRow)\n+      } else {\n+        // Find the next row from the left input that satisfied the bound condition\n+        if (!advanceLeftUntilBoundConditionSatisfied()) {\n+          joinedRow.withLeft(leftNullRow)\n+        }\n+      }\n+      true\n+    } else {\n+      // Right input has been exhausted\n+      false\n+    }\n+  }\n+\n+  private def advanceLeftUntilBoundConditionSatisfied(): Boolean = {\n+    var foundMatch: Boolean = false\n+    if (!foundMatch && leftIdx < smjScanner.getBufferedMatches.length) {"
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Why we check both `leftKeys` and `rightKeys` at here but only check `leftKeys` in the inner join operator?\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-10T23:27:53Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, RowIterator, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    // For left and right outer joins, the output is partitioned by the streamed input's join keys.\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    // For left and right outer joins, the output is ordered by the streamed input's join keys.\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n+\n+  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] = {\n+    // This must be ascending in order to agree with the `keyOrdering` defined in `doExecute()`.\n+    keys.map(SortOrder(_, Ascending))\n+  }\n+\n+  private def isUnsafeMode: Boolean = {\n+    (codegenEnabled && unsafeEnabled\n+      && UnsafeProjection.canSupport(leftKeys)\n+      && UnsafeProjection.canSupport(rightKeys)",
    "line": 86
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think that both `leftKeys` and `rightKeys` have the same types and `canSupport` currently only inspects the types. For clarity, however, it wouldn't hurt to add it.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-11T01:51:34Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{BinaryNode, RowIterator, SparkPlan}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs an sort merge outer join of two child relations.\n+ *\n+ * Note: this does not support full outer join yet; see SPARK-9730 for progress on this.\n+ */\n+@DeveloperApi\n+case class SortMergeOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode {\n+\n+  override def output: Seq[Attribute] = {\n+    joinType match {\n+      case LeftOuter =>\n+        left.output ++ right.output.map(_.withNullability(true))\n+      case RightOuter =>\n+        left.output.map(_.withNullability(true)) ++ right.output\n+      case x =>\n+        throw new IllegalArgumentException(\n+          s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = joinType match {\n+    // For left and right outer joins, the output is partitioned by the streamed input's join keys.\n+    case LeftOuter => left.outputPartitioning\n+    case RightOuter => right.outputPartitioning\n+    case x =>\n+      throw new IllegalArgumentException(\n+        s\"${getClass.getSimpleName} should not take $x as the JoinType\")\n+  }\n+\n+  override def outputOrdering: Seq[SortOrder] = joinType match {\n+    // For left and right outer joins, the output is ordered by the streamed input's join keys.\n+    case LeftOuter => requiredOrders(leftKeys)\n+    case RightOuter => requiredOrders(rightKeys)\n+    case x => throw new IllegalArgumentException(\n+      s\"SortMergeOuterJoin should not take $x as the JoinType\")\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] =\n+    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil\n+\n+  override def requiredChildOrdering: Seq[Seq[SortOrder]] =\n+    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil\n+\n+  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] = {\n+    // This must be ascending in order to agree with the `keyOrdering` defined in `doExecute()`.\n+    keys.map(SortOrder(_, Ascending))\n+  }\n+\n+  private def isUnsafeMode: Boolean = {\n+    (codegenEnabled && unsafeEnabled\n+      && UnsafeProjection.canSupport(leftKeys)\n+      && UnsafeProjection.canSupport(rightKeys)",
    "line": 86
  }],
  "prId": 7904
}]