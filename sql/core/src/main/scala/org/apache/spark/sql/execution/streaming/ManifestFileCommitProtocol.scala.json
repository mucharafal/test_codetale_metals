[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Is this just an optimization to avoid instantiating the fs for empty writes?\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T00:54:46Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.UUID\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.datasources.FileCommitProtocol\n+import org.apache.spark.sql.execution.datasources.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * A [[FileCommitProtocol]] that tracks the list of valid files in a manifest file, used in\n+ * structured streaming.\n+ *\n+ * @param path path to write the final output to.\n+ */\n+class ManifestFileCommitProtocol(path: String)\n+  extends FileCommitProtocol with Serializable with Logging {\n+\n+  // Track the list of files added by a task, only used on the executors.\n+  @transient private var addedFiles: ArrayBuffer[String] = _\n+\n+  @transient private var fileLog: FileStreamSinkLog = _\n+  private var batchId: Long = _\n+\n+  /**\n+   * Sets up the manifest log output and the batch id for this job.\n+   * Must be called before any other function.\n+   */\n+  def setupManifestOptions(fileLog: FileStreamSinkLog, batchId: Long): Unit = {\n+    this.fileLog = fileLog\n+    this.batchId = batchId\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    // Do nothing\n+  }\n+\n+  override def commitJob(jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    val fileStatuses = taskCommits.flatMap(_.obj.asInstanceOf[Seq[SinkFileStatus]]).toArray\n+\n+    if (fileLog.add(batchId, fileStatuses)) {\n+      logInfo(s\"Committed batch $batchId\")\n+    } else {\n+      throw new IllegalStateException(s\"Race while writing batch $batchId\")\n+    }\n+  }\n+\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    // Do nothing\n+  }\n+\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {\n+    addedFiles = new ArrayBuffer[String]\n+  }\n+\n+  override def newTaskTempFile(\n+      taskContext: TaskAttemptContext, dir: Option[String], ext: String): String = {\n+    // The file name looks like part-r-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003.gz.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    val uuid = UUID.randomUUID.toString\n+    val filename = f\"part-$split%05d-$uuid$ext\"\n+\n+    val file = dir.map { d =>\n+      new Path(new Path(path, d), filename).toString\n+    }.getOrElse {\n+      new Path(path, filename).toString\n+    }\n+\n+    addedFiles += file\n+    file\n+  }\n+\n+  override def commitTask(taskContext: TaskAttemptContext): TaskCommitMessage = {\n+    if (addedFiles.nonEmpty) {",
    "line": 100
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "I was copying the same logic from before -- but i think so...\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T01:03:56Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.UUID\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.datasources.FileCommitProtocol\n+import org.apache.spark.sql.execution.datasources.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * A [[FileCommitProtocol]] that tracks the list of valid files in a manifest file, used in\n+ * structured streaming.\n+ *\n+ * @param path path to write the final output to.\n+ */\n+class ManifestFileCommitProtocol(path: String)\n+  extends FileCommitProtocol with Serializable with Logging {\n+\n+  // Track the list of files added by a task, only used on the executors.\n+  @transient private var addedFiles: ArrayBuffer[String] = _\n+\n+  @transient private var fileLog: FileStreamSinkLog = _\n+  private var batchId: Long = _\n+\n+  /**\n+   * Sets up the manifest log output and the batch id for this job.\n+   * Must be called before any other function.\n+   */\n+  def setupManifestOptions(fileLog: FileStreamSinkLog, batchId: Long): Unit = {\n+    this.fileLog = fileLog\n+    this.batchId = batchId\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    // Do nothing\n+  }\n+\n+  override def commitJob(jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    val fileStatuses = taskCommits.flatMap(_.obj.asInstanceOf[Seq[SinkFileStatus]]).toArray\n+\n+    if (fileLog.add(batchId, fileStatuses)) {\n+      logInfo(s\"Committed batch $batchId\")\n+    } else {\n+      throw new IllegalStateException(s\"Race while writing batch $batchId\")\n+    }\n+  }\n+\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    // Do nothing\n+  }\n+\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {\n+    addedFiles = new ArrayBuffer[String]\n+  }\n+\n+  override def newTaskTempFile(\n+      taskContext: TaskAttemptContext, dir: Option[String], ext: String): String = {\n+    // The file name looks like part-r-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003.gz.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    val uuid = UUID.randomUUID.toString\n+    val filename = f\"part-$split%05d-$uuid$ext\"\n+\n+    val file = dir.map { d =>\n+      new Path(new Path(path, d), filename).toString\n+    }.getOrElse {\n+      new Path(path, filename).toString\n+    }\n+\n+    addedFiles += file\n+    file\n+  }\n+\n+  override def commitTask(taskContext: TaskAttemptContext): TaskCommitMessage = {\n+    if (addedFiles.nonEmpty) {",
    "line": 100
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "actually the other thing is that we are using the head. Technically we can use headOption and than map over it but it will be pretty weird ..\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T01:04:48Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.UUID\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.execution.datasources.FileCommitProtocol\n+import org.apache.spark.sql.execution.datasources.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * A [[FileCommitProtocol]] that tracks the list of valid files in a manifest file, used in\n+ * structured streaming.\n+ *\n+ * @param path path to write the final output to.\n+ */\n+class ManifestFileCommitProtocol(path: String)\n+  extends FileCommitProtocol with Serializable with Logging {\n+\n+  // Track the list of files added by a task, only used on the executors.\n+  @transient private var addedFiles: ArrayBuffer[String] = _\n+\n+  @transient private var fileLog: FileStreamSinkLog = _\n+  private var batchId: Long = _\n+\n+  /**\n+   * Sets up the manifest log output and the batch id for this job.\n+   * Must be called before any other function.\n+   */\n+  def setupManifestOptions(fileLog: FileStreamSinkLog, batchId: Long): Unit = {\n+    this.fileLog = fileLog\n+    this.batchId = batchId\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    // Do nothing\n+  }\n+\n+  override def commitJob(jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    val fileStatuses = taskCommits.flatMap(_.obj.asInstanceOf[Seq[SinkFileStatus]]).toArray\n+\n+    if (fileLog.add(batchId, fileStatuses)) {\n+      logInfo(s\"Committed batch $batchId\")\n+    } else {\n+      throw new IllegalStateException(s\"Race while writing batch $batchId\")\n+    }\n+  }\n+\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    require(fileLog != null, \"setupManifestOptions must be called before this function\")\n+    // Do nothing\n+  }\n+\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {\n+    addedFiles = new ArrayBuffer[String]\n+  }\n+\n+  override def newTaskTempFile(\n+      taskContext: TaskAttemptContext, dir: Option[String], ext: String): String = {\n+    // The file name looks like part-r-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003.gz.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    val uuid = UUID.randomUUID.toString\n+    val filename = f\"part-$split%05d-$uuid$ext\"\n+\n+    val file = dir.map { d =>\n+      new Path(new Path(path, d), filename).toString\n+    }.getOrElse {\n+      new Path(path, filename).toString\n+    }\n+\n+    addedFiles += file\n+    file\n+  }\n+\n+  override def commitTask(taskContext: TaskAttemptContext): TaskCommitMessage = {\n+    if (addedFiles.nonEmpty) {",
    "line": 100
  }],
  "prId": 15710
}]