[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "``` scala\n    val db = databaseName.getOrElse(catalog.getCurrentDatabase)\n```\n\nThen, use `Some` in the next statement.\n",
    "commit": "601aba50032830f29881fc83299cc970e53c4cb2",
    "createdAt": "2016-03-27T22:47:29Z",
    "diffHunk": "@@ -434,3 +440,57 @@ case class SetDatabaseCommand(databaseName: String) extends RunnableCommand {\n \n   override val output: Seq[Attribute] = Seq.empty\n }\n+\n+/**\n+  * A command for users to create a function.\n+  * The syntax of using this command in SQL is\n+  * {{{\n+  *   CREATE TEMPORARY FUNCTION function_name AS class_name;\n+  *   CREATE FUNCTION [db_name.]function_name AS class_name\n+  *     [USING JAR|FILE|ARCHIVE 'file_uri' [, JAR|FILE|ARCHIVE 'file_uri'] ];\n+  * }}}\n+  */\n+case class CreateFunction(\n+    databaseName: Option[String],\n+    functionName: String, alias: String,\n+    resources: Seq[(String, String)],\n+    isTemp: Boolean)(sql: String) extends RunnableCommand {\n+  override def run(sqlContext: SQLContext): Seq[Row] = {\n+    val catalog = sqlContext.sessionState.catalog\n+    val db = if (databaseName.isDefined) {\n+      databaseName\n+    } else {\n+      Some(catalog.getCurrentDatabase)\n+    }\n+    val functionIdentifier = FunctionIdentifier(functionName, db)\n+    catalog.createFunction(CatalogFunction(functionIdentifier, alias))\n+    Seq.empty[Row]\n+  }\n+\n+  override val output: Seq[Attribute] = Seq.empty\n+}\n+\n+/**\n+  * The DDL command that drops a function.\n+  * ifExists: returns an error if the function doesn't exist, unless this is true.\n+  * isTemp: indicates if it is a temporary function.\n+  */\n+case class DropFunction(\n+    databaseName: Option[String],\n+    functionName: String,\n+    ifExists: Boolean,\n+    isTemp: Boolean)(sql: String) extends RunnableCommand {\n+  override def run(sqlContext: SQLContext): Seq[Row] = {\n+    val catalog = sqlContext.sessionState.catalog\n+    val db = if (databaseName.isDefined) {\n+      databaseName\n+    } else {\n+      Some(catalog.getCurrentDatabase)\n+    }",
    "line": 74
  }],
  "prId": 11988
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Same rewrite here.\n",
    "commit": "601aba50032830f29881fc83299cc970e53c4cb2",
    "createdAt": "2016-03-27T22:47:56Z",
    "diffHunk": "@@ -434,3 +440,57 @@ case class SetDatabaseCommand(databaseName: String) extends RunnableCommand {\n \n   override val output: Seq[Attribute] = Seq.empty\n }\n+\n+/**\n+  * A command for users to create a function.\n+  * The syntax of using this command in SQL is\n+  * {{{\n+  *   CREATE TEMPORARY FUNCTION function_name AS class_name;\n+  *   CREATE FUNCTION [db_name.]function_name AS class_name\n+  *     [USING JAR|FILE|ARCHIVE 'file_uri' [, JAR|FILE|ARCHIVE 'file_uri'] ];\n+  * }}}\n+  */\n+case class CreateFunction(\n+    databaseName: Option[String],\n+    functionName: String, alias: String,\n+    resources: Seq[(String, String)],\n+    isTemp: Boolean)(sql: String) extends RunnableCommand {\n+  override def run(sqlContext: SQLContext): Seq[Row] = {\n+    val catalog = sqlContext.sessionState.catalog\n+    val db = if (databaseName.isDefined) {\n+      databaseName\n+    } else {\n+      Some(catalog.getCurrentDatabase)\n+    }",
    "line": 49
  }],
  "prId": 11988
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "``` scala\nimport org.apache.spark.sql.{Dataset, Row, SQLContext}\n```\n",
    "commit": "601aba50032830f29881fc83299cc970e53c4cb2",
    "createdAt": "2016-03-27T22:51:21Z",
    "diffHunk": "@@ -21,15 +21,21 @@ import java.util.NoSuchElementException\n \n import org.apache.spark.internal.Logging\n import org.apache.spark.rdd.RDD\n-import org.apache.spark.sql.{Dataset, Row, SQLContext}\n-import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.CatalogFunction\n import org.apache.spark.sql.catalyst.errors.TreeNodeException\n import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference}\n import org.apache.spark.sql.catalyst.plans.logical\n import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.CatalystTypeConverters\n+import org.apache.spark.sql.catalyst.FunctionIdentifier\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.TableIdentifier\n import org.apache.spark.sql.execution.SparkPlan\n import org.apache.spark.sql.internal.SQLConf\n import org.apache.spark.sql.types._\n+import org.apache.spark.sql.Dataset\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.SQLContext",
    "line": 20
  }],
  "prId": 11988
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "``` scala\nimport org.apache.spark.sql.catalyst.{CatalystTypeConverters, FunctionIdentifier, InternalRow, TableIdentifier}\n```\n",
    "commit": "601aba50032830f29881fc83299cc970e53c4cb2",
    "createdAt": "2016-03-27T22:52:16Z",
    "diffHunk": "@@ -21,15 +21,21 @@ import java.util.NoSuchElementException\n \n import org.apache.spark.internal.Logging\n import org.apache.spark.rdd.RDD\n-import org.apache.spark.sql.{Dataset, Row, SQLContext}\n-import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.catalog.CatalogFunction\n import org.apache.spark.sql.catalyst.errors.TreeNodeException\n import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference}\n import org.apache.spark.sql.catalyst.plans.logical\n import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.CatalystTypeConverters\n+import org.apache.spark.sql.catalyst.FunctionIdentifier\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.TableIdentifier",
    "line": 14
  }],
  "prId": 11988
}]