[{
  "comments": [{
    "author": {
      "login": "wesm"
    },
    "body": "Perhaps a more descriptive name, like `ArrowConverters`? ",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-01-26T19:40:19Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import scala.collection.JavaConverters._\n+import scala.language.implicitConversions\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+object Arrow {"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "wesm"
    },
    "body": "`\"BinaryValue\"`",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-01-26T19:44:32Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import scala.collection.JavaConverters._\n+import scala.language.implicitConversions\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+object Arrow {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: ${dataType}\")\n+    }\n+  }\n+\n+  /**\n+   * Transfer an array of InternalRow to an ArrowRecordBatch.\n+   */\n+  private[sql] def internalRowsToArrowRecordBatch(\n+      rows: Array[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+    val fieldAndBuf = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      internalRowToArrowBuf(rows, ordinal, field, allocator)\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1.flatten\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val recordBatch = new ArrowRecordBatch(rows.length,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Write a Field from array of InternalRow to an ArrowBuf.\n+   */\n+  private def internalRowToArrowBuf(\n+      rows: Array[InternalRow],\n+      ordinal: Int,\n+      field: StructField,\n+      allocator: RootAllocator): (Array[ArrowFieldNode], Array[ArrowBuf]) = {\n+    val numOfRows = rows.length\n+    val columnWriter = ColumnWriter(allocator, field.dataType)\n+    columnWriter.init(numOfRows)\n+    var index = 0\n+\n+    while(index < numOfRows) {\n+      val row = rows(index)\n+      if (row.isNullAt(ordinal)) {\n+        columnWriter.writeNull()\n+      } else {\n+        columnWriter.write(row, ordinal)\n+      }\n+      index += 1\n+    }\n+\n+    val (arrowFieldNodes, arrowBufs) = columnWriter.finish()\n+    (arrowFieldNodes.toArray, arrowBufs.toArray)\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(initialSize: Int): Unit\n+  def writeNull(): Unit\n+  def write(row: InternalRow, ordinal: Int): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(protected val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  protected def valueVector: BaseDataValueVector\n+  protected def valueMutator: BaseMutator\n+\n+  protected def setNull(): Unit\n+  protected def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(initialSize: Int): Unit = {\n+    valueVector.allocateNew()\n+  }\n+\n+  override def writeNull(): Unit = {\n+    setNull()\n+    nullCount += 1\n+    count += 1\n+  }\n+\n+  override def write(row: InternalRow, ordinal: Int): Unit = {\n+    setValue(row, ordinal)\n+    count += 1\n+  }\n+\n+  override def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers: Seq[ArrowBuf] = valueVector.getBuffers(true)\n+    (List(fieldNode), valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override protected val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override protected val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, bool2int(row.getBoolean(ordinal)))\n+}\n+\n+private[sql] class ShortColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableSmallIntVector\n+    = new NullableSmallIntVector(\"ShortValue\", allocator)\n+  override protected val valueMutator: NullableSmallIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getShort(ordinal))\n+}\n+\n+private[sql] class IntegerColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableIntVector\n+    = new NullableIntVector(\"IntValue\", allocator)\n+  override protected val valueMutator: NullableIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getInt(ordinal))\n+}\n+\n+private[sql] class LongColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableBigIntVector\n+    = new NullableBigIntVector(\"LongValue\", allocator)\n+  override protected val valueMutator: NullableBigIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getLong(ordinal))\n+}\n+\n+private[sql] class FloatColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableFloat4Vector\n+    = new NullableFloat4Vector(\"FloatValue\", allocator)\n+  override protected val valueMutator: NullableFloat4Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getFloat(ordinal))\n+}\n+\n+private[sql] class DoubleColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableFloat8Vector\n+    = new NullableFloat8Vector(\"DoubleValue\", allocator)\n+  override protected val valueMutator: NullableFloat8Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getDouble(ordinal))\n+}\n+\n+private[sql] class ByteColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableUInt1Vector\n+    = new NullableUInt1Vector(\"ByteValue\", allocator)\n+  override protected val valueMutator: NullableUInt1Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getByte(ordinal))\n+}\n+\n+private[sql] class UTF8StringColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)\n+  override protected val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    val bytes = row.getUTF8String(ordinal).getBytes\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class BinaryColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "wesm"
    },
    "body": "Can you add a comment explaining the difference between the value representations (date ordinal vs. Joda-compatible milliseconds timestamp)?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-01-26T19:45:47Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import scala.collection.JavaConverters._\n+import scala.language.implicitConversions\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+object Arrow {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: ${dataType}\")\n+    }\n+  }\n+\n+  /**\n+   * Transfer an array of InternalRow to an ArrowRecordBatch.\n+   */\n+  private[sql] def internalRowsToArrowRecordBatch(\n+      rows: Array[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+    val fieldAndBuf = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      internalRowToArrowBuf(rows, ordinal, field, allocator)\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1.flatten\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val recordBatch = new ArrowRecordBatch(rows.length,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Write a Field from array of InternalRow to an ArrowBuf.\n+   */\n+  private def internalRowToArrowBuf(\n+      rows: Array[InternalRow],\n+      ordinal: Int,\n+      field: StructField,\n+      allocator: RootAllocator): (Array[ArrowFieldNode], Array[ArrowBuf]) = {\n+    val numOfRows = rows.length\n+    val columnWriter = ColumnWriter(allocator, field.dataType)\n+    columnWriter.init(numOfRows)\n+    var index = 0\n+\n+    while(index < numOfRows) {\n+      val row = rows(index)\n+      if (row.isNullAt(ordinal)) {\n+        columnWriter.writeNull()\n+      } else {\n+        columnWriter.write(row, ordinal)\n+      }\n+      index += 1\n+    }\n+\n+    val (arrowFieldNodes, arrowBufs) = columnWriter.finish()\n+    (arrowFieldNodes.toArray, arrowBufs.toArray)\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(initialSize: Int): Unit\n+  def writeNull(): Unit\n+  def write(row: InternalRow, ordinal: Int): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(protected val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  protected def valueVector: BaseDataValueVector\n+  protected def valueMutator: BaseMutator\n+\n+  protected def setNull(): Unit\n+  protected def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(initialSize: Int): Unit = {\n+    valueVector.allocateNew()\n+  }\n+\n+  override def writeNull(): Unit = {\n+    setNull()\n+    nullCount += 1\n+    count += 1\n+  }\n+\n+  override def write(row: InternalRow, ordinal: Int): Unit = {\n+    setValue(row, ordinal)\n+    count += 1\n+  }\n+\n+  override def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers: Seq[ArrowBuf] = valueVector.getBuffers(true)\n+    (List(fieldNode), valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override protected val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override protected val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, bool2int(row.getBoolean(ordinal)))\n+}\n+\n+private[sql] class ShortColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableSmallIntVector\n+    = new NullableSmallIntVector(\"ShortValue\", allocator)\n+  override protected val valueMutator: NullableSmallIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getShort(ordinal))\n+}\n+\n+private[sql] class IntegerColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableIntVector\n+    = new NullableIntVector(\"IntValue\", allocator)\n+  override protected val valueMutator: NullableIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getInt(ordinal))\n+}\n+\n+private[sql] class LongColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableBigIntVector\n+    = new NullableBigIntVector(\"LongValue\", allocator)\n+  override protected val valueMutator: NullableBigIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getLong(ordinal))\n+}\n+\n+private[sql] class FloatColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableFloat4Vector\n+    = new NullableFloat4Vector(\"FloatValue\", allocator)\n+  override protected val valueMutator: NullableFloat4Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getFloat(ordinal))\n+}\n+\n+private[sql] class DoubleColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableFloat8Vector\n+    = new NullableFloat8Vector(\"DoubleValue\", allocator)\n+  override protected val valueMutator: NullableFloat8Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getDouble(ordinal))\n+}\n+\n+private[sql] class ByteColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableUInt1Vector\n+    = new NullableUInt1Vector(\"ByteValue\", allocator)\n+  override protected val valueMutator: NullableUInt1Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getByte(ordinal))\n+}\n+\n+private[sql] class UTF8StringColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)\n+  override protected val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    val bytes = row.getUTF8String(ordinal).getBytes\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class BinaryColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)\n+  override protected val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    val bytes = row.getBinary(ordinal)\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class DateColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableDateVector\n+    = new NullableDateVector(\"DateValue\", allocator)\n+  override protected val valueMutator: NullableDateVector#Mutator = valueVector.getMutator\n+\n+  override protected def setNull(): Unit = valueMutator.setNull(count)\n+  override protected def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    valueMutator.setSafe(count, row.getInt(ordinal).toLong * 24 * 3600 * 1000)"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "wesm"
    },
    "body": "Can you add a `TODO` that we should migrate to microsecond timestamps once ARROW-477 is in? ",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-01-26T19:46:49Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.spark.sql\n+\n+import scala.collection.JavaConverters._\n+import scala.language.implicitConversions\n+\n+import io.netty.buffer.ArrowBuf\n+import org.apache.arrow.memory.{BaseAllocator, RootAllocator}\n+import org.apache.arrow.vector._\n+import org.apache.arrow.vector.BaseValueVector.BaseMutator\n+import org.apache.arrow.vector.schema.{ArrowFieldNode, ArrowRecordBatch}\n+import org.apache.arrow.vector.types.{FloatingPointPrecision, TimeUnit}\n+import org.apache.arrow.vector.types.pojo.{ArrowType, Field, Schema}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.types._\n+\n+object Arrow {\n+\n+  /**\n+   * Map a Spark Dataset type to ArrowType.\n+   */\n+  private[sql] def sparkTypeToArrowType(dataType: DataType): ArrowType = {\n+    dataType match {\n+      case BooleanType => ArrowType.Bool.INSTANCE\n+      case ShortType => new ArrowType.Int(8 * ShortType.defaultSize, true)\n+      case IntegerType => new ArrowType.Int(8 * IntegerType.defaultSize, true)\n+      case LongType => new ArrowType.Int(8 * LongType.defaultSize, true)\n+      case FloatType => new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE)\n+      case DoubleType => new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)\n+      case ByteType => new ArrowType.Int(8, true)\n+      case StringType => ArrowType.Utf8.INSTANCE\n+      case BinaryType => ArrowType.Binary.INSTANCE\n+      case DateType => ArrowType.Date.INSTANCE\n+      case TimestampType => new ArrowType.Timestamp(TimeUnit.MILLISECOND)\n+      case _ => throw new UnsupportedOperationException(s\"Unsupported data type: ${dataType}\")\n+    }\n+  }\n+\n+  /**\n+   * Transfer an array of InternalRow to an ArrowRecordBatch.\n+   */\n+  private[sql] def internalRowsToArrowRecordBatch(\n+      rows: Array[InternalRow],\n+      schema: StructType,\n+      allocator: RootAllocator): ArrowRecordBatch = {\n+    val fieldAndBuf = schema.fields.zipWithIndex.map { case (field, ordinal) =>\n+      internalRowToArrowBuf(rows, ordinal, field, allocator)\n+    }.unzip\n+    val fieldNodes = fieldAndBuf._1.flatten\n+    val buffers = fieldAndBuf._2.flatten\n+\n+    val recordBatch = new ArrowRecordBatch(rows.length,\n+      fieldNodes.toList.asJava, buffers.toList.asJava)\n+\n+    buffers.foreach(_.release())\n+    recordBatch\n+  }\n+\n+  /**\n+   * Write a Field from array of InternalRow to an ArrowBuf.\n+   */\n+  private def internalRowToArrowBuf(\n+      rows: Array[InternalRow],\n+      ordinal: Int,\n+      field: StructField,\n+      allocator: RootAllocator): (Array[ArrowFieldNode], Array[ArrowBuf]) = {\n+    val numOfRows = rows.length\n+    val columnWriter = ColumnWriter(allocator, field.dataType)\n+    columnWriter.init(numOfRows)\n+    var index = 0\n+\n+    while(index < numOfRows) {\n+      val row = rows(index)\n+      if (row.isNullAt(ordinal)) {\n+        columnWriter.writeNull()\n+      } else {\n+        columnWriter.write(row, ordinal)\n+      }\n+      index += 1\n+    }\n+\n+    val (arrowFieldNodes, arrowBufs) = columnWriter.finish()\n+    (arrowFieldNodes.toArray, arrowBufs.toArray)\n+  }\n+\n+  /**\n+   * Convert a Spark Dataset schema to Arrow schema.\n+   */\n+  private[sql] def schemaToArrowSchema(schema: StructType): Schema = {\n+    val arrowFields = schema.fields.map { f =>\n+      new Field(f.name, f.nullable, sparkTypeToArrowType(f.dataType), List.empty[Field].asJava)\n+    }\n+    new Schema(arrowFields.toList.asJava)\n+  }\n+}\n+\n+private[sql] trait ColumnWriter {\n+  def init(initialSize: Int): Unit\n+  def writeNull(): Unit\n+  def write(row: InternalRow, ordinal: Int): Unit\n+\n+  /**\n+   * Clear the column writer and return the ArrowFieldNode and ArrowBuf.\n+   * This should be called only once after all the data is written.\n+   */\n+  def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf])\n+}\n+\n+/**\n+ * Base class for flat arrow column writer, i.e., column without children.\n+ */\n+private[sql] abstract class PrimitiveColumnWriter(protected val allocator: BaseAllocator)\n+    extends ColumnWriter {\n+  protected def valueVector: BaseDataValueVector\n+  protected def valueMutator: BaseMutator\n+\n+  protected def setNull(): Unit\n+  protected def setValue(row: InternalRow, ordinal: Int): Unit\n+\n+  protected var count = 0\n+  protected var nullCount = 0\n+\n+  override def init(initialSize: Int): Unit = {\n+    valueVector.allocateNew()\n+  }\n+\n+  override def writeNull(): Unit = {\n+    setNull()\n+    nullCount += 1\n+    count += 1\n+  }\n+\n+  override def write(row: InternalRow, ordinal: Int): Unit = {\n+    setValue(row, ordinal)\n+    count += 1\n+  }\n+\n+  override def finish(): (Seq[ArrowFieldNode], Seq[ArrowBuf]) = {\n+    valueMutator.setValueCount(count)\n+    val fieldNode = new ArrowFieldNode(count, nullCount)\n+    val valueBuffers: Seq[ArrowBuf] = valueVector.getBuffers(true)\n+    (List(fieldNode), valueBuffers)\n+  }\n+}\n+\n+private[sql] class BooleanColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  private def bool2int(b: Boolean): Int = if (b) 1 else 0\n+\n+  override protected val valueVector: NullableBitVector\n+    = new NullableBitVector(\"BooleanValue\", allocator)\n+  override protected val valueMutator: NullableBitVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, bool2int(row.getBoolean(ordinal)))\n+}\n+\n+private[sql] class ShortColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableSmallIntVector\n+    = new NullableSmallIntVector(\"ShortValue\", allocator)\n+  override protected val valueMutator: NullableSmallIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getShort(ordinal))\n+}\n+\n+private[sql] class IntegerColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableIntVector\n+    = new NullableIntVector(\"IntValue\", allocator)\n+  override protected val valueMutator: NullableIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getInt(ordinal))\n+}\n+\n+private[sql] class LongColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableBigIntVector\n+    = new NullableBigIntVector(\"LongValue\", allocator)\n+  override protected val valueMutator: NullableBigIntVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getLong(ordinal))\n+}\n+\n+private[sql] class FloatColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableFloat4Vector\n+    = new NullableFloat4Vector(\"FloatValue\", allocator)\n+  override protected val valueMutator: NullableFloat4Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getFloat(ordinal))\n+}\n+\n+private[sql] class DoubleColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableFloat8Vector\n+    = new NullableFloat8Vector(\"DoubleValue\", allocator)\n+  override protected val valueMutator: NullableFloat8Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getDouble(ordinal))\n+}\n+\n+private[sql] class ByteColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableUInt1Vector\n+    = new NullableUInt1Vector(\"ByteValue\", allocator)\n+  override protected val valueMutator: NullableUInt1Vector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit\n+    = valueMutator.setSafe(count, row.getByte(ordinal))\n+}\n+\n+private[sql] class UTF8StringColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)\n+  override protected val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    val bytes = row.getUTF8String(ordinal).getBytes\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class BinaryColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableVarBinaryVector\n+    = new NullableVarBinaryVector(\"UTF8StringValue\", allocator)\n+  override protected val valueMutator: NullableVarBinaryVector#Mutator = valueVector.getMutator\n+\n+  override def setNull(): Unit = valueMutator.setNull(count)\n+  override def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    val bytes = row.getBinary(ordinal)\n+    valueMutator.setSafe(count, bytes, 0, bytes.length)\n+  }\n+}\n+\n+private[sql] class DateColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableDateVector\n+    = new NullableDateVector(\"DateValue\", allocator)\n+  override protected val valueMutator: NullableDateVector#Mutator = valueVector.getMutator\n+\n+  override protected def setNull(): Unit = valueMutator.setNull(count)\n+  override protected def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    valueMutator.setSafe(count, row.getInt(ordinal).toLong * 24 * 3600 * 1000)\n+  }\n+}\n+\n+private[sql] class TimeStampColumnWriter(allocator: BaseAllocator)\n+    extends PrimitiveColumnWriter(allocator) {\n+  override protected val valueVector: NullableTimeStampVector\n+    = new NullableTimeStampVector(\"TimeStampValue\", allocator)\n+  override protected val valueMutator: NullableTimeStampVector#Mutator = valueVector.getMutator\n+\n+  override protected def setNull(): Unit = valueMutator.setNull(count)\n+\n+  override protected def setValue(row: InternalRow, ordinal: Int): Unit = {\n+    valueMutator.setSafe(count, row.getLong(ordinal) / 1000)"
  }],
  "prId": 15821
}]