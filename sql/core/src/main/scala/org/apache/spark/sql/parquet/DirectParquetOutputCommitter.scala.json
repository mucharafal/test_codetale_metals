[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Exception caught here may be also caused by writing the `_SUCCEEDED` mark file. Can we move the outer most `try`  into the first `if` block, and add a separate `try` in the second `if` block for writing the `_SUCCEEDED` mark file?\n",
    "commit": "54c6b157547ea16cc5482e9dfd396179022d5948",
    "createdAt": "2015-04-27T05:54:49Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.parquet\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+\n+import parquet.Log\n+import parquet.hadoop.util.ContextUtil\n+import parquet.hadoop.{ParquetFileReader, ParquetFileWriter, ParquetOutputCommitter, ParquetOutputFormat}\n+\n+private[parquet] class DirectParquetOutputCommitter(outputPath: Path, context: TaskAttemptContext)\n+  extends ParquetOutputCommitter(outputPath, context) {\n+  val LOG = Log.getLog(classOf[ParquetOutputCommitter])\n+\n+  override def getWorkPath(): Path = outputPath\n+  override def abortTask(taskContext: TaskAttemptContext): Unit = {}\n+  override def commitTask(taskContext: TaskAttemptContext): Unit = {}\n+  override def needsTaskCommit(taskContext: TaskAttemptContext): Boolean = true\n+  override def setupJob(jobContext: JobContext): Unit = {}\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {}\n+\n+  override def commitJob(jobContext: JobContext) {\n+    try {\n+      val configuration = ContextUtil.getConfiguration(jobContext)\n+      val fileSystem = outputPath.getFileSystem(configuration)\n+      if (configuration.getBoolean(ParquetOutputFormat.ENABLE_JOB_SUMMARY, true)) {\n+        val outputStatus = fileSystem.getFileStatus(outputPath)\n+        val footers = ParquetFileReader.readAllFootersInParallel(configuration, outputStatus)\n+        try {\n+          ParquetFileWriter.writeMetadataFile(configuration, outputPath, footers)\n+        } catch {\n+          case e: Exception => {\n+            LOG.warn(\"could not write summary file for \" + outputPath, e)\n+            val metadataPath = new Path(outputPath, ParquetFileWriter.PARQUET_METADATA_FILE)\n+            if (fileSystem.exists(metadataPath)) {\n+              fileSystem.delete(metadataPath, true)\n+            }\n+          }\n+        }\n+      }\n+      if (configuration.getBoolean(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", true)) {\n+        val successPath = new Path(outputPath, FileOutputCommitter.SUCCEEDED_FILE_NAME)\n+        fileSystem.create(successPath).close()\n+      }\n+    } catch {\n+      case e: Exception => LOG.warn(\"could not write summary file for \" + outputPath, e)"
  }],
  "prId": 5525
}]