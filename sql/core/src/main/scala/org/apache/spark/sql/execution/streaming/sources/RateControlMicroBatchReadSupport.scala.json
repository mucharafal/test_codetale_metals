[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why is this public? I thought it was a hack for the Kafka implementation. If so, I think it should be private[spark] or private[sql].",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-14T15:32:12Z",
    "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import org.apache.spark.sql.sources.v2.reader.streaming.{MicroBatchReadSupport, Offset}\n+\n+// A special `MicroBatchReadSupport` that can get latestOffset with a start offset.\n+trait RateControlMicroBatchReadSupport extends MicroBatchReadSupport {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "everything under `org.apache.spark.sql.execution` is private, that's why we don't add `private[...]` to the classes under `org.apache.spark.sql.execution`",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-15T02:05:22Z",
    "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.sources\n+\n+import org.apache.spark.sql.sources.v2.reader.streaming.{MicroBatchReadSupport, Offset}\n+\n+// A special `MicroBatchReadSupport` that can get latestOffset with a start offset.\n+trait RateControlMicroBatchReadSupport extends MicroBatchReadSupport {"
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this is under `org.apache.spark.sql.execution`, so it's an internal API.",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-21T15:11:43Z",
    "diffHunk": "@@ -17,21 +17,15 @@\n \n package org.apache.spark.sql.execution.streaming.sources\n \n-import org.apache.spark.sql.catalyst.InternalRow\n-import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n-import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter\n+import org.apache.spark.sql.sources.v2.reader.streaming.{MicroBatchReadSupport, Offset}\n \n-/**\n- * A [[DataSourceWriter]] used to hook V2 stream writers into a microbatch plan. It implements\n- * the non-streaming interface, forwarding the batch ID determined at construction to a wrapped\n- * streaming writer.\n- */\n-class MicroBatchWriter(batchId: Long, val writer: StreamWriter) extends DataSourceWriter {\n-  override def commit(messages: Array[WriterCommitMessage]): Unit = {\n-    writer.commit(batchId, messages)\n-  }\n+// A special `MicroBatchReadSupport` that can get latestOffset with a start offset.\n+trait RateControlMicroBatchReadSupport extends MicroBatchReadSupport {",
    "line": 19
  }],
  "prId": 22009
}]