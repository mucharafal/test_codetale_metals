[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thank you for your contribution, @yanlin-Lynn .\r\nWe need two more spaces for the params. Please refer the other functions.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-22T03:26:54Z",
    "diffHunk": "@@ -87,4 +95,15 @@ class JdbcRelationProvider extends CreatableRelationProvider\n \n     createRelation(sqlContext, parameters)\n   }\n+\n+  override def createStreamingWriteSupport(\n+    queryId: String,"
  }, {
    "author": {
      "login": "yanlin-Lynn"
    },
    "body": "thank you for you time, I will fix this.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T00:52:05Z",
    "diffHunk": "@@ -87,4 +95,15 @@ class JdbcRelationProvider extends CreatableRelationProvider\n \n     createRelation(sqlContext, parameters)\n   }\n+\n+  override def createStreamingWriteSupport(\n+    queryId: String,"
  }],
  "prId": 23369
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "I'm curious why to change this (JdbcRelationProvider -> JdbcSourceProvider) ? Is it related change?",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-28T09:53:15Z",
    "diffHunk": "@@ -17,12 +17,20 @@\n \n package org.apache.spark.sql.execution.datasources.jdbc\n \n+import scala.collection.JavaConverters._\n+\n import org.apache.spark.sql.{AnalysisException, DataFrame, SaveMode, SQLContext}\n import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils._\n import org.apache.spark.sql.sources.{BaseRelation, CreatableRelationProvider, DataSourceRegister, RelationProvider}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, StreamingWriteSupportProvider}\n+import org.apache.spark.sql.sources.v2.writer.streaming.StreamingWriteSupport\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.StructType\n \n-class JdbcRelationProvider extends CreatableRelationProvider\n-  with RelationProvider with DataSourceRegister {\n+class JdbcSourceProvider extends CreatableRelationProvider",
    "line": 16
  }, {
    "author": {
      "login": "yanlin-Lynn"
    },
    "body": "As I know, RelationProvider is used for batch computation, not for streaming. So I make this change, just as `KafkaSourceProvider` does.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-31T06:37:11Z",
    "diffHunk": "@@ -17,12 +17,20 @@\n \n package org.apache.spark.sql.execution.datasources.jdbc\n \n+import scala.collection.JavaConverters._\n+\n import org.apache.spark.sql.{AnalysisException, DataFrame, SaveMode, SQLContext}\n import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils._\n import org.apache.spark.sql.sources.{BaseRelation, CreatableRelationProvider, DataSourceRegister, RelationProvider}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, StreamingWriteSupportProvider}\n+import org.apache.spark.sql.sources.v2.writer.streaming.StreamingWriteSupport\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.StructType\n \n-class JdbcRelationProvider extends CreatableRelationProvider\n-  with RelationProvider with DataSourceRegister {\n+class JdbcSourceProvider extends CreatableRelationProvider",
    "line": 16
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Changing built-in data source name like that will have backward compatibility issue.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-31T08:22:08Z",
    "diffHunk": "@@ -17,12 +17,20 @@\n \n package org.apache.spark.sql.execution.datasources.jdbc\n \n+import scala.collection.JavaConverters._\n+\n import org.apache.spark.sql.{AnalysisException, DataFrame, SaveMode, SQLContext}\n import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils._\n import org.apache.spark.sql.sources.{BaseRelation, CreatableRelationProvider, DataSourceRegister, RelationProvider}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, StreamingWriteSupportProvider}\n+import org.apache.spark.sql.sources.v2.writer.streaming.StreamingWriteSupport\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.StructType\n \n-class JdbcRelationProvider extends CreatableRelationProvider\n-  with RelationProvider with DataSourceRegister {\n+class JdbcSourceProvider extends CreatableRelationProvider",
    "line": 16
  }, {
    "author": {
      "login": "yanlin-Lynn"
    },
    "body": "I think it's OK, because the `shortName` method is the same, so users using`format(\"jdbc\")` are not affected.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2019-01-01T09:42:58Z",
    "diffHunk": "@@ -17,12 +17,20 @@\n \n package org.apache.spark.sql.execution.datasources.jdbc\n \n+import scala.collection.JavaConverters._\n+\n import org.apache.spark.sql.{AnalysisException, DataFrame, SaveMode, SQLContext}\n import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils._\n import org.apache.spark.sql.sources.{BaseRelation, CreatableRelationProvider, DataSourceRegister, RelationProvider}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, StreamingWriteSupportProvider}\n+import org.apache.spark.sql.sources.v2.writer.streaming.StreamingWriteSupport\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.StructType\n \n-class JdbcRelationProvider extends CreatableRelationProvider\n-  with RelationProvider with DataSourceRegister {\n+class JdbcSourceProvider extends CreatableRelationProvider",
    "line": 16
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Although it is rare case, but it's still possible that if you use full classname then this will break it. Anyway, I don't see strong reason to support this renaming. You can see others' opinions too.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2019-01-01T12:33:27Z",
    "diffHunk": "@@ -17,12 +17,20 @@\n \n package org.apache.spark.sql.execution.datasources.jdbc\n \n+import scala.collection.JavaConverters._\n+\n import org.apache.spark.sql.{AnalysisException, DataFrame, SaveMode, SQLContext}\n import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils._\n import org.apache.spark.sql.sources.{BaseRelation, CreatableRelationProvider, DataSourceRegister, RelationProvider}\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, StreamingWriteSupportProvider}\n+import org.apache.spark.sql.sources.v2.writer.streaming.StreamingWriteSupport\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.StructType\n \n-class JdbcRelationProvider extends CreatableRelationProvider\n-  with RelationProvider with DataSourceRegister {\n+class JdbcSourceProvider extends CreatableRelationProvider",
    "line": 16
  }],
  "prId": 23369
}]