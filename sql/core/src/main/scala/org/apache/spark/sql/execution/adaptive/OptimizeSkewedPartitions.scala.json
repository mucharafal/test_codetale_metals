[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is different from local/coalesced shuffle reader as it reads only one reduce partition. Maybe better to call it `PostShufflePartitionReader`",
    "commit": "bbf585dbbcf41a47282a272ef7449db553eae9f3",
    "createdAt": "2019-11-11T17:42:30Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.MapOutputStatistics\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.joins.SortMergeJoinExec\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+case class OptimizeSkewedPartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private val supportedJoinTypes =\n+    Inner :: Cross :: LeftSemi :: LeftAnti :: LeftOuter :: RightOuter :: Nil\n+\n+  /**\n+   * A partition is considered as a skewed partition if its size is larger than the median\n+   * partition size * spark.sql.adaptive.skewedPartitionFactor and also larger than\n+   * spark.sql.adaptive.skewedPartitionSizeThreshold.\n+   */\n+  private def isSkewed(\n+     stats: MapOutputStatistics,\n+     partitionId: Int,\n+     medianSize: Long): Boolean = {\n+    val size = stats.bytesByPartitionId(partitionId)\n+    size > medianSize * conf.adaptiveSkewedFactor &&\n+      size > conf.adaptiveSkewedSizeThreshold\n+  }\n+\n+  private def medianSize(stats: MapOutputStatistics): Long = {\n+    val bytesLen = stats.bytesByPartitionId.length\n+    val bytes = stats.bytesByPartitionId.sorted\n+    if (bytes(bytesLen / 2) > 0) bytes(bytesLen / 2) else 1\n+  }\n+\n+  /**\n+   * To equally divide n elements into m buckets, basically each bucket should have n/m elements,\n+   * for the remaining n%m elements, add one more element to the first n%m buckets each. Returns\n+   * a sequence with length numBuckets and each value represents the start index of each bucket.\n+   */\n+  def equallyDivide(numElements: Int, numBuckets: Int): Seq[Int] = {\n+    val elementsPerBucket = numElements / numBuckets\n+    val remaining = numElements % numBuckets\n+    val splitPoint = (elementsPerBucket + 1) * remaining\n+    (0 until remaining).map(_ * (elementsPerBucket + 1)) ++\n+      (remaining until numBuckets).map(i => splitPoint + (i - remaining) * elementsPerBucket)\n+  }\n+\n+  /**\n+   * We split the partition into several splits. Each split reads the data from several map outputs\n+   * ranging from startMapId to endMapId(exclusive). This method calculates the split number and\n+   * the startMapId for all splits.\n+   */\n+  private def estimateMapIdStartIndices(\n+    stage: QueryStageExec,\n+    partitionId: Int,\n+    medianSize: Long): Array[Int] = {\n+    val metrics = getStatistics(stage)\n+    val size = metrics.bytesByPartitionId(partitionId)\n+    val factor = size / medianSize\n+    val numMappers = getShuffleStage(stage).\n+      plan.shuffleDependency.rdd.partitions.length\n+     val numSplits = Math.min(conf.adaptiveSkewedMaxSplits,\n+      Math.min(factor.toInt, numMappers))\n+    equallyDivide(numMappers, numSplits).toArray\n+  }\n+\n+  private def getShuffleStage(queryStage: QueryStageExec): ShuffleQueryStageExec = {\n+    queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+  }\n+\n+  private def getStatistics(queryStage: QueryStageExec): MapOutputStatistics = {\n+    val shuffleStage = queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+    val metrics = shuffleStage.mapOutputStatisticsFuture\n+    assert(metrics.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+    ThreadUtils.awaitResult(metrics, Duration.Zero)\n+  }\n+\n+  /**\n+   * Base optimization support check: the join type is supported and plan statistics is available.\n+   * Note that for some join types(like left outer), whether a certain partition can be optimized\n+   * also depends on the filed isSkewAndSupportsSplit.\n+   */\n+  private def supportOptimization(\n+    joinType: JoinType,\n+    leftStage: QueryStageExec,\n+    rightStage: QueryStageExec): Boolean = {\n+    val joinTypeSupported = supportedJoinTypes.contains(joinType)\n+    val shuffleStageCheck = ShuffleQueryStageExec.isShuffleQueryStageExec(leftStage) &&\n+      ShuffleQueryStageExec.isShuffleQueryStageExec(rightStage)\n+    val statisticsReady: Boolean = if (shuffleStageCheck) {\n+      getStatistics(leftStage) != null && getStatistics(rightStage) != null\n+    } else false\n+\n+    joinTypeSupported && statisticsReady\n+  }\n+\n+  private def supportSplitOnLeftPartition(joinType: JoinType) = joinType != RightOuter\n+\n+  private def supportSplitOnRightPartition(joinType: JoinType) = {\n+    joinType != LeftOuter && joinType != LeftSemi && joinType != LeftAnti\n+  }\n+\n+  def handleSkewJoin(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case smj @ SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+    SortExec(_, _, left: QueryStageExec, _),\n+    SortExec(_, _, right: QueryStageExec, _))\n+      if supportOptimization(joinType, left, right) =>\n+      val leftStats = getStatistics(left)\n+      val rightStats = getStatistics(right)\n+      val numPartitions = leftStats.bytesByPartitionId.length\n+\n+      val leftMedSize = medianSize(leftStats)\n+      val rightMedSize = medianSize(rightStats)\n+      logInfo(s\"HandlingSkewedJoin left medSize: ($leftMedSize)\" +\n+        s\" right medSize ($rightMedSize)\")\n+      logInfo(s\"left bytes Max : ${leftStats.bytesByPartitionId.max}\")\n+      logInfo(s\"right bytes Max : ${rightStats.bytesByPartitionId.max}\")\n+\n+      val skewedPartitions = mutable.HashSet[Int]()\n+      val subJoins = mutable.ArrayBuffer[SparkPlan](smj)\n+      for (partitionId <- 0 until numPartitions) {\n+        val isLeftSkew = isSkewed(leftStats, partitionId, leftMedSize)\n+        val isRightSkew = isSkewed(rightStats, partitionId, rightMedSize)\n+        val isSkewAndSupportsSplit =\n+          (isLeftSkew && supportSplitOnLeftPartition(joinType)) ||\n+            (isRightSkew && supportSplitOnRightPartition(joinType))\n+\n+        if (isSkewAndSupportsSplit) {\n+          skewedPartitions += partitionId\n+          val leftMapIdStartIndices = if (isLeftSkew && supportSplitOnLeftPartition(joinType)) {\n+            estimateMapIdStartIndices(left, partitionId, leftMedSize)\n+          } else {\n+            Array(0)\n+          }\n+          val rightMapIdStartIndices = if (isRightSkew && supportSplitOnRightPartition(joinType)) {\n+            estimateMapIdStartIndices(right, partitionId, rightMedSize)\n+          } else {\n+            Array(0)\n+          }\n+\n+          for (i <- 0 until leftMapIdStartIndices.length;\n+               j <- 0 until rightMapIdStartIndices.length) {\n+            val leftEndMapId = if (i == leftMapIdStartIndices.length - 1) {\n+              getShuffleStage(left).plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              leftMapIdStartIndices(i + 1)\n+            }\n+            val rightEndMapId = if (j == rightMapIdStartIndices.length - 1) {\n+              getShuffleStage(right).\n+                plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              rightMapIdStartIndices(j + 1)\n+            }\n+            // For the skewed partition, we set the id of shuffle query stage to -1.\n+            // And skip this shuffle query stage optimization in 'ReduceNumShufflePartitions' rule.\n+            val leftSkewedReader =\n+              SkewedShuffleReaderExec(getShuffleStage(left).copy(id = -1),\n+                partitionId, leftMapIdStartIndices(i), leftEndMapId)\n+\n+            val rightSkewedReader =\n+              SkewedShuffleReaderExec(getShuffleStage(right).copy(id = -1),\n+                partitionId, rightMapIdStartIndices(j), rightEndMapId)\n+\n+            subJoins +=\n+              SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+                leftSkewedReader, rightSkewedReader)\n+          }\n+        }\n+      }\n+      logInfo(s\"skewed partition number is ${skewedPartitions.size}\")\n+      if (skewedPartitions.size > 0) {\n+        getShuffleStage(left).skewedPartitions = skewedPartitions\n+        getShuffleStage(right).skewedPartitions = skewedPartitions\n+        UnionExec(subJoins.toList)\n+      } else {\n+        smj\n+      }\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    if (!conf.adaptiveSkewedJoinEnabled) {\n+      return  plan\n+    }\n+\n+    def collectShuffleStages(plan: SparkPlan): Seq[ShuffleQueryStageExec] = plan match {\n+      case _: LocalShuffleReaderExec => Nil\n+      case _: SkewedShuffleReaderExec => Nil\n+      case stage: ShuffleQueryStageExec => Seq(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => Seq(stage)\n+      case _ => plan.children.flatMap(collectShuffleStages)\n+    }\n+\n+    val shuffleStages = collectShuffleStages(plan)\n+\n+    if (shuffleStages.length == 2) {\n+      // Currently we only support handling skewed join for 2 table join.\n+      handleSkewJoin(plan)\n+    } else {\n+      plan\n+    }\n+  }\n+}\n+\n+case class SkewedShuffleReaderExec("
  }],
  "prId": 26434
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can easily get the data size of each mapper, shall we split mapper ranges based on data size?",
    "commit": "bbf585dbbcf41a47282a272ef7449db553eae9f3",
    "createdAt": "2019-11-11T17:48:27Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.MapOutputStatistics\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.joins.SortMergeJoinExec\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+case class OptimizeSkewedPartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private val supportedJoinTypes =\n+    Inner :: Cross :: LeftSemi :: LeftAnti :: LeftOuter :: RightOuter :: Nil\n+\n+  /**\n+   * A partition is considered as a skewed partition if its size is larger than the median\n+   * partition size * spark.sql.adaptive.skewedPartitionFactor and also larger than\n+   * spark.sql.adaptive.skewedPartitionSizeThreshold.\n+   */\n+  private def isSkewed(\n+     stats: MapOutputStatistics,\n+     partitionId: Int,\n+     medianSize: Long): Boolean = {\n+    val size = stats.bytesByPartitionId(partitionId)\n+    size > medianSize * conf.adaptiveSkewedFactor &&\n+      size > conf.adaptiveSkewedSizeThreshold\n+  }\n+\n+  private def medianSize(stats: MapOutputStatistics): Long = {\n+    val bytesLen = stats.bytesByPartitionId.length\n+    val bytes = stats.bytesByPartitionId.sorted\n+    if (bytes(bytesLen / 2) > 0) bytes(bytesLen / 2) else 1\n+  }\n+\n+  /**\n+   * To equally divide n elements into m buckets, basically each bucket should have n/m elements,\n+   * for the remaining n%m elements, add one more element to the first n%m buckets each. Returns\n+   * a sequence with length numBuckets and each value represents the start index of each bucket.\n+   */\n+  def equallyDivide(numElements: Int, numBuckets: Int): Seq[Int] = {\n+    val elementsPerBucket = numElements / numBuckets\n+    val remaining = numElements % numBuckets\n+    val splitPoint = (elementsPerBucket + 1) * remaining\n+    (0 until remaining).map(_ * (elementsPerBucket + 1)) ++\n+      (remaining until numBuckets).map(i => splitPoint + (i - remaining) * elementsPerBucket)\n+  }\n+\n+  /**\n+   * We split the partition into several splits. Each split reads the data from several map outputs\n+   * ranging from startMapId to endMapId(exclusive). This method calculates the split number and\n+   * the startMapId for all splits.\n+   */\n+  private def estimateMapIdStartIndices(\n+    stage: QueryStageExec,\n+    partitionId: Int,\n+    medianSize: Long): Array[Int] = {\n+    val metrics = getStatistics(stage)\n+    val size = metrics.bytesByPartitionId(partitionId)\n+    val factor = size / medianSize\n+    val numMappers = getShuffleStage(stage).\n+      plan.shuffleDependency.rdd.partitions.length"
  }],
  "prId": 26434
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we return `SinglePartition`? otherwise we may add shuffle between `PostShufflePartitionReader` and SMJ",
    "commit": "bbf585dbbcf41a47282a272ef7449db553eae9f3",
    "createdAt": "2019-11-12T17:49:42Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.{MapOutputStatistics, MapOutputTrackerMaster, SparkEnv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.joins.SortMergeJoinExec\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+case class OptimizeSkewedPartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private val supportedJoinTypes =\n+    Inner :: Cross :: LeftSemi :: LeftAnti :: LeftOuter :: RightOuter :: Nil\n+\n+  /**\n+   * A partition is considered as a skewed partition if its size is larger than the median\n+   * partition size * spark.sql.adaptive.skewedPartitionFactor and also larger than\n+   * spark.sql.adaptive.skewedPartitionSizeThreshold.\n+   */\n+  private def isSkewed(\n+     stats: MapOutputStatistics,\n+     partitionId: Int,\n+     medianSize: Long): Boolean = {\n+    val size = stats.bytesByPartitionId(partitionId)\n+    size > medianSize * conf.adaptiveSkewedFactor &&\n+      size > conf.adaptiveSkewedSizeThreshold\n+  }\n+\n+  private def medianSize(stats: MapOutputStatistics): Long = {\n+    val bytesLen = stats.bytesByPartitionId.length\n+    val bytes = stats.bytesByPartitionId.sorted\n+    if (bytes(bytesLen / 2) > 0) bytes(bytesLen / 2) else 1\n+  }\n+\n+  /*\n+  * Get all the map data size for specific reduce partitionId.\n+  */\n+  def getMapSizeForSpecificPartition(partitionId: Int, shuffleId: Int): Array[Long] = {\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster]\n+    mapOutputTracker.shuffleStatuses.get(shuffleId).\n+      get.mapStatuses.map{_.getSizeForBlock(partitionId)}\n+  }\n+\n+  /*\n+  * Split the mappers based on the map size of specific skewed reduce partitionId.\n+  */\n+  def splitMappersBasedDataSize(mapPartitionSize: Array[Long], numMappers: Int): Array[Int] = {\n+    val advisoryTargetPostShuffleInputSize = conf.targetPostShuffleInputSize\n+    val partitionStartIndices = ArrayBuffer[Int]()\n+    var i = 0\n+    var postMapPartitionSize: Long = mapPartitionSize(i)\n+    partitionStartIndices += i\n+    while (i < numMappers && i + 1 < numMappers) {\n+      val nextIndex = if (i + 1 < numMappers) {\n+        i + 1\n+      } else numMappers -1\n+\n+      if (postMapPartitionSize + mapPartitionSize(nextIndex) > advisoryTargetPostShuffleInputSize) {\n+        postMapPartitionSize = mapPartitionSize(nextIndex)\n+        partitionStartIndices += nextIndex\n+      } else {\n+        postMapPartitionSize += mapPartitionSize(nextIndex)\n+      }\n+      i += 1\n+    }\n+    partitionStartIndices.toArray\n+  }\n+\n+  /**\n+   * We split the partition into several splits. Each split reads the data from several map outputs\n+   * ranging from startMapId to endMapId(exclusive). This method calculates the split number and\n+   * the startMapId for all splits.\n+   */\n+  private def estimateMapIdStartIndices(\n+    stage: QueryStageExec,\n+    partitionId: Int,\n+    medianSize: Long): Array[Int] = {\n+    val dependency = getShuffleStage(stage).plan.shuffleDependency\n+    val shuffleId = dependency.shuffleHandle.shuffleId\n+    val mapSize = getMapSizeForSpecificPartition(partitionId, shuffleId)\n+    val numMappers = dependency.rdd.partitions.length\n+    splitMappersBasedDataSize(mapSize, numMappers)\n+  }\n+\n+  private def getShuffleStage(queryStage: QueryStageExec): ShuffleQueryStageExec = {\n+    queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+  }\n+\n+  private def getStatistics(queryStage: QueryStageExec): MapOutputStatistics = {\n+    val shuffleStage = queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+    val metrics = shuffleStage.mapOutputStatisticsFuture\n+    assert(metrics.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+    ThreadUtils.awaitResult(metrics, Duration.Zero)\n+  }\n+\n+  /**\n+   * Base optimization support check: the join type is supported and plan statistics is available.\n+   * Note that for some join types(like left outer), whether a certain partition can be optimized\n+   * also depends on the filed isSkewAndSupportsSplit.\n+   */\n+  private def supportOptimization(\n+    joinType: JoinType,\n+    leftStage: QueryStageExec,\n+    rightStage: QueryStageExec): Boolean = {\n+    val joinTypeSupported = supportedJoinTypes.contains(joinType)\n+    val shuffleStageCheck = ShuffleQueryStageExec.isShuffleQueryStageExec(leftStage) &&\n+      ShuffleQueryStageExec.isShuffleQueryStageExec(rightStage)\n+    val statisticsReady: Boolean = if (shuffleStageCheck) {\n+      getStatistics(leftStage) != null && getStatistics(rightStage) != null\n+    } else false\n+\n+    joinTypeSupported && statisticsReady\n+  }\n+\n+  private def supportSplitOnLeftPartition(joinType: JoinType) = joinType != RightOuter\n+\n+  private def supportSplitOnRightPartition(joinType: JoinType) = {\n+    joinType != LeftOuter && joinType != LeftSemi && joinType != LeftAnti\n+  }\n+\n+  def handleSkewJoin(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case smj @ SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+    SortExec(_, _, left: QueryStageExec, _),\n+    SortExec(_, _, right: QueryStageExec, _))\n+      if supportOptimization(joinType, left, right) =>\n+      val leftStats = getStatistics(left)\n+      val rightStats = getStatistics(right)\n+      val numPartitions = leftStats.bytesByPartitionId.length\n+\n+      val leftMedSize = medianSize(leftStats)\n+      val rightMedSize = medianSize(rightStats)\n+      logInfo(s\"HandlingSkewedJoin left medSize: ($leftMedSize)\" +\n+        s\" right medSize ($rightMedSize)\")\n+      logInfo(s\"left bytes Max : ${leftStats.bytesByPartitionId.max}\")\n+      logInfo(s\"right bytes Max : ${rightStats.bytesByPartitionId.max}\")\n+\n+      val skewedPartitions = mutable.HashSet[Int]()\n+      val subJoins = mutable.ArrayBuffer[SparkPlan](smj)\n+      for (partitionId <- 0 until numPartitions) {\n+        val isLeftSkew = isSkewed(leftStats, partitionId, leftMedSize)\n+        val isRightSkew = isSkewed(rightStats, partitionId, rightMedSize)\n+        val isSkewAndSupportsSplit =\n+          (isLeftSkew && supportSplitOnLeftPartition(joinType)) ||\n+            (isRightSkew && supportSplitOnRightPartition(joinType))\n+\n+        if (isSkewAndSupportsSplit) {\n+          skewedPartitions += partitionId\n+          val leftMapIdStartIndices = if (isLeftSkew && supportSplitOnLeftPartition(joinType)) {\n+            estimateMapIdStartIndices(left, partitionId, leftMedSize)\n+          } else {\n+            Array(0)\n+          }\n+          val rightMapIdStartIndices = if (isRightSkew && supportSplitOnRightPartition(joinType)) {\n+            estimateMapIdStartIndices(right, partitionId, rightMedSize)\n+          } else {\n+            Array(0)\n+          }\n+\n+          for (i <- 0 until leftMapIdStartIndices.length;\n+               j <- 0 until rightMapIdStartIndices.length) {\n+            val leftEndMapId = if (i == leftMapIdStartIndices.length - 1) {\n+              getShuffleStage(left).plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              leftMapIdStartIndices(i + 1)\n+            }\n+            val rightEndMapId = if (j == rightMapIdStartIndices.length - 1) {\n+              getShuffleStage(right).\n+                plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              rightMapIdStartIndices(j + 1)\n+            }\n+            // For the skewed partition, we set the id of shuffle query stage to -1.\n+            // And skip this shuffle query stage optimization in 'ReduceNumShufflePartitions' rule.\n+            val leftSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(left).copy(id = -1),\n+                partitionId, leftMapIdStartIndices(i), leftEndMapId)\n+\n+            val rightSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(right).copy(id = -1),\n+                partitionId, rightMapIdStartIndices(j), rightEndMapId)\n+\n+            subJoins +=\n+              SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+                leftSkewedReader, rightSkewedReader)\n+          }\n+        }\n+      }\n+      logInfo(s\"skewed partition number is ${skewedPartitions.size}\")\n+      if (skewedPartitions.size > 0) {\n+        getShuffleStage(left).skewedPartitions = skewedPartitions\n+        getShuffleStage(right).skewedPartitions = skewedPartitions\n+        UnionExec(subJoins.toList)\n+      } else {\n+        smj\n+      }\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    if (!conf.adaptiveSkewedJoinEnabled) {\n+      return  plan\n+    }\n+\n+    def collectShuffleStages(plan: SparkPlan): Seq[ShuffleQueryStageExec] = plan match {\n+      case _: LocalShuffleReaderExec => Nil\n+      case _: PostShufflePartitionReader => Nil\n+      case stage: ShuffleQueryStageExec => Seq(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => Seq(stage)\n+      case _ => plan.children.flatMap(collectShuffleStages)\n+    }\n+\n+    val shuffleStages = collectShuffleStages(plan)\n+\n+    if (shuffleStages.length == 2) {\n+      // Currently we only support handling skewed join for 2 table join.\n+      handleSkewJoin(plan)\n+    } else {\n+      plan\n+\n+    }\n+  }\n+}\n+\n+case class PostShufflePartitionReader(\n+    child: QueryStageExec,\n+    partitionIndex: Int,\n+    startMapId: Int,\n+    endMapId: Int) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def doCanonicalize(): SparkPlan = child.canonicalized\n+\n+  override def outputPartitioning: Partitioning = {\n+    UnknownPartitioning(1)",
    "line": 266
  }, {
    "author": {
      "login": "JkSelf"
    },
    "body": "Here we may not execute `ensureRequirements` again. So how add shuffle between `PostShufflePartitionReader` and SMJ?",
    "commit": "bbf585dbbcf41a47282a272ef7449db553eae9f3",
    "createdAt": "2019-11-13T02:59:03Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.{MapOutputStatistics, MapOutputTrackerMaster, SparkEnv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.joins.SortMergeJoinExec\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+case class OptimizeSkewedPartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private val supportedJoinTypes =\n+    Inner :: Cross :: LeftSemi :: LeftAnti :: LeftOuter :: RightOuter :: Nil\n+\n+  /**\n+   * A partition is considered as a skewed partition if its size is larger than the median\n+   * partition size * spark.sql.adaptive.skewedPartitionFactor and also larger than\n+   * spark.sql.adaptive.skewedPartitionSizeThreshold.\n+   */\n+  private def isSkewed(\n+     stats: MapOutputStatistics,\n+     partitionId: Int,\n+     medianSize: Long): Boolean = {\n+    val size = stats.bytesByPartitionId(partitionId)\n+    size > medianSize * conf.adaptiveSkewedFactor &&\n+      size > conf.adaptiveSkewedSizeThreshold\n+  }\n+\n+  private def medianSize(stats: MapOutputStatistics): Long = {\n+    val bytesLen = stats.bytesByPartitionId.length\n+    val bytes = stats.bytesByPartitionId.sorted\n+    if (bytes(bytesLen / 2) > 0) bytes(bytesLen / 2) else 1\n+  }\n+\n+  /*\n+  * Get all the map data size for specific reduce partitionId.\n+  */\n+  def getMapSizeForSpecificPartition(partitionId: Int, shuffleId: Int): Array[Long] = {\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster]\n+    mapOutputTracker.shuffleStatuses.get(shuffleId).\n+      get.mapStatuses.map{_.getSizeForBlock(partitionId)}\n+  }\n+\n+  /*\n+  * Split the mappers based on the map size of specific skewed reduce partitionId.\n+  */\n+  def splitMappersBasedDataSize(mapPartitionSize: Array[Long], numMappers: Int): Array[Int] = {\n+    val advisoryTargetPostShuffleInputSize = conf.targetPostShuffleInputSize\n+    val partitionStartIndices = ArrayBuffer[Int]()\n+    var i = 0\n+    var postMapPartitionSize: Long = mapPartitionSize(i)\n+    partitionStartIndices += i\n+    while (i < numMappers && i + 1 < numMappers) {\n+      val nextIndex = if (i + 1 < numMappers) {\n+        i + 1\n+      } else numMappers -1\n+\n+      if (postMapPartitionSize + mapPartitionSize(nextIndex) > advisoryTargetPostShuffleInputSize) {\n+        postMapPartitionSize = mapPartitionSize(nextIndex)\n+        partitionStartIndices += nextIndex\n+      } else {\n+        postMapPartitionSize += mapPartitionSize(nextIndex)\n+      }\n+      i += 1\n+    }\n+    partitionStartIndices.toArray\n+  }\n+\n+  /**\n+   * We split the partition into several splits. Each split reads the data from several map outputs\n+   * ranging from startMapId to endMapId(exclusive). This method calculates the split number and\n+   * the startMapId for all splits.\n+   */\n+  private def estimateMapIdStartIndices(\n+    stage: QueryStageExec,\n+    partitionId: Int,\n+    medianSize: Long): Array[Int] = {\n+    val dependency = getShuffleStage(stage).plan.shuffleDependency\n+    val shuffleId = dependency.shuffleHandle.shuffleId\n+    val mapSize = getMapSizeForSpecificPartition(partitionId, shuffleId)\n+    val numMappers = dependency.rdd.partitions.length\n+    splitMappersBasedDataSize(mapSize, numMappers)\n+  }\n+\n+  private def getShuffleStage(queryStage: QueryStageExec): ShuffleQueryStageExec = {\n+    queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+  }\n+\n+  private def getStatistics(queryStage: QueryStageExec): MapOutputStatistics = {\n+    val shuffleStage = queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+    val metrics = shuffleStage.mapOutputStatisticsFuture\n+    assert(metrics.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+    ThreadUtils.awaitResult(metrics, Duration.Zero)\n+  }\n+\n+  /**\n+   * Base optimization support check: the join type is supported and plan statistics is available.\n+   * Note that for some join types(like left outer), whether a certain partition can be optimized\n+   * also depends on the filed isSkewAndSupportsSplit.\n+   */\n+  private def supportOptimization(\n+    joinType: JoinType,\n+    leftStage: QueryStageExec,\n+    rightStage: QueryStageExec): Boolean = {\n+    val joinTypeSupported = supportedJoinTypes.contains(joinType)\n+    val shuffleStageCheck = ShuffleQueryStageExec.isShuffleQueryStageExec(leftStage) &&\n+      ShuffleQueryStageExec.isShuffleQueryStageExec(rightStage)\n+    val statisticsReady: Boolean = if (shuffleStageCheck) {\n+      getStatistics(leftStage) != null && getStatistics(rightStage) != null\n+    } else false\n+\n+    joinTypeSupported && statisticsReady\n+  }\n+\n+  private def supportSplitOnLeftPartition(joinType: JoinType) = joinType != RightOuter\n+\n+  private def supportSplitOnRightPartition(joinType: JoinType) = {\n+    joinType != LeftOuter && joinType != LeftSemi && joinType != LeftAnti\n+  }\n+\n+  def handleSkewJoin(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case smj @ SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+    SortExec(_, _, left: QueryStageExec, _),\n+    SortExec(_, _, right: QueryStageExec, _))\n+      if supportOptimization(joinType, left, right) =>\n+      val leftStats = getStatistics(left)\n+      val rightStats = getStatistics(right)\n+      val numPartitions = leftStats.bytesByPartitionId.length\n+\n+      val leftMedSize = medianSize(leftStats)\n+      val rightMedSize = medianSize(rightStats)\n+      logInfo(s\"HandlingSkewedJoin left medSize: ($leftMedSize)\" +\n+        s\" right medSize ($rightMedSize)\")\n+      logInfo(s\"left bytes Max : ${leftStats.bytesByPartitionId.max}\")\n+      logInfo(s\"right bytes Max : ${rightStats.bytesByPartitionId.max}\")\n+\n+      val skewedPartitions = mutable.HashSet[Int]()\n+      val subJoins = mutable.ArrayBuffer[SparkPlan](smj)\n+      for (partitionId <- 0 until numPartitions) {\n+        val isLeftSkew = isSkewed(leftStats, partitionId, leftMedSize)\n+        val isRightSkew = isSkewed(rightStats, partitionId, rightMedSize)\n+        val isSkewAndSupportsSplit =\n+          (isLeftSkew && supportSplitOnLeftPartition(joinType)) ||\n+            (isRightSkew && supportSplitOnRightPartition(joinType))\n+\n+        if (isSkewAndSupportsSplit) {\n+          skewedPartitions += partitionId\n+          val leftMapIdStartIndices = if (isLeftSkew && supportSplitOnLeftPartition(joinType)) {\n+            estimateMapIdStartIndices(left, partitionId, leftMedSize)\n+          } else {\n+            Array(0)\n+          }\n+          val rightMapIdStartIndices = if (isRightSkew && supportSplitOnRightPartition(joinType)) {\n+            estimateMapIdStartIndices(right, partitionId, rightMedSize)\n+          } else {\n+            Array(0)\n+          }\n+\n+          for (i <- 0 until leftMapIdStartIndices.length;\n+               j <- 0 until rightMapIdStartIndices.length) {\n+            val leftEndMapId = if (i == leftMapIdStartIndices.length - 1) {\n+              getShuffleStage(left).plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              leftMapIdStartIndices(i + 1)\n+            }\n+            val rightEndMapId = if (j == rightMapIdStartIndices.length - 1) {\n+              getShuffleStage(right).\n+                plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              rightMapIdStartIndices(j + 1)\n+            }\n+            // For the skewed partition, we set the id of shuffle query stage to -1.\n+            // And skip this shuffle query stage optimization in 'ReduceNumShufflePartitions' rule.\n+            val leftSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(left).copy(id = -1),\n+                partitionId, leftMapIdStartIndices(i), leftEndMapId)\n+\n+            val rightSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(right).copy(id = -1),\n+                partitionId, rightMapIdStartIndices(j), rightEndMapId)\n+\n+            subJoins +=\n+              SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+                leftSkewedReader, rightSkewedReader)\n+          }\n+        }\n+      }\n+      logInfo(s\"skewed partition number is ${skewedPartitions.size}\")\n+      if (skewedPartitions.size > 0) {\n+        getShuffleStage(left).skewedPartitions = skewedPartitions\n+        getShuffleStage(right).skewedPartitions = skewedPartitions\n+        UnionExec(subJoins.toList)\n+      } else {\n+        smj\n+      }\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    if (!conf.adaptiveSkewedJoinEnabled) {\n+      return  plan\n+    }\n+\n+    def collectShuffleStages(plan: SparkPlan): Seq[ShuffleQueryStageExec] = plan match {\n+      case _: LocalShuffleReaderExec => Nil\n+      case _: PostShufflePartitionReader => Nil\n+      case stage: ShuffleQueryStageExec => Seq(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => Seq(stage)\n+      case _ => plan.children.flatMap(collectShuffleStages)\n+    }\n+\n+    val shuffleStages = collectShuffleStages(plan)\n+\n+    if (shuffleStages.length == 2) {\n+      // Currently we only support handling skewed join for 2 table join.\n+      handleSkewJoin(plan)\n+    } else {\n+      plan\n+\n+    }\n+  }\n+}\n+\n+case class PostShufflePartitionReader(\n+    child: QueryStageExec,\n+    partitionIndex: Int,\n+    startMapId: Int,\n+    endMapId: Int) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def doCanonicalize(): SparkPlan = child.canonicalized\n+\n+  override def outputPartitioning: Partitioning = {\n+    UnknownPartitioning(1)",
    "line": 266
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah i see, then there is no difference.",
    "commit": "bbf585dbbcf41a47282a272ef7449db553eae9f3",
    "createdAt": "2019-11-13T08:18:47Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.{MapOutputStatistics, MapOutputTrackerMaster, SparkEnv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.joins.SortMergeJoinExec\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+case class OptimizeSkewedPartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private val supportedJoinTypes =\n+    Inner :: Cross :: LeftSemi :: LeftAnti :: LeftOuter :: RightOuter :: Nil\n+\n+  /**\n+   * A partition is considered as a skewed partition if its size is larger than the median\n+   * partition size * spark.sql.adaptive.skewedPartitionFactor and also larger than\n+   * spark.sql.adaptive.skewedPartitionSizeThreshold.\n+   */\n+  private def isSkewed(\n+     stats: MapOutputStatistics,\n+     partitionId: Int,\n+     medianSize: Long): Boolean = {\n+    val size = stats.bytesByPartitionId(partitionId)\n+    size > medianSize * conf.adaptiveSkewedFactor &&\n+      size > conf.adaptiveSkewedSizeThreshold\n+  }\n+\n+  private def medianSize(stats: MapOutputStatistics): Long = {\n+    val bytesLen = stats.bytesByPartitionId.length\n+    val bytes = stats.bytesByPartitionId.sorted\n+    if (bytes(bytesLen / 2) > 0) bytes(bytesLen / 2) else 1\n+  }\n+\n+  /*\n+  * Get all the map data size for specific reduce partitionId.\n+  */\n+  def getMapSizeForSpecificPartition(partitionId: Int, shuffleId: Int): Array[Long] = {\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster]\n+    mapOutputTracker.shuffleStatuses.get(shuffleId).\n+      get.mapStatuses.map{_.getSizeForBlock(partitionId)}\n+  }\n+\n+  /*\n+  * Split the mappers based on the map size of specific skewed reduce partitionId.\n+  */\n+  def splitMappersBasedDataSize(mapPartitionSize: Array[Long], numMappers: Int): Array[Int] = {\n+    val advisoryTargetPostShuffleInputSize = conf.targetPostShuffleInputSize\n+    val partitionStartIndices = ArrayBuffer[Int]()\n+    var i = 0\n+    var postMapPartitionSize: Long = mapPartitionSize(i)\n+    partitionStartIndices += i\n+    while (i < numMappers && i + 1 < numMappers) {\n+      val nextIndex = if (i + 1 < numMappers) {\n+        i + 1\n+      } else numMappers -1\n+\n+      if (postMapPartitionSize + mapPartitionSize(nextIndex) > advisoryTargetPostShuffleInputSize) {\n+        postMapPartitionSize = mapPartitionSize(nextIndex)\n+        partitionStartIndices += nextIndex\n+      } else {\n+        postMapPartitionSize += mapPartitionSize(nextIndex)\n+      }\n+      i += 1\n+    }\n+    partitionStartIndices.toArray\n+  }\n+\n+  /**\n+   * We split the partition into several splits. Each split reads the data from several map outputs\n+   * ranging from startMapId to endMapId(exclusive). This method calculates the split number and\n+   * the startMapId for all splits.\n+   */\n+  private def estimateMapIdStartIndices(\n+    stage: QueryStageExec,\n+    partitionId: Int,\n+    medianSize: Long): Array[Int] = {\n+    val dependency = getShuffleStage(stage).plan.shuffleDependency\n+    val shuffleId = dependency.shuffleHandle.shuffleId\n+    val mapSize = getMapSizeForSpecificPartition(partitionId, shuffleId)\n+    val numMappers = dependency.rdd.partitions.length\n+    splitMappersBasedDataSize(mapSize, numMappers)\n+  }\n+\n+  private def getShuffleStage(queryStage: QueryStageExec): ShuffleQueryStageExec = {\n+    queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+  }\n+\n+  private def getStatistics(queryStage: QueryStageExec): MapOutputStatistics = {\n+    val shuffleStage = queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+    val metrics = shuffleStage.mapOutputStatisticsFuture\n+    assert(metrics.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+    ThreadUtils.awaitResult(metrics, Duration.Zero)\n+  }\n+\n+  /**\n+   * Base optimization support check: the join type is supported and plan statistics is available.\n+   * Note that for some join types(like left outer), whether a certain partition can be optimized\n+   * also depends on the filed isSkewAndSupportsSplit.\n+   */\n+  private def supportOptimization(\n+    joinType: JoinType,\n+    leftStage: QueryStageExec,\n+    rightStage: QueryStageExec): Boolean = {\n+    val joinTypeSupported = supportedJoinTypes.contains(joinType)\n+    val shuffleStageCheck = ShuffleQueryStageExec.isShuffleQueryStageExec(leftStage) &&\n+      ShuffleQueryStageExec.isShuffleQueryStageExec(rightStage)\n+    val statisticsReady: Boolean = if (shuffleStageCheck) {\n+      getStatistics(leftStage) != null && getStatistics(rightStage) != null\n+    } else false\n+\n+    joinTypeSupported && statisticsReady\n+  }\n+\n+  private def supportSplitOnLeftPartition(joinType: JoinType) = joinType != RightOuter\n+\n+  private def supportSplitOnRightPartition(joinType: JoinType) = {\n+    joinType != LeftOuter && joinType != LeftSemi && joinType != LeftAnti\n+  }\n+\n+  def handleSkewJoin(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case smj @ SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+    SortExec(_, _, left: QueryStageExec, _),\n+    SortExec(_, _, right: QueryStageExec, _))\n+      if supportOptimization(joinType, left, right) =>\n+      val leftStats = getStatistics(left)\n+      val rightStats = getStatistics(right)\n+      val numPartitions = leftStats.bytesByPartitionId.length\n+\n+      val leftMedSize = medianSize(leftStats)\n+      val rightMedSize = medianSize(rightStats)\n+      logInfo(s\"HandlingSkewedJoin left medSize: ($leftMedSize)\" +\n+        s\" right medSize ($rightMedSize)\")\n+      logInfo(s\"left bytes Max : ${leftStats.bytesByPartitionId.max}\")\n+      logInfo(s\"right bytes Max : ${rightStats.bytesByPartitionId.max}\")\n+\n+      val skewedPartitions = mutable.HashSet[Int]()\n+      val subJoins = mutable.ArrayBuffer[SparkPlan](smj)\n+      for (partitionId <- 0 until numPartitions) {\n+        val isLeftSkew = isSkewed(leftStats, partitionId, leftMedSize)\n+        val isRightSkew = isSkewed(rightStats, partitionId, rightMedSize)\n+        val isSkewAndSupportsSplit =\n+          (isLeftSkew && supportSplitOnLeftPartition(joinType)) ||\n+            (isRightSkew && supportSplitOnRightPartition(joinType))\n+\n+        if (isSkewAndSupportsSplit) {\n+          skewedPartitions += partitionId\n+          val leftMapIdStartIndices = if (isLeftSkew && supportSplitOnLeftPartition(joinType)) {\n+            estimateMapIdStartIndices(left, partitionId, leftMedSize)\n+          } else {\n+            Array(0)\n+          }\n+          val rightMapIdStartIndices = if (isRightSkew && supportSplitOnRightPartition(joinType)) {\n+            estimateMapIdStartIndices(right, partitionId, rightMedSize)\n+          } else {\n+            Array(0)\n+          }\n+\n+          for (i <- 0 until leftMapIdStartIndices.length;\n+               j <- 0 until rightMapIdStartIndices.length) {\n+            val leftEndMapId = if (i == leftMapIdStartIndices.length - 1) {\n+              getShuffleStage(left).plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              leftMapIdStartIndices(i + 1)\n+            }\n+            val rightEndMapId = if (j == rightMapIdStartIndices.length - 1) {\n+              getShuffleStage(right).\n+                plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              rightMapIdStartIndices(j + 1)\n+            }\n+            // For the skewed partition, we set the id of shuffle query stage to -1.\n+            // And skip this shuffle query stage optimization in 'ReduceNumShufflePartitions' rule.\n+            val leftSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(left).copy(id = -1),\n+                partitionId, leftMapIdStartIndices(i), leftEndMapId)\n+\n+            val rightSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(right).copy(id = -1),\n+                partitionId, rightMapIdStartIndices(j), rightEndMapId)\n+\n+            subJoins +=\n+              SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+                leftSkewedReader, rightSkewedReader)\n+          }\n+        }\n+      }\n+      logInfo(s\"skewed partition number is ${skewedPartitions.size}\")\n+      if (skewedPartitions.size > 0) {\n+        getShuffleStage(left).skewedPartitions = skewedPartitions\n+        getShuffleStage(right).skewedPartitions = skewedPartitions\n+        UnionExec(subJoins.toList)\n+      } else {\n+        smj\n+      }\n+  }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = {\n+    if (!conf.adaptiveSkewedJoinEnabled) {\n+      return  plan\n+    }\n+\n+    def collectShuffleStages(plan: SparkPlan): Seq[ShuffleQueryStageExec] = plan match {\n+      case _: LocalShuffleReaderExec => Nil\n+      case _: PostShufflePartitionReader => Nil\n+      case stage: ShuffleQueryStageExec => Seq(stage)\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => Seq(stage)\n+      case _ => plan.children.flatMap(collectShuffleStages)\n+    }\n+\n+    val shuffleStages = collectShuffleStages(plan)\n+\n+    if (shuffleStages.length == 2) {\n+      // Currently we only support handling skewed join for 2 table join.\n+      handleSkewJoin(plan)\n+    } else {\n+      plan\n+\n+    }\n+  }\n+}\n+\n+case class PostShufflePartitionReader(\n+    child: QueryStageExec,\n+    partitionIndex: Int,\n+    startMapId: Int,\n+    endMapId: Int) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = child.output\n+\n+  override def doCanonicalize(): SparkPlan = child.canonicalized\n+\n+  override def outputPartitioning: Partitioning = {\n+    UnknownPartitioning(1)",
    "line": 266
  }],
  "prId": 26434
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "just for brainstorm: here we are joining 2 partitions, not 2 RDDs, so there will be no shuffle. Is it better to run hash join than SMJ? cc @maryannxue ",
    "commit": "bbf585dbbcf41a47282a272ef7449db553eae9f3",
    "createdAt": "2019-11-12T17:55:08Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.{MapOutputStatistics, MapOutputTrackerMaster, SparkEnv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.joins.SortMergeJoinExec\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+case class OptimizeSkewedPartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private val supportedJoinTypes =\n+    Inner :: Cross :: LeftSemi :: LeftAnti :: LeftOuter :: RightOuter :: Nil\n+\n+  /**\n+   * A partition is considered as a skewed partition if its size is larger than the median\n+   * partition size * spark.sql.adaptive.skewedPartitionFactor and also larger than\n+   * spark.sql.adaptive.skewedPartitionSizeThreshold.\n+   */\n+  private def isSkewed(\n+     stats: MapOutputStatistics,\n+     partitionId: Int,\n+     medianSize: Long): Boolean = {\n+    val size = stats.bytesByPartitionId(partitionId)\n+    size > medianSize * conf.adaptiveSkewedFactor &&\n+      size > conf.adaptiveSkewedSizeThreshold\n+  }\n+\n+  private def medianSize(stats: MapOutputStatistics): Long = {\n+    val bytesLen = stats.bytesByPartitionId.length\n+    val bytes = stats.bytesByPartitionId.sorted\n+    if (bytes(bytesLen / 2) > 0) bytes(bytesLen / 2) else 1\n+  }\n+\n+  /*\n+  * Get all the map data size for specific reduce partitionId.\n+  */\n+  def getMapSizeForSpecificPartition(partitionId: Int, shuffleId: Int): Array[Long] = {\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster]\n+    mapOutputTracker.shuffleStatuses.get(shuffleId).\n+      get.mapStatuses.map{_.getSizeForBlock(partitionId)}\n+  }\n+\n+  /*\n+  * Split the mappers based on the map size of specific skewed reduce partitionId.\n+  */\n+  def splitMappersBasedDataSize(mapPartitionSize: Array[Long], numMappers: Int): Array[Int] = {\n+    val advisoryTargetPostShuffleInputSize = conf.targetPostShuffleInputSize\n+    val partitionStartIndices = ArrayBuffer[Int]()\n+    var i = 0\n+    var postMapPartitionSize: Long = mapPartitionSize(i)\n+    partitionStartIndices += i\n+    while (i < numMappers && i + 1 < numMappers) {\n+      val nextIndex = if (i + 1 < numMappers) {\n+        i + 1\n+      } else numMappers -1\n+\n+      if (postMapPartitionSize + mapPartitionSize(nextIndex) > advisoryTargetPostShuffleInputSize) {\n+        postMapPartitionSize = mapPartitionSize(nextIndex)\n+        partitionStartIndices += nextIndex\n+      } else {\n+        postMapPartitionSize += mapPartitionSize(nextIndex)\n+      }\n+      i += 1\n+    }\n+    partitionStartIndices.toArray\n+  }\n+\n+  /**\n+   * We split the partition into several splits. Each split reads the data from several map outputs\n+   * ranging from startMapId to endMapId(exclusive). This method calculates the split number and\n+   * the startMapId for all splits.\n+   */\n+  private def estimateMapIdStartIndices(\n+    stage: QueryStageExec,\n+    partitionId: Int,\n+    medianSize: Long): Array[Int] = {\n+    val dependency = getShuffleStage(stage).plan.shuffleDependency\n+    val shuffleId = dependency.shuffleHandle.shuffleId\n+    val mapSize = getMapSizeForSpecificPartition(partitionId, shuffleId)\n+    val numMappers = dependency.rdd.partitions.length\n+    splitMappersBasedDataSize(mapSize, numMappers)\n+  }\n+\n+  private def getShuffleStage(queryStage: QueryStageExec): ShuffleQueryStageExec = {\n+    queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+  }\n+\n+  private def getStatistics(queryStage: QueryStageExec): MapOutputStatistics = {\n+    val shuffleStage = queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+    val metrics = shuffleStage.mapOutputStatisticsFuture\n+    assert(metrics.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+    ThreadUtils.awaitResult(metrics, Duration.Zero)\n+  }\n+\n+  /**\n+   * Base optimization support check: the join type is supported and plan statistics is available.\n+   * Note that for some join types(like left outer), whether a certain partition can be optimized\n+   * also depends on the filed isSkewAndSupportsSplit.\n+   */\n+  private def supportOptimization(\n+    joinType: JoinType,\n+    leftStage: QueryStageExec,\n+    rightStage: QueryStageExec): Boolean = {\n+    val joinTypeSupported = supportedJoinTypes.contains(joinType)\n+    val shuffleStageCheck = ShuffleQueryStageExec.isShuffleQueryStageExec(leftStage) &&\n+      ShuffleQueryStageExec.isShuffleQueryStageExec(rightStage)\n+    val statisticsReady: Boolean = if (shuffleStageCheck) {\n+      getStatistics(leftStage) != null && getStatistics(rightStage) != null\n+    } else false\n+\n+    joinTypeSupported && statisticsReady\n+  }\n+\n+  private def supportSplitOnLeftPartition(joinType: JoinType) = joinType != RightOuter\n+\n+  private def supportSplitOnRightPartition(joinType: JoinType) = {\n+    joinType != LeftOuter && joinType != LeftSemi && joinType != LeftAnti\n+  }\n+\n+  def handleSkewJoin(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case smj @ SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+    SortExec(_, _, left: QueryStageExec, _),\n+    SortExec(_, _, right: QueryStageExec, _))\n+      if supportOptimization(joinType, left, right) =>\n+      val leftStats = getStatistics(left)\n+      val rightStats = getStatistics(right)\n+      val numPartitions = leftStats.bytesByPartitionId.length\n+\n+      val leftMedSize = medianSize(leftStats)\n+      val rightMedSize = medianSize(rightStats)\n+      logInfo(s\"HandlingSkewedJoin left medSize: ($leftMedSize)\" +\n+        s\" right medSize ($rightMedSize)\")\n+      logInfo(s\"left bytes Max : ${leftStats.bytesByPartitionId.max}\")\n+      logInfo(s\"right bytes Max : ${rightStats.bytesByPartitionId.max}\")\n+\n+      val skewedPartitions = mutable.HashSet[Int]()\n+      val subJoins = mutable.ArrayBuffer[SparkPlan](smj)\n+      for (partitionId <- 0 until numPartitions) {\n+        val isLeftSkew = isSkewed(leftStats, partitionId, leftMedSize)\n+        val isRightSkew = isSkewed(rightStats, partitionId, rightMedSize)\n+        val isSkewAndSupportsSplit =\n+          (isLeftSkew && supportSplitOnLeftPartition(joinType)) ||\n+            (isRightSkew && supportSplitOnRightPartition(joinType))\n+\n+        if (isSkewAndSupportsSplit) {\n+          skewedPartitions += partitionId\n+          val leftMapIdStartIndices = if (isLeftSkew && supportSplitOnLeftPartition(joinType)) {\n+            estimateMapIdStartIndices(left, partitionId, leftMedSize)\n+          } else {\n+            Array(0)\n+          }\n+          val rightMapIdStartIndices = if (isRightSkew && supportSplitOnRightPartition(joinType)) {\n+            estimateMapIdStartIndices(right, partitionId, rightMedSize)\n+          } else {\n+            Array(0)\n+          }\n+\n+          for (i <- 0 until leftMapIdStartIndices.length;\n+               j <- 0 until rightMapIdStartIndices.length) {\n+            val leftEndMapId = if (i == leftMapIdStartIndices.length - 1) {\n+              getShuffleStage(left).plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              leftMapIdStartIndices(i + 1)\n+            }\n+            val rightEndMapId = if (j == rightMapIdStartIndices.length - 1) {\n+              getShuffleStage(right).\n+                plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              rightMapIdStartIndices(j + 1)\n+            }\n+            // For the skewed partition, we set the id of shuffle query stage to -1.\n+            // And skip this shuffle query stage optimization in 'ReduceNumShufflePartitions' rule.\n+            val leftSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(left).copy(id = -1),\n+                partitionId, leftMapIdStartIndices(i), leftEndMapId)\n+\n+            val rightSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(right).copy(id = -1),\n+                partitionId, rightMapIdStartIndices(j), rightEndMapId)\n+\n+            subJoins +=\n+              SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,",
    "line": 215
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Furthermore, if only one side is skew, maybe better to plan a broadcast hash join instead of a union of many SMJs?",
    "commit": "bbf585dbbcf41a47282a272ef7449db553eae9f3",
    "createdAt": "2019-11-12T17:56:17Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.adaptive\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration.Duration\n+\n+import org.apache.spark.{MapOutputStatistics, MapOutputTrackerMaster, SparkEnv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans._\n+import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, UnknownPartitioning}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.joins.SortMergeJoinExec\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.ThreadUtils\n+\n+case class OptimizeSkewedPartitions(conf: SQLConf) extends Rule[SparkPlan] {\n+\n+  private val supportedJoinTypes =\n+    Inner :: Cross :: LeftSemi :: LeftAnti :: LeftOuter :: RightOuter :: Nil\n+\n+  /**\n+   * A partition is considered as a skewed partition if its size is larger than the median\n+   * partition size * spark.sql.adaptive.skewedPartitionFactor and also larger than\n+   * spark.sql.adaptive.skewedPartitionSizeThreshold.\n+   */\n+  private def isSkewed(\n+     stats: MapOutputStatistics,\n+     partitionId: Int,\n+     medianSize: Long): Boolean = {\n+    val size = stats.bytesByPartitionId(partitionId)\n+    size > medianSize * conf.adaptiveSkewedFactor &&\n+      size > conf.adaptiveSkewedSizeThreshold\n+  }\n+\n+  private def medianSize(stats: MapOutputStatistics): Long = {\n+    val bytesLen = stats.bytesByPartitionId.length\n+    val bytes = stats.bytesByPartitionId.sorted\n+    if (bytes(bytesLen / 2) > 0) bytes(bytesLen / 2) else 1\n+  }\n+\n+  /*\n+  * Get all the map data size for specific reduce partitionId.\n+  */\n+  def getMapSizeForSpecificPartition(partitionId: Int, shuffleId: Int): Array[Long] = {\n+    val mapOutputTracker = SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster]\n+    mapOutputTracker.shuffleStatuses.get(shuffleId).\n+      get.mapStatuses.map{_.getSizeForBlock(partitionId)}\n+  }\n+\n+  /*\n+  * Split the mappers based on the map size of specific skewed reduce partitionId.\n+  */\n+  def splitMappersBasedDataSize(mapPartitionSize: Array[Long], numMappers: Int): Array[Int] = {\n+    val advisoryTargetPostShuffleInputSize = conf.targetPostShuffleInputSize\n+    val partitionStartIndices = ArrayBuffer[Int]()\n+    var i = 0\n+    var postMapPartitionSize: Long = mapPartitionSize(i)\n+    partitionStartIndices += i\n+    while (i < numMappers && i + 1 < numMappers) {\n+      val nextIndex = if (i + 1 < numMappers) {\n+        i + 1\n+      } else numMappers -1\n+\n+      if (postMapPartitionSize + mapPartitionSize(nextIndex) > advisoryTargetPostShuffleInputSize) {\n+        postMapPartitionSize = mapPartitionSize(nextIndex)\n+        partitionStartIndices += nextIndex\n+      } else {\n+        postMapPartitionSize += mapPartitionSize(nextIndex)\n+      }\n+      i += 1\n+    }\n+    partitionStartIndices.toArray\n+  }\n+\n+  /**\n+   * We split the partition into several splits. Each split reads the data from several map outputs\n+   * ranging from startMapId to endMapId(exclusive). This method calculates the split number and\n+   * the startMapId for all splits.\n+   */\n+  private def estimateMapIdStartIndices(\n+    stage: QueryStageExec,\n+    partitionId: Int,\n+    medianSize: Long): Array[Int] = {\n+    val dependency = getShuffleStage(stage).plan.shuffleDependency\n+    val shuffleId = dependency.shuffleHandle.shuffleId\n+    val mapSize = getMapSizeForSpecificPartition(partitionId, shuffleId)\n+    val numMappers = dependency.rdd.partitions.length\n+    splitMappersBasedDataSize(mapSize, numMappers)\n+  }\n+\n+  private def getShuffleStage(queryStage: QueryStageExec): ShuffleQueryStageExec = {\n+    queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+  }\n+\n+  private def getStatistics(queryStage: QueryStageExec): MapOutputStatistics = {\n+    val shuffleStage = queryStage match {\n+      case stage: ShuffleQueryStageExec => stage\n+      case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage\n+    }\n+    val metrics = shuffleStage.mapOutputStatisticsFuture\n+    assert(metrics.isCompleted, \"ShuffleQueryStageExec should already be ready\")\n+    ThreadUtils.awaitResult(metrics, Duration.Zero)\n+  }\n+\n+  /**\n+   * Base optimization support check: the join type is supported and plan statistics is available.\n+   * Note that for some join types(like left outer), whether a certain partition can be optimized\n+   * also depends on the filed isSkewAndSupportsSplit.\n+   */\n+  private def supportOptimization(\n+    joinType: JoinType,\n+    leftStage: QueryStageExec,\n+    rightStage: QueryStageExec): Boolean = {\n+    val joinTypeSupported = supportedJoinTypes.contains(joinType)\n+    val shuffleStageCheck = ShuffleQueryStageExec.isShuffleQueryStageExec(leftStage) &&\n+      ShuffleQueryStageExec.isShuffleQueryStageExec(rightStage)\n+    val statisticsReady: Boolean = if (shuffleStageCheck) {\n+      getStatistics(leftStage) != null && getStatistics(rightStage) != null\n+    } else false\n+\n+    joinTypeSupported && statisticsReady\n+  }\n+\n+  private def supportSplitOnLeftPartition(joinType: JoinType) = joinType != RightOuter\n+\n+  private def supportSplitOnRightPartition(joinType: JoinType) = {\n+    joinType != LeftOuter && joinType != LeftSemi && joinType != LeftAnti\n+  }\n+\n+  def handleSkewJoin(plan: SparkPlan): SparkPlan = plan.transformUp {\n+    case smj @ SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,\n+    SortExec(_, _, left: QueryStageExec, _),\n+    SortExec(_, _, right: QueryStageExec, _))\n+      if supportOptimization(joinType, left, right) =>\n+      val leftStats = getStatistics(left)\n+      val rightStats = getStatistics(right)\n+      val numPartitions = leftStats.bytesByPartitionId.length\n+\n+      val leftMedSize = medianSize(leftStats)\n+      val rightMedSize = medianSize(rightStats)\n+      logInfo(s\"HandlingSkewedJoin left medSize: ($leftMedSize)\" +\n+        s\" right medSize ($rightMedSize)\")\n+      logInfo(s\"left bytes Max : ${leftStats.bytesByPartitionId.max}\")\n+      logInfo(s\"right bytes Max : ${rightStats.bytesByPartitionId.max}\")\n+\n+      val skewedPartitions = mutable.HashSet[Int]()\n+      val subJoins = mutable.ArrayBuffer[SparkPlan](smj)\n+      for (partitionId <- 0 until numPartitions) {\n+        val isLeftSkew = isSkewed(leftStats, partitionId, leftMedSize)\n+        val isRightSkew = isSkewed(rightStats, partitionId, rightMedSize)\n+        val isSkewAndSupportsSplit =\n+          (isLeftSkew && supportSplitOnLeftPartition(joinType)) ||\n+            (isRightSkew && supportSplitOnRightPartition(joinType))\n+\n+        if (isSkewAndSupportsSplit) {\n+          skewedPartitions += partitionId\n+          val leftMapIdStartIndices = if (isLeftSkew && supportSplitOnLeftPartition(joinType)) {\n+            estimateMapIdStartIndices(left, partitionId, leftMedSize)\n+          } else {\n+            Array(0)\n+          }\n+          val rightMapIdStartIndices = if (isRightSkew && supportSplitOnRightPartition(joinType)) {\n+            estimateMapIdStartIndices(right, partitionId, rightMedSize)\n+          } else {\n+            Array(0)\n+          }\n+\n+          for (i <- 0 until leftMapIdStartIndices.length;\n+               j <- 0 until rightMapIdStartIndices.length) {\n+            val leftEndMapId = if (i == leftMapIdStartIndices.length - 1) {\n+              getShuffleStage(left).plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              leftMapIdStartIndices(i + 1)\n+            }\n+            val rightEndMapId = if (j == rightMapIdStartIndices.length - 1) {\n+              getShuffleStage(right).\n+                plan.shuffleDependency.rdd.partitions.length\n+            } else {\n+              rightMapIdStartIndices(j + 1)\n+            }\n+            // For the skewed partition, we set the id of shuffle query stage to -1.\n+            // And skip this shuffle query stage optimization in 'ReduceNumShufflePartitions' rule.\n+            val leftSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(left).copy(id = -1),\n+                partitionId, leftMapIdStartIndices(i), leftEndMapId)\n+\n+            val rightSkewedReader =\n+              PostShufflePartitionReader(getShuffleStage(right).copy(id = -1),\n+                partitionId, rightMapIdStartIndices(j), rightEndMapId)\n+\n+            subJoins +=\n+              SortMergeJoinExec(leftKeys, rightKeys, joinType, condition,",
    "line": 215
  }],
  "prId": 26434
}]