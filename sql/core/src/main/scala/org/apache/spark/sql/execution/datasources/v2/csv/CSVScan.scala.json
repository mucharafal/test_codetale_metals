[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "This is already handled in `CSVScanBuilder`.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-08T18:22:19Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.csv.CSVOptions\n+import org.apache.spark.sql.catalyst.expressions.ExprUtils\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2.TextBasedFileScan\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.reader.PartitionReaderFactory\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.util.SerializableConfiguration\n+\n+case class CSVScan(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    dataSchema: StructType,\n+    readSchema: StructType,\n+    options: DataSourceOptions)\n+  extends TextBasedFileScan(sparkSession, fileIndex, readSchema, options) {\n+\n+  private val optionsAsScala = options.asMap().asScala.toMap\n+  private lazy val parsedOptions: CSVOptions = new CSVOptions(\n+    optionsAsScala,\n+    columnPruning = sparkSession.sessionState.conf.csvColumnPruning,\n+    sparkSession.sessionState.conf.sessionLocalTimeZone,\n+    sparkSession.sessionState.conf.columnNameOfCorruptRecord)\n+\n+  override def isSplitable(path: Path): Boolean = {\n+    CSVDataSource(parsedOptions).isSplitable && super.isSplitable(path)\n+  }\n+\n+  override def createReaderFactory(): PartitionReaderFactory = {\n+    // Check a field requirement for corrupt records here to throw an exception in a driver side\n+    ExprUtils.verifyColumnNameOfCorruptRecord(dataSchema, parsedOptions.columnNameOfCorruptRecord)\n+\n+    if (readSchema.length == 1 &&\n+      readSchema.head.name == parsedOptions.columnNameOfCorruptRecord) {\n+      throw new AnalysisException(\n+        \"Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\\n\" +\n+          \"referenced columns only include the internal corrupt record column\\n\" +\n+          s\"(named _corrupt_record by default). For example:\\n\" +\n+          \"spark.read.schema(schema).csv(file).filter($\\\"_corrupt_record\\\".isNotNull).count()\\n\" +\n+          \"and spark.read.schema(schema).csv(file).select(\\\"_corrupt_record\\\").show().\\n\" +\n+          \"Instead, you can cache or save the parsed results and then send the same query.\\n\" +\n+          \"For example, val df = spark.read.schema(schema).csv(file).cache() and then\\n\" +\n+          \"df.filter($\\\"_corrupt_record\\\".isNotNull).count().\"\n+      )\n+    }\n+\n+    val hadoopConf =\n+      sparkSession.sessionState.newHadoopConfWithOptions(optionsAsScala)"
  }],
  "prId": 24005
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "I know this is a copied one. But, shall we remove `s` from `s\"`?",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-22T05:59:23Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.csv.CSVOptions\n+import org.apache.spark.sql.catalyst.expressions.ExprUtils\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2.TextBasedFileScan\n+import org.apache.spark.sql.sources.v2.reader.PartitionReaderFactory\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+import org.apache.spark.util.SerializableConfiguration\n+\n+case class CSVScan(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    dataSchema: StructType,\n+    readSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends TextBasedFileScan(sparkSession, fileIndex, readSchema, options) {\n+\n+  private val optionsAsScala = options.asScala.toMap\n+  private lazy val parsedOptions: CSVOptions = new CSVOptions(\n+    optionsAsScala,\n+    columnPruning = sparkSession.sessionState.conf.csvColumnPruning,\n+    sparkSession.sessionState.conf.sessionLocalTimeZone,\n+    sparkSession.sessionState.conf.columnNameOfCorruptRecord)\n+\n+  override def isSplitable(path: Path): Boolean = {\n+    CSVDataSource(parsedOptions).isSplitable && super.isSplitable(path)\n+  }\n+\n+  override def createReaderFactory(): PartitionReaderFactory = {\n+    // Check a field requirement for corrupt records here to throw an exception in a driver side\n+    ExprUtils.verifyColumnNameOfCorruptRecord(dataSchema, parsedOptions.columnNameOfCorruptRecord)\n+\n+    if (readSchema.length == 1 &&\n+      readSchema.head.name == parsedOptions.columnNameOfCorruptRecord) {\n+      throw new AnalysisException(\n+        \"Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\\n\" +\n+          \"referenced columns only include the internal corrupt record column\\n\" +\n+          s\"(named _corrupt_record by default). For example:\\n\" +"
  }],
  "prId": 24005
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Since this is used once, let's not make `val`.",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-22T07:00:22Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.csv.CSVOptions\n+import org.apache.spark.sql.catalyst.expressions.ExprUtils\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2.TextBasedFileScan\n+import org.apache.spark.sql.sources.v2.reader.PartitionReaderFactory\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+import org.apache.spark.util.SerializableConfiguration\n+\n+case class CSVScan(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    dataSchema: StructType,\n+    readSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends TextBasedFileScan(sparkSession, fileIndex, readSchema, options) {\n+\n+  private val optionsAsScala = options.asScala.toMap"
  }],
  "prId": 24005
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```scala\r\n- optionsAsScala,\r\n+ options.asScala.toMap,\r\n```",
    "commit": "689f17e4930694d1e7585ba82e0964507121868a",
    "createdAt": "2019-03-22T07:00:43Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.v2.csv\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.csv.CSVOptions\n+import org.apache.spark.sql.catalyst.expressions.ExprUtils\n+import org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex\n+import org.apache.spark.sql.execution.datasources.csv.CSVDataSource\n+import org.apache.spark.sql.execution.datasources.v2.TextBasedFileScan\n+import org.apache.spark.sql.sources.v2.reader.PartitionReaderFactory\n+import org.apache.spark.sql.types.{DataType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+import org.apache.spark.util.SerializableConfiguration\n+\n+case class CSVScan(\n+    sparkSession: SparkSession,\n+    fileIndex: PartitioningAwareFileIndex,\n+    dataSchema: StructType,\n+    readSchema: StructType,\n+    options: CaseInsensitiveStringMap)\n+  extends TextBasedFileScan(sparkSession, fileIndex, readSchema, options) {\n+\n+  private val optionsAsScala = options.asScala.toMap\n+  private lazy val parsedOptions: CSVOptions = new CSVOptions(\n+    optionsAsScala,"
  }],
  "prId": 24005
}]