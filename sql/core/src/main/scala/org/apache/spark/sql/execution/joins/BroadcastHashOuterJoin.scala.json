[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "This is not required as its the default.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-07T00:55:57Z",
    "diffHunk": "@@ -1,122 +1,122 @@\n-/*\r\n- * Licensed to the Apache Software Foundation (ASF) under one or more\r\n- * contributor license agreements.  See the NOTICE file distributed with\r\n- * this work for additional information regarding copyright ownership.\r\n- * The ASF licenses this file to You under the Apache License, Version 2.0\r\n- * (the \"License\"); you may not use this file except in compliance with\r\n- * the License.  You may obtain a copy of the License at\r\n- *\r\n- *    http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-\r\n-package org.apache.spark.sql.execution.joins\r\n-\r\n-import scala.concurrent._\r\n-import scala.concurrent.duration._\r\n-\r\n-import org.apache.spark.annotation.DeveloperApi\r\n-import org.apache.spark.rdd.RDD\r\n-import org.apache.spark.sql.catalyst.InternalRow\r\n-import org.apache.spark.sql.catalyst.expressions._\r\n-import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\r\n-import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\r\n-import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\r\n-import org.apache.spark.{InternalAccumulator, TaskContext}\r\n-\r\n-/**\r\n- * :: DeveloperApi ::\r\n- * Performs a outer hash join for two child relations.  When the output RDD of this operator is\r\n- * being constructed, a Spark job is asynchronously started to calculate the values for the\r\n- * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\r\n- * relation is not shuffled.\r\n- */\r\n-@DeveloperApi\r\n-case class BroadcastHashOuterJoin(\r\n-    leftKeys: Seq[Expression],\r\n-    rightKeys: Seq[Expression],\r\n-    joinType: JoinType,\r\n-    condition: Option[Expression],\r\n-    left: SparkPlan,\r\n-    right: SparkPlan) extends BinaryNode with HashOuterJoin {\r\n-\r\n-  val timeout = {\r\n-    val timeoutValue = sqlContext.conf.broadcastTimeout\r\n-    if (timeoutValue < 0) {\r\n-      Duration.Inf\r\n-    } else {\r\n-      timeoutValue.seconds\r\n-    }\r\n-  }\r\n-\r\n-  override def requiredChildDistribution: Seq[Distribution] =\r\n-    UnspecifiedDistribution :: UnspecifiedDistribution :: Nil\r\n-\r\n-  override def outputPartitioning: Partitioning = streamedPlan.outputPartitioning\r\n-\r\n-  // Use lazy so that we won't do broadcast when calling explain but still cache the broadcast value\r\n-  // for the same query.\r\n-  @transient\r\n-  private lazy val broadcastFuture = {\r\n-    // broadcastFuture is used in \"doExecute\". Therefore we can get the execution id correctly here.\r\n-    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\r\n-    future {\r\n-      // This will run in another thread. Set the execution id so that we can connect these jobs\r\n-      // with the correct execution.\r\n-      SQLExecution.withExecutionId(sparkContext, executionId) {\r\n-        // Note that we use .execute().collect() because we don't want to convert data to Scala\r\n-        // types\r\n-        val input: Array[InternalRow] = buildPlan.execute().map(_.copy()).collect()\r\n-        val hashed = HashedRelation(input.iterator, buildKeyGenerator, input.size)\r\n-        sparkContext.broadcast(hashed)\r\n-      }\r\n-    }(BroadcastHashJoin.broadcastHashJoinExecutionContext)\r\n-  }\r\n-\r\n-  protected override def doPrepare(): Unit = {\r\n-    broadcastFuture\r\n-  }\r\n-\r\n-  override def doExecute(): RDD[InternalRow] = {\r\n-    val broadcastRelation = Await.result(broadcastFuture, timeout)\r\n-\r\n-    streamedPlan.execute().mapPartitions { streamedIter =>\r\n-      val joinedRow = new JoinedRow()\r\n-      val hashTable = broadcastRelation.value\r\n-      val keyGenerator = streamedKeyGenerator\r\n-\r\n-      hashTable match {\r\n-        case unsafe: UnsafeHashedRelation =>\r\n-          TaskContext.get().internalMetricsToAccumulators(\r\n-            InternalAccumulator.PEAK_EXECUTION_MEMORY).add(unsafe.getUnsafeSize)\r\n-        case _ =>\r\n-      }\r\n-\r\n-      val resultProj = resultProjection\r\n-      joinType match {\r\n-        case LeftOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withLeft(currentRow)\r\n-            leftOuterIterator(rowKey, joinedRow, hashTable.get(rowKey), resultProj)\r\n-          })\r\n-\r\n-        case RightOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withRight(currentRow)\r\n-            rightOuterIterator(rowKey, hashTable.get(rowKey), joinedRow, resultProj)\r\n-          })\r\n-\r\n-        case x =>\r\n-          throw new IllegalArgumentException(\r\n-            s\"BroadcastHashOuterJoin should not take $x as the JoinType\")\r\n-      }\r\n-    }\r\n-  }\r\n-}\r\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\n+import org.apache.spark.{InternalAccumulator, TaskContext}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs a outer hash join for two child relations.  When the output RDD of this operator is\n+ * being constructed, a Spark job is asynchronously started to calculate the values for the\n+ * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\n+ * relation is not shuffled.\n+ */\n+@DeveloperApi\n+case class BroadcastHashOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  val timeout = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] ="
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'm not sure why this entire file ended up being marked as changed in the diff. I think that the original file might have contained Windows line terminator characters, leading to this giant change.  This was a carryover from the original code, but I'm going to fix this in my other EnsureRequirements patch.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-07T00:57:38Z",
    "diffHunk": "@@ -1,122 +1,122 @@\n-/*\r\n- * Licensed to the Apache Software Foundation (ASF) under one or more\r\n- * contributor license agreements.  See the NOTICE file distributed with\r\n- * this work for additional information regarding copyright ownership.\r\n- * The ASF licenses this file to You under the Apache License, Version 2.0\r\n- * (the \"License\"); you may not use this file except in compliance with\r\n- * the License.  You may obtain a copy of the License at\r\n- *\r\n- *    http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-\r\n-package org.apache.spark.sql.execution.joins\r\n-\r\n-import scala.concurrent._\r\n-import scala.concurrent.duration._\r\n-\r\n-import org.apache.spark.annotation.DeveloperApi\r\n-import org.apache.spark.rdd.RDD\r\n-import org.apache.spark.sql.catalyst.InternalRow\r\n-import org.apache.spark.sql.catalyst.expressions._\r\n-import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\r\n-import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\r\n-import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\r\n-import org.apache.spark.{InternalAccumulator, TaskContext}\r\n-\r\n-/**\r\n- * :: DeveloperApi ::\r\n- * Performs a outer hash join for two child relations.  When the output RDD of this operator is\r\n- * being constructed, a Spark job is asynchronously started to calculate the values for the\r\n- * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\r\n- * relation is not shuffled.\r\n- */\r\n-@DeveloperApi\r\n-case class BroadcastHashOuterJoin(\r\n-    leftKeys: Seq[Expression],\r\n-    rightKeys: Seq[Expression],\r\n-    joinType: JoinType,\r\n-    condition: Option[Expression],\r\n-    left: SparkPlan,\r\n-    right: SparkPlan) extends BinaryNode with HashOuterJoin {\r\n-\r\n-  val timeout = {\r\n-    val timeoutValue = sqlContext.conf.broadcastTimeout\r\n-    if (timeoutValue < 0) {\r\n-      Duration.Inf\r\n-    } else {\r\n-      timeoutValue.seconds\r\n-    }\r\n-  }\r\n-\r\n-  override def requiredChildDistribution: Seq[Distribution] =\r\n-    UnspecifiedDistribution :: UnspecifiedDistribution :: Nil\r\n-\r\n-  override def outputPartitioning: Partitioning = streamedPlan.outputPartitioning\r\n-\r\n-  // Use lazy so that we won't do broadcast when calling explain but still cache the broadcast value\r\n-  // for the same query.\r\n-  @transient\r\n-  private lazy val broadcastFuture = {\r\n-    // broadcastFuture is used in \"doExecute\". Therefore we can get the execution id correctly here.\r\n-    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\r\n-    future {\r\n-      // This will run in another thread. Set the execution id so that we can connect these jobs\r\n-      // with the correct execution.\r\n-      SQLExecution.withExecutionId(sparkContext, executionId) {\r\n-        // Note that we use .execute().collect() because we don't want to convert data to Scala\r\n-        // types\r\n-        val input: Array[InternalRow] = buildPlan.execute().map(_.copy()).collect()\r\n-        val hashed = HashedRelation(input.iterator, buildKeyGenerator, input.size)\r\n-        sparkContext.broadcast(hashed)\r\n-      }\r\n-    }(BroadcastHashJoin.broadcastHashJoinExecutionContext)\r\n-  }\r\n-\r\n-  protected override def doPrepare(): Unit = {\r\n-    broadcastFuture\r\n-  }\r\n-\r\n-  override def doExecute(): RDD[InternalRow] = {\r\n-    val broadcastRelation = Await.result(broadcastFuture, timeout)\r\n-\r\n-    streamedPlan.execute().mapPartitions { streamedIter =>\r\n-      val joinedRow = new JoinedRow()\r\n-      val hashTable = broadcastRelation.value\r\n-      val keyGenerator = streamedKeyGenerator\r\n-\r\n-      hashTable match {\r\n-        case unsafe: UnsafeHashedRelation =>\r\n-          TaskContext.get().internalMetricsToAccumulators(\r\n-            InternalAccumulator.PEAK_EXECUTION_MEMORY).add(unsafe.getUnsafeSize)\r\n-        case _ =>\r\n-      }\r\n-\r\n-      val resultProj = resultProjection\r\n-      joinType match {\r\n-        case LeftOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withLeft(currentRow)\r\n-            leftOuterIterator(rowKey, joinedRow, hashTable.get(rowKey), resultProj)\r\n-          })\r\n-\r\n-        case RightOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withRight(currentRow)\r\n-            rightOuterIterator(rowKey, hashTable.get(rowKey), joinedRow, resultProj)\r\n-          })\r\n-\r\n-        case x =>\r\n-          throw new IllegalArgumentException(\r\n-            s\"BroadcastHashOuterJoin should not take $x as the JoinType\")\r\n-      }\r\n-    }\r\n-  }\r\n-}\r\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\n+import org.apache.spark.{InternalAccumulator, TaskContext}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs a outer hash join for two child relations.  When the output RDD of this operator is\n+ * being constructed, a Spark job is asynchronously started to calculate the values for the\n+ * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\n+ * relation is not shuffled.\n+ */\n+@DeveloperApi\n+case class BroadcastHashOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  val timeout = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] ="
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'll just remove this here, since my EnsureRequirements patch is getting added later.\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-07T02:25:53Z",
    "diffHunk": "@@ -1,122 +1,122 @@\n-/*\r\n- * Licensed to the Apache Software Foundation (ASF) under one or more\r\n- * contributor license agreements.  See the NOTICE file distributed with\r\n- * this work for additional information regarding copyright ownership.\r\n- * The ASF licenses this file to You under the Apache License, Version 2.0\r\n- * (the \"License\"); you may not use this file except in compliance with\r\n- * the License.  You may obtain a copy of the License at\r\n- *\r\n- *    http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-\r\n-package org.apache.spark.sql.execution.joins\r\n-\r\n-import scala.concurrent._\r\n-import scala.concurrent.duration._\r\n-\r\n-import org.apache.spark.annotation.DeveloperApi\r\n-import org.apache.spark.rdd.RDD\r\n-import org.apache.spark.sql.catalyst.InternalRow\r\n-import org.apache.spark.sql.catalyst.expressions._\r\n-import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\r\n-import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\r\n-import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\r\n-import org.apache.spark.{InternalAccumulator, TaskContext}\r\n-\r\n-/**\r\n- * :: DeveloperApi ::\r\n- * Performs a outer hash join for two child relations.  When the output RDD of this operator is\r\n- * being constructed, a Spark job is asynchronously started to calculate the values for the\r\n- * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\r\n- * relation is not shuffled.\r\n- */\r\n-@DeveloperApi\r\n-case class BroadcastHashOuterJoin(\r\n-    leftKeys: Seq[Expression],\r\n-    rightKeys: Seq[Expression],\r\n-    joinType: JoinType,\r\n-    condition: Option[Expression],\r\n-    left: SparkPlan,\r\n-    right: SparkPlan) extends BinaryNode with HashOuterJoin {\r\n-\r\n-  val timeout = {\r\n-    val timeoutValue = sqlContext.conf.broadcastTimeout\r\n-    if (timeoutValue < 0) {\r\n-      Duration.Inf\r\n-    } else {\r\n-      timeoutValue.seconds\r\n-    }\r\n-  }\r\n-\r\n-  override def requiredChildDistribution: Seq[Distribution] =\r\n-    UnspecifiedDistribution :: UnspecifiedDistribution :: Nil\r\n-\r\n-  override def outputPartitioning: Partitioning = streamedPlan.outputPartitioning\r\n-\r\n-  // Use lazy so that we won't do broadcast when calling explain but still cache the broadcast value\r\n-  // for the same query.\r\n-  @transient\r\n-  private lazy val broadcastFuture = {\r\n-    // broadcastFuture is used in \"doExecute\". Therefore we can get the execution id correctly here.\r\n-    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\r\n-    future {\r\n-      // This will run in another thread. Set the execution id so that we can connect these jobs\r\n-      // with the correct execution.\r\n-      SQLExecution.withExecutionId(sparkContext, executionId) {\r\n-        // Note that we use .execute().collect() because we don't want to convert data to Scala\r\n-        // types\r\n-        val input: Array[InternalRow] = buildPlan.execute().map(_.copy()).collect()\r\n-        val hashed = HashedRelation(input.iterator, buildKeyGenerator, input.size)\r\n-        sparkContext.broadcast(hashed)\r\n-      }\r\n-    }(BroadcastHashJoin.broadcastHashJoinExecutionContext)\r\n-  }\r\n-\r\n-  protected override def doPrepare(): Unit = {\r\n-    broadcastFuture\r\n-  }\r\n-\r\n-  override def doExecute(): RDD[InternalRow] = {\r\n-    val broadcastRelation = Await.result(broadcastFuture, timeout)\r\n-\r\n-    streamedPlan.execute().mapPartitions { streamedIter =>\r\n-      val joinedRow = new JoinedRow()\r\n-      val hashTable = broadcastRelation.value\r\n-      val keyGenerator = streamedKeyGenerator\r\n-\r\n-      hashTable match {\r\n-        case unsafe: UnsafeHashedRelation =>\r\n-          TaskContext.get().internalMetricsToAccumulators(\r\n-            InternalAccumulator.PEAK_EXECUTION_MEMORY).add(unsafe.getUnsafeSize)\r\n-        case _ =>\r\n-      }\r\n-\r\n-      val resultProj = resultProjection\r\n-      joinType match {\r\n-        case LeftOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withLeft(currentRow)\r\n-            leftOuterIterator(rowKey, joinedRow, hashTable.get(rowKey), resultProj)\r\n-          })\r\n-\r\n-        case RightOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withRight(currentRow)\r\n-            rightOuterIterator(rowKey, hashTable.get(rowKey), joinedRow, resultProj)\r\n-          })\r\n-\r\n-        case x =>\r\n-          throw new IllegalArgumentException(\r\n-            s\"BroadcastHashOuterJoin should not take $x as the JoinType\")\r\n-      }\r\n-    }\r\n-  }\r\n-}\r\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\n+import org.apache.spark.{InternalAccumulator, TaskContext}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs a outer hash join for two child relations.  When the output RDD of this operator is\n+ * being constructed, a Spark job is asynchronously started to calculate the values for the\n+ * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\n+ * relation is not shuffled.\n+ */\n+@DeveloperApi\n+case class BroadcastHashOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  val timeout = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  override def requiredChildDistribution: Seq[Distribution] ="
  }],
  "prId": 7904
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "while you are at it -> override protected\n",
    "commit": "eabacca9864e609a1b085a2acbe10907929700a4",
    "createdAt": "2015-08-07T06:58:07Z",
    "diffHunk": "@@ -1,122 +1,119 @@\n-/*\r\n- * Licensed to the Apache Software Foundation (ASF) under one or more\r\n- * contributor license agreements.  See the NOTICE file distributed with\r\n- * this work for additional information regarding copyright ownership.\r\n- * The ASF licenses this file to You under the Apache License, Version 2.0\r\n- * (the \"License\"); you may not use this file except in compliance with\r\n- * the License.  You may obtain a copy of the License at\r\n- *\r\n- *    http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-\r\n-package org.apache.spark.sql.execution.joins\r\n-\r\n-import scala.concurrent._\r\n-import scala.concurrent.duration._\r\n-\r\n-import org.apache.spark.annotation.DeveloperApi\r\n-import org.apache.spark.rdd.RDD\r\n-import org.apache.spark.sql.catalyst.InternalRow\r\n-import org.apache.spark.sql.catalyst.expressions._\r\n-import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\r\n-import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\r\n-import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\r\n-import org.apache.spark.{InternalAccumulator, TaskContext}\r\n-\r\n-/**\r\n- * :: DeveloperApi ::\r\n- * Performs a outer hash join for two child relations.  When the output RDD of this operator is\r\n- * being constructed, a Spark job is asynchronously started to calculate the values for the\r\n- * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\r\n- * relation is not shuffled.\r\n- */\r\n-@DeveloperApi\r\n-case class BroadcastHashOuterJoin(\r\n-    leftKeys: Seq[Expression],\r\n-    rightKeys: Seq[Expression],\r\n-    joinType: JoinType,\r\n-    condition: Option[Expression],\r\n-    left: SparkPlan,\r\n-    right: SparkPlan) extends BinaryNode with HashOuterJoin {\r\n-\r\n-  val timeout = {\r\n-    val timeoutValue = sqlContext.conf.broadcastTimeout\r\n-    if (timeoutValue < 0) {\r\n-      Duration.Inf\r\n-    } else {\r\n-      timeoutValue.seconds\r\n-    }\r\n-  }\r\n-\r\n-  override def requiredChildDistribution: Seq[Distribution] =\r\n-    UnspecifiedDistribution :: UnspecifiedDistribution :: Nil\r\n-\r\n-  override def outputPartitioning: Partitioning = streamedPlan.outputPartitioning\r\n-\r\n-  // Use lazy so that we won't do broadcast when calling explain but still cache the broadcast value\r\n-  // for the same query.\r\n-  @transient\r\n-  private lazy val broadcastFuture = {\r\n-    // broadcastFuture is used in \"doExecute\". Therefore we can get the execution id correctly here.\r\n-    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\r\n-    future {\r\n-      // This will run in another thread. Set the execution id so that we can connect these jobs\r\n-      // with the correct execution.\r\n-      SQLExecution.withExecutionId(sparkContext, executionId) {\r\n-        // Note that we use .execute().collect() because we don't want to convert data to Scala\r\n-        // types\r\n-        val input: Array[InternalRow] = buildPlan.execute().map(_.copy()).collect()\r\n-        val hashed = HashedRelation(input.iterator, buildKeyGenerator, input.size)\r\n-        sparkContext.broadcast(hashed)\r\n-      }\r\n-    }(BroadcastHashJoin.broadcastHashJoinExecutionContext)\r\n-  }\r\n-\r\n-  protected override def doPrepare(): Unit = {\r\n-    broadcastFuture\r\n-  }\r\n-\r\n-  override def doExecute(): RDD[InternalRow] = {\r\n-    val broadcastRelation = Await.result(broadcastFuture, timeout)\r\n-\r\n-    streamedPlan.execute().mapPartitions { streamedIter =>\r\n-      val joinedRow = new JoinedRow()\r\n-      val hashTable = broadcastRelation.value\r\n-      val keyGenerator = streamedKeyGenerator\r\n-\r\n-      hashTable match {\r\n-        case unsafe: UnsafeHashedRelation =>\r\n-          TaskContext.get().internalMetricsToAccumulators(\r\n-            InternalAccumulator.PEAK_EXECUTION_MEMORY).add(unsafe.getUnsafeSize)\r\n-        case _ =>\r\n-      }\r\n-\r\n-      val resultProj = resultProjection\r\n-      joinType match {\r\n-        case LeftOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withLeft(currentRow)\r\n-            leftOuterIterator(rowKey, joinedRow, hashTable.get(rowKey), resultProj)\r\n-          })\r\n-\r\n-        case RightOuter =>\r\n-          streamedIter.flatMap(currentRow => {\r\n-            val rowKey = keyGenerator(currentRow)\r\n-            joinedRow.withRight(currentRow)\r\n-            rightOuterIterator(rowKey, hashTable.get(rowKey), joinedRow, resultProj)\r\n-          })\r\n-\r\n-        case x =>\r\n-          throw new IllegalArgumentException(\r\n-            s\"BroadcastHashOuterJoin should not take $x as the JoinType\")\r\n-      }\r\n-    }\r\n-  }\r\n-}\r\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.joins\n+\n+import scala.concurrent._\n+import scala.concurrent.duration._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.physical.{Distribution, Partitioning, UnspecifiedDistribution}\n+import org.apache.spark.sql.catalyst.plans.{JoinType, LeftOuter, RightOuter}\n+import org.apache.spark.sql.execution.{BinaryNode, SQLExecution, SparkPlan}\n+import org.apache.spark.{InternalAccumulator, TaskContext}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Performs a outer hash join for two child relations.  When the output RDD of this operator is\n+ * being constructed, a Spark job is asynchronously started to calculate the values for the\n+ * broadcasted relation.  This data is then placed in a Spark broadcast variable.  The streamed\n+ * relation is not shuffled.\n+ */\n+@DeveloperApi\n+case class BroadcastHashOuterJoin(\n+    leftKeys: Seq[Expression],\n+    rightKeys: Seq[Expression],\n+    joinType: JoinType,\n+    condition: Option[Expression],\n+    left: SparkPlan,\n+    right: SparkPlan) extends BinaryNode with OuterJoin {\n+\n+  val timeout = {\n+    val timeoutValue = sqlContext.conf.broadcastTimeout\n+    if (timeoutValue < 0) {\n+      Duration.Inf\n+    } else {\n+      timeoutValue.seconds\n+    }\n+  }\n+\n+  override def outputPartitioning: Partitioning = streamedPlan.outputPartitioning\n+\n+  // Use lazy so that we won't do broadcast when calling explain but still cache the broadcast value\n+  // for the same query.\n+  @transient\n+  private lazy val broadcastFuture = {\n+    // broadcastFuture is used in \"doExecute\". Therefore we can get the execution id correctly here.\n+    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)\n+    future {\n+      // This will run in another thread. Set the execution id so that we can connect these jobs\n+      // with the correct execution.\n+      SQLExecution.withExecutionId(sparkContext, executionId) {\n+        // Note that we use .execute().collect() because we don't want to convert data to Scala\n+        // types\n+        val input: Array[InternalRow] = buildPlan.execute().map(_.copy()).collect()\n+        val hashed = HashedRelation(input.iterator, buildKeyGenerator, input.size)\n+        sparkContext.broadcast(hashed)\n+      }\n+    }(BroadcastHashJoin.broadcastHashJoinExecutionContext)\n+  }\n+\n+  protected override def doPrepare(): Unit = {"
  }],
  "prId": 7904
}]