[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "here is my thoughts about the optimizable cases:\n\nFirst of all, only parition colums are required(which means we need to traverse down the plan tree and find table relation here)\n1. aggregate expression is partition columns, e.g. `SELECT col FROM tbl GROUP BY col`\n2. aggregate function on partition columns with DISTINCT, e.g. `SELECT count(DISTINCT a) FROM tbl GROUP BY b`\n3. aggregate function on partition columns which have same result with or without DISTINCT keyword, e.g. `SELECT sum(a) FROM tbl GROUP BY b`\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-27T08:03:00Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min).\n+ * Example: select Max(partition) from table.\n+ */\n+case class MetadataOnlyOptimizer(\n+    sparkSession: SparkSession,\n+    catalog: SessionCatalog) extends Rule[LogicalPlan] {\n+\n+  private def canSupportMetadataOnly(a: Aggregate): Boolean = {"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "instead of collecting project list and filter conditions, produce a `LogicalRDD` directly, can we just replace the table relation with `LogicalRDD` and still keep these `Filter` and `Project` operators?\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-27T08:06:50Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min).\n+ * Example: select Max(partition) from table.\n+ */\n+case class MetadataOnlyOptimizer(\n+    sparkSession: SparkSession,\n+    catalog: SessionCatalog) extends Rule[LogicalPlan] {\n+\n+  private def canSupportMetadataOnly(a: Aggregate): Boolean = {\n+    val aggregateExpressions = a.aggregateExpressions.flatMap { expr =>\n+      expr.collect {\n+        case agg: AggregateExpression => agg\n+      }\n+    }.distinct\n+    if (aggregateExpressions.isEmpty) {\n+      // Cannot support for aggregate that has no aggregateFunction.\n+      // example: select col1 from table group by col1.\n+      false\n+    } else {\n+      aggregateExpressions.forall { agg =>\n+        if (agg.isDistinct) {\n+          true\n+        } else {\n+          // If function can be evaluated on just the distinct values of a column, it can be used\n+          // by metadata-only optimizer.\n+          agg.aggregateFunction match {\n+            case max: Max => true\n+            case min: Min => true\n+            case hyperLog: HyperLogLogPlusPlus => true\n+            case _ => false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def collectAliases(fields: Seq[Expression]): Map[ExprId, Expression] = fields.collect {\n+    case a @ Alias(child, _) => a.toAttribute.exprId -> child\n+  }.toMap\n+\n+  private def substitute(aliases: Map[ExprId, Expression])(expr: Expression): Expression = {\n+    expr.transform {\n+      case a @ Alias(ref: AttributeReference, name) =>\n+        aliases.get(ref.exprId)\n+          .map(Alias(_, name)(a.exprId, a.qualifier, isGenerated = a.isGenerated))\n+          .getOrElse(a)\n+\n+      case a: AttributeReference =>\n+        aliases.get(a.exprId)\n+          .map(Alias(_, a.name)(a.exprId, a.qualifier, isGenerated = a.isGenerated)).getOrElse(a)\n+    }\n+  }\n+\n+  private def findRelation(plan: LogicalPlan)\n+      : (Option[LogicalPlan], Seq[NamedExpression], Seq[Expression], Map[ExprId, Expression]) = {\n+    plan match {\n+      case relation @ LogicalRelation(files: HadoopFsRelation, _, table)\n+        if files.partitionSchema.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case relation: CatalogRelation if relation.catalogTable.partitionColumnNames.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case p @ Project(fields, child) if fields.forall(_.deterministic) =>\n+        val (plan, _, filters, aliases) = findRelation(child)\n+        val substitutedFields = fields.map(substitute(aliases)).asInstanceOf[Seq[NamedExpression]]\n+        (plan, substitutedFields, filters, collectAliases(substitutedFields))\n+\n+      case f @ Filter(condition, child) if condition.deterministic =>\n+        val (plan, fields, filters, aliases) = findRelation(child)\n+        val substitutedCondition = substitute(aliases)(condition)\n+        (plan, fields, filters ++ Seq(substitutedCondition), aliases)\n+\n+      case _ => (None, Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+    }\n+  }\n+\n+  private def convertToMetadataOnlyPlan(\n+      parent: LogicalPlan,\n+      projectList: Seq[NamedExpression],\n+      filters: Seq[Expression],\n+      relation: LogicalPlan): LogicalPlan = relation match {\n+    case l @ LogicalRelation(files: HadoopFsRelation, _, _) =>\n+      val attributeMap = l.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = files.partitionSchema.map { field =>\n+        attributeMap.getOrElse(field.name, throw new AnalysisException(\n+          s\"Unable to resolve ${field.name} given [${l.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val selectedPartitions = files.location.listFiles(filters)\n+        val partitionValues = selectedPartitions.map(_.values)\n+        val valuesRdd = sparkSession.sparkContext.parallelize(partitionValues, 1)\n+        val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+        if (projectList.nonEmpty) {\n+          parent.withNewChildren(Project(projectList, valuesPlan) :: Nil)\n+        } else {\n+          parent.withNewChildren(valuesPlan :: Nil)\n+        }\n+      } else {\n+        parent\n+      }\n+\n+    case relation: CatalogRelation =>\n+      val attributeMap = relation.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = relation.catalogTable.partitionColumnNames.map { column =>\n+        attributeMap.getOrElse(column, throw new AnalysisException(\n+          s\"Unable to resolve ${column} given [${relation.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val partitionColumnDataTypes = partitionColumns.map(_.dataType)\n+        val partitionValues = catalog.getPartitionsByFilter(relation.catalogTable, filters)\n+          .map { p =>\n+            InternalRow.fromSeq(\n+              partitionColumns.map(a => p.spec(a.name)).zip(partitionColumnDataTypes).map {\n+                case (rawValue, dataType) => Cast(Literal(rawValue), dataType).eval(null)\n+              })\n+          }\n+        val valuesRdd = sparkSession.sparkContext.parallelize(partitionValues, 1)\n+        val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+        val filterPlan =\n+          filters.reduceLeftOption(And).map(Filter(_, valuesPlan)).getOrElse(valuesPlan)\n+        if (projectList.nonEmpty) {\n+          parent.withNewChildren(Project(projectList, filterPlan) :: Nil)\n+        } else {\n+          parent.withNewChildren(filterPlan :: Nil)\n+        }\n+      } else {\n+        parent\n+      }\n+\n+    case _ =>\n+      parent\n+  }\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!sparkSession.sessionState.conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+    plan.transform {\n+      case a @ Aggregate(_, _, child) if canSupportMetadataOnly(a) =>\n+        val (plan, projectList, filters, _) = findRelation(child)\n+        if (plan.isDefined) {\n+          convertToMetadataOnlyPlan(a, projectList, filters, plan.get)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "~~for the sake of simplicity, how about we remove this optimization and always apply filters after getting the partition values? We can implement it in follow-ups.~~\n\nSorry I replied to the wrong place...\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-27T08:12:06Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min).\n+ * Example: select Max(partition) from table.\n+ */\n+case class MetadataOnlyOptimizer(\n+    sparkSession: SparkSession,\n+    catalog: SessionCatalog) extends Rule[LogicalPlan] {\n+\n+  private def canSupportMetadataOnly(a: Aggregate): Boolean = {\n+    val aggregateExpressions = a.aggregateExpressions.flatMap { expr =>\n+      expr.collect {\n+        case agg: AggregateExpression => agg\n+      }\n+    }.distinct\n+    if (aggregateExpressions.isEmpty) {\n+      // Cannot support for aggregate that has no aggregateFunction.\n+      // example: select col1 from table group by col1.\n+      false\n+    } else {\n+      aggregateExpressions.forall { agg =>\n+        if (agg.isDistinct) {\n+          true\n+        } else {\n+          // If function can be evaluated on just the distinct values of a column, it can be used\n+          // by metadata-only optimizer.\n+          agg.aggregateFunction match {\n+            case max: Max => true\n+            case min: Min => true\n+            case hyperLog: HyperLogLogPlusPlus => true\n+            case _ => false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def collectAliases(fields: Seq[Expression]): Map[ExprId, Expression] = fields.collect {\n+    case a @ Alias(child, _) => a.toAttribute.exprId -> child\n+  }.toMap\n+\n+  private def substitute(aliases: Map[ExprId, Expression])(expr: Expression): Expression = {\n+    expr.transform {\n+      case a @ Alias(ref: AttributeReference, name) =>\n+        aliases.get(ref.exprId)\n+          .map(Alias(_, name)(a.exprId, a.qualifier, isGenerated = a.isGenerated))\n+          .getOrElse(a)\n+\n+      case a: AttributeReference =>\n+        aliases.get(a.exprId)\n+          .map(Alias(_, a.name)(a.exprId, a.qualifier, isGenerated = a.isGenerated)).getOrElse(a)\n+    }\n+  }\n+\n+  private def findRelation(plan: LogicalPlan)\n+      : (Option[LogicalPlan], Seq[NamedExpression], Seq[Expression], Map[ExprId, Expression]) = {\n+    plan match {\n+      case relation @ LogicalRelation(files: HadoopFsRelation, _, table)\n+        if files.partitionSchema.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case relation: CatalogRelation if relation.catalogTable.partitionColumnNames.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case p @ Project(fields, child) if fields.forall(_.deterministic) =>\n+        val (plan, _, filters, aliases) = findRelation(child)\n+        val substitutedFields = fields.map(substitute(aliases)).asInstanceOf[Seq[NamedExpression]]\n+        (plan, substitutedFields, filters, collectAliases(substitutedFields))\n+\n+      case f @ Filter(condition, child) if condition.deterministic =>\n+        val (plan, fields, filters, aliases) = findRelation(child)\n+        val substitutedCondition = substitute(aliases)(condition)\n+        (plan, fields, filters ++ Seq(substitutedCondition), aliases)\n+\n+      case _ => (None, Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+    }\n+  }\n+\n+  private def convertToMetadataOnlyPlan(\n+      parent: LogicalPlan,\n+      projectList: Seq[NamedExpression],\n+      filters: Seq[Expression],\n+      relation: LogicalPlan): LogicalPlan = relation match {\n+    case l @ LogicalRelation(files: HadoopFsRelation, _, _) =>\n+      val attributeMap = l.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = files.partitionSchema.map { field =>\n+        attributeMap.getOrElse(field.name, throw new AnalysisException(\n+          s\"Unable to resolve ${field.name} given [${l.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val selectedPartitions = files.location.listFiles(filters)\n+        val partitionValues = selectedPartitions.map(_.values)\n+        val valuesRdd = sparkSession.sparkContext.parallelize(partitionValues, 1)\n+        val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+        if (projectList.nonEmpty) {\n+          parent.withNewChildren(Project(projectList, valuesPlan) :: Nil)\n+        } else {\n+          parent.withNewChildren(valuesPlan :: Nil)\n+        }\n+      } else {\n+        parent\n+      }\n+\n+    case relation: CatalogRelation =>\n+      val attributeMap = relation.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = relation.catalogTable.partitionColumnNames.map { column =>\n+        attributeMap.getOrElse(column, throw new AnalysisException(\n+          s\"Unable to resolve ${column} given [${relation.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val partitionColumnDataTypes = partitionColumns.map(_.dataType)\n+        val partitionValues = catalog.getPartitionsByFilter(relation.catalogTable, filters)\n+          .map { p =>\n+            InternalRow.fromSeq(\n+              partitionColumns.map(a => p.spec(a.name)).zip(partitionColumnDataTypes).map {\n+                case (rawValue, dataType) => Cast(Literal(rawValue), dataType).eval(null)\n+              })\n+          }\n+        val valuesRdd = sparkSession.sparkContext.parallelize(partitionValues, 1)\n+        val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+        val filterPlan =\n+          filters.reduceLeftOption(And).map(Filter(_, valuesPlan)).getOrElse(valuesPlan)\n+        if (projectList.nonEmpty) {\n+          parent.withNewChildren(Project(projectList, filterPlan) :: Nil)\n+        } else {\n+          parent.withNewChildren(filterPlan :: Nil)\n+        }\n+      } else {\n+        parent\n+      }\n+\n+    case _ =>\n+      parent\n+  }\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!sparkSession.sessionState.conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+    plan.transform {\n+      case a @ Aggregate(_, _, child) if canSupportMetadataOnly(a) =>\n+        val (plan, projectList, filters, _) = findRelation(child)\n+        if (plan.isDefined) {\n+          convertToMetadataOnlyPlan(a, projectList, filters, plan.get)"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "the `filters` are not guaranteed to be applied, see https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala#L283-L292\n\nWe still need to apply the filters\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-27T08:10:53Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min).\n+ * Example: select Max(partition) from table.\n+ */\n+case class MetadataOnlyOptimizer(\n+    sparkSession: SparkSession,\n+    catalog: SessionCatalog) extends Rule[LogicalPlan] {\n+\n+  private def canSupportMetadataOnly(a: Aggregate): Boolean = {\n+    val aggregateExpressions = a.aggregateExpressions.flatMap { expr =>\n+      expr.collect {\n+        case agg: AggregateExpression => agg\n+      }\n+    }.distinct\n+    if (aggregateExpressions.isEmpty) {\n+      // Cannot support for aggregate that has no aggregateFunction.\n+      // example: select col1 from table group by col1.\n+      false\n+    } else {\n+      aggregateExpressions.forall { agg =>\n+        if (agg.isDistinct) {\n+          true\n+        } else {\n+          // If function can be evaluated on just the distinct values of a column, it can be used\n+          // by metadata-only optimizer.\n+          agg.aggregateFunction match {\n+            case max: Max => true\n+            case min: Min => true\n+            case hyperLog: HyperLogLogPlusPlus => true\n+            case _ => false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def collectAliases(fields: Seq[Expression]): Map[ExprId, Expression] = fields.collect {\n+    case a @ Alias(child, _) => a.toAttribute.exprId -> child\n+  }.toMap\n+\n+  private def substitute(aliases: Map[ExprId, Expression])(expr: Expression): Expression = {\n+    expr.transform {\n+      case a @ Alias(ref: AttributeReference, name) =>\n+        aliases.get(ref.exprId)\n+          .map(Alias(_, name)(a.exprId, a.qualifier, isGenerated = a.isGenerated))\n+          .getOrElse(a)\n+\n+      case a: AttributeReference =>\n+        aliases.get(a.exprId)\n+          .map(Alias(_, a.name)(a.exprId, a.qualifier, isGenerated = a.isGenerated)).getOrElse(a)\n+    }\n+  }\n+\n+  private def findRelation(plan: LogicalPlan)\n+      : (Option[LogicalPlan], Seq[NamedExpression], Seq[Expression], Map[ExprId, Expression]) = {\n+    plan match {\n+      case relation @ LogicalRelation(files: HadoopFsRelation, _, table)\n+        if files.partitionSchema.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case relation: CatalogRelation if relation.catalogTable.partitionColumnNames.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case p @ Project(fields, child) if fields.forall(_.deterministic) =>\n+        val (plan, _, filters, aliases) = findRelation(child)\n+        val substitutedFields = fields.map(substitute(aliases)).asInstanceOf[Seq[NamedExpression]]\n+        (plan, substitutedFields, filters, collectAliases(substitutedFields))\n+\n+      case f @ Filter(condition, child) if condition.deterministic =>\n+        val (plan, fields, filters, aliases) = findRelation(child)\n+        val substitutedCondition = substitute(aliases)(condition)\n+        (plan, fields, filters ++ Seq(substitutedCondition), aliases)\n+\n+      case _ => (None, Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+    }\n+  }\n+\n+  private def convertToMetadataOnlyPlan(\n+      parent: LogicalPlan,\n+      projectList: Seq[NamedExpression],\n+      filters: Seq[Expression],\n+      relation: LogicalPlan): LogicalPlan = relation match {\n+    case l @ LogicalRelation(files: HadoopFsRelation, _, _) =>\n+      val attributeMap = l.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = files.partitionSchema.map { field =>\n+        attributeMap.getOrElse(field.name, throw new AnalysisException(\n+          s\"Unable to resolve ${field.name} given [${l.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val selectedPartitions = files.location.listFiles(filters)\n+        val partitionValues = selectedPartitions.map(_.values)\n+        val valuesRdd = sparkSession.sparkContext.parallelize(partitionValues, 1)\n+        val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+        if (projectList.nonEmpty) {\n+          parent.withNewChildren(Project(projectList, valuesPlan) :: Nil)\n+        } else {\n+          parent.withNewChildren(valuesPlan :: Nil)\n+        }\n+      } else {\n+        parent\n+      }\n+\n+    case relation: CatalogRelation =>\n+      val attributeMap = relation.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = relation.catalogTable.partitionColumnNames.map { column =>\n+        attributeMap.getOrElse(column, throw new AnalysisException(\n+          s\"Unable to resolve ${column} given [${relation.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val partitionColumnDataTypes = partitionColumns.map(_.dataType)\n+        val partitionValues = catalog.getPartitionsByFilter(relation.catalogTable, filters)"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "for the sake of simplicity, how about we remove this optimization and always apply filters after getting the partition values? We can implement it in follow-ups.\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-27T13:56:30Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min).\n+ * Example: select Max(partition) from table.\n+ */\n+case class MetadataOnlyOptimizer(\n+    sparkSession: SparkSession,\n+    catalog: SessionCatalog) extends Rule[LogicalPlan] {\n+\n+  private def canSupportMetadataOnly(a: Aggregate): Boolean = {\n+    val aggregateExpressions = a.aggregateExpressions.flatMap { expr =>\n+      expr.collect {\n+        case agg: AggregateExpression => agg\n+      }\n+    }.distinct\n+    if (aggregateExpressions.isEmpty) {\n+      // Cannot support for aggregate that has no aggregateFunction.\n+      // example: select col1 from table group by col1.\n+      false\n+    } else {\n+      aggregateExpressions.forall { agg =>\n+        if (agg.isDistinct) {\n+          true\n+        } else {\n+          // If function can be evaluated on just the distinct values of a column, it can be used\n+          // by metadata-only optimizer.\n+          agg.aggregateFunction match {\n+            case max: Max => true\n+            case min: Min => true\n+            case hyperLog: HyperLogLogPlusPlus => true\n+            case _ => false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def collectAliases(fields: Seq[Expression]): Map[ExprId, Expression] = fields.collect {\n+    case a @ Alias(child, _) => a.toAttribute.exprId -> child\n+  }.toMap\n+\n+  private def substitute(aliases: Map[ExprId, Expression])(expr: Expression): Expression = {\n+    expr.transform {\n+      case a @ Alias(ref: AttributeReference, name) =>\n+        aliases.get(ref.exprId)\n+          .map(Alias(_, name)(a.exprId, a.qualifier, isGenerated = a.isGenerated))\n+          .getOrElse(a)\n+\n+      case a: AttributeReference =>\n+        aliases.get(a.exprId)\n+          .map(Alias(_, a.name)(a.exprId, a.qualifier, isGenerated = a.isGenerated)).getOrElse(a)\n+    }\n+  }\n+\n+  private def findRelation(plan: LogicalPlan)\n+      : (Option[LogicalPlan], Seq[NamedExpression], Seq[Expression], Map[ExprId, Expression]) = {\n+    plan match {\n+      case relation @ LogicalRelation(files: HadoopFsRelation, _, table)\n+        if files.partitionSchema.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case relation: CatalogRelation if relation.catalogTable.partitionColumnNames.nonEmpty =>\n+        (Some(relation), Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+\n+      case p @ Project(fields, child) if fields.forall(_.deterministic) =>\n+        val (plan, _, filters, aliases) = findRelation(child)\n+        val substitutedFields = fields.map(substitute(aliases)).asInstanceOf[Seq[NamedExpression]]\n+        (plan, substitutedFields, filters, collectAliases(substitutedFields))\n+\n+      case f @ Filter(condition, child) if condition.deterministic =>\n+        val (plan, fields, filters, aliases) = findRelation(child)\n+        val substitutedCondition = substitute(aliases)(condition)\n+        (plan, fields, filters ++ Seq(substitutedCondition), aliases)\n+\n+      case _ => (None, Seq.empty[NamedExpression], Seq.empty[Expression], Map.empty)\n+    }\n+  }\n+\n+  private def convertToMetadataOnlyPlan(\n+      parent: LogicalPlan,\n+      projectList: Seq[NamedExpression],\n+      filters: Seq[Expression],\n+      relation: LogicalPlan): LogicalPlan = relation match {\n+    case l @ LogicalRelation(files: HadoopFsRelation, _, _) =>\n+      val attributeMap = l.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = files.partitionSchema.map { field =>\n+        attributeMap.getOrElse(field.name, throw new AnalysisException(\n+          s\"Unable to resolve ${field.name} given [${l.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val selectedPartitions = files.location.listFiles(filters)\n+        val partitionValues = selectedPartitions.map(_.values)\n+        val valuesRdd = sparkSession.sparkContext.parallelize(partitionValues, 1)\n+        val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+        if (projectList.nonEmpty) {\n+          parent.withNewChildren(Project(projectList, valuesPlan) :: Nil)\n+        } else {\n+          parent.withNewChildren(valuesPlan :: Nil)\n+        }\n+      } else {\n+        parent\n+      }\n+\n+    case relation: CatalogRelation =>\n+      val attributeMap = relation.output.map(attr => (attr.name, attr)).toMap\n+      val partitionColumns = relation.catalogTable.partitionColumnNames.map { column =>\n+        attributeMap.getOrElse(column, throw new AnalysisException(\n+          s\"Unable to resolve ${column} given [${relation.output.map(_.name).mkString(\", \")}]\"))\n+      }\n+      val filterColumns = filters.flatMap(_.references)\n+      val projectSet = parent.references ++ AttributeSet(filterColumns)\n+      if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+        val partitionColumnDataTypes = partitionColumns.map(_.dataType)\n+        val partitionValues = catalog.getPartitionsByFilter(relation.catalogTable, filters)"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about\n\n```\nIt's used for operators that only need distinct values. Currently only [[Aggregate]] operator\nwhich satisfy the following conditions are supported:\n1. .....\n2. .....\n```\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-29T03:48:56Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min)."
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "where do we check it only requires partition columns?\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-29T03:54:45Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min).\n+ * First of all, scanning only partition columns are required, then the rule does the following\n+ * things here:\n+ * 1. aggregate expression is partition columns,\n+ *  e.g. SELECT col FROM tbl GROUP BY col or SELECT col FROM tbl GROUP BY cube(col).\n+ * 2. aggregate function on partition columns with DISTINCT,\n+ *  e.g. SELECT count(DISTINCT col) FROM tbl GROUP BY col.\n+ * 3. aggregate function on partition columns which have same result with DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class MetadataOnlyOptimizer(\n+    sparkSession: SparkSession,\n+    catalog: SessionCatalog) extends Rule[LogicalPlan] {\n+\n+  private def canSupportMetadataOnly(a: Aggregate): Boolean = {\n+    val aggregateExpressions = a.aggregateExpressions.flatMap { expr =>"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We need to think about it more carefully, i.e. how can the partition information propagate up from table relation?\nIt's obvious that `Filter` can retain all partition information, but for others, it's not trivial to explain.\n\nSince this PR definitely need more people to review, how about we only handle `Filter` for now and improve it later? Then it's easier for other people to review and get this PR in. Thanks!\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-06-29T04:04:58Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on metadata without scanning files.\n+ * It is used for distinct, distinct aggregations or distinct-like aggregations(example: Max/Min).\n+ * First of all, scanning only partition columns are required, then the rule does the following\n+ * things here:\n+ * 1. aggregate expression is partition columns,\n+ *  e.g. SELECT col FROM tbl GROUP BY col or SELECT col FROM tbl GROUP BY cube(col).\n+ * 2. aggregate function on partition columns with DISTINCT,\n+ *  e.g. SELECT count(DISTINCT col) FROM tbl GROUP BY col.\n+ * 3. aggregate function on partition columns which have same result with DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class MetadataOnlyOptimizer(\n+    sparkSession: SparkSession,\n+    catalog: SessionCatalog) extends Rule[LogicalPlan] {\n+\n+  private def canSupportMetadataOnly(a: Aggregate): Boolean = {\n+    val aggregateExpressions = a.aggregateExpressions.flatMap { expr =>\n+      expr.collect {\n+        case agg: AggregateExpression => agg\n+      }\n+    }.distinct\n+    if (aggregateExpressions.isEmpty) {\n+      // Support for aggregate that has no aggregateFunction when expressions are partition columns\n+      // example: select partitionCol from table group by partitionCol.\n+      // Moreover, multiple-distinct has been rewritted into it by RewriteDistinctAggregates.\n+      true\n+    } else {\n+      aggregateExpressions.forall { agg =>\n+        if (agg.isDistinct) {\n+          true\n+        } else {\n+          // If function can be evaluated on just the distinct values of a column, it can be used\n+          // by metadata-only optimizer.\n+          agg.aggregateFunction match {\n+            case max: Max => true\n+            case min: Min => true\n+            case hyperLog: HyperLogLogPlusPlus => true\n+            case _ => false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private def convertLogicalToMetadataOnly(\n+      project: LogicalPlan,\n+      filter: Option[Expression],\n+      logical: LogicalRelation,\n+      files: HadoopFsRelation): LogicalPlan = {\n+    val attributeMap = logical.output.map(attr => (attr.name, attr)).toMap\n+    val partitionColumns = files.partitionSchema.map { field =>\n+      attributeMap.getOrElse(field.name, throw new AnalysisException(\n+        s\"Unable to resolve ${field.name} given [${logical.output.map(_.name).mkString(\", \")}]\"))\n+    }\n+    val projectSet = filter.map(project.references ++ _.references).getOrElse(project.references)\n+    if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+      val selectedPartitions = files.location.listFiles(filter.map(Seq(_)).getOrElse(Seq.empty))\n+      val valuesRdd = sparkSession.sparkContext.parallelize(selectedPartitions.map(_.values), 1)\n+      val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+      valuesPlan\n+    } else {\n+      logical\n+    }\n+  }\n+\n+  private def convertCatalogToMetadataOnly(\n+      project: LogicalPlan,\n+      filter: Option[Expression],\n+      relation: CatalogRelation): LogicalPlan = {\n+    val attributeMap = relation.output.map(attr => (attr.name, attr)).toMap\n+    val partitionColumns = relation.catalogTable.partitionColumnNames.map { column =>\n+      attributeMap.getOrElse(column, throw new AnalysisException(\n+        s\"Unable to resolve ${column} given [${relation.output.map(_.name).mkString(\", \")}]\"))\n+    }\n+    val projectSet = filter.map(project.references ++ _.references).getOrElse(project.references)\n+    if (projectSet.subsetOf(AttributeSet(partitionColumns))) {\n+      val partitionColumnDataTypes = partitionColumns.map(_.dataType)\n+      val partitionValues = catalog.listPartitions(relation.catalogTable.identifier)\n+        .map { p =>\n+          InternalRow.fromSeq(\n+            partitionColumns.map(a => p.spec(a.name)).zip(partitionColumnDataTypes).map {\n+              case (rawValue, dataType) => Cast(Literal(rawValue), dataType).eval(null)\n+            })\n+        }\n+      val valuesRdd = sparkSession.sparkContext.parallelize(partitionValues, 1)\n+      val valuesPlan = LogicalRDD(partitionColumns, valuesRdd)(sparkSession)\n+      valuesPlan\n+    } else {\n+      relation\n+    }\n+  }\n+\n+  private def convertToMetadataOnly(plan: LogicalPlan): LogicalPlan = plan match {"
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this example is wrong, we can not aggregate on grouping columns, it should be `SELECT count(DISTINCT col1) FROM tbl GROUP BY col2`\n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-03T17:25:17Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col) FROM tbl GROUP BY col."
  }],
  "prId": 13494
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is something we need to discuss, there may be a lot of partition values and using `LocalRelation` may not give enough parallelism here.\ncc @yhuai @liancheng \n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-03T17:36:14Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col) FROM tbl GROUP BY col.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class MetadataOnlyOptimizer(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))"
  }, {
    "author": {
      "login": "lianhuiwang"
    },
    "body": "How about sizeInBytes of LocalRelation's statistics is used to determine the parallelism? Initial code is https://github.com/apache/spark/pull/13979/commits/2ca01f26df7572251136d2c059299f846cf8a3f1. \n",
    "commit": "030776ae49484c4e5db7f775344e5e40dff27e9a",
    "createdAt": "2016-07-04T10:41:56Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{CatalystConf, InternalRow}\n+import org.apache.spark.sql.catalyst.catalog.{CatalogRelation, SessionCatalog}\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.aggregate._\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n+\n+/**\n+ * When scanning only partition columns, get results based on partition data without scanning files.\n+ * It's used for operators that only need distinct values. Currently only [[Aggregate]] operator\n+ * which satisfy the following conditions are supported:\n+ * 1. aggregate expression is partition columns.\n+ *  e.g. SELECT col FROM tbl GROUP BY col.\n+ * 2. aggregate function on partition columns with DISTINCT.\n+ *  e.g. SELECT count(DISTINCT col) FROM tbl GROUP BY col.\n+ * 3. aggregate function on partition columns which have same result w or w/o DISTINCT keyword.\n+ *  e.g. SELECT Max(col2) FROM tbl GROUP BY col1.\n+ */\n+case class MetadataOnlyOptimizer(\n+    catalog: SessionCatalog,\n+    conf: CatalystConf) extends Rule[LogicalPlan] {\n+\n+  def apply(plan: LogicalPlan): LogicalPlan = {\n+    if (!conf.optimizerMetadataOnly) {\n+      return plan\n+    }\n+\n+    plan.transform {\n+      case a @ Aggregate(_, aggExprs, child @ PartitionedRelation(partAttrs, relation)) =>\n+        if (a.references.subsetOf(partAttrs)) {\n+          val aggFunctions = aggExprs.flatMap(_.collect {\n+            case agg: AggregateExpression => agg\n+          })\n+          val isPartitionDataOnly = aggFunctions.isEmpty || aggFunctions.forall { agg =>\n+            agg.isDistinct || (agg.aggregateFunction match {\n+              case _: Max => true\n+              case _: Min => true\n+              case _ => false\n+            })\n+          }\n+          if (isPartitionDataOnly) {\n+            a.withNewChildren(Seq(usePartitionData(child, relation)))\n+          } else {\n+            a\n+          }\n+        } else {\n+          a\n+        }\n+    }\n+  }\n+\n+  private def usePartitionData(child: LogicalPlan, relation: LogicalPlan): LogicalPlan = {\n+    child transform {\n+      case plan if plan eq relation =>\n+        relation match {\n+          case l @ LogicalRelation(fsRelation: HadoopFsRelation, _, _) =>\n+            val partColumns = fsRelation.partitionSchema.map(_.name.toLowerCase).toSet\n+            val partAttrs = l.output.filter(a => partColumns.contains(a.name.toLowerCase))\n+            val partitionData = fsRelation.location.listFiles(Nil)\n+            LocalRelation(partAttrs, partitionData.map(_.values))"
  }],
  "prId": 13494
}]