[{
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "better omit the body altogether",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-05T14:07:25Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+\n+/**\n+ * A logical command specialized for writing data out. `WriteOutFileCommand`s are\n+ * wrapped in `WrittenFileCommandExec` during execution.\n+ */\n+trait WriteOutFileCommand extends logical.Command {\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `WrittenFileCommandExe` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] =\n+    Map(\n+      // General metrics.\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\")\n+    )\n+\n+  def run(\n+      sparkSession: SparkSession,\n+      children: Seq[SparkPlan],\n+      metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row] = {\n+    throw new NotImplementedError"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "for consistency, better call this `<traitName>Exec`",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-05T14:08:40Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+\n+/**\n+ * A logical command specialized for writing data out. `WriteOutFileCommand`s are\n+ * wrapped in `WrittenFileCommandExec` during execution.\n+ */\n+trait WriteOutFileCommand extends logical.Command {\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `WrittenFileCommandExe` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] =\n+    Map(\n+      // General metrics.\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\")\n+    )\n+\n+  def run(\n+      sparkSession: SparkSession,\n+      children: Seq[SparkPlan],\n+      metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row] = {\n+    throw new NotImplementedError\n+  }\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `WriteOutFileCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class WrittenFileCommandExec("
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "FileWritingCommand ?",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-05T14:11:28Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+\n+/**\n+ * A logical command specialized for writing data out. `WriteOutFileCommand`s are\n+ * wrapped in `WrittenFileCommandExec` during execution.\n+ */\n+trait WriteOutFileCommand extends logical.Command {"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "should just be `metrics.keys`",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-05T14:18:05Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+\n+/**\n+ * A logical command specialized for writing data out. `WriteOutFileCommand`s are\n+ * wrapped in `WrittenFileCommandExec` during execution.\n+ */\n+trait WriteOutFileCommand extends logical.Command {\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `WrittenFileCommandExe` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] =\n+    Map(\n+      // General metrics.\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\")\n+    )\n+\n+  def run(\n+      sparkSession: SparkSession,\n+      children: Seq[SparkPlan],\n+      metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row] = {\n+    throw new NotImplementedError\n+  }\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `WriteOutFileCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class WrittenFileCommandExec(\n+    cmd: WriteOutFileCommand,\n+    children: Seq[SparkPlan]) extends CommandExec {\n+\n+  override lazy val metrics = cmd.metrics(sqlContext.sparkContext)\n+\n+  /**\n+   * The callback function used to update metrics returned from the operation of writing data out.\n+   */\n+  private def updateDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    val times = writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))\n+    val avgWritingTime = if (times.size > 0) {\n+      times.sum / times.size\n+    } else {\n+      0\n+    }\n+\n+    val metricsNames = Seq(\"numParts\", \"numFiles\", \"numOutputBytes\", \"numOutputRows\", \"avgTime\")"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "Is there no library/Utils function that computes the average value of a list? if not, better add one and use it here.",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-05T14:30:23Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+\n+/**\n+ * A logical command specialized for writing data out. `WriteOutFileCommand`s are\n+ * wrapped in `WrittenFileCommandExec` during execution.\n+ */\n+trait WriteOutFileCommand extends logical.Command {\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `WrittenFileCommandExe` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] =\n+    Map(\n+      // General metrics.\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\")\n+    )\n+\n+  def run(\n+      sparkSession: SparkSession,\n+      children: Seq[SparkPlan],\n+      metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row] = {\n+    throw new NotImplementedError\n+  }\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `WriteOutFileCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class WrittenFileCommandExec(\n+    cmd: WriteOutFileCommand,\n+    children: Seq[SparkPlan]) extends CommandExec {\n+\n+  override lazy val metrics = cmd.metrics(sqlContext.sparkContext)\n+\n+  /**\n+   * The callback function used to update metrics returned from the operation of writing data out.\n+   */\n+  private def updateDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    val times = writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))\n+    val avgWritingTime = if (times.size > 0) {\n+      times.sum / times.size\n+    } else {\n+      0\n+    }"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "I feel this function belongs to the trait, next to the method that creates these metrics.\r\nRight now you're spelling out this list of metric names in two different places (here + trait)",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-05T14:30:51Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+\n+/**\n+ * A logical command specialized for writing data out. `WriteOutFileCommand`s are\n+ * wrapped in `WrittenFileCommandExec` during execution.\n+ */\n+trait WriteOutFileCommand extends logical.Command {\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `WrittenFileCommandExe` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] =\n+    Map(\n+      // General metrics.\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\")\n+    )\n+\n+  def run(\n+      sparkSession: SparkSession,\n+      children: Seq[SparkPlan],\n+      metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row] = {\n+    throw new NotImplementedError\n+  }\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `WriteOutFileCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class WrittenFileCommandExec(\n+    cmd: WriteOutFileCommand,\n+    children: Seq[SparkPlan]) extends CommandExec {\n+\n+  override lazy val metrics = cmd.metrics(sqlContext.sparkContext)\n+\n+  /**\n+   * The callback function used to update metrics returned from the operation of writing data out.\n+   */\n+  private def updateDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L"
  }],
  "prId": 18159
}, {
  "comments": [{
    "author": {
      "login": "adrian-ionescu"
    },
    "body": "leave a note explaining how a `writeTime` of <=0 can occur and why we're excluding it here",
    "commit": "04e79d9255f923b9e5a740d20e110b62280310f6",
    "createdAt": "2017-06-05T14:34:10Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.command\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.catalyst.plans.logical\n+import org.apache.spark.sql.execution.{SparkPlan, SQLExecution}\n+import org.apache.spark.sql.execution.datasources.ExecutedWriteSummary\n+import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics}\n+\n+/**\n+ * A logical command specialized for writing data out. `WriteOutFileCommand`s are\n+ * wrapped in `WrittenFileCommandExec` during execution.\n+ */\n+trait WriteOutFileCommand extends logical.Command {\n+\n+  /**\n+   * Those metrics will be updated once the command finishes writing data out. Those metrics will\n+   * be taken by `WrittenFileCommandExe` as its metrics when showing in UI.\n+   */\n+  def metrics(sparkContext: SparkContext): Map[String, SQLMetric] =\n+    Map(\n+      // General metrics.\n+      \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n+      \"numParts\" -> SQLMetrics.createMetric(sparkContext, \"number of dynamic part\"),\n+      \"numFiles\" -> SQLMetrics.createMetric(sparkContext, \"number of written files\"),\n+      \"numOutputBytes\" -> SQLMetrics.createMetric(sparkContext, \"bytes of written output\"),\n+      \"avgTime\" -> SQLMetrics.createMetric(sparkContext, \"average writing time (ms)\")\n+    )\n+\n+  def run(\n+      sparkSession: SparkSession,\n+      children: Seq[SparkPlan],\n+      metricsCallback: (Seq[ExecutedWriteSummary]) => Unit): Seq[Row] = {\n+    throw new NotImplementedError\n+  }\n+}\n+\n+/**\n+ * A physical operator specialized to execute the run method of a `WriteOutFileCommand`,\n+ * save the result to prevent multiple executions, and record necessary metrics for UI.\n+ */\n+case class WrittenFileCommandExec(\n+    cmd: WriteOutFileCommand,\n+    children: Seq[SparkPlan]) extends CommandExec {\n+\n+  override lazy val metrics = cmd.metrics(sqlContext.sparkContext)\n+\n+  /**\n+   * The callback function used to update metrics returned from the operation of writing data out.\n+   */\n+  private def updateDriverMetrics(writeSummaries: Seq[ExecutedWriteSummary]): Unit = {\n+    var numPartitions = 0\n+    var numFiles = 0\n+    var totalNumBytes: Long = 0L\n+    var totalNumOutput: Long = 0L\n+\n+    writeSummaries.foreach { summary =>\n+      numPartitions += summary.updatedPartitions.size\n+      numFiles += summary.numOutputFile\n+      totalNumBytes += summary.numOutputBytes\n+      totalNumOutput += summary.numOutputRows\n+    }\n+\n+    val times = writeSummaries.flatMap(_.writingTimePerFile.filter(_ > 0))"
  }],
  "prId": 18159
}]