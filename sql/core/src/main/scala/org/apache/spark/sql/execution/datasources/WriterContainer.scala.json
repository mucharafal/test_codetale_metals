[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Simply added `iterator.hasNext` check.\n",
    "commit": "5f780a7ba60b2f0518897a9a369d27455cab47cb",
    "createdAt": "2016-05-03T10:45:28Z",
    "diffHunk": "@@ -239,48 +239,50 @@ private[sql] class DefaultWriterContainer(\n   extends BaseWriterContainer(relation, job, isAppend) {\n \n   def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n-    executorSideSetup(taskContext)\n-    val configuration = taskAttemptContext.getConfiguration\n-    configuration.set(\"spark.sql.sources.output.path\", outputPath)\n-    var writer = newOutputWriter(getWorkPath)\n-    writer.initConverter(dataSchema)\n-\n-    // If anything below fails, we should abort the task.\n-    try {\n-      Utils.tryWithSafeFinallyAndFailureCallbacks {\n-        while (iterator.hasNext) {\n-          val internalRow = iterator.next()\n-          writer.writeInternal(internalRow)\n-        }\n-        commitTask()\n-      }(catchBlock = abortTask())\n-    } catch {\n-      case t: Throwable =>\n-        throw new SparkException(\"Task failed while writing rows\", t)\n-    }\n+    if (iterator.hasNext) {",
    "line": 23
  }],
  "prId": 12855
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Here as well. Simply added `iterator.hasNext` check.\n",
    "commit": "5f780a7ba60b2f0518897a9a369d27455cab47cb",
    "createdAt": "2016-05-03T10:45:46Z",
    "diffHunk": "@@ -363,84 +365,87 @@ private[sql] class DynamicPartitionWriterContainer(\n   }\n \n   def writeRows(taskContext: TaskContext, iterator: Iterator[InternalRow]): Unit = {\n-    executorSideSetup(taskContext)\n-\n-    // We should first sort by partition columns, then bucket id, and finally sorting columns.\n-    val sortingExpressions: Seq[Expression] = partitionColumns ++ bucketIdExpression ++ sortColumns\n-    val getSortingKey = UnsafeProjection.create(sortingExpressions, inputSchema)\n-\n-    val sortingKeySchema = StructType(sortingExpressions.map {\n-      case a: Attribute => StructField(a.name, a.dataType, a.nullable)\n-      // The sorting expressions are all `Attribute` except bucket id.\n-      case _ => StructField(\"bucketId\", IntegerType, nullable = false)\n-    })\n-\n-    // Returns the data columns to be written given an input row\n-    val getOutputRow = UnsafeProjection.create(dataColumns, inputSchema)\n-\n-    // Returns the partition path given a partition key.\n-    val getPartitionString =\n-      UnsafeProjection.create(Concat(partitionStringExpression) :: Nil, partitionColumns)\n-\n-    // Sorts the data before write, so that we only need one writer at the same time.\n-    // TODO: inject a local sort operator in planning.\n-    val sorter = new UnsafeKVExternalSorter(\n-      sortingKeySchema,\n-      StructType.fromAttributes(dataColumns),\n-      SparkEnv.get.blockManager,\n-      SparkEnv.get.serializerManager,\n-      TaskContext.get().taskMemoryManager().pageSizeBytes)\n-\n-    while (iterator.hasNext) {\n-      val currentRow = iterator.next()\n-      sorter.insertKV(getSortingKey(currentRow), getOutputRow(currentRow))\n-    }\n-    logInfo(s\"Sorting complete. Writing out partition files one at a time.\")\n-\n-    val getBucketingKey: InternalRow => InternalRow = if (sortColumns.isEmpty) {\n-      identity\n-    } else {\n-      UnsafeProjection.create(sortingExpressions.dropRight(sortColumns.length).zipWithIndex.map {\n-        case (expr, ordinal) => BoundReference(ordinal, expr.dataType, expr.nullable)\n+    if (iterator.hasNext) {",
    "line": 130
  }],
  "prId": 12855
}]