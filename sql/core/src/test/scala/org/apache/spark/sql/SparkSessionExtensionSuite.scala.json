[{
  "comments": [{
    "author": {
      "login": "abellina"
    },
    "body": "maybe lets make the rule `PreRuleReplaceAddPlusExtraOne` as I see that 100 + 1 is not 102, but you stated this within the rule :) \r\n\r\nre: https://github.com/apache/spark/pull/24795/files#diff-b7face523211086b889c334ae4075ad2R549",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-14T19:05:55Z",
    "diffHunk": "@@ -116,6 +121,23 @@ class SparkSessionExtensionSuite extends SparkFunSuite {\n     }\n   }\n \n+  test(\"inject columnar\") {\n+    val extensions = create { extensions =>\n+      extensions.injectColumnar(session => MyColumarRule(PreRuleReplaceAdd(), MyPostRule()))\n+    }\n+    withSession(extensions) { session =>\n+      assert(session.sessionState.columnarRules.contains(\n+        MyColumarRule(PreRuleReplaceAdd(), MyPostRule())))\n+      import session.sqlContext.implicits._\n+      // repartitioning avoids having the add operation pushed up into the LocalTableScan\n+      val data = Seq((100L), (200L), (300L)).toDF(\"vals\").repartition(1)\n+      val result = data.selectExpr(\"vals + 1\").collect()"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "it looks identical plan. We would appreciate it if the testcase would have non-identical plan for ease of review.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-15T07:02:26Z",
    "diffHunk": "@@ -251,6 +276,368 @@ object MyExtensions {\n     (_: Seq[Expression]) => Literal(5, IntegerType))\n }\n \n+case class CloseableColumnBatchIterator(itr: Iterator[ColumnarBatch],\n+    f: ColumnarBatch => ColumnarBatch) extends Iterator[ColumnarBatch] {\n+  var cb: ColumnarBatch = null\n+\n+  private def closeCurrentBatch(): Unit = {\n+    if (cb != null) {\n+      cb.close\n+      cb = null\n+    }\n+  }\n+\n+  TaskContext.get().addTaskCompletionListener[Unit]((tc: TaskContext) => {\n+    closeCurrentBatch()\n+  })\n+\n+  override def hasNext: Boolean = {\n+    closeCurrentBatch()\n+    itr.hasNext\n+  }\n+\n+  override def next(): ColumnarBatch = {\n+    closeCurrentBatch()\n+    cb = f(itr.next())\n+    cb\n+  }\n+}\n+\n+object NoCloseColumnVector extends Logging {\n+  def wrapIfNeeded(cv: ColumnVector): NoCloseColumnVector = cv match {\n+    case ref: NoCloseColumnVector =>\n+      ref\n+    case vec => NoCloseColumnVector(vec)\n+  }\n+}\n+\n+/**\n+ * Provide a ColumnVector so ColumnarExpression can close temporary values without\n+ * having to guess what type it really is.\n+ */\n+case class NoCloseColumnVector(wrapped: ColumnVector) extends ColumnVector(wrapped.dataType) {\n+  private var refCount = 1\n+\n+  /**\n+   * Don't actually close the ColumnVector this wraps.  The producer of the vector will take\n+   * care of that.\n+   */\n+  override def close(): Unit = {\n+    // Empty\n+  }\n+\n+\n+  override def hasNull: Boolean = wrapped.hasNull\n+\n+\n+  override def numNulls(): Int = wrapped.numNulls\n+\n+\n+  override def isNullAt(rowId: Int): Boolean = wrapped.isNullAt(rowId)\n+\n+\n+  override def getBoolean(rowId: Int): Boolean = wrapped.getBoolean(rowId)\n+\n+\n+  override def getByte(rowId: Int): Byte = wrapped.getByte(rowId)\n+\n+\n+  override def getShort(rowId: Int): Short = wrapped.getShort(rowId)\n+\n+\n+  override def getInt(rowId: Int): Int = wrapped.getInt(rowId)\n+\n+\n+  override def getLong(rowId: Int): Long = wrapped.getLong(rowId)\n+\n+\n+  override def getFloat(rowId: Int): Float = wrapped.getFloat(rowId)\n+\n+\n+  override def getDouble(rowId: Int): Double = wrapped.getDouble(rowId)\n+\n+\n+  override def getArray(rowId: Int): ColumnarArray = wrapped.getArray(rowId)\n+\n+\n+  override def getMap(ordinal: Int): ColumnarMap = wrapped.getMap(ordinal)\n+\n+\n+  override def getDecimal(rowId: Int, precision: Int, scale: Int): Decimal =\n+    wrapped.getDecimal(rowId, precision, scale)\n+\n+\n+  override def getUTF8String(rowId: Int): UTF8String = wrapped.getUTF8String(rowId)\n+\n+\n+  override def getBinary(rowId: Int): Array[Byte] = wrapped.getBinary(rowId)\n+\n+\n+  override protected def getChild(ordinal: Int): ColumnVector = wrapped.getChild(ordinal)\n+}\n+\n+trait ColumnarExpression extends Expression with Serializable {\n+  /**\n+   * Returns true if this expression supports columnar processing through [[columnarEval]].\n+   */\n+  def supportsColumnar: Boolean = true\n+\n+  /**\n+   * Returns the result of evaluating this expression on the entire\n+   * [[org.apache.spark.sql.vectorized.ColumnarBatch]]. The result of\n+   * calling this may be a single [[org.apache.spark.sql.vectorized.ColumnVector]] or a scalar\n+   * value. Scalar values typically happen if they are a part of the expression i.e. col(\"a\") + 100.\n+   * In this case the 100 is a [[org.apache.spark.sql.catalyst.expressions.Literal]] that\n+   * [[org.apache.spark.sql.catalyst.expressions.Add]] would have to be able to handle.\n+   *\n+   * By convention any [[org.apache.spark.sql.vectorized.ColumnVector]] returned by [[columnarEval]]\n+   * is owned by the caller and will need to be closed by them. This can happen by putting it into\n+   * a [[org.apache.spark.sql.vectorized.ColumnarBatch]] and closing the batch or by closing the\n+   * vector directly if it is a temporary value.\n+   */\n+  def columnarEval(batch: ColumnarBatch): Any = {\n+    throw new IllegalStateException(s\"Internal Error ${this.getClass} has column support mismatch\")\n+  }\n+\n+  // We need to override equals because we are subclassing a case class\n+  override def equals(other: Any): Boolean = {\n+    if (!super.equals(other)) {\n+      return false\n+    }\n+    return other.isInstanceOf[ColumnarExpression]\n+  }\n+\n+  override def hashCode(): Int = super.hashCode()\n+\n+}\n+\n+object ColumnarBindReferences extends Logging {\n+\n+  // Mostly copied from BoundAttribute.scala so we can do columnar processing\n+  def bindReference[A <: ColumnarExpression](\n+      expression: A,\n+      input: AttributeSeq,\n+      allowFailures: Boolean = false): A = {\n+    expression.transform { case a: AttributeReference =>\n+      val ordinal = input.indexOf(a.exprId)\n+      if (ordinal == -1) {\n+        if (allowFailures) {\n+          a\n+        } else {\n+          sys.error(s\"Couldn't find $a in ${input.attrs.mkString(\"[\", \",\", \"]\")}\")\n+        }\n+      } else {\n+        new ColumnarBoundReference(ordinal, a.dataType, input(ordinal).nullable)\n+      }\n+    }.asInstanceOf[A]\n+  }\n+\n+  /**\n+   * A helper function to bind given expressions to an input schema.\n+   */\n+  def bindReferences[A <: ColumnarExpression](\n+      expressions: Seq[A],\n+      input: AttributeSeq): Seq[A] = {\n+    expressions.map(ColumnarBindReferences.bindReference(_, input))\n+  }\n+}\n+\n+class ColumnarBoundReference(ordinal: Int, dataType: DataType, nullable: Boolean)\n+  extends BoundReference(ordinal, dataType, nullable) with ColumnarExpression {\n+\n+  override def columnarEval(batch: ColumnarBatch): Any = {\n+    // Because of the convention that the returned ColumnVector must be closed by the\n+    // caller we wrap this column vector so a close is a NOOP, and let the original source\n+    // of the vector close it.\n+    NoCloseColumnVector.wrapIfNeeded(batch.column(ordinal))\n+  }\n+}\n+\n+class ColumnarAlias(child: ColumnarExpression, name: String)(\n+    override val exprId: ExprId = NamedExpression.newExprId,\n+    override val qualifier: Seq[String] = Seq.empty,\n+    override val explicitMetadata: Option[Metadata] = None)\n+  extends Alias(child, name)(exprId, qualifier, explicitMetadata)\n+  with ColumnarExpression {\n+\n+  override def columnarEval(batch: ColumnarBatch): Any = child.columnarEval(batch)\n+}\n+\n+class ColumnarAttributeReference(\n+    name: String,\n+    dataType: DataType,\n+    nullable: Boolean = true,\n+    override val metadata: Metadata = Metadata.empty)(\n+    override val exprId: ExprId = NamedExpression.newExprId,\n+    override val qualifier: Seq[String] = Seq.empty[String])\n+  extends AttributeReference(name, dataType, nullable, metadata)(exprId, qualifier)\n+  with ColumnarExpression {\n+\n+  // No columnar eval is needed because this must be bound before it is evaluated\n+}\n+\n+class ColumnarLiteral (value: Any, dataType: DataType) extends Literal(value, dataType)\n+  with ColumnarExpression {\n+  override def columnarEval(batch: ColumnarBatch): Any = value\n+}\n+\n+/**\n+ * A version of ProjectExec that adds in columnar support.\n+ */\n+class ColumnarProjectExec(projectList: Seq[NamedExpression], child: SparkPlan)\n+  extends ProjectExec(projectList, child) {\n+\n+  override def supportsColumnar: Boolean =\n+    projectList.forall(_.asInstanceOf[ColumnarExpression].supportsColumnar)\n+\n+  // Disable code generation\n+  override def supportCodegen: Boolean = false\n+\n+  override def doExecuteColumnar() : RDD[ColumnarBatch] = {\n+    val boundProjectList: Seq[Any] =\n+      ColumnarBindReferences.bindReferences(\n+        projectList.asInstanceOf[Seq[ColumnarExpression]], child.output)\n+    val rdd = child.executeColumnar()\n+    rdd.mapPartitions((itr) => CloseableColumnBatchIterator(itr,\n+      (cb) => {\n+        val newColumns = boundProjectList.map(\n+          expr => expr.asInstanceOf[ColumnarExpression].columnarEval(cb).asInstanceOf[ColumnVector]\n+        ).toArray\n+        new ColumnarBatch(newColumns, cb.numRows())\n+      })\n+    )\n+  }\n+\n+  // We have to override equals because subclassing a case class like ProjectExec is not that clean\n+  // One of the issues is that the generated equals will see ColumnarProjectExec and ProjectExec\n+  // as being equal and this can result in the withNewChildren method not actually replacing\n+  // anything\n+  override def equals(other: Any): Boolean = {\n+    if (!super.equals(other)) {\n+      return false\n+    }\n+    return other.isInstanceOf[ColumnarProjectExec]\n+  }\n+\n+  override def hashCode(): Int = super.hashCode()\n+}\n+\n+/**\n+ * A version of add that supports columnar processing for longs.  This version is broken\n+ * on purpose so it adds the numbers plus 1 so that the tests can show that it was replaced.\n+ */\n+class BrokenColumnarAdd(left: ColumnarExpression, right: ColumnarExpression)\n+  extends Add(left, right) with ColumnarExpression {\n+\n+  override def supportsColumnar(): Boolean = left.supportsColumnar && right.supportsColumnar\n+\n+  override def columnarEval(batch: ColumnarBatch): Any = {\n+    var lhs: Any = null\n+    var rhs: Any = null\n+    var ret: Any = null\n+    try {\n+      lhs = left.columnarEval(batch)\n+      rhs = right.columnarEval(batch)\n+\n+      if (lhs == null || rhs == null) {\n+        ret = null\n+      } else if (lhs.isInstanceOf[ColumnVector] && rhs.isInstanceOf[ColumnVector]) {\n+        val l = lhs.asInstanceOf[ColumnVector]\n+        val r = rhs.asInstanceOf[ColumnVector]\n+        val result = new OnHeapColumnVector(batch.numRows(), dataType)\n+        ret = result\n+\n+        for (i <- 0 until batch.numRows()) {\n+          result.appendLong(l.getLong(i) + r.getLong(i) + 1) // BUG to show we replaced Add\n+        }\n+      } else if (rhs.isInstanceOf[ColumnVector]) {\n+        val l = lhs.asInstanceOf[Long]\n+        val r = rhs.asInstanceOf[ColumnVector]\n+        val result = new OnHeapColumnVector(batch.numRows(), dataType)\n+        ret = result\n+\n+        for (i <- 0 until batch.numRows()) {\n+          result.appendLong(l + r.getLong(i) + 1) // BUG to show we replaced Add\n+        }\n+      } else if (lhs.isInstanceOf[ColumnVector]) {\n+        val l = lhs.asInstanceOf[ColumnVector]\n+        val r = rhs.asInstanceOf[Long]\n+        val result = new OnHeapColumnVector(batch.numRows(), dataType)\n+        ret = result\n+\n+        for (i <- 0 until batch.numRows()) {\n+          result.appendLong(l.getLong(i) + r + 1) // BUG to show we replaced Add\n+        }\n+      } else {\n+        ret = nullSafeEval(lhs, rhs)\n+      }\n+    } finally {\n+      if (lhs != null && lhs.isInstanceOf[ColumnVector]) {\n+        lhs.asInstanceOf[ColumnVector].close()\n+      }\n+      if (rhs != null && rhs.isInstanceOf[ColumnVector]) {\n+        rhs.asInstanceOf[ColumnVector].close()\n+      }\n+    }\n+    ret\n+  }\n+}\n+\n+class CannotReplaceException(str: String) extends RuntimeException(str) {\n+\n+}\n+\n+case class PreRuleReplaceAddWithBrokenVersion() extends Rule[SparkPlan] {\n+  def replaceWithColumnarExpression(exp: Expression): ColumnarExpression = exp match {\n+    case a: Alias =>\n+      new ColumnarAlias(replaceWithColumnarExpression(a.child),\n+        a.name)(a.exprId, a.qualifier, a.explicitMetadata)\n+    case att: AttributeReference =>\n+      new ColumnarAttributeReference(att.name, att.dataType, att.nullable,\n+        att.metadata)(att.exprId, att.qualifier)\n+    case lit: Literal =>\n+      new ColumnarLiteral(lit.value, lit.dataType)\n+    case add: Add if (add.dataType == LongType) &&\n+      (add.left.dataType == LongType) &&\n+      (add.right.dataType == LongType) =>\n+      // Add only supports Longs for now.\n+      new BrokenColumnarAdd(replaceWithColumnarExpression(add.left),\n+        replaceWithColumnarExpression(add.right))\n+    case exp =>\n+      throw new CannotReplaceException(s\"expression \" +\n+        s\"${exp.getClass} ${exp} is not currently supported.\")\n+  }\n+\n+  def replaceWithColumnarPlan(plan: SparkPlan): SparkPlan =\n+    try {\n+      plan match {\n+        case plan: ProjectExec =>\n+          new ColumnarProjectExec(plan.projectList.map((exp) =>\n+            replaceWithColumnarExpression(exp).asInstanceOf[NamedExpression]),\n+            replaceWithColumnarPlan(plan.child))\n+        case p =>\n+          logWarning(s\"Columnar processing for ${p.getClass} is not currently supported.\")\n+          p.withNewChildren(p.children.map(replaceWithColumnarPlan))\n+      }\n+    } catch {\n+      case exp: CannotReplaceException =>\n+        logWarning(s\"Columnar processing for ${plan.getClass} is not currently supported\" +\n+          s\"because ${exp.getMessage}\")\n+        plan\n+    }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = replaceWithColumnarPlan(plan)\n+}\n+\n+case class MyPostRule() extends Rule[SparkPlan] {\n+  override def apply(plan: SparkPlan): SparkPlan = plan"
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "Sure, I'll look at inserting in a noop of some kind to show that it was replaced. ",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-17T12:38:09Z",
    "diffHunk": "@@ -251,6 +276,368 @@ object MyExtensions {\n     (_: Seq[Expression]) => Literal(5, IntegerType))\n }\n \n+case class CloseableColumnBatchIterator(itr: Iterator[ColumnarBatch],\n+    f: ColumnarBatch => ColumnarBatch) extends Iterator[ColumnarBatch] {\n+  var cb: ColumnarBatch = null\n+\n+  private def closeCurrentBatch(): Unit = {\n+    if (cb != null) {\n+      cb.close\n+      cb = null\n+    }\n+  }\n+\n+  TaskContext.get().addTaskCompletionListener[Unit]((tc: TaskContext) => {\n+    closeCurrentBatch()\n+  })\n+\n+  override def hasNext: Boolean = {\n+    closeCurrentBatch()\n+    itr.hasNext\n+  }\n+\n+  override def next(): ColumnarBatch = {\n+    closeCurrentBatch()\n+    cb = f(itr.next())\n+    cb\n+  }\n+}\n+\n+object NoCloseColumnVector extends Logging {\n+  def wrapIfNeeded(cv: ColumnVector): NoCloseColumnVector = cv match {\n+    case ref: NoCloseColumnVector =>\n+      ref\n+    case vec => NoCloseColumnVector(vec)\n+  }\n+}\n+\n+/**\n+ * Provide a ColumnVector so ColumnarExpression can close temporary values without\n+ * having to guess what type it really is.\n+ */\n+case class NoCloseColumnVector(wrapped: ColumnVector) extends ColumnVector(wrapped.dataType) {\n+  private var refCount = 1\n+\n+  /**\n+   * Don't actually close the ColumnVector this wraps.  The producer of the vector will take\n+   * care of that.\n+   */\n+  override def close(): Unit = {\n+    // Empty\n+  }\n+\n+\n+  override def hasNull: Boolean = wrapped.hasNull\n+\n+\n+  override def numNulls(): Int = wrapped.numNulls\n+\n+\n+  override def isNullAt(rowId: Int): Boolean = wrapped.isNullAt(rowId)\n+\n+\n+  override def getBoolean(rowId: Int): Boolean = wrapped.getBoolean(rowId)\n+\n+\n+  override def getByte(rowId: Int): Byte = wrapped.getByte(rowId)\n+\n+\n+  override def getShort(rowId: Int): Short = wrapped.getShort(rowId)\n+\n+\n+  override def getInt(rowId: Int): Int = wrapped.getInt(rowId)\n+\n+\n+  override def getLong(rowId: Int): Long = wrapped.getLong(rowId)\n+\n+\n+  override def getFloat(rowId: Int): Float = wrapped.getFloat(rowId)\n+\n+\n+  override def getDouble(rowId: Int): Double = wrapped.getDouble(rowId)\n+\n+\n+  override def getArray(rowId: Int): ColumnarArray = wrapped.getArray(rowId)\n+\n+\n+  override def getMap(ordinal: Int): ColumnarMap = wrapped.getMap(ordinal)\n+\n+\n+  override def getDecimal(rowId: Int, precision: Int, scale: Int): Decimal =\n+    wrapped.getDecimal(rowId, precision, scale)\n+\n+\n+  override def getUTF8String(rowId: Int): UTF8String = wrapped.getUTF8String(rowId)\n+\n+\n+  override def getBinary(rowId: Int): Array[Byte] = wrapped.getBinary(rowId)\n+\n+\n+  override protected def getChild(ordinal: Int): ColumnVector = wrapped.getChild(ordinal)\n+}\n+\n+trait ColumnarExpression extends Expression with Serializable {\n+  /**\n+   * Returns true if this expression supports columnar processing through [[columnarEval]].\n+   */\n+  def supportsColumnar: Boolean = true\n+\n+  /**\n+   * Returns the result of evaluating this expression on the entire\n+   * [[org.apache.spark.sql.vectorized.ColumnarBatch]]. The result of\n+   * calling this may be a single [[org.apache.spark.sql.vectorized.ColumnVector]] or a scalar\n+   * value. Scalar values typically happen if they are a part of the expression i.e. col(\"a\") + 100.\n+   * In this case the 100 is a [[org.apache.spark.sql.catalyst.expressions.Literal]] that\n+   * [[org.apache.spark.sql.catalyst.expressions.Add]] would have to be able to handle.\n+   *\n+   * By convention any [[org.apache.spark.sql.vectorized.ColumnVector]] returned by [[columnarEval]]\n+   * is owned by the caller and will need to be closed by them. This can happen by putting it into\n+   * a [[org.apache.spark.sql.vectorized.ColumnarBatch]] and closing the batch or by closing the\n+   * vector directly if it is a temporary value.\n+   */\n+  def columnarEval(batch: ColumnarBatch): Any = {\n+    throw new IllegalStateException(s\"Internal Error ${this.getClass} has column support mismatch\")\n+  }\n+\n+  // We need to override equals because we are subclassing a case class\n+  override def equals(other: Any): Boolean = {\n+    if (!super.equals(other)) {\n+      return false\n+    }\n+    return other.isInstanceOf[ColumnarExpression]\n+  }\n+\n+  override def hashCode(): Int = super.hashCode()\n+\n+}\n+\n+object ColumnarBindReferences extends Logging {\n+\n+  // Mostly copied from BoundAttribute.scala so we can do columnar processing\n+  def bindReference[A <: ColumnarExpression](\n+      expression: A,\n+      input: AttributeSeq,\n+      allowFailures: Boolean = false): A = {\n+    expression.transform { case a: AttributeReference =>\n+      val ordinal = input.indexOf(a.exprId)\n+      if (ordinal == -1) {\n+        if (allowFailures) {\n+          a\n+        } else {\n+          sys.error(s\"Couldn't find $a in ${input.attrs.mkString(\"[\", \",\", \"]\")}\")\n+        }\n+      } else {\n+        new ColumnarBoundReference(ordinal, a.dataType, input(ordinal).nullable)\n+      }\n+    }.asInstanceOf[A]\n+  }\n+\n+  /**\n+   * A helper function to bind given expressions to an input schema.\n+   */\n+  def bindReferences[A <: ColumnarExpression](\n+      expressions: Seq[A],\n+      input: AttributeSeq): Seq[A] = {\n+    expressions.map(ColumnarBindReferences.bindReference(_, input))\n+  }\n+}\n+\n+class ColumnarBoundReference(ordinal: Int, dataType: DataType, nullable: Boolean)\n+  extends BoundReference(ordinal, dataType, nullable) with ColumnarExpression {\n+\n+  override def columnarEval(batch: ColumnarBatch): Any = {\n+    // Because of the convention that the returned ColumnVector must be closed by the\n+    // caller we wrap this column vector so a close is a NOOP, and let the original source\n+    // of the vector close it.\n+    NoCloseColumnVector.wrapIfNeeded(batch.column(ordinal))\n+  }\n+}\n+\n+class ColumnarAlias(child: ColumnarExpression, name: String)(\n+    override val exprId: ExprId = NamedExpression.newExprId,\n+    override val qualifier: Seq[String] = Seq.empty,\n+    override val explicitMetadata: Option[Metadata] = None)\n+  extends Alias(child, name)(exprId, qualifier, explicitMetadata)\n+  with ColumnarExpression {\n+\n+  override def columnarEval(batch: ColumnarBatch): Any = child.columnarEval(batch)\n+}\n+\n+class ColumnarAttributeReference(\n+    name: String,\n+    dataType: DataType,\n+    nullable: Boolean = true,\n+    override val metadata: Metadata = Metadata.empty)(\n+    override val exprId: ExprId = NamedExpression.newExprId,\n+    override val qualifier: Seq[String] = Seq.empty[String])\n+  extends AttributeReference(name, dataType, nullable, metadata)(exprId, qualifier)\n+  with ColumnarExpression {\n+\n+  // No columnar eval is needed because this must be bound before it is evaluated\n+}\n+\n+class ColumnarLiteral (value: Any, dataType: DataType) extends Literal(value, dataType)\n+  with ColumnarExpression {\n+  override def columnarEval(batch: ColumnarBatch): Any = value\n+}\n+\n+/**\n+ * A version of ProjectExec that adds in columnar support.\n+ */\n+class ColumnarProjectExec(projectList: Seq[NamedExpression], child: SparkPlan)\n+  extends ProjectExec(projectList, child) {\n+\n+  override def supportsColumnar: Boolean =\n+    projectList.forall(_.asInstanceOf[ColumnarExpression].supportsColumnar)\n+\n+  // Disable code generation\n+  override def supportCodegen: Boolean = false\n+\n+  override def doExecuteColumnar() : RDD[ColumnarBatch] = {\n+    val boundProjectList: Seq[Any] =\n+      ColumnarBindReferences.bindReferences(\n+        projectList.asInstanceOf[Seq[ColumnarExpression]], child.output)\n+    val rdd = child.executeColumnar()\n+    rdd.mapPartitions((itr) => CloseableColumnBatchIterator(itr,\n+      (cb) => {\n+        val newColumns = boundProjectList.map(\n+          expr => expr.asInstanceOf[ColumnarExpression].columnarEval(cb).asInstanceOf[ColumnVector]\n+        ).toArray\n+        new ColumnarBatch(newColumns, cb.numRows())\n+      })\n+    )\n+  }\n+\n+  // We have to override equals because subclassing a case class like ProjectExec is not that clean\n+  // One of the issues is that the generated equals will see ColumnarProjectExec and ProjectExec\n+  // as being equal and this can result in the withNewChildren method not actually replacing\n+  // anything\n+  override def equals(other: Any): Boolean = {\n+    if (!super.equals(other)) {\n+      return false\n+    }\n+    return other.isInstanceOf[ColumnarProjectExec]\n+  }\n+\n+  override def hashCode(): Int = super.hashCode()\n+}\n+\n+/**\n+ * A version of add that supports columnar processing for longs.  This version is broken\n+ * on purpose so it adds the numbers plus 1 so that the tests can show that it was replaced.\n+ */\n+class BrokenColumnarAdd(left: ColumnarExpression, right: ColumnarExpression)\n+  extends Add(left, right) with ColumnarExpression {\n+\n+  override def supportsColumnar(): Boolean = left.supportsColumnar && right.supportsColumnar\n+\n+  override def columnarEval(batch: ColumnarBatch): Any = {\n+    var lhs: Any = null\n+    var rhs: Any = null\n+    var ret: Any = null\n+    try {\n+      lhs = left.columnarEval(batch)\n+      rhs = right.columnarEval(batch)\n+\n+      if (lhs == null || rhs == null) {\n+        ret = null\n+      } else if (lhs.isInstanceOf[ColumnVector] && rhs.isInstanceOf[ColumnVector]) {\n+        val l = lhs.asInstanceOf[ColumnVector]\n+        val r = rhs.asInstanceOf[ColumnVector]\n+        val result = new OnHeapColumnVector(batch.numRows(), dataType)\n+        ret = result\n+\n+        for (i <- 0 until batch.numRows()) {\n+          result.appendLong(l.getLong(i) + r.getLong(i) + 1) // BUG to show we replaced Add\n+        }\n+      } else if (rhs.isInstanceOf[ColumnVector]) {\n+        val l = lhs.asInstanceOf[Long]\n+        val r = rhs.asInstanceOf[ColumnVector]\n+        val result = new OnHeapColumnVector(batch.numRows(), dataType)\n+        ret = result\n+\n+        for (i <- 0 until batch.numRows()) {\n+          result.appendLong(l + r.getLong(i) + 1) // BUG to show we replaced Add\n+        }\n+      } else if (lhs.isInstanceOf[ColumnVector]) {\n+        val l = lhs.asInstanceOf[ColumnVector]\n+        val r = rhs.asInstanceOf[Long]\n+        val result = new OnHeapColumnVector(batch.numRows(), dataType)\n+        ret = result\n+\n+        for (i <- 0 until batch.numRows()) {\n+          result.appendLong(l.getLong(i) + r + 1) // BUG to show we replaced Add\n+        }\n+      } else {\n+        ret = nullSafeEval(lhs, rhs)\n+      }\n+    } finally {\n+      if (lhs != null && lhs.isInstanceOf[ColumnVector]) {\n+        lhs.asInstanceOf[ColumnVector].close()\n+      }\n+      if (rhs != null && rhs.isInstanceOf[ColumnVector]) {\n+        rhs.asInstanceOf[ColumnVector].close()\n+      }\n+    }\n+    ret\n+  }\n+}\n+\n+class CannotReplaceException(str: String) extends RuntimeException(str) {\n+\n+}\n+\n+case class PreRuleReplaceAddWithBrokenVersion() extends Rule[SparkPlan] {\n+  def replaceWithColumnarExpression(exp: Expression): ColumnarExpression = exp match {\n+    case a: Alias =>\n+      new ColumnarAlias(replaceWithColumnarExpression(a.child),\n+        a.name)(a.exprId, a.qualifier, a.explicitMetadata)\n+    case att: AttributeReference =>\n+      new ColumnarAttributeReference(att.name, att.dataType, att.nullable,\n+        att.metadata)(att.exprId, att.qualifier)\n+    case lit: Literal =>\n+      new ColumnarLiteral(lit.value, lit.dataType)\n+    case add: Add if (add.dataType == LongType) &&\n+      (add.left.dataType == LongType) &&\n+      (add.right.dataType == LongType) =>\n+      // Add only supports Longs for now.\n+      new BrokenColumnarAdd(replaceWithColumnarExpression(add.left),\n+        replaceWithColumnarExpression(add.right))\n+    case exp =>\n+      throw new CannotReplaceException(s\"expression \" +\n+        s\"${exp.getClass} ${exp} is not currently supported.\")\n+  }\n+\n+  def replaceWithColumnarPlan(plan: SparkPlan): SparkPlan =\n+    try {\n+      plan match {\n+        case plan: ProjectExec =>\n+          new ColumnarProjectExec(plan.projectList.map((exp) =>\n+            replaceWithColumnarExpression(exp).asInstanceOf[NamedExpression]),\n+            replaceWithColumnarPlan(plan.child))\n+        case p =>\n+          logWarning(s\"Columnar processing for ${p.getClass} is not currently supported.\")\n+          p.withNewChildren(p.children.map(replaceWithColumnarPlan))\n+      }\n+    } catch {\n+      case exp: CannotReplaceException =>\n+        logWarning(s\"Columnar processing for ${plan.getClass} is not currently supported\" +\n+          s\"because ${exp.getMessage}\")\n+        plan\n+    }\n+\n+  override def apply(plan: SparkPlan): SparkPlan = replaceWithColumnarPlan(plan)\n+}\n+\n+case class MyPostRule() extends Rule[SparkPlan] {\n+  override def apply(plan: SparkPlan): SparkPlan = plan"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "nit: can you use 1 newline here and below instead of 2?",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-18T17:13:04Z",
    "diffHunk": "@@ -251,6 +286,388 @@ object MyExtensions {\n     (_: Seq[Expression]) => Literal(5, IntegerType))\n }\n \n+case class CloseableColumnBatchIterator(itr: Iterator[ColumnarBatch],\n+    f: ColumnarBatch => ColumnarBatch) extends Iterator[ColumnarBatch] {\n+  var cb: ColumnarBatch = null\n+\n+  private def closeCurrentBatch(): Unit = {\n+    if (cb != null) {\n+      cb.close\n+      cb = null\n+    }\n+  }\n+\n+  TaskContext.get().addTaskCompletionListener[Unit]((tc: TaskContext) => {\n+    closeCurrentBatch()\n+  })\n+\n+  override def hasNext: Boolean = {\n+    closeCurrentBatch()\n+    itr.hasNext\n+  }\n+\n+  override def next(): ColumnarBatch = {\n+    closeCurrentBatch()\n+    cb = f(itr.next())\n+    cb\n+  }\n+}\n+\n+object NoCloseColumnVector extends Logging {\n+  def wrapIfNeeded(cv: ColumnVector): NoCloseColumnVector = cv match {\n+    case ref: NoCloseColumnVector =>\n+      ref\n+    case vec => NoCloseColumnVector(vec)\n+  }\n+}\n+\n+/**\n+ * Provide a ColumnVector so ColumnarExpression can close temporary values without\n+ * having to guess what type it really is.\n+ */\n+case class NoCloseColumnVector(wrapped: ColumnVector) extends ColumnVector(wrapped.dataType) {\n+  private var refCount = 1\n+\n+  /**\n+   * Don't actually close the ColumnVector this wraps.  The producer of the vector will take\n+   * care of that.\n+   */\n+  override def close(): Unit = {\n+    // Empty\n+  }\n+\n+"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: how about ` import org.apache.spark.sql.catalyst.expressions._`. It looks too long",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-20T15:33:55Z",
    "diffHunk": "@@ -16,14 +16,19 @@\n  */\n package org.apache.spark.sql\n \n-import org.apache.spark.SparkFunSuite\n+import org.apache.spark.{SparkFunSuite, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n-import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionInfo, Literal}\n+import org.apache.spark.sql.catalyst.expressions.{Add, Alias, AttributeReference, AttributeSeq, BoundReference, Expression, ExpressionInfo, ExprId, Literal, NamedExpression}"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "ditto",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-20T15:34:11Z",
    "diffHunk": "@@ -16,14 +16,19 @@\n  */\n package org.apache.spark.sql\n \n-import org.apache.spark.SparkFunSuite\n+import org.apache.spark.{SparkFunSuite, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n-import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionInfo, Literal}\n+import org.apache.spark.sql.catalyst.expressions.{Add, Alias, AttributeReference, AttributeSeq, BoundReference, Expression, ExpressionInfo, ExprId, Literal, NamedExpression}\n import org.apache.spark.sql.catalyst.parser.{CatalystSqlParser, ParserInterface}\n import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n import org.apache.spark.sql.catalyst.rules.Rule\n-import org.apache.spark.sql.execution.{SparkPlan, SparkStrategy}\n-import org.apache.spark.sql.types.{DataType, IntegerType, StructType}\n+import org.apache.spark.sql.execution.{ColumnarRule, ColumnarToRowExec, ProjectExec, RowToColumnarExec, SparkPlan, SparkStrategy}"
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "How about the following? The `sum` will return the unique value that corresponds to the combination.\r\n\r\n```\r\ncase rep: ... => 1\r\ncase proj: ... => 4\r\ncase c2r: ... => 13\r\n```",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-06-20T16:48:50Z",
    "diffHunk": "@@ -116,6 +121,34 @@ class SparkSessionExtensionSuite extends SparkFunSuite {\n     }\n   }\n \n+  test(\"inject columnar\") {\n+    val extensions = create { extensions =>\n+      extensions.injectColumnar(session =>\n+        MyColumarRule(PreRuleReplaceAddWithBrokenVersion(), MyPostRule()))\n+    }\n+    withSession(extensions) { session =>\n+      assert(session.sessionState.columnarRules.contains(\n+        MyColumarRule(PreRuleReplaceAddWithBrokenVersion(), MyPostRule())))\n+      import session.sqlContext.implicits._\n+      // repartitioning avoids having the add operation pushed up into the LocalTableScan\n+      val data = Seq((100L), (200L), (300L)).toDF(\"vals\").repartition(1)\n+      val df = data.selectExpr(\"vals + 1\")\n+      // Verify that both pre and post processing of the plan worked.\n+      val found = df.queryExecution.executedPlan.collect {\n+        case rep: ReplacedRowToColumnarExec => 1",
    "line": 42
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I understand that this is in test not an API, but other people may look at this test to learn how to implement columnar operator, and I feel the current example is not that good.\r\n\r\nIIUC, the goal is:\r\n1. users can write a rule to replace an arbitrary SQL operator with a custom optimized columnar version\r\n2. Spark automatically insert column-to-row and row-to-column operators around the columnar operator.\r\n\r\nFor 1, I think a pretty simple approach is, take in an expression tree, compile it to a columnar processor that can execute the expression tree in a columnar fashion. We don't need to create a `ColumnarExpression`, which seems over complicated to me as a column processor.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-07-01T03:45:54Z",
    "diffHunk": "@@ -251,6 +286,371 @@ object MyExtensions {\n     (_: Seq[Expression]) => Literal(5, IntegerType))\n }\n \n+case class CloseableColumnBatchIterator(itr: Iterator[ColumnarBatch],\n+    f: ColumnarBatch => ColumnarBatch) extends Iterator[ColumnarBatch] {\n+  var cb: ColumnarBatch = null\n+\n+  private def closeCurrentBatch(): Unit = {\n+    if (cb != null) {\n+      cb.close\n+      cb = null\n+    }\n+  }\n+\n+  TaskContext.get().addTaskCompletionListener[Unit]((tc: TaskContext) => {\n+    closeCurrentBatch()\n+  })\n+\n+  override def hasNext: Boolean = {\n+    closeCurrentBatch()\n+    itr.hasNext\n+  }\n+\n+  override def next(): ColumnarBatch = {\n+    closeCurrentBatch()\n+    cb = f(itr.next())\n+    cb\n+  }\n+}\n+\n+object NoCloseColumnVector extends Logging {\n+  def wrapIfNeeded(cv: ColumnVector): NoCloseColumnVector = cv match {\n+    case ref: NoCloseColumnVector =>\n+      ref\n+    case vec => NoCloseColumnVector(vec)\n+  }\n+}\n+\n+/**\n+ * Provide a ColumnVector so ColumnarExpression can close temporary values without\n+ * having to guess what type it really is.\n+ */\n+case class NoCloseColumnVector(wrapped: ColumnVector) extends ColumnVector(wrapped.dataType) {\n+  private var refCount = 1\n+\n+  /**\n+   * Don't actually close the ColumnVector this wraps.  The producer of the vector will take\n+   * care of that.\n+   */\n+  override def close(): Unit = {\n+    // Empty\n+  }\n+\n+  override def hasNull: Boolean = wrapped.hasNull\n+\n+  override def numNulls(): Int = wrapped.numNulls\n+\n+  override def isNullAt(rowId: Int): Boolean = wrapped.isNullAt(rowId)\n+\n+  override def getBoolean(rowId: Int): Boolean = wrapped.getBoolean(rowId)\n+\n+  override def getByte(rowId: Int): Byte = wrapped.getByte(rowId)\n+\n+  override def getShort(rowId: Int): Short = wrapped.getShort(rowId)\n+\n+  override def getInt(rowId: Int): Int = wrapped.getInt(rowId)\n+\n+  override def getLong(rowId: Int): Long = wrapped.getLong(rowId)\n+\n+  override def getFloat(rowId: Int): Float = wrapped.getFloat(rowId)\n+\n+  override def getDouble(rowId: Int): Double = wrapped.getDouble(rowId)\n+\n+  override def getArray(rowId: Int): ColumnarArray = wrapped.getArray(rowId)\n+\n+  override def getMap(ordinal: Int): ColumnarMap = wrapped.getMap(ordinal)\n+\n+  override def getDecimal(rowId: Int, precision: Int, scale: Int): Decimal =\n+    wrapped.getDecimal(rowId, precision, scale)\n+\n+  override def getUTF8String(rowId: Int): UTF8String = wrapped.getUTF8String(rowId)\n+\n+  override def getBinary(rowId: Int): Array[Byte] = wrapped.getBinary(rowId)\n+\n+  override protected def getChild(ordinal: Int): ColumnVector = wrapped.getChild(ordinal)\n+}\n+\n+trait ColumnarExpression extends Expression with Serializable {\n+  /**\n+   * Returns true if this expression supports columnar processing through [[columnarEval]].\n+   */\n+  def supportsColumnar: Boolean = true\n+\n+  /**\n+   * Returns the result of evaluating this expression on the entire\n+   * [[org.apache.spark.sql.vectorized.ColumnarBatch]]. The result of\n+   * calling this may be a single [[org.apache.spark.sql.vectorized.ColumnVector]] or a scalar\n+   * value. Scalar values typically happen if they are a part of the expression i.e. col(\"a\") + 100.",
    "line": 166
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "The main reason it is this way is that originally I had Expression support columnar as well but as a part of the review it changed so I made minimal changes to the example to match.\r\nI do find it simpler from a development standpoint to have a one to one mapping.  I can then write unit tests with just inputs and verify that the outputs match exactly.   But yes there probably are simpler ways to do this depending on the columnar library you are using.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-07-01T12:14:29Z",
    "diffHunk": "@@ -251,6 +286,371 @@ object MyExtensions {\n     (_: Seq[Expression]) => Literal(5, IntegerType))\n }\n \n+case class CloseableColumnBatchIterator(itr: Iterator[ColumnarBatch],\n+    f: ColumnarBatch => ColumnarBatch) extends Iterator[ColumnarBatch] {\n+  var cb: ColumnarBatch = null\n+\n+  private def closeCurrentBatch(): Unit = {\n+    if (cb != null) {\n+      cb.close\n+      cb = null\n+    }\n+  }\n+\n+  TaskContext.get().addTaskCompletionListener[Unit]((tc: TaskContext) => {\n+    closeCurrentBatch()\n+  })\n+\n+  override def hasNext: Boolean = {\n+    closeCurrentBatch()\n+    itr.hasNext\n+  }\n+\n+  override def next(): ColumnarBatch = {\n+    closeCurrentBatch()\n+    cb = f(itr.next())\n+    cb\n+  }\n+}\n+\n+object NoCloseColumnVector extends Logging {\n+  def wrapIfNeeded(cv: ColumnVector): NoCloseColumnVector = cv match {\n+    case ref: NoCloseColumnVector =>\n+      ref\n+    case vec => NoCloseColumnVector(vec)\n+  }\n+}\n+\n+/**\n+ * Provide a ColumnVector so ColumnarExpression can close temporary values without\n+ * having to guess what type it really is.\n+ */\n+case class NoCloseColumnVector(wrapped: ColumnVector) extends ColumnVector(wrapped.dataType) {\n+  private var refCount = 1\n+\n+  /**\n+   * Don't actually close the ColumnVector this wraps.  The producer of the vector will take\n+   * care of that.\n+   */\n+  override def close(): Unit = {\n+    // Empty\n+  }\n+\n+  override def hasNull: Boolean = wrapped.hasNull\n+\n+  override def numNulls(): Int = wrapped.numNulls\n+\n+  override def isNullAt(rowId: Int): Boolean = wrapped.isNullAt(rowId)\n+\n+  override def getBoolean(rowId: Int): Boolean = wrapped.getBoolean(rowId)\n+\n+  override def getByte(rowId: Int): Byte = wrapped.getByte(rowId)\n+\n+  override def getShort(rowId: Int): Short = wrapped.getShort(rowId)\n+\n+  override def getInt(rowId: Int): Int = wrapped.getInt(rowId)\n+\n+  override def getLong(rowId: Int): Long = wrapped.getLong(rowId)\n+\n+  override def getFloat(rowId: Int): Float = wrapped.getFloat(rowId)\n+\n+  override def getDouble(rowId: Int): Double = wrapped.getDouble(rowId)\n+\n+  override def getArray(rowId: Int): ColumnarArray = wrapped.getArray(rowId)\n+\n+  override def getMap(ordinal: Int): ColumnarMap = wrapped.getMap(ordinal)\n+\n+  override def getDecimal(rowId: Int, precision: Int, scale: Int): Decimal =\n+    wrapped.getDecimal(rowId, precision, scale)\n+\n+  override def getUTF8String(rowId: Int): UTF8String = wrapped.getUTF8String(rowId)\n+\n+  override def getBinary(rowId: Int): Array[Byte] = wrapped.getBinary(rowId)\n+\n+  override protected def getChild(ordinal: Int): ColumnVector = wrapped.getChild(ordinal)\n+}\n+\n+trait ColumnarExpression extends Expression with Serializable {\n+  /**\n+   * Returns true if this expression supports columnar processing through [[columnarEval]].\n+   */\n+  def supportsColumnar: Boolean = true\n+\n+  /**\n+   * Returns the result of evaluating this expression on the entire\n+   * [[org.apache.spark.sql.vectorized.ColumnarBatch]]. The result of\n+   * calling this may be a single [[org.apache.spark.sql.vectorized.ColumnVector]] or a scalar\n+   * value. Scalar values typically happen if they are a part of the expression i.e. col(\"a\") + 100.",
    "line": 166
  }],
  "prId": 24795
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Not a big deal but for my own understanding, why is it `case class`?",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-07-04T10:05:24Z",
    "diffHunk": "@@ -251,6 +286,371 @@ object MyExtensions {\n     (_: Seq[Expression]) => Literal(5, IntegerType))\n }\n \n+case class CloseableColumnBatchIterator(itr: Iterator[ColumnarBatch],",
    "line": 72
  }, {
    "author": {
      "login": "revans2"
    },
    "body": "It doesn't have to be a case class, just made it so I didn't need to add in the new, but I am happy to change it if you want me to in a follow on PR.",
    "commit": "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "createdAt": "2019-07-08T13:01:34Z",
    "diffHunk": "@@ -251,6 +286,371 @@ object MyExtensions {\n     (_: Seq[Expression]) => Literal(5, IntegerType))\n }\n \n+case class CloseableColumnBatchIterator(itr: Iterator[ColumnarBatch],",
    "line": 72
  }],
  "prId": 24795
}]