[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "No need to add a new test suite. Just need to add a test case in `JDBCWriteSuite.scala`\r\n\r\n```\r\n  test(\"INSERT null to a NOT NULL column\") {\r\n    val e = intercept[SparkException] {\r\n      sql(\"INSERT INTO PEOPLE1 values (null, null)\")\r\n    }.getMessage\r\n    assert(e.contains(\"NULL not allowed for column \\\"NAME\\\"\"))\r\n  }\r\n```",
    "commit": "c071c2f9144672487c15bd624d0d5f47130e1f08",
    "createdAt": "2017-06-28T04:25:24Z",
    "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import java.io.File\n+import java.sql.DriverManager\n+import java.util.Properties\n+\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.util.Utils\n+import org.scalatest.BeforeAndAfter\n+\n+class JdbcInsertSuite extends DataSourceTest with BeforeAndAfter with SharedSQLContext {\n+  import testImplicits._\n+\n+  val url = \"jdbc:h2:mem:testdb0\"\n+  val urlWithUserAndPass = \"jdbc:h2:mem:testdb0;user=testUser;password=testPass\"\n+  var conn: java.sql.Connection = null\n+\n+\n+  protected override lazy val sql = spark.sql _\n+\n+  before {\n+    Utils.classForName(\"org.h2.Driver\")\n+    val properties = new Properties()\n+    properties.setProperty(\"user\", \"testUser\")\n+    properties.setProperty(\"password\", \"testPass\")\n+    properties.setProperty(\"rowId\", \"false\")\n+\n+    conn = DriverManager.getConnection(url, properties)\n+    conn.prepareStatement(\"create schema test\").executeUpdate()\n+    conn.prepareStatement(\n+      \"create table test.timestamp_test (id bigint(11) DEFAULT NULL, time_stamp TIMESTAMP NOT NULL)\").\n+      executeUpdate()\n+\n+    conn.commit()\n+\n+    sql(\n+      s\"\"\"\n+         |CREATE OR REPLACE TEMPORARY VIEW timestamp_test\n+         |USING org.apache.spark.sql.jdbc\n+         |OPTIONS (url '$url', dbtable 'test.timestamp_test', user 'testUser', password 'testPass')\n+       \"\"\".stripMargin.replaceAll(\"\\n\", \" \"))\n+  }\n+\n+  after {\n+    spark.catalog.dropTempView(\"jdbcTable\")\n+\n+    conn.prepareStatement(\"drop table test.timestamp_test\").executeUpdate()\n+    conn.prepareStatement(\"drop schema test\").executeUpdate()\n+\n+    conn.commit()\n+    conn.close()\n+  }\n+\n+  test(\"SPARK-19726 - Faild to insert null timestamp value to mysql using spark jdbc\") {\n+\n+    val message = intercept[Exception] {\n+      sql(\n+        s\"\"\"\n+           |INSERT INTO timestamp_test values(111, null)\n+      \"\"\".stripMargin)\n+    }.getMessage\n+\n+    assert(\n+      message.contains(\"NULL not allowed for column \\\"TIME_STAMP\\\"\"),\n+      \"It is not allowed to insert null into timestamp column which is defined not null.\"\n+    )"
  }],
  "prId": 18445
}]