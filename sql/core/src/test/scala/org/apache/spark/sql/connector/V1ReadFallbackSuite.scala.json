[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Can SQLContext be passed in when converting?",
    "commit": "211c580dc00d9f44f76ef556f9e30db6c902fed6",
    "createdAt": "2019-10-29T20:28:58Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.connector\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.connector.catalog.{Identifier, SupportsRead, Table, TableCapability, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.connector.read.{Scan, ScanBuilder, SupportsPushDownFilters, SupportsPushDownRequiredColumns, V1Scan}\n+import org.apache.spark.sql.execution.RowDataSourceScanExec\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, GreaterThan, TableScan}\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+abstract class V1ReadFallbackSuite extends QueryTest with SharedSparkSession {\n+  protected def baseTableScan(): DataFrame\n+\n+  test(\"full scan\") {\n+    val df = baseTableScan()\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    checkAnswer(df, Seq(Row(1, 10), Row(2, 20), Row(3, 30)))\n+  }\n+\n+  test(\"column pruning\") {\n+    val df = baseTableScan().select(\"i\")\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    assert(v1Scan.head.output.map(_.name) == Seq(\"i\"))\n+    checkAnswer(df, Seq(Row(1), Row(2), Row(3)))\n+  }\n+\n+  test(\"filter push down\") {\n+    val df = baseTableScan().filter(\"i > 1 and j < 30\")\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    // `j < 30` can't be pushed.\n+    assert(v1Scan.head.handledFilters.size == 1)\n+    checkAnswer(df, Seq(Row(2, 20)))\n+  }\n+\n+  test(\"filter push down + column pruning\") {\n+    val df = baseTableScan().filter(\"i > 1\").select(\"i\")\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    assert(v1Scan.head.output.map(_.name) == Seq(\"i\"))\n+    assert(v1Scan.head.handledFilters.size == 1)\n+    checkAnswer(df, Seq(Row(2), Row(3)))\n+  }\n+}\n+\n+class V1ReadFallbackWithDataFrameReaderSuite extends V1ReadFallbackSuite {\n+  override protected def baseTableScan(): DataFrame = {\n+    spark.read.format(classOf[V1ReadFallbackTableProvider].getName).load()\n+  }\n+}\n+\n+class V1ReadFallbackWithCatalogSuite extends V1ReadFallbackSuite {\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    spark.conf.set(\"spark.sql.catalog.read_fallback\", classOf[V1ReadFallbackCatalog].getName)\n+    sql(\"CREATE TABLE read_fallback.tbl(i int, j int) USING foo\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    spark.conf.unset(\"spark.sql.catalog.read_fallback\")\n+    super.afterAll()\n+  }\n+\n+  override protected def baseTableScan(): DataFrame = {\n+    spark.table(\"read_fallback.tbl\")\n+  }\n+}\n+\n+class V1ReadFallbackCatalog extends BasicInMemoryTableCatalog {\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    // To simplify the test implementation, only support fixed schema.\n+    if (schema != V1ReadFallbackCatalog.schema || partitions.nonEmpty) {\n+      throw new UnsupportedOperationException\n+    }\n+    val table = new TableWithV1ReadFallback\n+    tables.put(ident, table)\n+    table\n+  }\n+}\n+\n+object V1ReadFallbackCatalog {\n+  val schema = new StructType().add(\"i\", \"int\").add(\"j\", \"int\")\n+}\n+\n+class V1ReadFallbackTableProvider extends TableProvider {\n+  override def getTable(options: CaseInsensitiveStringMap): Table = {\n+    new TableWithV1ReadFallback\n+  }\n+}\n+\n+class TableWithV1ReadFallback extends Table with SupportsRead {\n+  override def name(): String = \"v1-read-fallback\"\n+\n+  override def schema(): StructType = V1ReadFallbackCatalog.schema\n+\n+  override def capabilities(): util.Set[TableCapability] = {\n+    Set(TableCapability.BATCH_READ).asJava\n+  }\n+\n+  override def newScanBuilder(options: CaseInsensitiveStringMap): ScanBuilder = {\n+    new V1ReadFallbackScanBuilder\n+  }\n+\n+  private class V1ReadFallbackScanBuilder extends ScanBuilder\n+    with SupportsPushDownRequiredColumns with SupportsPushDownFilters {\n+\n+    private var requiredSchema: StructType = schema()\n+    override def pruneColumns(requiredSchema: StructType): Unit = {\n+      this.requiredSchema = requiredSchema\n+    }\n+\n+    private var filters: Array[Filter] = Array.empty\n+    override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n+      val (supported, unsupported) = filters.partition {\n+        case GreaterThan(\"i\", _: Int) => true\n+        case _ => false\n+      }\n+      this.filters = supported\n+      unsupported\n+    }\n+    override def pushedFilters(): Array[Filter] = filters\n+\n+    override def build(): Scan = new V1ReadFallbackScan(requiredSchema, filters)\n+  }\n+\n+  private class V1ReadFallbackScan(\n+      requiredSchema: StructType,\n+      filters: Array[Filter]) extends V1Scan {\n+    override def readSchema(): StructType = requiredSchema\n+    override def toV1Relation(): BaseRelation = {\n+      new BaseRelation with TableScan {\n+        override def sqlContext: SQLContext = SparkSession.active.sqlContext"
  }],
  "prId": 26231
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "This should return the string version of the identifier it was loaded with.",
    "commit": "211c580dc00d9f44f76ef556f9e30db6c902fed6",
    "createdAt": "2019-10-29T20:30:10Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.connector\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.connector.catalog.{Identifier, SupportsRead, Table, TableCapability, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.connector.read.{Scan, ScanBuilder, SupportsPushDownFilters, SupportsPushDownRequiredColumns, V1Scan}\n+import org.apache.spark.sql.execution.RowDataSourceScanExec\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, GreaterThan, TableScan}\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+abstract class V1ReadFallbackSuite extends QueryTest with SharedSparkSession {\n+  protected def baseTableScan(): DataFrame\n+\n+  test(\"full scan\") {\n+    val df = baseTableScan()\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    checkAnswer(df, Seq(Row(1, 10), Row(2, 20), Row(3, 30)))\n+  }\n+\n+  test(\"column pruning\") {\n+    val df = baseTableScan().select(\"i\")\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    assert(v1Scan.head.output.map(_.name) == Seq(\"i\"))\n+    checkAnswer(df, Seq(Row(1), Row(2), Row(3)))\n+  }\n+\n+  test(\"filter push down\") {\n+    val df = baseTableScan().filter(\"i > 1 and j < 30\")\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    // `j < 30` can't be pushed.\n+    assert(v1Scan.head.handledFilters.size == 1)\n+    checkAnswer(df, Seq(Row(2, 20)))\n+  }\n+\n+  test(\"filter push down + column pruning\") {\n+    val df = baseTableScan().filter(\"i > 1\").select(\"i\")\n+    val v1Scan = df.queryExecution.executedPlan.collect {\n+      case s: RowDataSourceScanExec => s\n+    }\n+    assert(v1Scan.length == 1)\n+    assert(v1Scan.head.output.map(_.name) == Seq(\"i\"))\n+    assert(v1Scan.head.handledFilters.size == 1)\n+    checkAnswer(df, Seq(Row(2), Row(3)))\n+  }\n+}\n+\n+class V1ReadFallbackWithDataFrameReaderSuite extends V1ReadFallbackSuite {\n+  override protected def baseTableScan(): DataFrame = {\n+    spark.read.format(classOf[V1ReadFallbackTableProvider].getName).load()\n+  }\n+}\n+\n+class V1ReadFallbackWithCatalogSuite extends V1ReadFallbackSuite {\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    spark.conf.set(\"spark.sql.catalog.read_fallback\", classOf[V1ReadFallbackCatalog].getName)\n+    sql(\"CREATE TABLE read_fallback.tbl(i int, j int) USING foo\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    spark.conf.unset(\"spark.sql.catalog.read_fallback\")\n+    super.afterAll()\n+  }\n+\n+  override protected def baseTableScan(): DataFrame = {\n+    spark.table(\"read_fallback.tbl\")\n+  }\n+}\n+\n+class V1ReadFallbackCatalog extends BasicInMemoryTableCatalog {\n+  override def createTable(\n+      ident: Identifier,\n+      schema: StructType,\n+      partitions: Array[Transform],\n+      properties: util.Map[String, String]): Table = {\n+    // To simplify the test implementation, only support fixed schema.\n+    if (schema != V1ReadFallbackCatalog.schema || partitions.nonEmpty) {\n+      throw new UnsupportedOperationException\n+    }\n+    val table = new TableWithV1ReadFallback\n+    tables.put(ident, table)\n+    table\n+  }\n+}\n+\n+object V1ReadFallbackCatalog {\n+  val schema = new StructType().add(\"i\", \"int\").add(\"j\", \"int\")\n+}\n+\n+class V1ReadFallbackTableProvider extends TableProvider {\n+  override def getTable(options: CaseInsensitiveStringMap): Table = {\n+    new TableWithV1ReadFallback\n+  }\n+}\n+\n+class TableWithV1ReadFallback extends Table with SupportsRead {\n+  override def name(): String = \"v1-read-fallback\""
  }],
  "prId": 26231
}]