[{
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Should these be used the same variable name, `sparkContext` or `sc`?",
    "commit": "3886e093089cd70d655b64c5995a3d438a03ff18",
    "createdAt": "2017-03-29T05:59:29Z",
    "diffHunk": "@@ -477,9 +477,11 @@ private case class MyPlan(sc: SparkContext, expectedValue: Long) extends LeafExe\n \n   override def doExecute(): RDD[InternalRow] = {\n     longMetric(\"dummy\") += expectedValue\n-    sc.listenerBus.post(SparkListenerDriverAccumUpdates(\n-      sc.getLocalProperty(SQLExecution.EXECUTION_ID_KEY).toLong,\n-      metrics.values.map(m => m.id -> m.value).toSeq))\n+\n+    SQLMetrics.postDriverMetricUpdates(\n+      sparkContext,\n+      sc.getLocalProperty(SQLExecution.EXECUTION_ID_KEY),"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "hm for some reason both works. i agree it'd be better to be the same.\r\n",
    "commit": "3886e093089cd70d655b64c5995a3d438a03ff18",
    "createdAt": "2017-03-29T06:35:54Z",
    "diffHunk": "@@ -477,9 +477,11 @@ private case class MyPlan(sc: SparkContext, expectedValue: Long) extends LeafExe\n \n   override def doExecute(): RDD[InternalRow] = {\n     longMetric(\"dummy\") += expectedValue\n-    sc.listenerBus.post(SparkListenerDriverAccumUpdates(\n-      sc.getLocalProperty(SQLExecution.EXECUTION_ID_KEY).toLong,\n-      metrics.values.map(m => m.id -> m.value).toSeq))\n+\n+    SQLMetrics.postDriverMetricUpdates(\n+      sparkContext,\n+      sc.getLocalProperty(SQLExecution.EXECUTION_ID_KEY),"
  }],
  "prId": 17464
}]