[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shouldn't we use `sql` here? ",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-23T11:10:38Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.parquet\n+\n+import java.io.File\n+\n+import scala.util.Try\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark to measure parquet write performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object ParquetWriteBenchmark {\n+  val conf = new SparkConf()\n+  conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"parquet-write-benchmark\")\n+    .config(conf)\n+    .getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withSQLConf(pairs: (String, String)*)(f: => Unit): Unit = {\n+    val (keys, values) = pairs.unzip\n+    val currentValues = keys.map(key => Try(spark.conf.get(key)).toOption)\n+    (keys, values).zipped.foreach(spark.conf.set)\n+    try f finally {\n+      keys.zip(currentValues).foreach {\n+        case (key, Some(value)) => spark.conf.set(key, value)\n+        case (key, None) => spark.conf.unset(key)\n+      }\n+    }\n+  }\n+\n+  def runSQL(name: String, sql: String, values: Int): Unit = {\n+    withTempTable(\"t1\") {\n+      spark.range(values).createOrReplaceTempView(\"t1\")\n+      val benchmark = new Benchmark(name, values)\n+      benchmark.addCase(\"Parquet Writer\") { _ =>\n+        withTempPath { dir =>\n+          spark.sql(\"select cast(id as INT) as id from t1\").write.parquet(dir.getCanonicalPath)"
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we don't need this",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-23T11:11:38Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.parquet\n+\n+import java.io.File\n+\n+import scala.util.Try\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark to measure parquet write performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object ParquetWriteBenchmark {\n+  val conf = new SparkConf()\n+  conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"parquet-write-benchmark\")\n+    .config(conf)\n+    .getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withSQLConf(pairs: (String, String)*)(f: => Unit): Unit = {\n+    val (keys, values) = pairs.unzip\n+    val currentValues = keys.map(key => Try(spark.conf.get(key)).toOption)\n+    (keys, values).zipped.foreach(spark.conf.set)\n+    try f finally {\n+      keys.zip(currentValues).foreach {\n+        case (key, Some(value)) => spark.conf.set(key, value)\n+        case (key, None) => spark.conf.unset(key)\n+      }\n+    }\n+  }\n+\n+  def runSQL(name: String, sql: String, values: Int): Unit = {\n+    withTempTable(\"t1\") {\n+      spark.range(values).createOrReplaceTempView(\"t1\")\n+      val benchmark = new Benchmark(name, values)\n+      benchmark.addCase(\"Parquet Writer\") { _ =>\n+        withTempPath { dir =>\n+          spark.sql(\"select cast(id as INT) as id from t1\").write.parquet(dir.getCanonicalPath)\n+        }\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def intWriteBenchmark(values: Int): Unit = {\n+    runSQL(\"Output Single Int Column\", \"select cast(id as INT) as id from t1\", values)\n+  }\n+\n+  def intStringScanBenchmark(values: Int): Unit = {\n+    runSQL(name = \"Output Int and String Column\",\n+      sql = \"select cast(id as INT) as c1, cast(id as STRING) as c2 from t1\",\n+      values = values)\n+  }\n+\n+  def stringWithNullsScanBenchmark(values: Int, fractionOfNulls: Double): Unit = {"
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: Is there any reason to use named argument for this `runSQL` and not to use name argument in another `runSQL`?",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-23T12:47:56Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.parquet\n+\n+import java.io.File\n+\n+import scala.util.Try\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark to measure parquet write performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object ParquetWriteBenchmark {\n+  val conf = new SparkConf()\n+  conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"parquet-write-benchmark\")\n+    .config(conf)\n+    .getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withSQLConf(pairs: (String, String)*)(f: => Unit): Unit = {\n+    val (keys, values) = pairs.unzip\n+    val currentValues = keys.map(key => Try(spark.conf.get(key)).toOption)\n+    (keys, values).zipped.foreach(spark.conf.set)\n+    try f finally {\n+      keys.zip(currentValues).foreach {\n+        case (key, Some(value)) => spark.conf.set(key, value)\n+        case (key, None) => spark.conf.unset(key)\n+      }\n+    }\n+  }\n+\n+  def runSQL(name: String, sql: String, values: Int): Unit = {\n+    withTempTable(\"t1\") {\n+      spark.range(values).createOrReplaceTempView(\"t1\")\n+      val benchmark = new Benchmark(name, values)\n+      benchmark.addCase(\"Parquet Writer\") { _ =>\n+        withTempPath { dir =>\n+          spark.sql(sql).write.parquet(dir.getCanonicalPath)\n+        }\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def intWriteBenchmark(values: Int): Unit = {\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+\n+    Output Single Int Column:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Parquet Writer                                2536 / 2610          6.2         161.3       1.0X\n+    */\n+    runSQL(\"Output Single Int Column\", \"select cast(id as INT) as id from t1\", values)\n+  }\n+\n+  def intStringWriteBenchmark(values: Int): Unit = {\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+\n+    Output Int and String Column:            Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Parquet Writer                                4644 / 4673          2.3         442.9       1.0X\n+    */\n+    runSQL(name = \"Output Int and String Column\","
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "let's also use named parameter here.",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-23T14:03:45Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.parquet\n+\n+import java.io.File\n+\n+import scala.util.Try\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark to measure parquet write performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object ParquetWriteBenchmark {\n+  val conf = new SparkConf()\n+  conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"parquet-write-benchmark\")\n+    .config(conf)\n+    .getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withSQLConf(pairs: (String, String)*)(f: => Unit): Unit = {\n+    val (keys, values) = pairs.unzip\n+    val currentValues = keys.map(key => Try(spark.conf.get(key)).toOption)\n+    (keys, values).zipped.foreach(spark.conf.set)\n+    try f finally {\n+      keys.zip(currentValues).foreach {\n+        case (key, Some(value)) => spark.conf.set(key, value)\n+        case (key, None) => spark.conf.unset(key)\n+      }\n+    }\n+  }\n+\n+  def runSQL(name: String, sql: String, values: Int): Unit = {\n+    withTempTable(\"t1\") {\n+      spark.range(values).createOrReplaceTempView(\"t1\")\n+      val benchmark = new Benchmark(name, values)\n+      benchmark.addCase(\"Parquet Writer\") { _ =>\n+        withTempPath { dir =>\n+          spark.sql(sql).write.parquet(dir.getCanonicalPath)\n+        }\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def intWriteBenchmark(values: Int): Unit = {\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+\n+    Output Single Int Column:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Parquet Writer                                2536 / 2610          6.2         161.3       1.0X\n+    */\n+    runSQL(\"Output Single Int Column\", \"select cast(id as INT) as id from t1\", values)"
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we should also test bucket.",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-23T14:05:54Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.parquet\n+\n+import java.io.File\n+\n+import scala.util.Try\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark to measure parquet write performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object ParquetWriteBenchmark {\n+  val conf = new SparkConf()\n+  conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"parquet-write-benchmark\")\n+    .config(conf)\n+    .getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withSQLConf(pairs: (String, String)*)(f: => Unit): Unit = {\n+    val (keys, values) = pairs.unzip\n+    val currentValues = keys.map(key => Try(spark.conf.get(key)).toOption)\n+    (keys, values).zipped.foreach(spark.conf.set)\n+    try f finally {\n+      keys.zip(currentValues).foreach {\n+        case (key, Some(value)) => spark.conf.set(key, value)\n+        case (key, None) => spark.conf.unset(key)\n+      }\n+    }\n+  }\n+\n+  def runSQL(name: String, sql: String, values: Int): Unit = {\n+    withTempTable(\"t1\") {\n+      spark.range(values).createOrReplaceTempView(\"t1\")\n+      val benchmark = new Benchmark(name, values)\n+      benchmark.addCase(\"Parquet Writer\") { _ =>\n+        withTempPath { dir =>\n+          spark.sql(sql).write.parquet(dir.getCanonicalPath)\n+        }\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def intWriteBenchmark(values: Int): Unit = {\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+\n+    Output Single Int Column:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Parquet Writer                                2536 / 2610          6.2         161.3       1.0X\n+    */\n+    runSQL(\"Output Single Int Column\", \"select cast(id as INT) as id from t1\", values)\n+  }\n+\n+  def intStringWriteBenchmark(values: Int): Unit = {\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+\n+    Output Int and String Column:            Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Parquet Writer                                4644 / 4673          2.3         442.9       1.0X\n+    */\n+    runSQL(name = \"Output Int and String Column\",\n+      sql = \"select cast(id as INT) as c1, cast(id as STRING) as c2 from t1\",\n+      values = values)\n+  }\n+\n+  def partitionTableWriteBenchmark(values: Int): Unit = {\n+    withTempTable(\"t1\") {\n+      spark.range(values).createOrReplaceTempView(\"t1\")\n+      val benchmark = new Benchmark(\"Partitioned Table\", values)\n+      benchmark.addCase(\"Parquet Writer\") { _ =>\n+        withTempPath { dir =>\n+          spark.sql(\"select id % 2 as p, cast(id as INT) as id from t1\")\n+            .write.partitionBy(\"p\").parquet(dir.getCanonicalPath)\n+        }\n+      }\n+\n+      /*\n+      Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+\n+      Partitioned Table:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+      ---------------------------------------------------------------------------------------------\n+      Parquet Writer                             4163 / 4173          3.8         264.7       1.0X\n+      */\n+      benchmark.run()\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+    intWriteBenchmark(1024 * 1024 * 15)\n+    intStringWriteBenchmark(1024 * 1024 * 10)\n+    partitionTableWriteBenchmark(1024 * 1024 * 15)"
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "the benchmark workflow should be\r\n1. create a table\r\n2. run INSERT OVERWRITE\r\n\r\nthen all the cases can use `runSQL`",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-23T14:08:02Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.datasources.parquet\n+\n+import java.io.File\n+\n+import scala.util.Try\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark to measure parquet write performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object ParquetWriteBenchmark {\n+  val conf = new SparkConf()\n+  conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"parquet-write-benchmark\")\n+    .config(conf)\n+    .getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withSQLConf(pairs: (String, String)*)(f: => Unit): Unit = {\n+    val (keys, values) = pairs.unzip\n+    val currentValues = keys.map(key => Try(spark.conf.get(key)).toOption)\n+    (keys, values).zipped.foreach(spark.conf.set)\n+    try f finally {\n+      keys.zip(currentValues).foreach {\n+        case (key, Some(value)) => spark.conf.set(key, value)\n+        case (key, None) => spark.conf.unset(key)\n+      }\n+    }\n+  }\n+\n+  def runSQL(name: String, sql: String, values: Int): Unit = {"
  }],
  "prId": 21409
}]