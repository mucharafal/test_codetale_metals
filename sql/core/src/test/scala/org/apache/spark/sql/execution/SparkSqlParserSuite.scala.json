[{
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Although we have covered most of the cases in `DDLSuite` and `DDLCommandSuite`, it is still useful to test sql parser directly in `SparkSqlParserSuite`.\n",
    "commit": "605aeb60bedfbc042c237360345951ec5593e7a3",
    "createdAt": "2016-10-04T17:24:01Z",
    "diffHunk": "@@ -67,9 +86,331 @@ class SparkSqlParserSuite extends PlanTest {\n       DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = None), isExtended = true))\n     assertEqual(\"describe function foo.bar\",\n       DescribeFunctionCommand(\n-        FunctionIdentifier(\"bar\", database = Option(\"foo\")), isExtended = false))\n+        FunctionIdentifier(\"bar\", database = Some(\"foo\")), isExtended = false))\n     assertEqual(\"describe function extended f.bar\",\n-      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Option(\"f\")), isExtended = true))\n+      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Some(\"f\")), isExtended = true))\n+  }\n+\n+  private def createTableUsing(\n+      table: String,\n+      database: Option[String] = None,\n+      tableType: CatalogTableType = CatalogTableType.MANAGED,\n+      storage: CatalogStorageFormat = CatalogStorageFormat.empty,\n+      schema: StructType = new StructType,\n+      provider: Option[String] = Some(\"parquet\"),\n+      partitionColumnNames: Seq[String] = Seq.empty,\n+      bucketSpec: Option[BucketSpec] = None,\n+      mode: SaveMode = SaveMode.ErrorIfExists,\n+      query: Option[LogicalPlan] = None): CreateTable = {\n+    CreateTable(\n+      CatalogTable(\n+        identifier = TableIdentifier(table, database),\n+        tableType = tableType,\n+        storage = storage,\n+        schema = schema,\n+        provider = provider,\n+        partitionColumnNames = partitionColumnNames,\n+        bucketSpec = bucketSpec\n+      ), mode, query\n+    )\n+  }\n+\n+  private def createTempViewUsing(\n+      table: String,\n+      database: Option[String] = None,\n+      schema: Option[StructType] = None,\n+      replace: Boolean = true,\n+      provider: String = \"parquet\",\n+      options: Map[String, String] = Map.empty): LogicalPlan = {\n+    CreateTempViewUsing(TableIdentifier(table, database), schema, replace, provider, options)\n+  }\n+\n+  private def createTable(\n+      table: String,\n+      database: Option[String] = None,\n+      tableType: CatalogTableType = CatalogTableType.MANAGED,\n+      storage: CatalogStorageFormat = CatalogStorageFormat.empty.copy(\n+        inputFormat = HiveSerDe.sourceToSerDe(\"textfile\").get.inputFormat,\n+        outputFormat = HiveSerDe.sourceToSerDe(\"textfile\").get.outputFormat),\n+      schema: StructType = new StructType,\n+      provider: Option[String] = Some(\"hive\"),\n+      partitionColumnNames: Seq[String] = Seq.empty,\n+      comment: Option[String] = None,\n+      mode: SaveMode = SaveMode.ErrorIfExists,\n+      query: Option[LogicalPlan] = None): CreateTable = {\n+    CreateTable(\n+      CatalogTable(\n+        identifier = TableIdentifier(table, database),\n+        tableType = tableType,\n+        storage = storage,\n+        schema = schema,\n+        provider = provider,\n+        partitionColumnNames = partitionColumnNames,\n+        comment = comment\n+      ), mode, query\n+    )\n+  }\n+\n+  test(\"create table using - createTableHeader\") {"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Most of these unit tests are not related to this change, lets move these into a separate PR. Just put a couple of cases (at least one positive and one negative case) in here testing your change.\n",
    "commit": "605aeb60bedfbc042c237360345951ec5593e7a3",
    "createdAt": "2016-10-05T01:42:01Z",
    "diffHunk": "@@ -67,9 +86,331 @@ class SparkSqlParserSuite extends PlanTest {\n       DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = None), isExtended = true))\n     assertEqual(\"describe function foo.bar\",\n       DescribeFunctionCommand(\n-        FunctionIdentifier(\"bar\", database = Option(\"foo\")), isExtended = false))\n+        FunctionIdentifier(\"bar\", database = Some(\"foo\")), isExtended = false))\n     assertEqual(\"describe function extended f.bar\",\n-      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Option(\"f\")), isExtended = true))\n+      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Some(\"f\")), isExtended = true))\n+  }\n+\n+  private def createTableUsing(\n+      table: String,\n+      database: Option[String] = None,\n+      tableType: CatalogTableType = CatalogTableType.MANAGED,\n+      storage: CatalogStorageFormat = CatalogStorageFormat.empty,\n+      schema: StructType = new StructType,\n+      provider: Option[String] = Some(\"parquet\"),\n+      partitionColumnNames: Seq[String] = Seq.empty,\n+      bucketSpec: Option[BucketSpec] = None,\n+      mode: SaveMode = SaveMode.ErrorIfExists,\n+      query: Option[LogicalPlan] = None): CreateTable = {\n+    CreateTable(\n+      CatalogTable(\n+        identifier = TableIdentifier(table, database),\n+        tableType = tableType,\n+        storage = storage,\n+        schema = schema,\n+        provider = provider,\n+        partitionColumnNames = partitionColumnNames,\n+        bucketSpec = bucketSpec\n+      ), mode, query\n+    )\n+  }\n+\n+  private def createTempViewUsing(\n+      table: String,\n+      database: Option[String] = None,\n+      schema: Option[StructType] = None,\n+      replace: Boolean = true,\n+      provider: String = \"parquet\",\n+      options: Map[String, String] = Map.empty): LogicalPlan = {\n+    CreateTempViewUsing(TableIdentifier(table, database), schema, replace, provider, options)\n+  }\n+\n+  private def createTable(\n+      table: String,\n+      database: Option[String] = None,\n+      tableType: CatalogTableType = CatalogTableType.MANAGED,\n+      storage: CatalogStorageFormat = CatalogStorageFormat.empty.copy(\n+        inputFormat = HiveSerDe.sourceToSerDe(\"textfile\").get.inputFormat,\n+        outputFormat = HiveSerDe.sourceToSerDe(\"textfile\").get.outputFormat),\n+      schema: StructType = new StructType,\n+      provider: Option[String] = Some(\"hive\"),\n+      partitionColumnNames: Seq[String] = Seq.empty,\n+      comment: Option[String] = None,\n+      mode: SaveMode = SaveMode.ErrorIfExists,\n+      query: Option[LogicalPlan] = None): CreateTable = {\n+    CreateTable(\n+      CatalogTable(\n+        identifier = TableIdentifier(table, database),\n+        tableType = tableType,\n+        storage = storage,\n+        schema = schema,\n+        provider = provider,\n+        partitionColumnNames = partitionColumnNames,\n+        comment = comment\n+      ), mode, query\n+    )\n+  }\n+\n+  test(\"create table using - createTableHeader\") {"
  }],
  "prId": 15346
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "The colon issue of table schema is inconvenient to test in `DataTypeParserSuite`, so we test the cases here.\n",
    "commit": "605aeb60bedfbc042c237360345951ec5593e7a3",
    "createdAt": "2016-10-05T09:51:32Z",
    "diffHunk": "@@ -67,9 +86,133 @@ class SparkSqlParserSuite extends PlanTest {\n       DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = None), isExtended = true))\n     assertEqual(\"describe function foo.bar\",\n       DescribeFunctionCommand(\n-        FunctionIdentifier(\"bar\", database = Option(\"foo\")), isExtended = false))\n+        FunctionIdentifier(\"bar\", database = Some(\"foo\")), isExtended = false))\n     assertEqual(\"describe function extended f.bar\",\n-      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Option(\"f\")), isExtended = true))\n+      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Some(\"f\")), isExtended = true))\n+  }\n+\n+  private def createTableUsing(\n+      table: String,\n+      database: Option[String] = None,\n+      tableType: CatalogTableType = CatalogTableType.MANAGED,\n+      storage: CatalogStorageFormat = CatalogStorageFormat.empty,\n+      schema: StructType = new StructType,\n+      provider: Option[String] = Some(\"parquet\"),\n+      partitionColumnNames: Seq[String] = Seq.empty,\n+      bucketSpec: Option[BucketSpec] = None,\n+      mode: SaveMode = SaveMode.ErrorIfExists,\n+      query: Option[LogicalPlan] = None): CreateTable = {\n+    CreateTable(\n+      CatalogTable(\n+        identifier = TableIdentifier(table, database),\n+        tableType = tableType,\n+        storage = storage,\n+        schema = schema,\n+        provider = provider,\n+        partitionColumnNames = partitionColumnNames,\n+        bucketSpec = bucketSpec\n+      ), mode, query\n+    )\n+  }\n+\n+  private def createTempViewUsing(\n+      table: String,\n+      database: Option[String] = None,\n+      schema: Option[StructType] = None,\n+      replace: Boolean = true,\n+      provider: String = \"parquet\",\n+      options: Map[String, String] = Map.empty): LogicalPlan = {\n+    CreateTempViewUsing(TableIdentifier(table, database), schema, replace, provider, options)\n+  }\n+\n+  private def createTable(\n+      table: String,\n+      database: Option[String] = None,\n+      tableType: CatalogTableType = CatalogTableType.MANAGED,\n+      storage: CatalogStorageFormat = CatalogStorageFormat.empty.copy(\n+        inputFormat = HiveSerDe.sourceToSerDe(\"textfile\").get.inputFormat,\n+        outputFormat = HiveSerDe.sourceToSerDe(\"textfile\").get.outputFormat),\n+      schema: StructType = new StructType,\n+      provider: Option[String] = Some(\"hive\"),\n+      partitionColumnNames: Seq[String] = Seq.empty,\n+      comment: Option[String] = None,\n+      mode: SaveMode = SaveMode.ErrorIfExists,\n+      query: Option[LogicalPlan] = None): CreateTable = {\n+    CreateTable(\n+      CatalogTable(\n+        identifier = TableIdentifier(table, database),\n+        tableType = tableType,\n+        storage = storage,\n+        schema = schema,\n+        provider = provider,\n+        partitionColumnNames = partitionColumnNames,\n+        comment = comment\n+      ), mode, query\n+    )\n   }\n \n+  test(\"create table - schema\") {\n+    assertEqual(\"CREATE TABLE my_tab(a INT COMMENT 'test', b STRING)\",\n+      createTable(\n+        table = \"my_tab\",\n+        schema = (new StructType)\n+          .add(\"a\", IntegerType, nullable = true, \"test\")\n+          .add(\"b\", StringType)\n+      )\n+    )\n+    assertEqual(\"CREATE TABLE my_tab(a INT COMMENT 'test', b STRING) \" +\n+      \"PARTITIONED BY (c INT, d STRING COMMENT 'test2')\",\n+      createTable(\n+        table = \"my_tab\",\n+        schema = (new StructType)\n+          .add(\"a\", IntegerType, nullable = true, \"test\")\n+          .add(\"b\", StringType)\n+          .add(\"c\", IntegerType)\n+          .add(\"d\", StringType, nullable = true, \"test2\"),\n+        partitionColumnNames = Seq(\"c\", \"d\")\n+      )\n+    )\n+    assertEqual(\"CREATE TABLE my_tab(id BIGINT, nested STRUCT<col1: STRING,col2: INT>)\",\n+      createTable(\n+        table = \"my_tab\",\n+        schema = (new StructType)\n+          .add(\"id\", LongType)\n+          .add(\"nested\", (new StructType)\n+            .add(\"col1\", StringType)\n+            .add(\"col2\", IntegerType)\n+          )\n+      )\n+    )\n+    // Partitioned by a StructType should be accepted by `SparkSqlParser` but will fail an analyze\n+    // rule in `AnalyzeCreateTable`.\n+    assertEqual(\"CREATE TABLE my_tab(a INT COMMENT 'test', b STRING) \" +\n+      \"PARTITIONED BY (nested STRUCT<col1: STRING,col2: INT>)\",\n+      createTable(\n+        table = \"my_tab\",\n+        schema = (new StructType)\n+          .add(\"a\", IntegerType, nullable = true, \"test\")\n+          .add(\"b\", StringType)\n+          .add(\"nested\", (new StructType)\n+            .add(\"col1\", StringType)\n+            .add(\"col2\", IntegerType)\n+          ),\n+        partitionColumnNames = Seq(\"nested\")\n+      )\n+    )\n+    intercept(\"CREATE TABLE my_tab(a: INT COMMENT 'test', b: STRING)\",",
    "line": 163
  }],
  "prId": 15346
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "yeah, this newly added function seems unused.\n",
    "commit": "605aeb60bedfbc042c237360345951ec5593e7a3",
    "createdAt": "2016-10-10T11:52:33Z",
    "diffHunk": "@@ -68,9 +87,134 @@ class SparkSqlParserSuite extends PlanTest {\n       DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = None), isExtended = true))\n     assertEqual(\"describe function foo.bar\",\n       DescribeFunctionCommand(\n-        FunctionIdentifier(\"bar\", database = Option(\"foo\")), isExtended = false))\n+        FunctionIdentifier(\"bar\", database = Some(\"foo\")), isExtended = false))\n     assertEqual(\"describe function extended f.bar\",\n-      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Option(\"f\")), isExtended = true))\n+      DescribeFunctionCommand(FunctionIdentifier(\"bar\", database = Some(\"f\")), isExtended = true))\n+  }\n+\n+  private def createTableUsing(\n+      table: String,\n+      database: Option[String] = None,\n+      tableType: CatalogTableType = CatalogTableType.MANAGED,\n+      storage: CatalogStorageFormat = CatalogStorageFormat.empty,\n+      schema: StructType = new StructType,\n+      provider: Option[String] = Some(\"parquet\"),\n+      partitionColumnNames: Seq[String] = Seq.empty,\n+      bucketSpec: Option[BucketSpec] = None,\n+      mode: SaveMode = SaveMode.ErrorIfExists,\n+      query: Option[LogicalPlan] = None): CreateTable = {\n+    CreateTable(\n+      CatalogTable(\n+        identifier = TableIdentifier(table, database),\n+        tableType = tableType,\n+        storage = storage,\n+        schema = schema,\n+        provider = provider,\n+        partitionColumnNames = partitionColumnNames,\n+        bucketSpec = bucketSpec\n+      ), mode, query\n+    )\n+  }\n+\n+  private def createTempViewUsing(",
    "line": 79
  }],
  "prId": 15346
}]