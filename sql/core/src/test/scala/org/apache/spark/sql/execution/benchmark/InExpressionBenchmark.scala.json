[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "It seems that we missed `200` only here.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T06:06:22Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      runByteBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 100, largeNumRows, minNumIters)"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "I skipped this intentionally to avoid hitting overflow for byte values. We can change the way we generate bytes: `(Byte.MinValue to Byte.MinValue + numItems).map(v => s\"${v}Y\")`. It just looks a bit more involved and I wanted to keep it consistent with other cases. I do not mind changing, though. Let me know your opinion.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T19:21:27Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      runByteBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 100, largeNumRows, minNumIters)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Oh, I got it. And, +1 for the new way starting from `MinValue`.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T19:40:17Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      runByteBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 100, largeNumRows, minNumIters)"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`FractionalProxy.to` is deprecated in Scala 2.12 and is already removed at Scala 2.13. Shall we avoid this syntax?\r\n- https://github.com/scala/scala/blob/2.12.x/src/library/scala/runtime/ScalaNumberProxy.scala#L73",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T06:22:09Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Like [line 64](https://github.com/apache/spark/pull/23291/files#diff-4f10bb3e29874544edecf6788e33088fR64), shall we use literal? `Y` is the postfix for tinyint.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T06:46:27Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "ditto. `S` is the postfix for small int literal.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T06:46:47Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Also, this will generate `decimal`. Please note that `1.0` is `decimal(2,1)`. For double literal, please use `1.0D`.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T06:48:44Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)",
    "line": 83
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "And, shorter is better.\r\n```scala\r\nSeq(5, 10, 25, 50, 100, 200).foreach { numItems =>\r\n  runShortBenchmark(numItems, largeNumRows, minNumIters)\r\n}\r\n```",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T06:54:01Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      runByteBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 100, largeNumRows, minNumIters)\n+\n+      runShortBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 100, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 200, largeNumRows, minNumIters)"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "What about this?\r\n\r\n```\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    val largeNumRows = 10000000\r\n    val smallNumRows = 1000000\r\n    val minNumIters = 5\r\n\r\n    runBenchmark(\"In Expression Benchmark\") {\r\n      Seq(5, 10, 25, 50, 100, 200).foreach { numItems =>\r\n        runByteBenchmark(numItems, largeNumRows, minNumIters)\r\n        runShortBenchmark(numItems, largeNumRows, minNumIters)\r\n        runIntBenchmark(numItems, largeNumRows, minNumIters)\r\n        runLongBenchmark(numItems, largeNumRows, minNumIters)\r\n        runFloatBenchmark(numItems, largeNumRows, minNumIters)\r\n        runDoubleBenchmark(numItems, largeNumRows, minNumIters)\r\n        runSmallDecimalBenchmark(numItems, smallNumRows, minNumIters)\r\n        runLargeDecimalBenchmark(numItems, smallNumRows, minNumIters)\r\n        runStringBenchmark(numItems, smallNumRows, minNumIters)\r\n        runTimestampBenchmark(numItems, largeNumRows, minNumIters)\r\n        runDateBenchmark(numItems, largeNumRows, minNumIters)\r\n        runArrayBenchmark(numItems, smallNumRows, minNumIters)\r\n        runStructBenchmark(numItems, smallNumRows, minNumIters)\r\n      }\r\n    }\r\n  }\r\n```",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T19:57:27Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      runByteBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 100, largeNumRows, minNumIters)\n+\n+      runShortBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 100, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 200, largeNumRows, minNumIters)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "It'll be difficult to see the result. You need to go back and forward to see the single type trends.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-19T20:19:43Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      runByteBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runByteBenchmark(numItems = 100, largeNumRows, minNumIters)\n+\n+      runShortBenchmark(numItems = 5, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 10, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 25, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 50, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 100, largeNumRows, minNumIters)\n+      runShortBenchmark(numItems = 200, largeNumRows, minNumIters)"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Just `org.apache.spark.sql.types._`?",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-12T09:17:31Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "Done",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-14T08:41:24Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Why some benchmarks use large num rows, others use small num rows?",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-12T09:23:57Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val numItemsSeq = Seq(5, 10, 25, 50, 100, 200)\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      numItemsSeq.foreach { numItems =>\n+        runByteBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runShortBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runIntBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runLongBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runFloatBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runDoubleBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runSmallDecimalBenchmark(numItems, smallNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runLargeDecimalBenchmark(numItems, smallNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runStringBenchmark(numItems, smallNumRows, minNumIters)",
    "line": 198
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "This is done to shorten the execution time for benchmarks that take longer. The ratio stays identical.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-14T08:27:20Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runBenchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Unit = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values.mkString(\",\")})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.run()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val numItemsSeq = Seq(5, 10, 25, 50, 100, 200)\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"In Expression Benchmark\") {\n+      numItemsSeq.foreach { numItems =>\n+        runByteBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runShortBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runIntBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runLongBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runFloatBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runDoubleBenchmark(numItems, largeNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runSmallDecimalBenchmark(numItems, smallNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runLargeDecimalBenchmark(numItems, smallNumRows, minNumIters)\n+      }\n+      numItemsSeq.foreach { numItems =>\n+        runStringBenchmark(numItems, smallNumRows, minNumIters)",
    "line": 198
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "The range of `numItems` (max: 200) is relatively smaller than `numRows` (max: 10000000). So most of time, In/InSet expressions can't find the value in given list. I think it is naturally bad for In expression's performance since it compares all values in the list.\r\n\r\nIs this a fair performance comparison between In and InSet expressions?",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-12T09:59:10Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))",
    "line": 131
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "I think we are interested in the worst case performance. I believe this benchmark is close to a real use case as most of the time we search for a relatively small set of values in a huge distributed data set.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-14T08:31:24Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))",
    "line": 131
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Can you add this description in the beginning comment of benchmark code? It is better to explain this is for worse case performance comparisons.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-14T08:57:25Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))",
    "line": 131
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "I am still not sure here. The main idea behind converting `In` to `InSet` is to avoid iterating through all elements. This benchmark mostly tests when this optimization works and when it makes things even worse. I tend to think that having `In` expression on a real data set that exits after a few branches is rather an edge case. I can be convinced but if there is no strong community's opinion on this, I would keep it as it is.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-15T09:29:17Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))",
    "line": 131
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "@viirya what about having this?\r\n\r\n```\r\n/**\r\n * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\r\n *\r\n * Specifically, this class compares the if-based approach, which might iterate through all items\r\n * inside the IN value list, to other options with better worst-case time complexities (e.g., sets).\r\n *\r\n * To run this benchmark:\r\n * {{{\r\n *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\r\n *   2. build/sbt \"sql/test:runMain <this class>\"\r\n *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\r\n *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\r\n * }}}\r\n */\r\n```",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-15T13:54:38Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))",
    "line": 131
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "LGTM, thanks @aokolnychyi ",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2019-01-15T14:06:40Z",
    "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType, ShortType, StringType, TimestampType}\n+\n+/**\n+ * A benchmark that compares the performance of different ways to evaluate SQL IN expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InExpressionBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InExpressionBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private def runByteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems bytes\"\n+    val values = (Byte.MinValue until Byte.MinValue + numItems).map(v => s\"${v}Y\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ByteType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runShortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"${v}S\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(ShortType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runIntBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(0, numRows).select($\"id\".cast(IntegerType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLongBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(0, numRows).toDF(\"id\")\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runFloatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(FloatType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDoubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems doubles\"\n+    val values = (1 to numItems).map(v => s\"$v.0D\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DoubleType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runSmallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runLargeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(StringType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runTimestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:00.$m' AS timestamp)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runDateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(0, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runArrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(0, numRows).select(array($\"id\").as(\"id\"))\n+    runBenchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  private def runStructBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Unit = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(0, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))",
    "line": 131
  }],
  "prId": 23291
}]