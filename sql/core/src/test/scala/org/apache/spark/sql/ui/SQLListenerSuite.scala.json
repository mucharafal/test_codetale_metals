[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "is this test potentially flaky due to the race condition you described in `SQLExecution.scala`? If we go into the `finally` there before the listener bus has a chance to post the events then this will fail right? Maybe this is not an actual concern.\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-08-03T21:06:02Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import java.util.Properties\n+\n+import org.apache.spark.{SparkException, SparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SQLExecution\n+import org.apache.spark.sql.test.TestSQLContext\n+\n+class SQLListenerSuite extends SparkFunSuite {\n+\n+  private def createTestDataFrame: DataFrame = {\n+    import TestSQLContext.implicits._\n+    Seq(\n+      (1, 1),\n+      (2, 2)\n+    ).toDF().filter(\"_1 > 1\")\n+  }\n+\n+  private def createProperties(executionId: Long): Properties = {\n+    val properties = new Properties()\n+    properties.setProperty(SQLExecution.EXECUTION_ID_KEY, executionId.toString)\n+    properties\n+  }\n+\n+  private def createStageInfo(stageId: Int, attemptId: Int): StageInfo = new StageInfo(\n+    stageId = stageId,\n+    attemptId = attemptId,\n+    // The following fields are not used in tests\n+    name = \"\",\n+    numTasks = 0,\n+    rddInfos = Nil,\n+    parentIds = Nil,\n+    details = \"\"\n+  )\n+\n+  private def createTaskInfo(taskId: Int, attempt: Int): TaskInfo = new TaskInfo(\n+    taskId = taskId,\n+    attempt = attempt,\n+    // The following fields are not used in tests\n+    index = 0,\n+    launchTime = 0,\n+    executorId = \"\",\n+    host = \"\",\n+    taskLocality = null,\n+    speculative = false\n+  )\n+\n+  private def createTaskMetrics(accumulatorUpdates: Map[Long, Any]): TaskMetrics = {\n+    val metrics = new TaskMetrics\n+    metrics.setAccumulatorsUpdater(() => accumulatorUpdates)\n+    metrics.updateAccumulators()\n+    metrics\n+  }\n+\n+  test(\"basic\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    val accumulatorIds =\n+      SparkPlanGraph(df.queryExecution.executedPlan).nodes.flatMap(_.metrics.map(_.accumulatorId))\n+    // Assume all accumulators are long\n+    var accumulatorValue = 0L\n+    val accumulatorUpdates = accumulatorIds.map { id =>\n+      accumulatorValue += 1L\n+      (id, accumulatorValue)\n+    }.toMap\n+\n+    listener.onExecutionStart(\n+    executionId, \"test\", \"test\", df, System.currentTimeMillis())\n+\n+    val executionUIData = listener.executionIdToData(0)\n+\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Seq(\n+        createStageInfo(0, 0),\n+        createStageInfo(1, 0)\n+      ),\n+      createProperties(executionId)))\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(0, 0)))\n+\n+    assert(listener.getExecutionMetrics(0).isEmpty)\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 0, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 0, createTaskMetrics(accumulatorUpdates.mapValues(_ * 2)))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 3))\n+\n+    // Retrying a stage should reset the metrics\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(0, 1)))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 1, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 1, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    // Ignore the task end for the first attempt\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 100))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    // Finish two tasks\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 1,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 2))))\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 1,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(1, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 5))\n+\n+    // Summit a new stage\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(1, 0)))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 1, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 1, 0, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 7))\n+\n+    // Finish two tasks\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 1,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 1,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(1, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 11))\n+\n+    assert(executionUIData.runningJobs === Seq(0))\n+    assert(executionUIData.succeededJobs.isEmpty)\n+    assert(executionUIData.failedJobs.isEmpty)\n+\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs === Seq(0))\n+    assert(executionUIData.failedJobs.isEmpty)\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 11))\n+  }\n+\n+  test(\"onExecutionEnd happens before onJobEnd(JobSucceeded)\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    listener.onExecutionStart(\n+      executionId, \"test\", \"test\", createTestDataFrame, System.currentTimeMillis())\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Nil,\n+      createProperties(executionId)))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+\n+    val executionUIData = listener.executionIdToData(0)\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs === Seq(0))\n+    assert(executionUIData.failedJobs.isEmpty)\n+  }\n+\n+  test(\"onExecutionEnd happens before multiple onJobEnd(JobSucceeded)s\") {",
    "line": 242
  }],
  "prId": 7774
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "This test floods the logs with stack traces.  Could we use [`quietly`](https://github.com/apache/spark/blob/d83c2f9f0b08d6d5d369d9fae04cdb15448e7f0d/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala#L27) to avoid making them hard to read?\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-12-17T18:34:45Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import java.util.Properties\n+\n+import org.apache.spark.{SparkException, SparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SQLExecution\n+import org.apache.spark.sql.test.TestSQLContext\n+\n+class SQLListenerSuite extends SparkFunSuite {\n+\n+  private def createTestDataFrame: DataFrame = {\n+    import TestSQLContext.implicits._\n+    Seq(\n+      (1, 1),\n+      (2, 2)\n+    ).toDF().filter(\"_1 > 1\")\n+  }\n+\n+  private def createProperties(executionId: Long): Properties = {\n+    val properties = new Properties()\n+    properties.setProperty(SQLExecution.EXECUTION_ID_KEY, executionId.toString)\n+    properties\n+  }\n+\n+  private def createStageInfo(stageId: Int, attemptId: Int): StageInfo = new StageInfo(\n+    stageId = stageId,\n+    attemptId = attemptId,\n+    // The following fields are not used in tests\n+    name = \"\",\n+    numTasks = 0,\n+    rddInfos = Nil,\n+    parentIds = Nil,\n+    details = \"\"\n+  )\n+\n+  private def createTaskInfo(taskId: Int, attempt: Int): TaskInfo = new TaskInfo(\n+    taskId = taskId,\n+    attempt = attempt,\n+    // The following fields are not used in tests\n+    index = 0,\n+    launchTime = 0,\n+    executorId = \"\",\n+    host = \"\",\n+    taskLocality = null,\n+    speculative = false\n+  )\n+\n+  private def createTaskMetrics(accumulatorUpdates: Map[Long, Any]): TaskMetrics = {\n+    val metrics = new TaskMetrics\n+    metrics.setAccumulatorsUpdater(() => accumulatorUpdates)\n+    metrics.updateAccumulators()\n+    metrics\n+  }\n+\n+  test(\"basic\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    val accumulatorIds =\n+      SparkPlanGraph(df.queryExecution.executedPlan).nodes.flatMap(_.metrics.map(_.accumulatorId))\n+    // Assume all accumulators are long\n+    var accumulatorValue = 0L\n+    val accumulatorUpdates = accumulatorIds.map { id =>\n+      accumulatorValue += 1L\n+      (id, accumulatorValue)\n+    }.toMap\n+\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+\n+    val executionUIData = listener.executionIdToData(0)\n+\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Seq(\n+        createStageInfo(0, 0),\n+        createStageInfo(1, 0)\n+      ),\n+      createProperties(executionId)))\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(0, 0)))\n+\n+    assert(listener.getExecutionMetrics(0).isEmpty)\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 0, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 0, createTaskMetrics(accumulatorUpdates.mapValues(_ * 2)))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 3))\n+\n+    // Retrying a stage should reset the metrics\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(0, 1)))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 1, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 1, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    // Ignore the task end for the first attempt\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 100))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    // Finish two tasks\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 1,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 2))))\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 1,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(1, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 5))\n+\n+    // Summit a new stage\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(1, 0)))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 1, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 1, 0, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 7))\n+\n+    // Finish two tasks\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 1,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 1,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(1, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 11))\n+\n+    assert(executionUIData.runningJobs === Seq(0))\n+    assert(executionUIData.succeededJobs.isEmpty)\n+    assert(executionUIData.failedJobs.isEmpty)\n+\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs === Seq(0))\n+    assert(executionUIData.failedJobs.isEmpty)\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 11))\n+  }\n+\n+  test(\"onExecutionEnd happens before onJobEnd(JobSucceeded)\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Nil,\n+      createProperties(executionId)))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+\n+    val executionUIData = listener.executionIdToData(0)\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs === Seq(0))\n+    assert(executionUIData.failedJobs.isEmpty)\n+  }\n+\n+  test(\"onExecutionEnd happens before multiple onJobEnd(JobSucceeded)s\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Nil,\n+      createProperties(executionId)))\n+    listener.onJobEnd(SparkListenerJobEnd(\n+        jobId = 0,\n+        time = System.currentTimeMillis(),\n+        JobSucceeded\n+    ))\n+\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 1,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Nil,\n+      createProperties(executionId)))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 1,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+\n+    val executionUIData = listener.executionIdToData(0)\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs.sorted === Seq(0, 1))\n+    assert(executionUIData.failedJobs.isEmpty)\n+  }\n+\n+  test(\"onExecutionEnd happens before onJobEnd(JobFailed)\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Seq.empty,\n+      createProperties(executionId)))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobFailed(new RuntimeException(\"Oops\"))\n+    ))\n+\n+    val executionUIData = listener.executionIdToData(0)\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs.isEmpty)\n+    assert(executionUIData.failedJobs === Seq(0))\n+  }\n+\n+  ignore(\"no memory leak\") {",
    "line": 311
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Sure. Will fix it.\n",
    "commit": "5a2bc9937bc26e014842b720fd2096294c9272b7",
    "createdAt": "2015-12-17T18:38:40Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.ui\n+\n+import java.util.Properties\n+\n+import org.apache.spark.{SparkException, SparkContext, SparkConf, SparkFunSuite}\n+import org.apache.spark.executor.TaskMetrics\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.{DataFrame, SQLContext}\n+import org.apache.spark.sql.execution.SQLExecution\n+import org.apache.spark.sql.test.TestSQLContext\n+\n+class SQLListenerSuite extends SparkFunSuite {\n+\n+  private def createTestDataFrame: DataFrame = {\n+    import TestSQLContext.implicits._\n+    Seq(\n+      (1, 1),\n+      (2, 2)\n+    ).toDF().filter(\"_1 > 1\")\n+  }\n+\n+  private def createProperties(executionId: Long): Properties = {\n+    val properties = new Properties()\n+    properties.setProperty(SQLExecution.EXECUTION_ID_KEY, executionId.toString)\n+    properties\n+  }\n+\n+  private def createStageInfo(stageId: Int, attemptId: Int): StageInfo = new StageInfo(\n+    stageId = stageId,\n+    attemptId = attemptId,\n+    // The following fields are not used in tests\n+    name = \"\",\n+    numTasks = 0,\n+    rddInfos = Nil,\n+    parentIds = Nil,\n+    details = \"\"\n+  )\n+\n+  private def createTaskInfo(taskId: Int, attempt: Int): TaskInfo = new TaskInfo(\n+    taskId = taskId,\n+    attempt = attempt,\n+    // The following fields are not used in tests\n+    index = 0,\n+    launchTime = 0,\n+    executorId = \"\",\n+    host = \"\",\n+    taskLocality = null,\n+    speculative = false\n+  )\n+\n+  private def createTaskMetrics(accumulatorUpdates: Map[Long, Any]): TaskMetrics = {\n+    val metrics = new TaskMetrics\n+    metrics.setAccumulatorsUpdater(() => accumulatorUpdates)\n+    metrics.updateAccumulators()\n+    metrics\n+  }\n+\n+  test(\"basic\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    val accumulatorIds =\n+      SparkPlanGraph(df.queryExecution.executedPlan).nodes.flatMap(_.metrics.map(_.accumulatorId))\n+    // Assume all accumulators are long\n+    var accumulatorValue = 0L\n+    val accumulatorUpdates = accumulatorIds.map { id =>\n+      accumulatorValue += 1L\n+      (id, accumulatorValue)\n+    }.toMap\n+\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+\n+    val executionUIData = listener.executionIdToData(0)\n+\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Seq(\n+        createStageInfo(0, 0),\n+        createStageInfo(1, 0)\n+      ),\n+      createProperties(executionId)))\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(0, 0)))\n+\n+    assert(listener.getExecutionMetrics(0).isEmpty)\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 0, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 0, createTaskMetrics(accumulatorUpdates.mapValues(_ * 2)))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 3))\n+\n+    // Retrying a stage should reset the metrics\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(0, 1)))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 0, 1, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 0, 1, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    // Ignore the task end for the first attempt\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 100))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 2))\n+\n+    // Finish two tasks\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 1,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 2))))\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 0,\n+      stageAttemptId = 1,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(1, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 5))\n+\n+    // Summit a new stage\n+    listener.onStageSubmitted(SparkListenerStageSubmitted(createStageInfo(1, 0)))\n+\n+    listener.onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate(\"\", Seq(\n+      // (task id, stage id, stage attempt, metrics)\n+      (0L, 1, 0, createTaskMetrics(accumulatorUpdates)),\n+      (1L, 1, 0, createTaskMetrics(accumulatorUpdates))\n+    )))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 7))\n+\n+    // Finish two tasks\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 1,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(0, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+    listener.onTaskEnd(SparkListenerTaskEnd(\n+      stageId = 1,\n+      stageAttemptId = 0,\n+      taskType = \"\",\n+      reason = null,\n+      createTaskInfo(1, 0),\n+      createTaskMetrics(accumulatorUpdates.mapValues(_ * 3))))\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 11))\n+\n+    assert(executionUIData.runningJobs === Seq(0))\n+    assert(executionUIData.succeededJobs.isEmpty)\n+    assert(executionUIData.failedJobs.isEmpty)\n+\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs === Seq(0))\n+    assert(executionUIData.failedJobs.isEmpty)\n+\n+    assert(listener.getExecutionMetrics(0) === accumulatorUpdates.mapValues(_ * 11))\n+  }\n+\n+  test(\"onExecutionEnd happens before onJobEnd(JobSucceeded)\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Nil,\n+      createProperties(executionId)))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+\n+    val executionUIData = listener.executionIdToData(0)\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs === Seq(0))\n+    assert(executionUIData.failedJobs.isEmpty)\n+  }\n+\n+  test(\"onExecutionEnd happens before multiple onJobEnd(JobSucceeded)s\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Nil,\n+      createProperties(executionId)))\n+    listener.onJobEnd(SparkListenerJobEnd(\n+        jobId = 0,\n+        time = System.currentTimeMillis(),\n+        JobSucceeded\n+    ))\n+\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 1,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Nil,\n+      createProperties(executionId)))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 1,\n+      time = System.currentTimeMillis(),\n+      JobSucceeded\n+    ))\n+\n+    val executionUIData = listener.executionIdToData(0)\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs.sorted === Seq(0, 1))\n+    assert(executionUIData.failedJobs.isEmpty)\n+  }\n+\n+  test(\"onExecutionEnd happens before onJobEnd(JobFailed)\") {\n+    val listener = new SQLListener(TestSQLContext)\n+    val executionId = 0\n+    val df = createTestDataFrame\n+    listener.onExecutionStart(\n+      executionId,\n+      \"test\",\n+      \"test\",\n+      df.queryExecution.toString,\n+      SparkPlanGraph(df.queryExecution.executedPlan),\n+      System.currentTimeMillis())\n+    listener.onJobStart(SparkListenerJobStart(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      stageInfos = Seq.empty,\n+      createProperties(executionId)))\n+    listener.onExecutionEnd(executionId, System.currentTimeMillis())\n+    listener.onJobEnd(SparkListenerJobEnd(\n+      jobId = 0,\n+      time = System.currentTimeMillis(),\n+      JobFailed(new RuntimeException(\"Oops\"))\n+    ))\n+\n+    val executionUIData = listener.executionIdToData(0)\n+    assert(executionUIData.runningJobs.isEmpty)\n+    assert(executionUIData.succeededJobs.isEmpty)\n+    assert(executionUIData.failedJobs === Seq(0))\n+  }\n+\n+  ignore(\"no memory leak\") {",
    "line": 311
  }],
  "prId": 7774
}]