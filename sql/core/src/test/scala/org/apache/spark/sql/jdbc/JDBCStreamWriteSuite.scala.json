[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "~Did you run `bin/scalastyle` in your mac? If not, please run and fix the style errors.~\r\nIt seems to pass the scalastyle test. Let's add space like this: `BeforeAndAfter{` -> `BeforeAndAfter {`.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-22T03:29:55Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter{"
  }, {
    "author": {
      "login": "yanlin-Lynn"
    },
    "body": "OK. I will fix it.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T01:30:16Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter{"
  }],
  "prId": 23369
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "nit: indentation here looks odd",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T08:03:31Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter {\n+  import testImplicits._\n+\n+  val url = \"jdbc:h2:mem:testdb\"\n+  val jdbcTableName = \"stream_test_table\"\n+  val driverClassName = \"org.h2.Driver\"\n+  val createTableSql = s\"\"\"\n+      |CREATE TABLE ${jdbcTableName}(\n+      | name VARCHAR(32),\n+      | value LONG,\n+      | PRIMARY KEY (name)\n+      |)\"\"\".stripMargin\n+\n+  var conn: java.sql.Connection = null\n+\n+  val testH2Dialect = new JdbcDialect {\n+    override def canHandle(url: String) : Boolean = url.startsWith(\"jdbc:h2\")\n+    override def isCascadingTruncateTable(): Option[Boolean] = Some(false)\n+  }\n+\n+  before {\n+    Utils.classForName(driverClassName)\n+    conn = DriverManager.getConnection(url)\n+    conn.prepareStatement(createTableSql).executeUpdate()\n+  }\n+\n+  after {\n+    conn.close()\n+  }\n+\n+  test(\"Basic Write\") {\n+    withTempDir { checkpointDir => {\n+        val input = MemoryStream[Int]\n+        val query = input.toDF().map { row =>\n+          val value = row.getInt(0)\n+          TestData(s\"name_$value\", value.toLong)\n+        }.writeStream\n+          .format(\"jdbc\")\n+          .option(JDBCOptions.JDBC_URL, url)\n+          .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+          .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+          .start()\n+        try {\n+          input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write sub columns\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)\n+      }.select(\"name\").writeStream // write just one `name` column\n+        .format(\"jdbc\")\n+        .option(JDBCOptions.JDBC_URL, url)\n+        .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+        .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .start()\n+      try {\n+        input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write same data\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)\n+      }.writeStream\n+        .format(\"jdbc\")\n+        .option(JDBCOptions.JDBC_URL, url)\n+        .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+        .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .start()\n+      try {\n+        input.addData(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+    }"
  }],
  "prId": 23369
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "nit: can we `row.getLong(0)` directly?",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T08:04:18Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter {\n+  import testImplicits._\n+\n+  val url = \"jdbc:h2:mem:testdb\"\n+  val jdbcTableName = \"stream_test_table\"\n+  val driverClassName = \"org.h2.Driver\"\n+  val createTableSql = s\"\"\"\n+      |CREATE TABLE ${jdbcTableName}(\n+      | name VARCHAR(32),\n+      | value LONG,\n+      | PRIMARY KEY (name)\n+      |)\"\"\".stripMargin\n+\n+  var conn: java.sql.Connection = null\n+\n+  val testH2Dialect = new JdbcDialect {\n+    override def canHandle(url: String) : Boolean = url.startsWith(\"jdbc:h2\")\n+    override def isCascadingTruncateTable(): Option[Boolean] = Some(false)\n+  }\n+\n+  before {\n+    Utils.classForName(driverClassName)\n+    conn = DriverManager.getConnection(url)\n+    conn.prepareStatement(createTableSql).executeUpdate()\n+  }\n+\n+  after {\n+    conn.close()\n+  }\n+\n+  test(\"Basic Write\") {\n+    withTempDir { checkpointDir => {\n+        val input = MemoryStream[Int]\n+        val query = input.toDF().map { row =>\n+          val value = row.getInt(0)\n+          TestData(s\"name_$value\", value.toLong)\n+        }.writeStream\n+          .format(\"jdbc\")\n+          .option(JDBCOptions.JDBC_URL, url)\n+          .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+          .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+          .start()\n+        try {\n+          input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write sub columns\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)\n+      }.select(\"name\").writeStream // write just one `name` column\n+        .format(\"jdbc\")\n+        .option(JDBCOptions.JDBC_URL, url)\n+        .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+        .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .start()\n+      try {\n+        input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write same data\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)"
  }, {
    "author": {
      "login": "yanlin-Lynn"
    },
    "body": "using row.getLong(0) directly will cause ClassCastException, part of the exception stack is as follows:\r\njava.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long\r\n        at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)\r\n        at org.apache.spark.sql.Row.getLong(Row.scala:232)\r\n        at org.apache.spark.sql.Row.getLong$(Row.scala:232)\r\n        at org.apache.spark.sql.catalyst.expressions.GenericRow.getLong(rows.scala:166)\r\nNeed to change the input data schema also, I will change the input data type, using \"val  input = MemoryStream[Long]\".",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T10:14:19Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter {\n+  import testImplicits._\n+\n+  val url = \"jdbc:h2:mem:testdb\"\n+  val jdbcTableName = \"stream_test_table\"\n+  val driverClassName = \"org.h2.Driver\"\n+  val createTableSql = s\"\"\"\n+      |CREATE TABLE ${jdbcTableName}(\n+      | name VARCHAR(32),\n+      | value LONG,\n+      | PRIMARY KEY (name)\n+      |)\"\"\".stripMargin\n+\n+  var conn: java.sql.Connection = null\n+\n+  val testH2Dialect = new JdbcDialect {\n+    override def canHandle(url: String) : Boolean = url.startsWith(\"jdbc:h2\")\n+    override def isCascadingTruncateTable(): Option[Boolean] = Some(false)\n+  }\n+\n+  before {\n+    Utils.classForName(driverClassName)\n+    conn = DriverManager.getConnection(url)\n+    conn.prepareStatement(createTableSql).executeUpdate()\n+  }\n+\n+  after {\n+    conn.close()\n+  }\n+\n+  test(\"Basic Write\") {\n+    withTempDir { checkpointDir => {\n+        val input = MemoryStream[Int]\n+        val query = input.toDF().map { row =>\n+          val value = row.getInt(0)\n+          TestData(s\"name_$value\", value.toLong)\n+        }.writeStream\n+          .format(\"jdbc\")\n+          .option(JDBCOptions.JDBC_URL, url)\n+          .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+          .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+          .start()\n+        try {\n+          input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write sub columns\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)\n+      }.select(\"name\").writeStream // write just one `name` column\n+        .format(\"jdbc\")\n+        .option(JDBCOptions.JDBC_URL, url)\n+        .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+        .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .start()\n+      try {\n+        input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write same data\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, then lets just do row.getInt.toLong. not a big deal though.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T10:20:06Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter {\n+  import testImplicits._\n+\n+  val url = \"jdbc:h2:mem:testdb\"\n+  val jdbcTableName = \"stream_test_table\"\n+  val driverClassName = \"org.h2.Driver\"\n+  val createTableSql = s\"\"\"\n+      |CREATE TABLE ${jdbcTableName}(\n+      | name VARCHAR(32),\n+      | value LONG,\n+      | PRIMARY KEY (name)\n+      |)\"\"\".stripMargin\n+\n+  var conn: java.sql.Connection = null\n+\n+  val testH2Dialect = new JdbcDialect {\n+    override def canHandle(url: String) : Boolean = url.startsWith(\"jdbc:h2\")\n+    override def isCascadingTruncateTable(): Option[Boolean] = Some(false)\n+  }\n+\n+  before {\n+    Utils.classForName(driverClassName)\n+    conn = DriverManager.getConnection(url)\n+    conn.prepareStatement(createTableSql).executeUpdate()\n+  }\n+\n+  after {\n+    conn.close()\n+  }\n+\n+  test(\"Basic Write\") {\n+    withTempDir { checkpointDir => {\n+        val input = MemoryStream[Int]\n+        val query = input.toDF().map { row =>\n+          val value = row.getInt(0)\n+          TestData(s\"name_$value\", value.toLong)\n+        }.writeStream\n+          .format(\"jdbc\")\n+          .option(JDBCOptions.JDBC_URL, url)\n+          .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+          .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+          .start()\n+        try {\n+          input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write sub columns\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)\n+      }.select(\"name\").writeStream // write just one `name` column\n+        .format(\"jdbc\")\n+        .option(JDBCOptions.JDBC_URL, url)\n+        .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+        .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .start()\n+      try {\n+        input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write same data\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)"
  }],
  "prId": 23369
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "nit: indentation looks odd",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T08:04:32Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter {\n+  import testImplicits._\n+\n+  val url = \"jdbc:h2:mem:testdb\"\n+  val jdbcTableName = \"stream_test_table\"\n+  val driverClassName = \"org.h2.Driver\"\n+  val createTableSql = s\"\"\"\n+      |CREATE TABLE ${jdbcTableName}(\n+      | name VARCHAR(32),\n+      | value LONG,\n+      | PRIMARY KEY (name)\n+      |)\"\"\".stripMargin\n+\n+  var conn: java.sql.Connection = null\n+\n+  val testH2Dialect = new JdbcDialect {\n+    override def canHandle(url: String) : Boolean = url.startsWith(\"jdbc:h2\")\n+    override def isCascadingTruncateTable(): Option[Boolean] = Some(false)\n+  }\n+\n+  before {\n+    Utils.classForName(driverClassName)\n+    conn = DriverManager.getConnection(url)\n+    conn.prepareStatement(createTableSql).executeUpdate()\n+  }\n+\n+  after {\n+    conn.close()\n+  }\n+\n+  test(\"Basic Write\") {\n+    withTempDir { checkpointDir => {\n+        val input = MemoryStream[Int]\n+        val query = input.toDF().map { row =>\n+          val value = row.getInt(0)\n+          TestData(s\"name_$value\", value.toLong)\n+        }.writeStream\n+          .format(\"jdbc\")\n+          .option(JDBCOptions.JDBC_URL, url)\n+          .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+          .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+          .start()\n+        try {\n+          input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write sub columns\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)\n+      }.select(\"name\").writeStream // write just one `name` column\n+        .format(\"jdbc\")\n+        .option(JDBCOptions.JDBC_URL, url)\n+        .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+        .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .start()\n+      try {\n+        input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+    }"
  }, {
    "author": {
      "login": "yanlin-Lynn"
    },
    "body": "en, I will fix it and commit.",
    "commit": "aa67565d4240cdbde738f7a9b2d9bf1bd7cf8beb",
    "createdAt": "2018-12-25T10:25:11Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc\n+\n+import java.sql.DriverManager\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.Utils\n+\n+private case class TestData(name: String, value: Long)\n+\n+class JDBCStreamWriteSuite extends StreamTest with BeforeAndAfter {\n+  import testImplicits._\n+\n+  val url = \"jdbc:h2:mem:testdb\"\n+  val jdbcTableName = \"stream_test_table\"\n+  val driverClassName = \"org.h2.Driver\"\n+  val createTableSql = s\"\"\"\n+      |CREATE TABLE ${jdbcTableName}(\n+      | name VARCHAR(32),\n+      | value LONG,\n+      | PRIMARY KEY (name)\n+      |)\"\"\".stripMargin\n+\n+  var conn: java.sql.Connection = null\n+\n+  val testH2Dialect = new JdbcDialect {\n+    override def canHandle(url: String) : Boolean = url.startsWith(\"jdbc:h2\")\n+    override def isCascadingTruncateTable(): Option[Boolean] = Some(false)\n+  }\n+\n+  before {\n+    Utils.classForName(driverClassName)\n+    conn = DriverManager.getConnection(url)\n+    conn.prepareStatement(createTableSql).executeUpdate()\n+  }\n+\n+  after {\n+    conn.close()\n+  }\n+\n+  test(\"Basic Write\") {\n+    withTempDir { checkpointDir => {\n+        val input = MemoryStream[Int]\n+        val query = input.toDF().map { row =>\n+          val value = row.getInt(0)\n+          TestData(s\"name_$value\", value.toLong)\n+        }.writeStream\n+          .format(\"jdbc\")\n+          .option(JDBCOptions.JDBC_URL, url)\n+          .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+          .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+          .start()\n+        try {\n+          input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+    val result = conn\n+      .prepareStatement(s\"select count(*) as count from $jdbcTableName\")\n+      .executeQuery()\n+    assert(result.next())\n+    assert(result.getInt(\"count\") == 10)\n+  }\n+\n+  test(\"Write sub columns\") {\n+    withTempDir { checkpointDir => {\n+      val input = MemoryStream[Int]\n+      val query = input.toDF().map { row =>\n+        val value = row.getInt(0)\n+        TestData(s\"name_$value\", value.toLong)\n+      }.select(\"name\").writeStream // write just one `name` column\n+        .format(\"jdbc\")\n+        .option(JDBCOptions.JDBC_URL, url)\n+        .option(JDBCOptions.JDBC_TABLE_NAME, jdbcTableName)\n+        .option(JDBCOptions.JDBC_DRIVER_CLASS, driverClassName)\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .start()\n+      try {\n+        input.addData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+    }"
  }],
  "prId": 23369
}]