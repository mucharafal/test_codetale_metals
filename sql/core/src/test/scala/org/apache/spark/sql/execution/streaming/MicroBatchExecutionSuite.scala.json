[{
  "comments": [{
    "author": {
      "login": "ulysses-you"
    },
    "body": "In order to avoid `StreamMetadata.write()` affect results, this code init metadata file.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-12T01:26:20Z",
    "diffHunk": "@@ -68,4 +73,44 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n-}\n+\n+  test(\"SPARK-28597: Add config to retry spark streaming's meta log when it met\") {\n+    val s = MemoryStream[Int]\n+    val df = s.toDF()\n+    // Specified checkpointLocation manually to init metadata file\n+    val tmp =\n+      new File(System.getProperty(\"java.io.tmpdir\"), UUID.randomUUID().toString).getCanonicalPath\n+    testStream(s.toDF())("
  }],
  "prId": 25333
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Let's remove `SPARK-28597: ` from the test case name since this is a new feature.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-12T23:01:18Z",
    "diffHunk": "@@ -68,4 +73,44 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n+\n+  test(\"SPARK-28597: Add config to retry spark streaming's meta log when it met\") {"
  }],
  "prId": 25333
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Let's remove this new empty line.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-12T23:11:52Z",
    "diffHunk": "@@ -17,11 +17,16 @@\n \n package org.apache.spark.sql.execution.streaming\n \n+import java.io.File\n+import java.util.UUID\n+\n import org.scalatest.BeforeAndAfter\n \n import org.apache.spark.sql.functions.{count, window}\n+import org.apache.spark.sql.internal.SQLConf\n import org.apache.spark.sql.streaming.StreamTest\n \n+"
  }],
  "prId": 25333
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "In the test suite, we use `withSQLConf` instead of `setConfString` and `unsetConf` combination.\r\nPlease refer `CheckpointFileManagerSuite` for the usage of `withSQLConf`.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-12T23:14:09Z",
    "diffHunk": "@@ -68,4 +73,44 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n+\n+  test(\"SPARK-28597: Add config to retry spark streaming's meta log when it met\") {\n+    val s = MemoryStream[Int]\n+    val df = s.toDF()\n+    // Specified checkpointLocation manually to init metadata file\n+    val tmp =\n+      new File(System.getProperty(\"java.io.tmpdir\"), UUID.randomUUID().toString).getCanonicalPath\n+    testStream(s.toDF())(\n+      StartStream(checkpointLocation = tmp)\n+    )\n+\n+    // fail with less retries\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key,\n+      classOf[FakeFileSystemBasedCheckpointFileManager].getName)"
  }],
  "prId": 25333
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Ditto for `STREAMING_META_DATA_NUM_RETRIES`. In this test case, you are trying to reset with default value, but this test case can fail at line 94. Then, the other test case will use this `1` accidentally. That is the reason why we use `withSQLConf`.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-12T23:15:55Z",
    "diffHunk": "@@ -68,4 +73,44 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n+\n+  test(\"SPARK-28597: Add config to retry spark streaming's meta log when it met\") {\n+    val s = MemoryStream[Int]\n+    val df = s.toDF()\n+    // Specified checkpointLocation manually to init metadata file\n+    val tmp =\n+      new File(System.getProperty(\"java.io.tmpdir\"), UUID.randomUUID().toString).getCanonicalPath\n+    testStream(s.toDF())(\n+      StartStream(checkpointLocation = tmp)\n+    )\n+\n+    // fail with less retries\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key,\n+      classOf[FakeFileSystemBasedCheckpointFileManager].getName)\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_META_DATA_NUM_RETRIES.key,\n+      1.toString)"
  }, {
    "author": {
      "login": "ulysses-you"
    },
    "body": "Thanks for your remind, I will check and fix them.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-13T00:38:11Z",
    "diffHunk": "@@ -68,4 +73,44 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n+\n+  test(\"SPARK-28597: Add config to retry spark streaming's meta log when it met\") {\n+    val s = MemoryStream[Int]\n+    val df = s.toDF()\n+    // Specified checkpointLocation manually to init metadata file\n+    val tmp =\n+      new File(System.getProperty(\"java.io.tmpdir\"), UUID.randomUUID().toString).getCanonicalPath\n+    testStream(s.toDF())(\n+      StartStream(checkpointLocation = tmp)\n+    )\n+\n+    // fail with less retries\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key,\n+      classOf[FakeFileSystemBasedCheckpointFileManager].getName)\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_META_DATA_NUM_RETRIES.key,\n+      1.toString)"
  }],
  "prId": 25333
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`intercept[Throwable]`? It's unclear what you want to test.\r\nIt's a good habit to check the exact exception type and the exact error message always.\r\nPlease search `intercept` use cases in the Spark code base. It will be helpful for you.\r\n",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-12T23:17:25Z",
    "diffHunk": "@@ -68,4 +73,44 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n+\n+  test(\"SPARK-28597: Add config to retry spark streaming's meta log when it met\") {\n+    val s = MemoryStream[Int]\n+    val df = s.toDF()\n+    // Specified checkpointLocation manually to init metadata file\n+    val tmp =\n+      new File(System.getProperty(\"java.io.tmpdir\"), UUID.randomUUID().toString).getCanonicalPath\n+    testStream(s.toDF())(\n+      StartStream(checkpointLocation = tmp)\n+    )\n+\n+    // fail with less retries\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key,\n+      classOf[FakeFileSystemBasedCheckpointFileManager].getName)\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_META_DATA_NUM_RETRIES.key,\n+      1.toString)\n+    intercept[Throwable] {\n+      testStream(s.toDF())(\n+        StartStream(checkpointLocation = tmp),\n+        AddData(s, 1),\n+        CheckAnswer(1)\n+      )\n+    }"
  }, {
    "author": {
      "login": "ulysses-you"
    },
    "body": "Throwable is underlying exception, I will check the message for it.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-08-13T00:37:29Z",
    "diffHunk": "@@ -68,4 +73,44 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n+\n+  test(\"SPARK-28597: Add config to retry spark streaming's meta log when it met\") {\n+    val s = MemoryStream[Int]\n+    val df = s.toDF()\n+    // Specified checkpointLocation manually to init metadata file\n+    val tmp =\n+      new File(System.getProperty(\"java.io.tmpdir\"), UUID.randomUUID().toString).getCanonicalPath\n+    testStream(s.toDF())(\n+      StartStream(checkpointLocation = tmp)\n+    )\n+\n+    // fail with less retries\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key,\n+      classOf[FakeFileSystemBasedCheckpointFileManager].getName)\n+    df.sparkSession.sessionState.conf.setConfString(\n+      SQLConf.STREAMING_META_DATA_NUM_RETRIES.key,\n+      1.toString)\n+    intercept[Throwable] {\n+      testStream(s.toDF())(\n+        StartStream(checkpointLocation = tmp),\n+        AddData(s, 1),\n+        CheckAnswer(1)\n+      )\n+    }"
  }],
  "prId": 25333
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "You can just use `withTempDir` and `withTempPath`.",
    "commit": "6123efb982bfab112a4ff14f1b0fa6592fc180b6",
    "createdAt": "2019-10-11T04:30:22Z",
    "diffHunk": "@@ -68,4 +72,43 @@ class MicroBatchExecutionSuite extends StreamTest with BeforeAndAfter {\n       CheckNewAnswer((25, 1), (30, 1))   // This should not throw the error reported in SPARK-24156\n     )\n   }\n+\n+  test(\"Add config to retry spark streaming's meta log when it met\") {\n+    val s = MemoryStream[Int]\n+    // Specified checkpointLocation manually to init metadata file\n+    val tmp =\n+      new File(System.getProperty(\"java.io.tmpdir\"), UUID.randomUUID().toString).getCanonicalPath"
  }],
  "prId": 25333
}]