[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "What about these tests?\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T01:01:42Z",
    "diffHunk": "@@ -17,106 +17,16 @@\n \n package org.apache.spark.sql.streaming\n \n-import java.io.File\n-\n-import org.apache.commons.io.FileUtils\n-import org.apache.commons.io.filefilter.{DirectoryFileFilter, RegexFileFilter}\n-\n import org.apache.spark.sql._\n-import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n import org.apache.spark.sql.execution.DataSourceScanExec\n import org.apache.spark.sql.execution.datasources._\n-import org.apache.spark.sql.execution.streaming.{FileStreamSinkWriter, MemoryStream, MetadataLogFileIndex}\n-import org.apache.spark.sql.functions._\n-import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, MetadataLogFileIndex}\n import org.apache.spark.sql.types.{IntegerType, StructField, StructType}\n import org.apache.spark.util.Utils\n \n class FileStreamSinkSuite extends StreamTest {\n   import testImplicits._\n \n-\n-  test(\"FileStreamSinkWriter - unpartitioned data\") {",
    "line": 24
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "They were testing code that's been deleted completely and is now purely redundant with all the tests we have for the batch write path.\n",
    "commit": "a2ea180b4cab053cdcdf47351a3031bffbd501b7",
    "createdAt": "2016-11-02T01:03:11Z",
    "diffHunk": "@@ -17,106 +17,16 @@\n \n package org.apache.spark.sql.streaming\n \n-import java.io.File\n-\n-import org.apache.commons.io.FileUtils\n-import org.apache.commons.io.filefilter.{DirectoryFileFilter, RegexFileFilter}\n-\n import org.apache.spark.sql._\n-import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n import org.apache.spark.sql.execution.DataSourceScanExec\n import org.apache.spark.sql.execution.datasources._\n-import org.apache.spark.sql.execution.streaming.{FileStreamSinkWriter, MemoryStream, MetadataLogFileIndex}\n-import org.apache.spark.sql.functions._\n-import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, MetadataLogFileIndex}\n import org.apache.spark.sql.types.{IntegerType, StructField, StructType}\n import org.apache.spark.util.Utils\n \n class FileStreamSinkSuite extends StreamTest {\n   import testImplicits._\n \n-\n-  test(\"FileStreamSinkWriter - unpartitioned data\") {",
    "line": 24
  }],
  "prId": 15710
}]