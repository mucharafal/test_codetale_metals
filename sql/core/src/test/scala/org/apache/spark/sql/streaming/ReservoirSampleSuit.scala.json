[{
  "comments": [{
    "author": {
      "login": "uncleGen"
    },
    "body": "new unit test needs to be improved.",
    "commit": "02d44aa06f025dc1d69a7abbcf59691ce7ee0e4e",
    "createdAt": "2017-03-03T01:45:35Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+\n+class ReservoirSampleSuit extends StateStoreMetricsTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"streaming reservoir sample: reservoir size is larger than stream data size - update mode\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().reservoir(4)\n+\n+    testStream(result, Update)(\n+      AddData(inputData, \"a\", \"b\"),\n+      CheckAnswer(Row(\"a\"), Row(\"b\")),\n+      AddData(inputData, \"a\"),\n+      CheckAnswer(Row(\"a\"), Row(\"b\"), Row(\"a\"))\n+    )\n+  }\n+\n+  test(\"streaming reservoir sample: reservoir size is less than stream data size - update mode\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().reservoir(1)\n+\n+    testStream(result, Update)(\n+      AddData(inputData, \"a\", \"a\"),\n+      CheckLastBatch(Row(\"a\")),\n+      AddData(inputData, \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\"),\n+      CheckLastBatch(Row(\"b\"))\n+    )\n+  }\n+\n+  test(\"streaming reservoir sample with aggregation - update mode\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().reservoir(3).groupBy(\"value\").count()\n+\n+    testStream(result, Update)(\n+      AddData(inputData, \"a\"),\n+      CheckAnswer(Row(\"a\", 1)),\n+      AddData(inputData, \"b\"),\n+      CheckAnswer(Row(\"a\", 1), Row(\"b\", 1))\n+    )\n+  }\n+\n+  test(\"streaming reservoir sample with watermark\") {\n+    val inputData = MemoryStream[Int]\n+    val result = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .reservoir(10)\n+      .select($\"eventTime\".cast(\"long\").as[Long])\n+\n+    testStream(result, Update)(\n+      AddData(inputData, (1 to 1).flatMap(_ => (11 to 15)): _*),\n+      CheckLastBatch(11 to 15: _*),\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(25),\n+      AddData(inputData, 25), // Drop states less than watermark\n+      CheckLastBatch(25),\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      AddData(inputData, 45), // Advance watermark to 35 seconds\n+      CheckLastBatch(45),\n+      AddData(inputData, 25), // Should not emit anything as data less than watermark\n+      CheckLastBatch()\n+    )\n+  }\n+\n+  test(\"streaming reservoir sample with aggregation - complete mode\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().select($\"_1\" as \"key\", $\"_2\" as \"value\")\n+      .reservoir(3).groupBy(\"key\").max(\"value\")\n+\n+    testStream(result, Complete)(\n+      AddData(inputData, (\"a\", 1)),\n+      CheckAnswer(Row(\"a\", 1)),\n+      AddData(inputData, (\"b\", 2)),\n+      CheckAnswer(Row(\"a\", 1), Row(\"b\", 2)),\n+      StopStream,\n+      StartStream(),\n+      AddData(inputData, (\"a\", 10)),\n+      CheckAnswer(Row(\"a\", 10), Row(\"b\", 2)),\n+      AddData(inputData, (1 to 10).map(e => (\"c\", 100)): _*),\n+      CheckAnswer(Row(\"a\", 10), Row(\"b\", 2), Row(\"c\", 100))\n+    )\n+  }\n+\n+  test(\"batch reservoir sample\") {\n+    val df = spark.createDataset(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 0))\n+    assert(df.reservoir(3).count() == 3, \"\")\n+  }\n+\n+  test(\"batch reservoir sample after aggregation\") {\n+    val df = spark.createDataset((1 to 10).map(e => (e, s\"val_$e\")))\n+        .select($\"_1\" as \"key\", $\"_2\" as \"value\")\n+        .groupBy(\"value\").count()\n+    assert(df.reservoir(3).count() == 3, \"\")\n+  }\n+\n+  test(\"batch reservoir sample before aggregation\") {\n+    val df = spark.createDataset((1 to 10).map(e => (e, s\"val_$e\")))\n+      .select($\"_1\" as \"key\", $\"_2\" as \"value\")\n+      .reservoir(3)\n+      .groupBy(\"value\").count()\n+    assert(df.count() == 3, \"\")\n+  }\n+}",
    "line": 134
  }],
  "prId": 17141
}]