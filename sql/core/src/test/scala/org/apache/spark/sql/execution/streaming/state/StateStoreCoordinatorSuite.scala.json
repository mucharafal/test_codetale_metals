[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```\r\n-import org.apache.spark.sql.internal.SQLConf\r\n+import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\r\n```",
    "commit": "b39fb625b18d5a07d3076406a73e0db8d519ae4b",
    "createdAt": "2019-07-10T22:48:27Z",
    "diffHunk": "@@ -27,6 +27,7 @@ import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n import org.apache.spark.sql.SparkSession\n import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamingQueryWrapper}\n import org.apache.spark.sql.functions.count\n+import org.apache.spark.sql.internal.SQLConf"
  }],
  "prId": 25059
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```\r\n-      spark.conf.set(SQLConf.SHUFFLE_PARTITIONS.key, \"1\")\r\n+      spark.conf.set(SHUFFLE_PARTITIONS.key, \"1\")\r\n```",
    "commit": "b39fb625b18d5a07d3076406a73e0db8d519ae4b",
    "createdAt": "2019-07-10T22:48:38Z",
    "diffHunk": "@@ -124,7 +125,7 @@ class StateStoreCoordinatorSuite extends SparkFunSuite with SharedSparkContext {\n       import spark.implicits._\n       coordRef = spark.streams.stateStoreCoordinator\n       implicit val sqlContext = spark.sqlContext\n-      spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n+      spark.conf.set(SQLConf.SHUFFLE_PARTITIONS.key, \"1\")"
  }],
  "prId": 25059
}]