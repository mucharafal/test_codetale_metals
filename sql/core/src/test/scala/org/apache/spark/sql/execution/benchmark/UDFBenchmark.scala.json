[{
  "comments": [{
    "author": {
      "login": "skambha"
    },
    "body": "Looks like a typo -- should this be \"long/nullable int to primitive\" ",
    "commit": "010b3d449403a2d215a18b425722810470147ad6",
    "createdAt": "2019-05-22T00:49:31Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{IntegerType, StringType}\n+\n+/**\n+ * Synthetic benchmark for Scala User Defined Functions.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <sql core test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result:\n+ *      SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/UDFBenchmark-results.txt\".\n+ * }}}\n+ */\n+object UDFBenchmark extends SqlBasedBenchmark {\n+\n+  private def doRunBenchmarkWithMixedTypes(udf: UserDefinedFunction, cardinality: Int): Unit = {\n+    val idCol = col(\"id\")\n+    val nullableIntCol = when(\n+      idCol % 2 === 0, idCol.cast(IntegerType)).otherwise(Literal(null, IntegerType))\n+    val stringCol = idCol.cast(StringType)\n+    spark.range(cardinality).select(\n+      udf(idCol, nullableIntCol, stringCol)).write.format(\"noop\").save()\n+  }\n+\n+  private def doRunBenchmarkWithPrimitiveTypes(\n+      udf: UserDefinedFunction, cardinality: Int): Unit = {\n+    val idCol = col(\"id\")\n+    val nullableIntCol = when(\n+      idCol % 2 === 0, idCol.cast(IntegerType)).otherwise(Literal(null, IntegerType))\n+    spark.range(cardinality).select(udf(idCol, nullableIntCol)).write.format(\"noop\").save()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val cardinality = 100000\n+    runBenchmark(\"UDF with mixed input types\") {\n+      codegenBenchmark(\"long/nullable int/string to string\", cardinality) {\n+        val sampleUDF = udf {(a: Long, b: java.lang.Integer, c: String) =>\n+          s\"$a,$b,$c\"\n+        }\n+        doRunBenchmarkWithMixedTypes(sampleUDF, cardinality)\n+      }\n+\n+      codegenBenchmark(\"long/nullable int/string to option\", cardinality) {\n+        val sampleUDF = udf {(_: Long, b: java.lang.Integer, _: String) =>\n+          Option(b)\n+        }\n+        doRunBenchmarkWithMixedTypes(sampleUDF, cardinality)\n+      }\n+\n+      codegenBenchmark(\"long/nullable int to primitive\", cardinality) {\n+        val sampleUDF = udf {(a: Long, b: java.lang.Integer, _: String) =>\n+          Option(b).map(_.longValue()).getOrElse(a)\n+        }\n+        doRunBenchmarkWithMixedTypes(sampleUDF, cardinality)\n+      }\n+    }\n+\n+    runBenchmark(\"UDF with primitive types\") {\n+      codegenBenchmark(\"long/nullable int to string\", cardinality) {\n+        val sampleUDF = udf {(a: Long, b: java.lang.Integer) =>\n+          s\"$a,$b\"\n+        }\n+        doRunBenchmarkWithPrimitiveTypes(sampleUDF, cardinality)\n+      }\n+\n+      codegenBenchmark(\"long/nullable int to option\", cardinality) {\n+        val sampleUDF = udf {(_: Long, b: java.lang.Integer) =>\n+          Option(b)\n+        }\n+        doRunBenchmarkWithPrimitiveTypes(sampleUDF, cardinality)\n+      }\n+\n+      codegenBenchmark(\"long/nullable int/string to primitive\", cardinality) {"
  }],
  "prId": 24636
}, {
  "comments": [{
    "author": {
      "login": "skambha"
    },
    "body": "\"long/nullable int to primitive\" , change to \"long/nullable int/string to primitive\"",
    "commit": "010b3d449403a2d215a18b425722810470147ad6",
    "createdAt": "2019-05-22T00:51:08Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{IntegerType, StringType}\n+\n+/**\n+ * Synthetic benchmark for Scala User Defined Functions.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <sql core test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result:\n+ *      SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/UDFBenchmark-results.txt\".\n+ * }}}\n+ */\n+object UDFBenchmark extends SqlBasedBenchmark {\n+\n+  private def doRunBenchmarkWithMixedTypes(udf: UserDefinedFunction, cardinality: Int): Unit = {\n+    val idCol = col(\"id\")\n+    val nullableIntCol = when(\n+      idCol % 2 === 0, idCol.cast(IntegerType)).otherwise(Literal(null, IntegerType))\n+    val stringCol = idCol.cast(StringType)\n+    spark.range(cardinality).select(\n+      udf(idCol, nullableIntCol, stringCol)).write.format(\"noop\").save()\n+  }\n+\n+  private def doRunBenchmarkWithPrimitiveTypes(\n+      udf: UserDefinedFunction, cardinality: Int): Unit = {\n+    val idCol = col(\"id\")\n+    val nullableIntCol = when(\n+      idCol % 2 === 0, idCol.cast(IntegerType)).otherwise(Literal(null, IntegerType))\n+    spark.range(cardinality).select(udf(idCol, nullableIntCol)).write.format(\"noop\").save()\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val cardinality = 100000\n+    runBenchmark(\"UDF with mixed input types\") {\n+      codegenBenchmark(\"long/nullable int/string to string\", cardinality) {\n+        val sampleUDF = udf {(a: Long, b: java.lang.Integer, c: String) =>\n+          s\"$a,$b,$c\"\n+        }\n+        doRunBenchmarkWithMixedTypes(sampleUDF, cardinality)\n+      }\n+\n+      codegenBenchmark(\"long/nullable int/string to option\", cardinality) {\n+        val sampleUDF = udf {(_: Long, b: java.lang.Integer, _: String) =>\n+          Option(b)\n+        }\n+        doRunBenchmarkWithMixedTypes(sampleUDF, cardinality)\n+      }\n+\n+      codegenBenchmark(\"long/nullable int to primitive\", cardinality) {"
  }],
  "prId": 24636
}, {
  "comments": [{
    "author": {
      "login": "skambha"
    },
    "body": "Thanks @mgaido91 for your work on this performance improvement.  I'm curious if you tried the JIRA test case from @JoshRosen with your changes.  How close does this get us?  Also do you think it might be worthwhile to add that test in this benchmark suite as well. ",
    "commit": "010b3d449403a2d215a18b425722810470147ad6",
    "createdAt": "2019-05-22T01:03:11Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{IntegerType, StringType}\n+\n+/**\n+ * Synthetic benchmark for Scala User Defined Functions.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <sql core test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result:\n+ *      SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/UDFBenchmark-results.txt\".\n+ * }}}\n+ */\n+object UDFBenchmark extends SqlBasedBenchmark {",
    "line": 38
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "Well, everything can be added. If you think it is critical, we can add it. Indeed, the test reported in the description is very similar, as it is doing a `+ 1`, which is not so different from an identity. I think the point here is to identify how much from the overhead is saved, and the tests performed show that the overhead is reduced significantly.",
    "commit": "010b3d449403a2d215a18b425722810470147ad6",
    "createdAt": "2019-05-22T07:45:47Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{IntegerType, StringType}\n+\n+/**\n+ * Synthetic benchmark for Scala User Defined Functions.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <sql core test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result:\n+ *      SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/UDFBenchmark-results.txt\".\n+ * }}}\n+ */\n+object UDFBenchmark extends SqlBasedBenchmark {",
    "line": 38
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "Anyway I added it, as shown in the results the overhead is now ~ 20% instead of ~50%",
    "commit": "010b3d449403a2d215a18b425722810470147ad6",
    "createdAt": "2019-05-22T08:31:27Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{IntegerType, StringType}\n+\n+/**\n+ * Synthetic benchmark for Scala User Defined Functions.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <sql core test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result:\n+ *      SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/UDFBenchmark-results.txt\".\n+ * }}}\n+ */\n+object UDFBenchmark extends SqlBasedBenchmark {",
    "line": 38
  }],
  "prId": 24636
}]