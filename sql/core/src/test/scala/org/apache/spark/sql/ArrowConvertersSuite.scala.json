[{
  "comments": [{
    "author": {
      "login": "julienledem"
    },
    "body": "there should be code in parquet-vector.jar for this",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T03:23:15Z",
    "diffHunk": "@@ -0,0 +1,567 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets\n+import java.sql.{Date, Timestamp}\n+import java.text.SimpleDateFormat\n+import java.util.Locale\n+\n+import com.google.common.io.Files\n+import org.apache.arrow.vector.{VectorLoader, VectorSchemaRoot}\n+import org.apache.arrow.vector.file.json.JsonFileReader\n+import org.apache.arrow.vector.util.Validator\n+import org.json4s.jackson.JsonMethods._\n+import org.json4s.JsonAST._\n+import org.json4s.JsonDSL._\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n+  import testImplicits._\n+\n+  private var tempDataPath: String = _\n+\n+  private def collectAsArrow(df: DataFrame,\n+                             converter: Option[ArrowConverters] = None): ArrowPayload = {\n+    val cnvtr = converter.getOrElse(new ArrowConverters)\n+    val payloadByteArrays = df.toArrowPayloadBytes().collect()\n+    cnvtr.readPayloadByteArrays(payloadByteArrays)\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    tempDataPath = Utils.createTempDir(namePrefix = \"arrow\").getAbsolutePath\n+  }\n+\n+  test(\"collect to arrow record batch\") {\n+    val indexData = (1 to 6).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(indexData)\n+    assert(arrowPayload.nonEmpty)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length == indexData.rdd.getNumPartitions)\n+    val rowCount = arrowBatches.map(batch => batch.getLength).sum\n+    assert(rowCount === indexData.count())\n+    arrowBatches.foreach(batch => assert(batch.getNodes.size() > 0))\n+    arrowBatches.foreach(batch => batch.close())\n+  }\n+\n+  test(\"numeric type conversion\") {\n+    collectAndValidate(indexData)\n+    collectAndValidate(shortData)\n+    collectAndValidate(intData)\n+    collectAndValidate(longData)\n+    collectAndValidate(floatData)\n+    collectAndValidate(doubleData)\n+  }\n+\n+  test(\"mixed numeric type conversion\") {\n+    collectAndValidate(mixedNumericData)\n+  }\n+\n+  test(\"boolean type conversion\") {\n+    collectAndValidate(boolData)\n+  }\n+\n+  test(\"string type conversion\") {\n+    collectAndValidate(stringData)\n+  }\n+\n+  test(\"byte type conversion\") {\n+    collectAndValidate(byteData)\n+  }\n+\n+  test(\"timestamp conversion\") {\n+    collectAndValidate(timestampData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"date conversion\") {\n+    // collectAndValidate(dateTimeData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"binary type conversion\") {\n+    // collectAndValidate(binaryData)\n+  }\n+\n+  test(\"floating-point NaN\") {\n+    collectAndValidate(floatNaNData)\n+  }\n+\n+  test(\"partitioned DataFrame\") {\n+    val converter = new ArrowConverters\n+    val schema = testData2.schema\n+    val arrowPayload = collectAsArrow(testData2, Some(converter))\n+    val arrowBatches = arrowPayload.toArray\n+    // NOTE: testData2 should have 2 partitions -> 2 arrow batches in payload\n+    assert(arrowBatches.length === 2)\n+    val pl1 = new ArrowStaticPayload(arrowBatches(0))\n+    val pl2 = new ArrowStaticPayload(arrowBatches(1))\n+    // Generate JSON files\n+    val a = List[Int](1, 1, 2, 2, 3, 3)\n+    val b = List[Int](1, 2, 1, 2, 1, 2)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b\", is_signed = true, 32, nullable = false))\n+    def getBatch(x: Seq[Int], y: Seq[Int]): JSONRecordBatch = {\n+      val columns = Seq(new PrimitiveColumn(\"a\", x.length, x.map(_ => true), x),\n+        new PrimitiveColumn(\"b\", y.length, y.map(_ => true), y))\n+      new JSONRecordBatch(x.length, columns)\n+    }\n+    val json1 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.take(3), b.take(3))))\n+    val json2 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.takeRight(3), b.takeRight(3))))\n+    val tempFile1 = new File(tempDataPath, \"testData2-ints-part1.json\")\n+    val tempFile2 = new File(tempDataPath, \"testData2-ints-part2.json\")\n+    json1.write(tempFile1)\n+    json2.write(tempFile2)\n+    validateConversion(schema, pl1, tempFile1, Some(converter))\n+    validateConversion(schema, pl2, tempFile2, Some(converter))\n+  }\n+\n+  test(\"empty frame collect\") {\n+    val arrowPayload = collectAsArrow(spark.emptyDataFrame)\n+    assert(arrowPayload.isEmpty)\n+  }\n+\n+  test(\"empty partition collect\") {\n+    val emptyPart = spark.sparkContext.parallelize(Seq(1), 2).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(emptyPart)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length === 2)\n+    assert(arrowBatches.count(_.getLength == 0) === 1)\n+    assert(arrowBatches.count(_.getLength == 1) === 1)\n+  }\n+\n+  test(\"unsupported types\") {\n+    def runUnsupported(block: => Unit): Unit = {\n+      val msg = intercept[SparkException] {\n+        block\n+      }\n+      assert(msg.getMessage.contains(\"Unsupported data type\"))\n+      assert(msg.getCause.getClass === classOf[UnsupportedOperationException])\n+    }\n+\n+    runUnsupported { collectAsArrow(decimalData) }\n+    runUnsupported { collectAsArrow(arrayData.toDF()) }\n+    runUnsupported { collectAsArrow(mapData.toDF()) }\n+    runUnsupported { collectAsArrow(complexData) }\n+  }\n+\n+  test(\"test Arrow Validator\") {\n+    val sdata = shortData\n+    val idata = intData\n+\n+    // Different schema\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(sdata.df, idata.json, idata.file))\n+    }\n+\n+    // Different values\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(idata.df.sort($\"a_i\".desc), idata.json, idata.file))\n+    }\n+  }\n+\n+  /** Test that a converted DataFrame to Arrow record batch equals batch read from JSON file */\n+  private def collectAndValidate(data: DataTuple): Unit = {\n+    val converter = new ArrowConverters\n+    // NOTE: coalesce to single partition because can only load 1 batch in validator\n+    val arrowPayload = collectAsArrow(data.df.coalesce(1), Some(converter))\n+    val tempFile = new File(tempDataPath, data.file)\n+    data.json.write(tempFile)\n+    validateConversion(data.df.schema, arrowPayload, tempFile, Some(converter))\n+  }\n+\n+  private def validateConversion(sparkSchema: StructType,\n+                                 arrowPayload: ArrowPayload,\n+                                 jsonFile: File,\n+                                 converterOpt: Option[ArrowConverters] = None): Unit = {\n+    val converter = converterOpt.getOrElse(new ArrowConverters)\n+    val jsonReader = new JsonFileReader(jsonFile, converter.allocator)\n+\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(sparkSchema)\n+    val jsonSchema = jsonReader.start()\n+    Validator.compareSchemas(arrowSchema, jsonSchema)\n+\n+    val arrowRoot = new VectorSchemaRoot(arrowSchema, converter.allocator)\n+    val vectorLoader = new VectorLoader(arrowRoot)\n+    arrowPayload.foreach(vectorLoader.load)\n+    val jsonRoot = jsonReader.read()\n+    Validator.compareVectorSchemaRoot(arrowRoot, jsonRoot)\n+  }\n+\n+  // Create Spark DataFrame and matching Arrow JSON at same time for validation\n+  private case class DataTuple(df: DataFrame, json: JSONFile, file: String)\n+\n+  private def indexData: DataTuple = {\n+    val data = List[Int](1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"i\", is_signed = true, 32, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"i\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"i\"), new JSONFile(schema, Seq(batch)), \"indexData-ints.json\")\n+  }\n+\n+  private def shortData: DataTuple = {\n+    val a_s = List[Short](1, -1, 2, -2, 32767, -32768)\n+    val b_s = List[Option[Short]](Some(1), None, None, Some(-2), None, Some(-32768))\n+    val fields = Seq(new IntegerType(\"a_s\", is_signed = true, 16, nullable = false),\n+      new IntegerType(\"b_s\", is_signed = true, 16, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val b_s_values = b_s.map(_.map(_.toInt).getOrElse(0))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_s\", a_s.length, a_s.map(_ => true), a_s.map(_.toInt)),\n+      new PrimitiveColumn(\"b_s\", b_s.length, b_s.map(_.isDefined), b_s_values))\n+    val batch = new JSONRecordBatch(a_s.length, columns)\n+    val df = a_s.zip(b_s).toDF(\"a_s\", \"b_s\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-16bit.json\")\n+  }\n+\n+  private def intData: DataTuple = {\n+    val a_i = List[Int](1, -1, 2, -2, 2147483647, -2147483648)\n+    val b_i = List[Option[Int]](Some(1), None, None, Some(-2), None, Some(-2147483648))\n+    val fields = Seq(new IntegerType(\"a_i\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b_i\", is_signed = true, 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_i\", a_i.length, a_i.map(_ => true), a_i),\n+      new PrimitiveColumn(\"b_i\", b_i.length, b_i.map(_.isDefined), b_i.map(_.getOrElse(0))))\n+    val batch = new JSONRecordBatch(a_i.length, columns)\n+    val df = a_i.zip(b_i).toDF(\"a_i\", \"b_i\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-32bit.json\")\n+  }\n+\n+  private def longData: DataTuple = {\n+    val a_l = List[Long](1, -1, 2, -2, 9223372036854775807L, -9223372036854775808L)\n+    val b_l = List[Option[Long]](Some(1), None, None, Some(-2), None, Some(-9223372036854775808L))\n+    val fields = Seq(new IntegerType(\"a_l\", is_signed = true, 64, nullable = false),\n+      new IntegerType(\"b_l\", is_signed = true, 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_l\", a_l.length, a_l.map(_ => true), a_l),\n+      new PrimitiveColumn(\"b_l\", b_l.length, b_l.map(_.isDefined), b_l.map(_.getOrElse(0L))))\n+    val batch = new JSONRecordBatch(a_l.length, columns)\n+    val df = a_l.zip(b_l).toDF(\"a_l\", \"b_l\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-64bit.json\")\n+  }\n+\n+  private def floatData: DataTuple = {\n+    val a_f = List(1.0f, 2.0f, 0.01f, 200.0f, 0.0001f, 20000.0f)\n+    val b_f = List[Option[Float]](Some(1.1f), None, None, Some(2.2f), None, Some(3.3f))\n+    val fields = Seq(new FloatingPointType(\"a_f\", 32, nullable = false),\n+      new FloatingPointType(\"b_f\", 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_f\", a_f.length, a_f.map(_ => true), a_f),\n+      new PrimitiveColumn(\"b_f\", b_f.length, b_f.map(_.isDefined), b_f.map(_.getOrElse(0.0f))))\n+    val batch = new JSONRecordBatch(a_f.length, columns)\n+    val df = a_f.zip(b_f).toDF(\"a_f\", \"b_f\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-single_precision.json\")\n+  }\n+\n+  private def doubleData: DataTuple = {\n+    val a_d = List(1.0, 2.0, 0.01, 200.0, 0.0001, 20000.0)\n+    val b_d = List[Option[Double]](Some(1.1), None, None, Some(2.2), None, Some(3.3))\n+    val fields = Seq(new FloatingPointType(\"a_d\", 64, nullable = false),\n+      new FloatingPointType(\"b_d\", 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_d\", a_d.length, a_d.map(_ => true), a_d),\n+      new PrimitiveColumn(\"b_d\", b_d.length, b_d.map(_.isDefined), b_d.map(_.getOrElse(0.0))))\n+    val batch = new JSONRecordBatch(a_d.length, columns)\n+    val df = a_d.zip(b_d).toDF(\"a_d\", \"b_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-double_precision.json\")\n+  }\n+\n+  private def mixedNumericData: DataTuple = {\n+    val data = List(1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 16, nullable = false),\n+      new FloatingPointType(\"b\", 32, nullable = false),\n+      new IntegerType(\"c\", is_signed = true, 32, nullable = false),\n+      new FloatingPointType(\"d\", 64, nullable = false),\n+      new IntegerType(\"e\", is_signed = true, 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"b\", data.length, data.map(_ => true), data.map(_.toFloat)),\n+      new PrimitiveColumn(\"c\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"d\", data.length, data.map(_ => true), data.map(_.toDouble)),\n+      new PrimitiveColumn(\"e\", data.length, data.map(_ => true), data)\n+    )\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    val data_tuples = for (d <- data) yield {\n+      (d.toShort, d.toFloat, d.toInt, d.toDouble, d.toLong)\n+    }\n+    val df = data_tuples.toDF(\"a\", \"b\", \"c\", \"d\", \"e\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"mixed_numeric_types.json\")\n+  }\n+\n+  private def boolData: DataTuple = {\n+    val data = Seq(true, true, false, true)\n+    val fields = Seq(new BooleanType(\"a_bool\", nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_bool\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_bool\"), new JSONFile(schema, Seq(batch)), \"boolData.json\")\n+  }\n+\n+  private def stringData: DataTuple = {\n+    val upperCase = Seq(\"A\", \"B\", \"C\")\n+    val lowerCase = Seq(\"a\", \"b\", \"c\")\n+    val nullStr = Seq(\"ab\", \"CDE\", null)\n+    val fields = Seq(new StringType(\"upper_case\", nullable = true),\n+      new StringType(\"lower_case\", nullable = true),\n+      new StringType(\"null_str\", nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new StringColumn(\"upper_case\", upperCase.length, upperCase.map(_ => true), upperCase),\n+      new StringColumn(\"lower_case\", lowerCase.length, lowerCase.map(_ => true), lowerCase),\n+      new StringColumn(\"null_str\", nullStr.length, nullStr.map(_ != null),\n+        nullStr.map { s => if (s == null) \"\" else s}\n+      ))\n+    val batch = new JSONRecordBatch(upperCase.length, columns)\n+    val df = (upperCase, lowerCase, nullStr).zipped.toList\n+      .toDF(\"upper_case\", \"lower_case\", \"null_str\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"stringData.json\")\n+  }\n+\n+  private def byteData: DataTuple = {\n+    val data = List[Byte](1.toByte, (-1).toByte, 64.toByte, Byte.MaxValue)\n+    val fields = Seq(new IntegerType(\"a_byte\", is_signed = true, 8, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_byte\", data.length, data.map(_ => true), data.map(_.toInt)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_byte\"), new JSONFile(schema, Seq(batch)), \"byteData.json\")\n+  }\n+\n+  private def floatNaNData: DataTuple = {\n+    val fnan = Seq(1.2F, Float.NaN)\n+    val dnan = Seq(Double.NaN, 1.2)\n+    val fields = Seq(new FloatingPointType(\"NaN_f\", 32, nullable = false),\n+      new FloatingPointType(\"NaN_d\", 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"NaN_f\", fnan.length, fnan.map(_ => true), fnan),\n+      new PrimitiveColumn(\"NaN_d\", dnan.length, dnan.map(_ => true), dnan))\n+    val batch = new JSONRecordBatch(fnan.length, columns)\n+    val df = fnan.zip(dnan).toDF(\"NaN_f\", \"NaN_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"nanData-floating_point.json\")\n+  }\n+\n+  private def timestampData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val data = Seq(ts1, ts2)\n+    val schema = new JSONSchema(Seq(new TimestampType(\"c_timestamp\")))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"c_timestamp\", data.length, data.map(_ => true), data.map(_.getTime)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"c_timestamp\"), new JSONFile(schema, Seq(batch)), \"timestampData.json\")\n+  }\n+\n+  private def dateData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d2 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val df = Seq((d1, sdf.format(d1), ts1), (d2, sdf.format(d2), ts2))\n+      .toDF(\"a_date\", \"b_string\", \"c_timestamp\")\n+    val jsonFile = new JSONFile(new JSONSchema(Seq.empty[DataType]), Seq.empty[JSONRecordBatch])\n+    DataTuple(df, jsonFile, \"dateData.json\")\n+  }\n+\n+  /**\n+   * Arrow JSON Format Data Generation\n+   * Referenced from https://github.com/apache/arrow/blob/master/integration/integration_test.py\n+   */"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "Thanks, I'll take a look",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-03-10T18:52:00Z",
    "diffHunk": "@@ -0,0 +1,567 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets\n+import java.sql.{Date, Timestamp}\n+import java.text.SimpleDateFormat\n+import java.util.Locale\n+\n+import com.google.common.io.Files\n+import org.apache.arrow.vector.{VectorLoader, VectorSchemaRoot}\n+import org.apache.arrow.vector.file.json.JsonFileReader\n+import org.apache.arrow.vector.util.Validator\n+import org.json4s.jackson.JsonMethods._\n+import org.json4s.JsonAST._\n+import org.json4s.JsonDSL._\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n+  import testImplicits._\n+\n+  private var tempDataPath: String = _\n+\n+  private def collectAsArrow(df: DataFrame,\n+                             converter: Option[ArrowConverters] = None): ArrowPayload = {\n+    val cnvtr = converter.getOrElse(new ArrowConverters)\n+    val payloadByteArrays = df.toArrowPayloadBytes().collect()\n+    cnvtr.readPayloadByteArrays(payloadByteArrays)\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    tempDataPath = Utils.createTempDir(namePrefix = \"arrow\").getAbsolutePath\n+  }\n+\n+  test(\"collect to arrow record batch\") {\n+    val indexData = (1 to 6).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(indexData)\n+    assert(arrowPayload.nonEmpty)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length == indexData.rdd.getNumPartitions)\n+    val rowCount = arrowBatches.map(batch => batch.getLength).sum\n+    assert(rowCount === indexData.count())\n+    arrowBatches.foreach(batch => assert(batch.getNodes.size() > 0))\n+    arrowBatches.foreach(batch => batch.close())\n+  }\n+\n+  test(\"numeric type conversion\") {\n+    collectAndValidate(indexData)\n+    collectAndValidate(shortData)\n+    collectAndValidate(intData)\n+    collectAndValidate(longData)\n+    collectAndValidate(floatData)\n+    collectAndValidate(doubleData)\n+  }\n+\n+  test(\"mixed numeric type conversion\") {\n+    collectAndValidate(mixedNumericData)\n+  }\n+\n+  test(\"boolean type conversion\") {\n+    collectAndValidate(boolData)\n+  }\n+\n+  test(\"string type conversion\") {\n+    collectAndValidate(stringData)\n+  }\n+\n+  test(\"byte type conversion\") {\n+    collectAndValidate(byteData)\n+  }\n+\n+  test(\"timestamp conversion\") {\n+    collectAndValidate(timestampData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"date conversion\") {\n+    // collectAndValidate(dateTimeData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"binary type conversion\") {\n+    // collectAndValidate(binaryData)\n+  }\n+\n+  test(\"floating-point NaN\") {\n+    collectAndValidate(floatNaNData)\n+  }\n+\n+  test(\"partitioned DataFrame\") {\n+    val converter = new ArrowConverters\n+    val schema = testData2.schema\n+    val arrowPayload = collectAsArrow(testData2, Some(converter))\n+    val arrowBatches = arrowPayload.toArray\n+    // NOTE: testData2 should have 2 partitions -> 2 arrow batches in payload\n+    assert(arrowBatches.length === 2)\n+    val pl1 = new ArrowStaticPayload(arrowBatches(0))\n+    val pl2 = new ArrowStaticPayload(arrowBatches(1))\n+    // Generate JSON files\n+    val a = List[Int](1, 1, 2, 2, 3, 3)\n+    val b = List[Int](1, 2, 1, 2, 1, 2)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b\", is_signed = true, 32, nullable = false))\n+    def getBatch(x: Seq[Int], y: Seq[Int]): JSONRecordBatch = {\n+      val columns = Seq(new PrimitiveColumn(\"a\", x.length, x.map(_ => true), x),\n+        new PrimitiveColumn(\"b\", y.length, y.map(_ => true), y))\n+      new JSONRecordBatch(x.length, columns)\n+    }\n+    val json1 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.take(3), b.take(3))))\n+    val json2 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.takeRight(3), b.takeRight(3))))\n+    val tempFile1 = new File(tempDataPath, \"testData2-ints-part1.json\")\n+    val tempFile2 = new File(tempDataPath, \"testData2-ints-part2.json\")\n+    json1.write(tempFile1)\n+    json2.write(tempFile2)\n+    validateConversion(schema, pl1, tempFile1, Some(converter))\n+    validateConversion(schema, pl2, tempFile2, Some(converter))\n+  }\n+\n+  test(\"empty frame collect\") {\n+    val arrowPayload = collectAsArrow(spark.emptyDataFrame)\n+    assert(arrowPayload.isEmpty)\n+  }\n+\n+  test(\"empty partition collect\") {\n+    val emptyPart = spark.sparkContext.parallelize(Seq(1), 2).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(emptyPart)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length === 2)\n+    assert(arrowBatches.count(_.getLength == 0) === 1)\n+    assert(arrowBatches.count(_.getLength == 1) === 1)\n+  }\n+\n+  test(\"unsupported types\") {\n+    def runUnsupported(block: => Unit): Unit = {\n+      val msg = intercept[SparkException] {\n+        block\n+      }\n+      assert(msg.getMessage.contains(\"Unsupported data type\"))\n+      assert(msg.getCause.getClass === classOf[UnsupportedOperationException])\n+    }\n+\n+    runUnsupported { collectAsArrow(decimalData) }\n+    runUnsupported { collectAsArrow(arrayData.toDF()) }\n+    runUnsupported { collectAsArrow(mapData.toDF()) }\n+    runUnsupported { collectAsArrow(complexData) }\n+  }\n+\n+  test(\"test Arrow Validator\") {\n+    val sdata = shortData\n+    val idata = intData\n+\n+    // Different schema\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(sdata.df, idata.json, idata.file))\n+    }\n+\n+    // Different values\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(idata.df.sort($\"a_i\".desc), idata.json, idata.file))\n+    }\n+  }\n+\n+  /** Test that a converted DataFrame to Arrow record batch equals batch read from JSON file */\n+  private def collectAndValidate(data: DataTuple): Unit = {\n+    val converter = new ArrowConverters\n+    // NOTE: coalesce to single partition because can only load 1 batch in validator\n+    val arrowPayload = collectAsArrow(data.df.coalesce(1), Some(converter))\n+    val tempFile = new File(tempDataPath, data.file)\n+    data.json.write(tempFile)\n+    validateConversion(data.df.schema, arrowPayload, tempFile, Some(converter))\n+  }\n+\n+  private def validateConversion(sparkSchema: StructType,\n+                                 arrowPayload: ArrowPayload,\n+                                 jsonFile: File,\n+                                 converterOpt: Option[ArrowConverters] = None): Unit = {\n+    val converter = converterOpt.getOrElse(new ArrowConverters)\n+    val jsonReader = new JsonFileReader(jsonFile, converter.allocator)\n+\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(sparkSchema)\n+    val jsonSchema = jsonReader.start()\n+    Validator.compareSchemas(arrowSchema, jsonSchema)\n+\n+    val arrowRoot = new VectorSchemaRoot(arrowSchema, converter.allocator)\n+    val vectorLoader = new VectorLoader(arrowRoot)\n+    arrowPayload.foreach(vectorLoader.load)\n+    val jsonRoot = jsonReader.read()\n+    Validator.compareVectorSchemaRoot(arrowRoot, jsonRoot)\n+  }\n+\n+  // Create Spark DataFrame and matching Arrow JSON at same time for validation\n+  private case class DataTuple(df: DataFrame, json: JSONFile, file: String)\n+\n+  private def indexData: DataTuple = {\n+    val data = List[Int](1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"i\", is_signed = true, 32, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"i\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"i\"), new JSONFile(schema, Seq(batch)), \"indexData-ints.json\")\n+  }\n+\n+  private def shortData: DataTuple = {\n+    val a_s = List[Short](1, -1, 2, -2, 32767, -32768)\n+    val b_s = List[Option[Short]](Some(1), None, None, Some(-2), None, Some(-32768))\n+    val fields = Seq(new IntegerType(\"a_s\", is_signed = true, 16, nullable = false),\n+      new IntegerType(\"b_s\", is_signed = true, 16, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val b_s_values = b_s.map(_.map(_.toInt).getOrElse(0))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_s\", a_s.length, a_s.map(_ => true), a_s.map(_.toInt)),\n+      new PrimitiveColumn(\"b_s\", b_s.length, b_s.map(_.isDefined), b_s_values))\n+    val batch = new JSONRecordBatch(a_s.length, columns)\n+    val df = a_s.zip(b_s).toDF(\"a_s\", \"b_s\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-16bit.json\")\n+  }\n+\n+  private def intData: DataTuple = {\n+    val a_i = List[Int](1, -1, 2, -2, 2147483647, -2147483648)\n+    val b_i = List[Option[Int]](Some(1), None, None, Some(-2), None, Some(-2147483648))\n+    val fields = Seq(new IntegerType(\"a_i\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b_i\", is_signed = true, 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_i\", a_i.length, a_i.map(_ => true), a_i),\n+      new PrimitiveColumn(\"b_i\", b_i.length, b_i.map(_.isDefined), b_i.map(_.getOrElse(0))))\n+    val batch = new JSONRecordBatch(a_i.length, columns)\n+    val df = a_i.zip(b_i).toDF(\"a_i\", \"b_i\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-32bit.json\")\n+  }\n+\n+  private def longData: DataTuple = {\n+    val a_l = List[Long](1, -1, 2, -2, 9223372036854775807L, -9223372036854775808L)\n+    val b_l = List[Option[Long]](Some(1), None, None, Some(-2), None, Some(-9223372036854775808L))\n+    val fields = Seq(new IntegerType(\"a_l\", is_signed = true, 64, nullable = false),\n+      new IntegerType(\"b_l\", is_signed = true, 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_l\", a_l.length, a_l.map(_ => true), a_l),\n+      new PrimitiveColumn(\"b_l\", b_l.length, b_l.map(_.isDefined), b_l.map(_.getOrElse(0L))))\n+    val batch = new JSONRecordBatch(a_l.length, columns)\n+    val df = a_l.zip(b_l).toDF(\"a_l\", \"b_l\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-64bit.json\")\n+  }\n+\n+  private def floatData: DataTuple = {\n+    val a_f = List(1.0f, 2.0f, 0.01f, 200.0f, 0.0001f, 20000.0f)\n+    val b_f = List[Option[Float]](Some(1.1f), None, None, Some(2.2f), None, Some(3.3f))\n+    val fields = Seq(new FloatingPointType(\"a_f\", 32, nullable = false),\n+      new FloatingPointType(\"b_f\", 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_f\", a_f.length, a_f.map(_ => true), a_f),\n+      new PrimitiveColumn(\"b_f\", b_f.length, b_f.map(_.isDefined), b_f.map(_.getOrElse(0.0f))))\n+    val batch = new JSONRecordBatch(a_f.length, columns)\n+    val df = a_f.zip(b_f).toDF(\"a_f\", \"b_f\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-single_precision.json\")\n+  }\n+\n+  private def doubleData: DataTuple = {\n+    val a_d = List(1.0, 2.0, 0.01, 200.0, 0.0001, 20000.0)\n+    val b_d = List[Option[Double]](Some(1.1), None, None, Some(2.2), None, Some(3.3))\n+    val fields = Seq(new FloatingPointType(\"a_d\", 64, nullable = false),\n+      new FloatingPointType(\"b_d\", 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_d\", a_d.length, a_d.map(_ => true), a_d),\n+      new PrimitiveColumn(\"b_d\", b_d.length, b_d.map(_.isDefined), b_d.map(_.getOrElse(0.0))))\n+    val batch = new JSONRecordBatch(a_d.length, columns)\n+    val df = a_d.zip(b_d).toDF(\"a_d\", \"b_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-double_precision.json\")\n+  }\n+\n+  private def mixedNumericData: DataTuple = {\n+    val data = List(1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 16, nullable = false),\n+      new FloatingPointType(\"b\", 32, nullable = false),\n+      new IntegerType(\"c\", is_signed = true, 32, nullable = false),\n+      new FloatingPointType(\"d\", 64, nullable = false),\n+      new IntegerType(\"e\", is_signed = true, 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"b\", data.length, data.map(_ => true), data.map(_.toFloat)),\n+      new PrimitiveColumn(\"c\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"d\", data.length, data.map(_ => true), data.map(_.toDouble)),\n+      new PrimitiveColumn(\"e\", data.length, data.map(_ => true), data)\n+    )\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    val data_tuples = for (d <- data) yield {\n+      (d.toShort, d.toFloat, d.toInt, d.toDouble, d.toLong)\n+    }\n+    val df = data_tuples.toDF(\"a\", \"b\", \"c\", \"d\", \"e\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"mixed_numeric_types.json\")\n+  }\n+\n+  private def boolData: DataTuple = {\n+    val data = Seq(true, true, false, true)\n+    val fields = Seq(new BooleanType(\"a_bool\", nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_bool\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_bool\"), new JSONFile(schema, Seq(batch)), \"boolData.json\")\n+  }\n+\n+  private def stringData: DataTuple = {\n+    val upperCase = Seq(\"A\", \"B\", \"C\")\n+    val lowerCase = Seq(\"a\", \"b\", \"c\")\n+    val nullStr = Seq(\"ab\", \"CDE\", null)\n+    val fields = Seq(new StringType(\"upper_case\", nullable = true),\n+      new StringType(\"lower_case\", nullable = true),\n+      new StringType(\"null_str\", nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new StringColumn(\"upper_case\", upperCase.length, upperCase.map(_ => true), upperCase),\n+      new StringColumn(\"lower_case\", lowerCase.length, lowerCase.map(_ => true), lowerCase),\n+      new StringColumn(\"null_str\", nullStr.length, nullStr.map(_ != null),\n+        nullStr.map { s => if (s == null) \"\" else s}\n+      ))\n+    val batch = new JSONRecordBatch(upperCase.length, columns)\n+    val df = (upperCase, lowerCase, nullStr).zipped.toList\n+      .toDF(\"upper_case\", \"lower_case\", \"null_str\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"stringData.json\")\n+  }\n+\n+  private def byteData: DataTuple = {\n+    val data = List[Byte](1.toByte, (-1).toByte, 64.toByte, Byte.MaxValue)\n+    val fields = Seq(new IntegerType(\"a_byte\", is_signed = true, 8, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_byte\", data.length, data.map(_ => true), data.map(_.toInt)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_byte\"), new JSONFile(schema, Seq(batch)), \"byteData.json\")\n+  }\n+\n+  private def floatNaNData: DataTuple = {\n+    val fnan = Seq(1.2F, Float.NaN)\n+    val dnan = Seq(Double.NaN, 1.2)\n+    val fields = Seq(new FloatingPointType(\"NaN_f\", 32, nullable = false),\n+      new FloatingPointType(\"NaN_d\", 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"NaN_f\", fnan.length, fnan.map(_ => true), fnan),\n+      new PrimitiveColumn(\"NaN_d\", dnan.length, dnan.map(_ => true), dnan))\n+    val batch = new JSONRecordBatch(fnan.length, columns)\n+    val df = fnan.zip(dnan).toDF(\"NaN_f\", \"NaN_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"nanData-floating_point.json\")\n+  }\n+\n+  private def timestampData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val data = Seq(ts1, ts2)\n+    val schema = new JSONSchema(Seq(new TimestampType(\"c_timestamp\")))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"c_timestamp\", data.length, data.map(_ => true), data.map(_.getTime)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"c_timestamp\"), new JSONFile(schema, Seq(batch)), \"timestampData.json\")\n+  }\n+\n+  private def dateData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d2 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val df = Seq((d1, sdf.format(d1), ts1), (d2, sdf.format(d2), ts2))\n+      .toDF(\"a_date\", \"b_string\", \"c_timestamp\")\n+    val jsonFile = new JSONFile(new JSONSchema(Seq.empty[DataType]), Seq.empty[JSONRecordBatch])\n+    DataTuple(df, jsonFile, \"dateData.json\")\n+  }\n+\n+  /**\n+   * Arrow JSON Format Data Generation\n+   * Referenced from https://github.com/apache/arrow/blob/master/integration/integration_test.py\n+   */"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "separate these into different test cases, and please inline the data directly in each test case. It's pretty annoying to have to jump around.\r\n",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T07:19:56Z",
    "diffHunk": "@@ -0,0 +1,568 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets\n+import java.sql.{Date, Timestamp}\n+import java.text.SimpleDateFormat\n+import java.util.Locale\n+\n+import com.google.common.io.Files\n+import org.apache.arrow.vector.{VectorLoader, VectorSchemaRoot}\n+import org.apache.arrow.vector.file.json.JsonFileReader\n+import org.apache.arrow.vector.util.Validator\n+import org.json4s.jackson.JsonMethods._\n+import org.json4s.JsonAST._\n+import org.json4s.JsonDSL._\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n+  import testImplicits._\n+\n+  private var tempDataPath: String = _\n+\n+  private def collectAsArrow(df: DataFrame,\n+                             converter: Option[ArrowConverters] = None): ArrowPayload = {\n+    val cnvtr = converter.getOrElse(new ArrowConverters)\n+    val payloadByteArrays = df.toArrowPayloadBytes().collect()\n+    cnvtr.readPayloadByteArrays(payloadByteArrays)\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    tempDataPath = Utils.createTempDir(namePrefix = \"arrow\").getAbsolutePath\n+  }\n+\n+  test(\"collect to arrow record batch\") {\n+    val indexData = (1 to 6).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(indexData)\n+    assert(arrowPayload.nonEmpty)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length == indexData.rdd.getNumPartitions)\n+    val rowCount = arrowBatches.map(batch => batch.getLength).sum\n+    assert(rowCount === indexData.count())\n+    arrowBatches.foreach(batch => assert(batch.getNodes.size() > 0))\n+    arrowBatches.foreach(batch => batch.close())\n+  }\n+\n+  test(\"numeric type conversion\") {\n+    collectAndValidate(indexData)"
  }],
  "prId": 15821
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "rather than defining the json using objects and serialize them, can we just put the json as a string inline? that'd be much easier to inspect ...\r\n",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-20T07:23:06Z",
    "diffHunk": "@@ -0,0 +1,568 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets\n+import java.sql.{Date, Timestamp}\n+import java.text.SimpleDateFormat\n+import java.util.Locale\n+\n+import com.google.common.io.Files\n+import org.apache.arrow.vector.{VectorLoader, VectorSchemaRoot}\n+import org.apache.arrow.vector.file.json.JsonFileReader\n+import org.apache.arrow.vector.util.Validator\n+import org.json4s.jackson.JsonMethods._\n+import org.json4s.JsonAST._\n+import org.json4s.JsonDSL._\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n+  import testImplicits._\n+\n+  private var tempDataPath: String = _\n+\n+  private def collectAsArrow(df: DataFrame,\n+                             converter: Option[ArrowConverters] = None): ArrowPayload = {\n+    val cnvtr = converter.getOrElse(new ArrowConverters)\n+    val payloadByteArrays = df.toArrowPayloadBytes().collect()\n+    cnvtr.readPayloadByteArrays(payloadByteArrays)\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    tempDataPath = Utils.createTempDir(namePrefix = \"arrow\").getAbsolutePath\n+  }\n+\n+  test(\"collect to arrow record batch\") {\n+    val indexData = (1 to 6).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(indexData)\n+    assert(arrowPayload.nonEmpty)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length == indexData.rdd.getNumPartitions)\n+    val rowCount = arrowBatches.map(batch => batch.getLength).sum\n+    assert(rowCount === indexData.count())\n+    arrowBatches.foreach(batch => assert(batch.getNodes.size() > 0))\n+    arrowBatches.foreach(batch => batch.close())\n+  }\n+\n+  test(\"numeric type conversion\") {\n+    collectAndValidate(indexData)\n+    collectAndValidate(shortData)\n+    collectAndValidate(intData)\n+    collectAndValidate(longData)\n+    collectAndValidate(floatData)\n+    collectAndValidate(doubleData)\n+  }\n+\n+  test(\"mixed numeric type conversion\") {\n+    collectAndValidate(mixedNumericData)\n+  }\n+\n+  test(\"boolean type conversion\") {\n+    collectAndValidate(boolData)\n+  }\n+\n+  test(\"string type conversion\") {\n+    collectAndValidate(stringData)\n+  }\n+\n+  test(\"byte type conversion\") {\n+    collectAndValidate(byteData)\n+  }\n+\n+  ignore(\"timestamp conversion\") {\n+    collectAndValidate(timestampData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"date conversion\") {\n+    // collectAndValidate(dateTimeData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"binary type conversion\") {\n+    // collectAndValidate(binaryData)\n+  }\n+\n+  test(\"floating-point NaN\") {\n+    collectAndValidate(floatNaNData)\n+  }\n+\n+  test(\"partitioned DataFrame\") {\n+    val converter = new ArrowConverters\n+    val schema = testData2.schema\n+    val arrowPayload = collectAsArrow(testData2, Some(converter))\n+    val arrowBatches = arrowPayload.toArray\n+    // NOTE: testData2 should have 2 partitions -> 2 arrow batches in payload\n+    assert(arrowBatches.length === 2)\n+    val pl1 = new ArrowStaticPayload(arrowBatches(0))\n+    val pl2 = new ArrowStaticPayload(arrowBatches(1))\n+    // Generate JSON files\n+    val a = List[Int](1, 1, 2, 2, 3, 3)\n+    val b = List[Int](1, 2, 1, 2, 1, 2)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b\", is_signed = true, 32, nullable = false))\n+    def getBatch(x: Seq[Int], y: Seq[Int]): JSONRecordBatch = {\n+      val columns = Seq(new PrimitiveColumn(\"a\", x.length, x.map(_ => true), x),\n+        new PrimitiveColumn(\"b\", y.length, y.map(_ => true), y))\n+      new JSONRecordBatch(x.length, columns)\n+    }\n+    val json1 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.take(3), b.take(3))))\n+    val json2 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.takeRight(3), b.takeRight(3))))\n+    val tempFile1 = new File(tempDataPath, \"testData2-ints-part1.json\")\n+    val tempFile2 = new File(tempDataPath, \"testData2-ints-part2.json\")\n+    json1.write(tempFile1)\n+    json2.write(tempFile2)\n+    validateConversion(schema, pl1, tempFile1, Some(converter))\n+    validateConversion(schema, pl2, tempFile2, Some(converter))\n+  }\n+\n+  test(\"empty frame collect\") {\n+    val arrowPayload = collectAsArrow(spark.emptyDataFrame)\n+    assert(arrowPayload.isEmpty)\n+  }\n+\n+  test(\"empty partition collect\") {\n+    val emptyPart = spark.sparkContext.parallelize(Seq(1), 2).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(emptyPart)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length === 2)\n+    assert(arrowBatches.count(_.getLength == 0) === 1)\n+    assert(arrowBatches.count(_.getLength == 1) === 1)\n+  }\n+\n+  test(\"unsupported types\") {\n+    def runUnsupported(block: => Unit): Unit = {\n+      val msg = intercept[SparkException] {\n+        block\n+      }\n+      assert(msg.getMessage.contains(\"Unsupported data type\"))\n+      assert(msg.getCause.getClass === classOf[UnsupportedOperationException])\n+    }\n+\n+    runUnsupported { collectAsArrow(decimalData) }\n+    runUnsupported { collectAsArrow(arrayData.toDF()) }\n+    runUnsupported { collectAsArrow(mapData.toDF()) }\n+    runUnsupported { collectAsArrow(complexData) }\n+  }\n+\n+  test(\"test Arrow Validator\") {\n+    val sdata = shortData\n+    val idata = intData\n+\n+    // Different schema\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(sdata.df, idata.json, idata.file))\n+    }\n+\n+    // Different values\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(idata.df.sort($\"a_i\".desc), idata.json, idata.file))\n+    }\n+  }\n+\n+  /** Test that a converted DataFrame to Arrow record batch equals batch read from JSON file */\n+  private def collectAndValidate(data: DataTuple): Unit = {\n+    val converter = new ArrowConverters\n+    // NOTE: coalesce to single partition because can only load 1 batch in validator\n+    val arrowPayload = collectAsArrow(data.df.coalesce(1), Some(converter))\n+    val tempFile = new File(tempDataPath, data.file)\n+    data.json.write(tempFile)\n+    validateConversion(data.df.schema, arrowPayload, tempFile, Some(converter))\n+  }\n+\n+  private def validateConversion(sparkSchema: StructType,\n+                                 arrowPayload: ArrowPayload,\n+                                 jsonFile: File,\n+                                 converterOpt: Option[ArrowConverters] = None): Unit = {\n+    val converter = converterOpt.getOrElse(new ArrowConverters)\n+    val jsonReader = new JsonFileReader(jsonFile, converter.allocator)\n+\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(sparkSchema)\n+    val jsonSchema = jsonReader.start()\n+    Validator.compareSchemas(arrowSchema, jsonSchema)\n+\n+    val arrowRoot = new VectorSchemaRoot(arrowSchema, converter.allocator)\n+    val vectorLoader = new VectorLoader(arrowRoot)\n+    arrowPayload.foreach(vectorLoader.load)\n+    val jsonRoot = jsonReader.read()\n+    Validator.compareVectorSchemaRoot(arrowRoot, jsonRoot)\n+  }\n+\n+  // Create Spark DataFrame and matching Arrow JSON at same time for validation\n+  private case class DataTuple(df: DataFrame, json: JSONFile, file: String)\n+\n+  private def indexData: DataTuple = {\n+    val data = List[Int](1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"i\", is_signed = true, 32, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"i\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"i\"), new JSONFile(schema, Seq(batch)), \"indexData-ints.json\")\n+  }\n+\n+  private def shortData: DataTuple = {\n+    val a_s = List[Short](1, -1, 2, -2, 32767, -32768)\n+    val b_s = List[Option[Short]](Some(1), None, None, Some(-2), None, Some(-32768))\n+    val fields = Seq(new IntegerType(\"a_s\", is_signed = true, 16, nullable = false),\n+      new IntegerType(\"b_s\", is_signed = true, 16, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val b_s_values = b_s.map(_.map(_.toInt).getOrElse(0))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_s\", a_s.length, a_s.map(_ => true), a_s.map(_.toInt)),\n+      new PrimitiveColumn(\"b_s\", b_s.length, b_s.map(_.isDefined), b_s_values))\n+    val batch = new JSONRecordBatch(a_s.length, columns)\n+    val df = a_s.zip(b_s).toDF(\"a_s\", \"b_s\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-16bit.json\")\n+  }\n+\n+  private def intData: DataTuple = {\n+    val a_i = List[Int](1, -1, 2, -2, 2147483647, -2147483648)\n+    val b_i = List[Option[Int]](Some(1), None, None, Some(-2), None, Some(-2147483648))\n+    val fields = Seq(new IntegerType(\"a_i\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b_i\", is_signed = true, 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_i\", a_i.length, a_i.map(_ => true), a_i),\n+      new PrimitiveColumn(\"b_i\", b_i.length, b_i.map(_.isDefined), b_i.map(_.getOrElse(0))))\n+    val batch = new JSONRecordBatch(a_i.length, columns)\n+    val df = a_i.zip(b_i).toDF(\"a_i\", \"b_i\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-32bit.json\")\n+  }\n+\n+  private def longData: DataTuple = {\n+    val a_l = List[Long](1, -1, 2, -2, 9223372036854775807L, -9223372036854775808L)\n+    val b_l = List[Option[Long]](Some(1), None, None, Some(-2), None, Some(-9223372036854775808L))\n+    val fields = Seq(new IntegerType(\"a_l\", is_signed = true, 64, nullable = false),\n+      new IntegerType(\"b_l\", is_signed = true, 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_l\", a_l.length, a_l.map(_ => true), a_l),\n+      new PrimitiveColumn(\"b_l\", b_l.length, b_l.map(_.isDefined), b_l.map(_.getOrElse(0L))))\n+    val batch = new JSONRecordBatch(a_l.length, columns)\n+    val df = a_l.zip(b_l).toDF(\"a_l\", \"b_l\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-64bit.json\")\n+  }\n+\n+  private def floatData: DataTuple = {\n+    val a_f = List(1.0f, 2.0f, 0.01f, 200.0f, 0.0001f, 20000.0f)\n+    val b_f = List[Option[Float]](Some(1.1f), None, None, Some(2.2f), None, Some(3.3f))\n+    val fields = Seq(new FloatingPointType(\"a_f\", 32, nullable = false),\n+      new FloatingPointType(\"b_f\", 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_f\", a_f.length, a_f.map(_ => true), a_f),\n+      new PrimitiveColumn(\"b_f\", b_f.length, b_f.map(_.isDefined), b_f.map(_.getOrElse(0.0f))))\n+    val batch = new JSONRecordBatch(a_f.length, columns)\n+    val df = a_f.zip(b_f).toDF(\"a_f\", \"b_f\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-single_precision.json\")\n+  }\n+\n+  private def doubleData: DataTuple = {\n+    val a_d = List(1.0, 2.0, 0.01, 200.0, 0.0001, 20000.0)\n+    val b_d = List[Option[Double]](Some(1.1), None, None, Some(2.2), None, Some(3.3))\n+    val fields = Seq(new FloatingPointType(\"a_d\", 64, nullable = false),\n+      new FloatingPointType(\"b_d\", 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_d\", a_d.length, a_d.map(_ => true), a_d),\n+      new PrimitiveColumn(\"b_d\", b_d.length, b_d.map(_.isDefined), b_d.map(_.getOrElse(0.0))))\n+    val batch = new JSONRecordBatch(a_d.length, columns)\n+    val df = a_d.zip(b_d).toDF(\"a_d\", \"b_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-double_precision.json\")\n+  }\n+\n+  private def mixedNumericData: DataTuple = {\n+    val data = List(1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 16, nullable = false),\n+      new FloatingPointType(\"b\", 32, nullable = false),\n+      new IntegerType(\"c\", is_signed = true, 32, nullable = false),\n+      new FloatingPointType(\"d\", 64, nullable = false),\n+      new IntegerType(\"e\", is_signed = true, 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"b\", data.length, data.map(_ => true), data.map(_.toFloat)),\n+      new PrimitiveColumn(\"c\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"d\", data.length, data.map(_ => true), data.map(_.toDouble)),\n+      new PrimitiveColumn(\"e\", data.length, data.map(_ => true), data)\n+    )\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    val data_tuples = for (d <- data) yield {\n+      (d.toShort, d.toFloat, d.toInt, d.toDouble, d.toLong)\n+    }\n+    val df = data_tuples.toDF(\"a\", \"b\", \"c\", \"d\", \"e\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"mixed_numeric_types.json\")\n+  }\n+\n+  private def boolData: DataTuple = {\n+    val data = Seq(true, true, false, true)\n+    val fields = Seq(new BooleanType(\"a_bool\", nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_bool\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_bool\"), new JSONFile(schema, Seq(batch)), \"boolData.json\")\n+  }\n+\n+  private def stringData: DataTuple = {\n+    val upperCase = Seq(\"A\", \"B\", \"C\")\n+    val lowerCase = Seq(\"a\", \"b\", \"c\")\n+    val nullStr = Seq(\"ab\", \"CDE\", null)\n+    val fields = Seq(new StringType(\"upper_case\", nullable = true),\n+      new StringType(\"lower_case\", nullable = true),\n+      new StringType(\"null_str\", nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new StringColumn(\"upper_case\", upperCase.length, upperCase.map(_ => true), upperCase),\n+      new StringColumn(\"lower_case\", lowerCase.length, lowerCase.map(_ => true), lowerCase),\n+      new StringColumn(\"null_str\", nullStr.length, nullStr.map(_ != null),\n+        nullStr.map { s => if (s == null) \"\" else s}\n+      ))\n+    val batch = new JSONRecordBatch(upperCase.length, columns)\n+    val df = (upperCase, lowerCase, nullStr).zipped.toList\n+      .toDF(\"upper_case\", \"lower_case\", \"null_str\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"stringData.json\")\n+  }\n+\n+  private def byteData: DataTuple = {\n+    val data = List[Byte](1.toByte, (-1).toByte, 64.toByte, Byte.MaxValue)\n+    val fields = Seq(new IntegerType(\"a_byte\", is_signed = true, 8, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_byte\", data.length, data.map(_ => true), data.map(_.toInt)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_byte\"), new JSONFile(schema, Seq(batch)), \"byteData.json\")\n+  }\n+\n+  private def floatNaNData: DataTuple = {\n+    val fnan = Seq(1.2F, Float.NaN)\n+    val dnan = Seq(Double.NaN, 1.2)\n+    val fields = Seq(new FloatingPointType(\"NaN_f\", 32, nullable = false),\n+      new FloatingPointType(\"NaN_d\", 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"NaN_f\", fnan.length, fnan.map(_ => true), fnan),\n+      new PrimitiveColumn(\"NaN_d\", dnan.length, dnan.map(_ => true), dnan))\n+    val batch = new JSONRecordBatch(fnan.length, columns)\n+    val df = fnan.zip(dnan).toDF(\"NaN_f\", \"NaN_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"nanData-floating_point.json\")\n+  }\n+\n+  private def timestampData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val data = Seq(ts1, ts2)\n+    val schema = new JSONSchema(Seq(new TimestampType(\"c_timestamp\")))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"c_timestamp\", data.length, data.map(_ => true), data.map(_.getTime)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"c_timestamp\"), new JSONFile(schema, Seq(batch)), \"timestampData.json\")\n+  }\n+\n+  private def dateData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d2 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val df = Seq((d1, sdf.format(d1), ts1), (d2, sdf.format(d2), ts2))\n+      .toDF(\"a_date\", \"b_string\", \"c_timestamp\")\n+    val jsonFile = new JSONFile(new JSONSchema(Seq.empty[DataType]), Seq.empty[JSONRecordBatch])\n+    DataTuple(df, jsonFile, \"dateData.json\")\n+  }\n+\n+  /**\n+   * Arrow JSON Format Data Generation\n+   * Referenced from https://github.com/apache/arrow/blob/master/integration/integration_test.py\n+   * TODO: Look into using JSON generation from parquet-vector.jar\n+   */\n+\n+  private abstract class DataType(name: String, nullable: Boolean) {"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "I thought the easiest and most compact strategy for testing was to 1) define common data, 2) write the data as JSON, 3) make a DataFrame from data and convert to ArrowRecordBatch, 4) send batch and JSON file to Arrow validator where it can be checked.\r\n\r\nI think the JSON files are too big to inline and would create a huge file if have them as strings.  I could try this out for one of the test cases if you like and we can see how it turns out?",
    "commit": "44d7a2a3fedb4f4bec167d763d0df3d6448bbe49",
    "createdAt": "2017-04-27T01:54:08Z",
    "diffHunk": "@@ -0,0 +1,568 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets\n+import java.sql.{Date, Timestamp}\n+import java.text.SimpleDateFormat\n+import java.util.Locale\n+\n+import com.google.common.io.Files\n+import org.apache.arrow.vector.{VectorLoader, VectorSchemaRoot}\n+import org.apache.arrow.vector.file.json.JsonFileReader\n+import org.apache.arrow.vector.util.Validator\n+import org.json4s.jackson.JsonMethods._\n+import org.json4s.JsonAST._\n+import org.json4s.JsonDSL._\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.Utils\n+\n+\n+class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n+  import testImplicits._\n+\n+  private var tempDataPath: String = _\n+\n+  private def collectAsArrow(df: DataFrame,\n+                             converter: Option[ArrowConverters] = None): ArrowPayload = {\n+    val cnvtr = converter.getOrElse(new ArrowConverters)\n+    val payloadByteArrays = df.toArrowPayloadBytes().collect()\n+    cnvtr.readPayloadByteArrays(payloadByteArrays)\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    tempDataPath = Utils.createTempDir(namePrefix = \"arrow\").getAbsolutePath\n+  }\n+\n+  test(\"collect to arrow record batch\") {\n+    val indexData = (1 to 6).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(indexData)\n+    assert(arrowPayload.nonEmpty)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length == indexData.rdd.getNumPartitions)\n+    val rowCount = arrowBatches.map(batch => batch.getLength).sum\n+    assert(rowCount === indexData.count())\n+    arrowBatches.foreach(batch => assert(batch.getNodes.size() > 0))\n+    arrowBatches.foreach(batch => batch.close())\n+  }\n+\n+  test(\"numeric type conversion\") {\n+    collectAndValidate(indexData)\n+    collectAndValidate(shortData)\n+    collectAndValidate(intData)\n+    collectAndValidate(longData)\n+    collectAndValidate(floatData)\n+    collectAndValidate(doubleData)\n+  }\n+\n+  test(\"mixed numeric type conversion\") {\n+    collectAndValidate(mixedNumericData)\n+  }\n+\n+  test(\"boolean type conversion\") {\n+    collectAndValidate(boolData)\n+  }\n+\n+  test(\"string type conversion\") {\n+    collectAndValidate(stringData)\n+  }\n+\n+  test(\"byte type conversion\") {\n+    collectAndValidate(byteData)\n+  }\n+\n+  ignore(\"timestamp conversion\") {\n+    collectAndValidate(timestampData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"date conversion\") {\n+    // collectAndValidate(dateTimeData)\n+  }\n+\n+  // TODO: Not currently supported in Arrow JSON reader\n+  ignore(\"binary type conversion\") {\n+    // collectAndValidate(binaryData)\n+  }\n+\n+  test(\"floating-point NaN\") {\n+    collectAndValidate(floatNaNData)\n+  }\n+\n+  test(\"partitioned DataFrame\") {\n+    val converter = new ArrowConverters\n+    val schema = testData2.schema\n+    val arrowPayload = collectAsArrow(testData2, Some(converter))\n+    val arrowBatches = arrowPayload.toArray\n+    // NOTE: testData2 should have 2 partitions -> 2 arrow batches in payload\n+    assert(arrowBatches.length === 2)\n+    val pl1 = new ArrowStaticPayload(arrowBatches(0))\n+    val pl2 = new ArrowStaticPayload(arrowBatches(1))\n+    // Generate JSON files\n+    val a = List[Int](1, 1, 2, 2, 3, 3)\n+    val b = List[Int](1, 2, 1, 2, 1, 2)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b\", is_signed = true, 32, nullable = false))\n+    def getBatch(x: Seq[Int], y: Seq[Int]): JSONRecordBatch = {\n+      val columns = Seq(new PrimitiveColumn(\"a\", x.length, x.map(_ => true), x),\n+        new PrimitiveColumn(\"b\", y.length, y.map(_ => true), y))\n+      new JSONRecordBatch(x.length, columns)\n+    }\n+    val json1 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.take(3), b.take(3))))\n+    val json2 = new JSONFile(new JSONSchema(fields), Seq(getBatch(a.takeRight(3), b.takeRight(3))))\n+    val tempFile1 = new File(tempDataPath, \"testData2-ints-part1.json\")\n+    val tempFile2 = new File(tempDataPath, \"testData2-ints-part2.json\")\n+    json1.write(tempFile1)\n+    json2.write(tempFile2)\n+    validateConversion(schema, pl1, tempFile1, Some(converter))\n+    validateConversion(schema, pl2, tempFile2, Some(converter))\n+  }\n+\n+  test(\"empty frame collect\") {\n+    val arrowPayload = collectAsArrow(spark.emptyDataFrame)\n+    assert(arrowPayload.isEmpty)\n+  }\n+\n+  test(\"empty partition collect\") {\n+    val emptyPart = spark.sparkContext.parallelize(Seq(1), 2).toDF(\"i\")\n+    val arrowPayload = collectAsArrow(emptyPart)\n+    val arrowBatches = arrowPayload.toArray\n+    assert(arrowBatches.length === 2)\n+    assert(arrowBatches.count(_.getLength == 0) === 1)\n+    assert(arrowBatches.count(_.getLength == 1) === 1)\n+  }\n+\n+  test(\"unsupported types\") {\n+    def runUnsupported(block: => Unit): Unit = {\n+      val msg = intercept[SparkException] {\n+        block\n+      }\n+      assert(msg.getMessage.contains(\"Unsupported data type\"))\n+      assert(msg.getCause.getClass === classOf[UnsupportedOperationException])\n+    }\n+\n+    runUnsupported { collectAsArrow(decimalData) }\n+    runUnsupported { collectAsArrow(arrayData.toDF()) }\n+    runUnsupported { collectAsArrow(mapData.toDF()) }\n+    runUnsupported { collectAsArrow(complexData) }\n+  }\n+\n+  test(\"test Arrow Validator\") {\n+    val sdata = shortData\n+    val idata = intData\n+\n+    // Different schema\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(sdata.df, idata.json, idata.file))\n+    }\n+\n+    // Different values\n+    intercept[IllegalArgumentException] {\n+      collectAndValidate(DataTuple(idata.df.sort($\"a_i\".desc), idata.json, idata.file))\n+    }\n+  }\n+\n+  /** Test that a converted DataFrame to Arrow record batch equals batch read from JSON file */\n+  private def collectAndValidate(data: DataTuple): Unit = {\n+    val converter = new ArrowConverters\n+    // NOTE: coalesce to single partition because can only load 1 batch in validator\n+    val arrowPayload = collectAsArrow(data.df.coalesce(1), Some(converter))\n+    val tempFile = new File(tempDataPath, data.file)\n+    data.json.write(tempFile)\n+    validateConversion(data.df.schema, arrowPayload, tempFile, Some(converter))\n+  }\n+\n+  private def validateConversion(sparkSchema: StructType,\n+                                 arrowPayload: ArrowPayload,\n+                                 jsonFile: File,\n+                                 converterOpt: Option[ArrowConverters] = None): Unit = {\n+    val converter = converterOpt.getOrElse(new ArrowConverters)\n+    val jsonReader = new JsonFileReader(jsonFile, converter.allocator)\n+\n+    val arrowSchema = ArrowConverters.schemaToArrowSchema(sparkSchema)\n+    val jsonSchema = jsonReader.start()\n+    Validator.compareSchemas(arrowSchema, jsonSchema)\n+\n+    val arrowRoot = new VectorSchemaRoot(arrowSchema, converter.allocator)\n+    val vectorLoader = new VectorLoader(arrowRoot)\n+    arrowPayload.foreach(vectorLoader.load)\n+    val jsonRoot = jsonReader.read()\n+    Validator.compareVectorSchemaRoot(arrowRoot, jsonRoot)\n+  }\n+\n+  // Create Spark DataFrame and matching Arrow JSON at same time for validation\n+  private case class DataTuple(df: DataFrame, json: JSONFile, file: String)\n+\n+  private def indexData: DataTuple = {\n+    val data = List[Int](1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"i\", is_signed = true, 32, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"i\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"i\"), new JSONFile(schema, Seq(batch)), \"indexData-ints.json\")\n+  }\n+\n+  private def shortData: DataTuple = {\n+    val a_s = List[Short](1, -1, 2, -2, 32767, -32768)\n+    val b_s = List[Option[Short]](Some(1), None, None, Some(-2), None, Some(-32768))\n+    val fields = Seq(new IntegerType(\"a_s\", is_signed = true, 16, nullable = false),\n+      new IntegerType(\"b_s\", is_signed = true, 16, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val b_s_values = b_s.map(_.map(_.toInt).getOrElse(0))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_s\", a_s.length, a_s.map(_ => true), a_s.map(_.toInt)),\n+      new PrimitiveColumn(\"b_s\", b_s.length, b_s.map(_.isDefined), b_s_values))\n+    val batch = new JSONRecordBatch(a_s.length, columns)\n+    val df = a_s.zip(b_s).toDF(\"a_s\", \"b_s\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-16bit.json\")\n+  }\n+\n+  private def intData: DataTuple = {\n+    val a_i = List[Int](1, -1, 2, -2, 2147483647, -2147483648)\n+    val b_i = List[Option[Int]](Some(1), None, None, Some(-2), None, Some(-2147483648))\n+    val fields = Seq(new IntegerType(\"a_i\", is_signed = true, 32, nullable = false),\n+      new IntegerType(\"b_i\", is_signed = true, 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_i\", a_i.length, a_i.map(_ => true), a_i),\n+      new PrimitiveColumn(\"b_i\", b_i.length, b_i.map(_.isDefined), b_i.map(_.getOrElse(0))))\n+    val batch = new JSONRecordBatch(a_i.length, columns)\n+    val df = a_i.zip(b_i).toDF(\"a_i\", \"b_i\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-32bit.json\")\n+  }\n+\n+  private def longData: DataTuple = {\n+    val a_l = List[Long](1, -1, 2, -2, 9223372036854775807L, -9223372036854775808L)\n+    val b_l = List[Option[Long]](Some(1), None, None, Some(-2), None, Some(-9223372036854775808L))\n+    val fields = Seq(new IntegerType(\"a_l\", is_signed = true, 64, nullable = false),\n+      new IntegerType(\"b_l\", is_signed = true, 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_l\", a_l.length, a_l.map(_ => true), a_l),\n+      new PrimitiveColumn(\"b_l\", b_l.length, b_l.map(_.isDefined), b_l.map(_.getOrElse(0L))))\n+    val batch = new JSONRecordBatch(a_l.length, columns)\n+    val df = a_l.zip(b_l).toDF(\"a_l\", \"b_l\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"integer-64bit.json\")\n+  }\n+\n+  private def floatData: DataTuple = {\n+    val a_f = List(1.0f, 2.0f, 0.01f, 200.0f, 0.0001f, 20000.0f)\n+    val b_f = List[Option[Float]](Some(1.1f), None, None, Some(2.2f), None, Some(3.3f))\n+    val fields = Seq(new FloatingPointType(\"a_f\", 32, nullable = false),\n+      new FloatingPointType(\"b_f\", 32, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_f\", a_f.length, a_f.map(_ => true), a_f),\n+      new PrimitiveColumn(\"b_f\", b_f.length, b_f.map(_.isDefined), b_f.map(_.getOrElse(0.0f))))\n+    val batch = new JSONRecordBatch(a_f.length, columns)\n+    val df = a_f.zip(b_f).toDF(\"a_f\", \"b_f\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-single_precision.json\")\n+  }\n+\n+  private def doubleData: DataTuple = {\n+    val a_d = List(1.0, 2.0, 0.01, 200.0, 0.0001, 20000.0)\n+    val b_d = List[Option[Double]](Some(1.1), None, None, Some(2.2), None, Some(3.3))\n+    val fields = Seq(new FloatingPointType(\"a_d\", 64, nullable = false),\n+      new FloatingPointType(\"b_d\", 64, nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_d\", a_d.length, a_d.map(_ => true), a_d),\n+      new PrimitiveColumn(\"b_d\", b_d.length, b_d.map(_.isDefined), b_d.map(_.getOrElse(0.0))))\n+    val batch = new JSONRecordBatch(a_d.length, columns)\n+    val df = a_d.zip(b_d).toDF(\"a_d\", \"b_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"floating_point-double_precision.json\")\n+  }\n+\n+  private def mixedNumericData: DataTuple = {\n+    val data = List(1, 2, 3, 4, 5, 6)\n+    val fields = Seq(new IntegerType(\"a\", is_signed = true, 16, nullable = false),\n+      new FloatingPointType(\"b\", 32, nullable = false),\n+      new IntegerType(\"c\", is_signed = true, 32, nullable = false),\n+      new FloatingPointType(\"d\", 64, nullable = false),\n+      new IntegerType(\"e\", is_signed = true, 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"b\", data.length, data.map(_ => true), data.map(_.toFloat)),\n+      new PrimitiveColumn(\"c\", data.length, data.map(_ => true), data),\n+      new PrimitiveColumn(\"d\", data.length, data.map(_ => true), data.map(_.toDouble)),\n+      new PrimitiveColumn(\"e\", data.length, data.map(_ => true), data)\n+    )\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    val data_tuples = for (d <- data) yield {\n+      (d.toShort, d.toFloat, d.toInt, d.toDouble, d.toLong)\n+    }\n+    val df = data_tuples.toDF(\"a\", \"b\", \"c\", \"d\", \"e\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"mixed_numeric_types.json\")\n+  }\n+\n+  private def boolData: DataTuple = {\n+    val data = Seq(true, true, false, true)\n+    val fields = Seq(new BooleanType(\"a_bool\", nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"a_bool\", data.length, data.map(_ => true), data))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_bool\"), new JSONFile(schema, Seq(batch)), \"boolData.json\")\n+  }\n+\n+  private def stringData: DataTuple = {\n+    val upperCase = Seq(\"A\", \"B\", \"C\")\n+    val lowerCase = Seq(\"a\", \"b\", \"c\")\n+    val nullStr = Seq(\"ab\", \"CDE\", null)\n+    val fields = Seq(new StringType(\"upper_case\", nullable = true),\n+      new StringType(\"lower_case\", nullable = true),\n+      new StringType(\"null_str\", nullable = true))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new StringColumn(\"upper_case\", upperCase.length, upperCase.map(_ => true), upperCase),\n+      new StringColumn(\"lower_case\", lowerCase.length, lowerCase.map(_ => true), lowerCase),\n+      new StringColumn(\"null_str\", nullStr.length, nullStr.map(_ != null),\n+        nullStr.map { s => if (s == null) \"\" else s}\n+      ))\n+    val batch = new JSONRecordBatch(upperCase.length, columns)\n+    val df = (upperCase, lowerCase, nullStr).zipped.toList\n+      .toDF(\"upper_case\", \"lower_case\", \"null_str\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"stringData.json\")\n+  }\n+\n+  private def byteData: DataTuple = {\n+    val data = List[Byte](1.toByte, (-1).toByte, 64.toByte, Byte.MaxValue)\n+    val fields = Seq(new IntegerType(\"a_byte\", is_signed = true, 8, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(\n+      new PrimitiveColumn(\"a_byte\", data.length, data.map(_ => true), data.map(_.toInt)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"a_byte\"), new JSONFile(schema, Seq(batch)), \"byteData.json\")\n+  }\n+\n+  private def floatNaNData: DataTuple = {\n+    val fnan = Seq(1.2F, Float.NaN)\n+    val dnan = Seq(Double.NaN, 1.2)\n+    val fields = Seq(new FloatingPointType(\"NaN_f\", 32, nullable = false),\n+      new FloatingPointType(\"NaN_d\", 64, nullable = false))\n+    val schema = new JSONSchema(fields)\n+    val columns = Seq(new PrimitiveColumn(\"NaN_f\", fnan.length, fnan.map(_ => true), fnan),\n+      new PrimitiveColumn(\"NaN_d\", dnan.length, dnan.map(_ => true), dnan))\n+    val batch = new JSONRecordBatch(fnan.length, columns)\n+    val df = fnan.zip(dnan).toDF(\"NaN_f\", \"NaN_d\")\n+    DataTuple(df, new JSONFile(schema, Seq(batch)), \"nanData-floating_point.json\")\n+  }\n+\n+  private def timestampData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val data = Seq(ts1, ts2)\n+    val schema = new JSONSchema(Seq(new TimestampType(\"c_timestamp\")))\n+    val columns = Seq(\n+      new PrimitiveColumn(\"c_timestamp\", data.length, data.map(_ => true), data.map(_.getTime)))\n+    val batch = new JSONRecordBatch(data.length, columns)\n+    DataTuple(data.toDF(\"c_timestamp\"), new JSONFile(schema, Seq(batch)), \"timestampData.json\")\n+  }\n+\n+  private def dateData: DataTuple = {\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d2 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val ts1 = new Timestamp(sdf.parse(\"2013-04-08 01:10:15.567 UTC\").getTime)\n+    val ts2 = new Timestamp(sdf.parse(\"2013-04-08 13:10:10.789 UTC\").getTime)\n+    val df = Seq((d1, sdf.format(d1), ts1), (d2, sdf.format(d2), ts2))\n+      .toDF(\"a_date\", \"b_string\", \"c_timestamp\")\n+    val jsonFile = new JSONFile(new JSONSchema(Seq.empty[DataType]), Seq.empty[JSONRecordBatch])\n+    DataTuple(df, jsonFile, \"dateData.json\")\n+  }\n+\n+  /**\n+   * Arrow JSON Format Data Generation\n+   * Referenced from https://github.com/apache/arrow/blob/master/integration/integration_test.py\n+   * TODO: Look into using JSON generation from parquet-vector.jar\n+   */\n+\n+  private abstract class DataType(name: String, nullable: Boolean) {"
  }],
  "prId": 15821
}]