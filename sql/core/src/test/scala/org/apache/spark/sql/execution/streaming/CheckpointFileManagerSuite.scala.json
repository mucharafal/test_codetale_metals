[{
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "plagiarizer! :P",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T00:50:56Z",
    "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.util.Random\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.util.Utils\n+\n+abstract class CheckpointFileManagerTests extends SparkFunSuite {\n+\n+  def createManager(path: Path): CheckpointFileManager\n+\n+  test(\"mkdirs, list, createAtomic, open, delete\") {\n+    withTempPath { p =>\n+      val basePath = new Path(p.getAbsolutePath)\n+      val fm = createManager(basePath)\n+      // Mkdirs\n+      val dir = new Path(s\"$basePath/dir/subdir/subsubdir\")\n+      assert(!fm.exists(dir))\n+      fm.mkdirs(dir)\n+      assert(fm.exists(dir))\n+      fm.mkdirs(dir)\n+\n+      // List\n+      val acceptAllFilter = new PathFilter {\n+        override def accept(path: Path): Boolean = true\n+      }\n+      val rejectAllFilter = new PathFilter {\n+        override def accept(path: Path): Boolean = false\n+      }\n+      assert(fm.list(basePath, acceptAllFilter).exists(_.getPath.getName == \"dir\"))\n+      assert(fm.list(basePath, rejectAllFilter).length === 0)\n+\n+      // Create atomic without overwrite\n+      var path = new Path(s\"$dir/file\")\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = false).cancel()\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = false).close()\n+      assert(fm.exists(path))\n+      intercept[IOException] {\n+        // should throw exception since file exists and overwrite is false\n+        fm.createAtomic(path, overwriteIfPossible = false).close()\n+      }\n+\n+      // Create atomic with overwrite if possible\n+      path = new Path(s\"$dir/file2\")\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = true).cancel()\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = true).close()\n+      assert(fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = true).close()  // should not throw exception\n+\n+      // Open and delete\n+      fm.open(path).close()\n+      fm.delete(path)\n+      assert(!fm.exists(path))\n+      intercept[IOException] {\n+        fm.open(path)\n+      }\n+      fm.delete(path) // should not throw exception\n+    }\n+  }\n+\n+  protected def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+}\n+\n+class CheckpointFileManagerSuite extends SparkFunSuite with SharedSparkSession {\n+\n+  test(\"CheckpointFileManager.create() should pick up user-specified class from conf\") {\n+    withSQLConf(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key ->\n+        classOf[TestCheckpointFileManager].getName) {\n+      val fileManager =\n+        CheckpointFileManager.create(new Path(\"/\"), spark.sessionState.newHadoopConf)\n+      assert(fileManager.isInstanceOf[TestCheckpointFileManager])\n+    }\n+  }\n+\n+  test(\"CheckpointFileManager.create() should fallback from FileContext to FileSystem\") {\n+    import FakeFileSystem.scheme\n+    spark.conf.set(\n+      s\"fs.$scheme.impl\",\n+      classOf[FakeFileSystem].getName)\n+    withTempDir { temp =>\n+      val metadataLog = new HDFSMetadataLog[String](spark, s\"$scheme://${temp.toURI.getPath}\")\n+      assert(metadataLog.add(0, \"batch0\"))\n+      assert(metadataLog.getLatest() === Some(0 -> \"batch0\"))\n+      assert(metadataLog.get(0) === Some(\"batch0\"))\n+      assert(metadataLog.get(None, Some(0)) === Array(0 -> \"batch0\"))\n+\n+\n+      val metadataLog2 = new HDFSMetadataLog[String](spark, s\"$scheme://${temp.toURI.getPath}\")\n+      assert(metadataLog2.get(0) === Some(\"batch0\"))\n+      assert(metadataLog2.getLatest() === Some(0 -> \"batch0\"))\n+      assert(metadataLog2.get(None, Some(0)) === Array(0 -> \"batch0\"))\n+\n+    }\n+  }\n+}\n+\n+class FileContextBasedCheckpointFileManagerSuite extends CheckpointFileManagerTests {\n+  override def createManager(path: Path): CheckpointFileManager = {\n+    new FileContextBasedCheckpointFileManager(path, new Configuration())\n+  }\n+}\n+\n+class FileSystemBasedCheckpointFileManagerSuite extends CheckpointFileManagerTests {\n+  override def createManager(path: Path): CheckpointFileManager = {\n+    new FileSystemBasedCheckpointFileManager(path, new Configuration())\n+  }\n+}\n+\n+\n+/** A fake implementation to test different characteristics of CheckpointFileManager interface */\n+class TestCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends FileSystemBasedCheckpointFileManager(path, hadoopConf) {\n+\n+  import CheckpointFileManager._\n+\n+  override def createAtomic(path: Path, overwrite: Boolean): CancellableFSDataOutputStream = {\n+    if (TestCheckpointFileManager.shouldFailInCreateAtomic) {\n+      TestCheckpointFileManager.cancelCalledInCreateAtomic = false\n+    }\n+    val originalOut = super.createAtomic(path, overwrite)\n+\n+    new CancellableFSDataOutputStream(originalOut) {\n+      override def close(): Unit = {\n+        if (TestCheckpointFileManager.shouldFailInCreateAtomic) {\n+          throw new IOException(\"Copy failed intentionally\")\n+        }\n+        super.close()\n+      }\n+\n+      override def cancel(): Unit = {\n+        TestCheckpointFileManager.cancelCalledInCreateAtomic = true\n+        originalOut.cancel()\n+      }\n+    }\n+  }\n+}\n+\n+object TestCheckpointFileManager {\n+  @volatile var shouldFailInCreateAtomic = false\n+  @volatile var cancelCalledInCreateAtomic = false\n+}\n+\n+\n+/** FakeFileSystem to test fallback of the HDFSMetadataLog from FileContext to FileSystem API */\n+private class FakeFileSystem extends RawLocalFileSystem {\n+  import FakeFileSystem.scheme\n+\n+  override def getUri: URI = {\n+    URI.create(s\"$scheme:///\")\n+  }\n+}\n+\n+private object FakeFileSystem {\n+  val scheme = s\"HDFSMetadataLogSuite${math.abs(Random.nextInt)}\""
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Hahaha.",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-12T01:05:55Z",
    "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.util.Random\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.util.Utils\n+\n+abstract class CheckpointFileManagerTests extends SparkFunSuite {\n+\n+  def createManager(path: Path): CheckpointFileManager\n+\n+  test(\"mkdirs, list, createAtomic, open, delete\") {\n+    withTempPath { p =>\n+      val basePath = new Path(p.getAbsolutePath)\n+      val fm = createManager(basePath)\n+      // Mkdirs\n+      val dir = new Path(s\"$basePath/dir/subdir/subsubdir\")\n+      assert(!fm.exists(dir))\n+      fm.mkdirs(dir)\n+      assert(fm.exists(dir))\n+      fm.mkdirs(dir)\n+\n+      // List\n+      val acceptAllFilter = new PathFilter {\n+        override def accept(path: Path): Boolean = true\n+      }\n+      val rejectAllFilter = new PathFilter {\n+        override def accept(path: Path): Boolean = false\n+      }\n+      assert(fm.list(basePath, acceptAllFilter).exists(_.getPath.getName == \"dir\"))\n+      assert(fm.list(basePath, rejectAllFilter).length === 0)\n+\n+      // Create atomic without overwrite\n+      var path = new Path(s\"$dir/file\")\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = false).cancel()\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = false).close()\n+      assert(fm.exists(path))\n+      intercept[IOException] {\n+        // should throw exception since file exists and overwrite is false\n+        fm.createAtomic(path, overwriteIfPossible = false).close()\n+      }\n+\n+      // Create atomic with overwrite if possible\n+      path = new Path(s\"$dir/file2\")\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = true).cancel()\n+      assert(!fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = true).close()\n+      assert(fm.exists(path))\n+      fm.createAtomic(path, overwriteIfPossible = true).close()  // should not throw exception\n+\n+      // Open and delete\n+      fm.open(path).close()\n+      fm.delete(path)\n+      assert(!fm.exists(path))\n+      intercept[IOException] {\n+        fm.open(path)\n+      }\n+      fm.delete(path) // should not throw exception\n+    }\n+  }\n+\n+  protected def withTempPath(f: File => Unit): Unit = {\n+    val path = Utils.createTempDir()\n+    path.delete()\n+    try f(path) finally Utils.deleteRecursively(path)\n+  }\n+}\n+\n+class CheckpointFileManagerSuite extends SparkFunSuite with SharedSparkSession {\n+\n+  test(\"CheckpointFileManager.create() should pick up user-specified class from conf\") {\n+    withSQLConf(\n+      SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key ->\n+        classOf[TestCheckpointFileManager].getName) {\n+      val fileManager =\n+        CheckpointFileManager.create(new Path(\"/\"), spark.sessionState.newHadoopConf)\n+      assert(fileManager.isInstanceOf[TestCheckpointFileManager])\n+    }\n+  }\n+\n+  test(\"CheckpointFileManager.create() should fallback from FileContext to FileSystem\") {\n+    import FakeFileSystem.scheme\n+    spark.conf.set(\n+      s\"fs.$scheme.impl\",\n+      classOf[FakeFileSystem].getName)\n+    withTempDir { temp =>\n+      val metadataLog = new HDFSMetadataLog[String](spark, s\"$scheme://${temp.toURI.getPath}\")\n+      assert(metadataLog.add(0, \"batch0\"))\n+      assert(metadataLog.getLatest() === Some(0 -> \"batch0\"))\n+      assert(metadataLog.get(0) === Some(\"batch0\"))\n+      assert(metadataLog.get(None, Some(0)) === Array(0 -> \"batch0\"))\n+\n+\n+      val metadataLog2 = new HDFSMetadataLog[String](spark, s\"$scheme://${temp.toURI.getPath}\")\n+      assert(metadataLog2.get(0) === Some(\"batch0\"))\n+      assert(metadataLog2.getLatest() === Some(0 -> \"batch0\"))\n+      assert(metadataLog2.get(None, Some(0)) === Array(0 -> \"batch0\"))\n+\n+    }\n+  }\n+}\n+\n+class FileContextBasedCheckpointFileManagerSuite extends CheckpointFileManagerTests {\n+  override def createManager(path: Path): CheckpointFileManager = {\n+    new FileContextBasedCheckpointFileManager(path, new Configuration())\n+  }\n+}\n+\n+class FileSystemBasedCheckpointFileManagerSuite extends CheckpointFileManagerTests {\n+  override def createManager(path: Path): CheckpointFileManager = {\n+    new FileSystemBasedCheckpointFileManager(path, new Configuration())\n+  }\n+}\n+\n+\n+/** A fake implementation to test different characteristics of CheckpointFileManager interface */\n+class TestCheckpointFileManager(path: Path, hadoopConf: Configuration)\n+  extends FileSystemBasedCheckpointFileManager(path, hadoopConf) {\n+\n+  import CheckpointFileManager._\n+\n+  override def createAtomic(path: Path, overwrite: Boolean): CancellableFSDataOutputStream = {\n+    if (TestCheckpointFileManager.shouldFailInCreateAtomic) {\n+      TestCheckpointFileManager.cancelCalledInCreateAtomic = false\n+    }\n+    val originalOut = super.createAtomic(path, overwrite)\n+\n+    new CancellableFSDataOutputStream(originalOut) {\n+      override def close(): Unit = {\n+        if (TestCheckpointFileManager.shouldFailInCreateAtomic) {\n+          throw new IOException(\"Copy failed intentionally\")\n+        }\n+        super.close()\n+      }\n+\n+      override def cancel(): Unit = {\n+        TestCheckpointFileManager.cancelCalledInCreateAtomic = true\n+        originalOut.cancel()\n+      }\n+    }\n+  }\n+}\n+\n+object TestCheckpointFileManager {\n+  @volatile var shouldFailInCreateAtomic = false\n+  @volatile var cancelCalledInCreateAtomic = false\n+}\n+\n+\n+/** FakeFileSystem to test fallback of the HDFSMetadataLog from FileContext to FileSystem API */\n+private class FakeFileSystem extends RawLocalFileSystem {\n+  import FakeFileSystem.scheme\n+\n+  override def getUri: URI = {\n+    URI.create(s\"$scheme:///\")\n+  }\n+}\n+\n+private object FakeFileSystem {\n+  val scheme = s\"HDFSMetadataLogSuite${math.abs(Random.nextInt)}\""
  }],
  "prId": 21048
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "if that fm.exists() call was replaced with an fm.getFileStatus() operation, as suggested earlier, this assert could be come `assert(fm.getFileStatus(dir).isDirectory)`",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T11:17:58Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.util.Random\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.util.quietly\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.util.Utils\n+\n+abstract class CheckpointFileManagerTests extends SparkFunSuite {\n+\n+  def createManager(path: Path): CheckpointFileManager\n+\n+  test(\"mkdirs, list, createAtomic, open, delete\") {\n+    withTempPath { p =>\n+      val basePath = new Path(p.getAbsolutePath)\n+      val fm = createManager(basePath)\n+      // Mkdirs\n+      val dir = new Path(s\"$basePath/dir/subdir/subsubdir\")\n+      assert(!fm.exists(dir))\n+      fm.mkdirs(dir)\n+      assert(fm.exists(dir))\n+      fm.mkdirs(dir)",
    "line": 46
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "This is a test. I think it is fine. ",
    "commit": "c5b0c98257e39d6af2dd8f702b8cbc9f9e6fabe9",
    "createdAt": "2018-04-13T19:23:39Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.io._\n+import java.net.URI\n+\n+import scala.util.Random\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs._\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.util.quietly\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.util.Utils\n+\n+abstract class CheckpointFileManagerTests extends SparkFunSuite {\n+\n+  def createManager(path: Path): CheckpointFileManager\n+\n+  test(\"mkdirs, list, createAtomic, open, delete\") {\n+    withTempPath { p =>\n+      val basePath = new Path(p.getAbsolutePath)\n+      val fm = createManager(basePath)\n+      // Mkdirs\n+      val dir = new Path(s\"$basePath/dir/subdir/subsubdir\")\n+      assert(!fm.exists(dir))\n+      fm.mkdirs(dir)\n+      assert(fm.exists(dir))\n+      fm.mkdirs(dir)",
    "line": 46
  }],
  "prId": 21048
}]