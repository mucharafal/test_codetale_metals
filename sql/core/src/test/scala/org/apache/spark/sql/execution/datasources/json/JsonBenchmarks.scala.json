[{
  "comments": [{
    "author": {
      "login": "wangyum"
    },
    "body": "Please update `without sbt` usage to:\r\n```\r\nbin/spark-submit --class <this class> --jars <spark core test jar>,<spark catalyst test jar> <spark sql test jar>\r\n```",
    "commit": "422df479aae55b52e17eccf1f46675c9b7369ddd",
    "createdAt": "2018-10-29T08:27:33Z",
    "diffHunk": "@@ -16,32 +16,30 @@\n  */\n package org.apache.spark.sql.execution.datasources.json\n \n-import java.io.File\n-\n-import org.apache.spark.SparkConf\n import org.apache.spark.benchmark.Benchmark\n-import org.apache.spark.sql.{Row, SparkSession}\n-import org.apache.spark.sql.catalyst.plans.SQLHelper\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n import org.apache.spark.sql.functions.lit\n import org.apache.spark.sql.types._\n \n /**\n  * The benchmarks aims to measure performance of JSON parsing when encoding is set and isn't.\n- * To run this:\n- *  spark-submit --class <this class> --jars <spark sql test jar>\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Also update the usage in description:\r\n```console\r\nbin/spark-submit --class org.apache.spark.sql.execution.datasources.json.JSONBenchmarks --jars ./core/target/spark-core_2.11-3.0.0-SNAPSHOT-tests.jar,./sql/catalyst/target/spark-catalyst_2.11-3.0.0-SNAPSHOT-tests.jar ./sql/core/target/spark-sql_2.11-3.0.0-SNAPSHOT-tests.jar\r\n```",
    "commit": "422df479aae55b52e17eccf1f46675c9b7369ddd",
    "createdAt": "2018-10-29T08:27:55Z",
    "diffHunk": "@@ -16,32 +16,30 @@\n  */\n package org.apache.spark.sql.execution.datasources.json\n \n-import java.io.File\n-\n-import org.apache.spark.SparkConf\n import org.apache.spark.benchmark.Benchmark\n-import org.apache.spark.sql.{Row, SparkSession}\n-import org.apache.spark.sql.catalyst.plans.SQLHelper\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n import org.apache.spark.sql.functions.lit\n import org.apache.spark.sql.types._\n \n /**\n  * The benchmarks aims to measure performance of JSON parsing when encoding is set and isn't.\n- * To run this:\n- *  spark-submit --class <this class> --jars <spark sql test jar>\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>"
  }],
  "prId": 22844
}, {
  "comments": [{
    "author": {
      "login": "wangyum"
    },
    "body": "Sorry @heary-cao I mean update here to:\r\n```\r\nbin/spark-submit --class <this class> --jars <spark core test jar>,<spark catalyst test jar> <spark sql test jar>\r\n```\r\nand update [PR description](https://github.com/apache/spark/pull/22844#issue-225971331) to:\r\n```\r\nbin/spark-submit --class org.apache.spark.sql.execution.datasources.json.JSONBenchmarks --jars ./core/target/spark-core_2.11-3.0.0-SNAPSHOT-tests.jar,./sql/catalyst/target/spark-catalyst_2.11-3.0.0-SNAPSHOT-tests.jar ./sql/core/target/spark-sql_2.11-3.0.0-SNAPSHOT-tests.jar\r\n```",
    "commit": "422df479aae55b52e17eccf1f46675c9b7369ddd",
    "createdAt": "2018-10-29T09:01:10Z",
    "diffHunk": "@@ -16,32 +16,33 @@\n  */\n package org.apache.spark.sql.execution.datasources.json\n \n-import java.io.File\n-\n-import org.apache.spark.SparkConf\n import org.apache.spark.benchmark.Benchmark\n-import org.apache.spark.sql.{Row, SparkSession}\n-import org.apache.spark.sql.catalyst.plans.SQLHelper\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n import org.apache.spark.sql.functions.lit\n import org.apache.spark.sql.types._\n \n /**\n  * The benchmarks aims to measure performance of JSON parsing when encoding is set and isn't.\n- * To run this:\n- *  spark-submit --class <this class> --jars <spark sql test jar>\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class  org.apache.spark.sql.execution.datasources.json.JSONBenchmarks"
  }],
  "prId": 22844
}, {
  "comments": [{
    "author": {
      "login": "yucai"
    },
    "body": "#22872 has updated `runBenchmarkSuite`'s signature.\r\n```suggestion\r\n  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n```",
    "commit": "422df479aae55b52e17eccf1f46675c9b7369ddd",
    "createdAt": "2018-10-29T16:52:14Z",
    "diffHunk": "@@ -195,23 +170,16 @@ object JSONBenchmarks extends SQLHelper {\n         ds.count()\n       }\n \n-      /*\n-      Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n-\n-      Count a dataset with 10 columns:      Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      ---------------------------------------------------------------------------------------------\n-      Select 10 columns + count()               9961 / 10006          1.0         996.1       1.0X\n-      Select 1 column + count()                  8355 / 8470          1.2         835.5       1.2X\n-      count()                                    2104 / 2156          4.8         210.4       4.7X\n-      */\n       benchmark.run()\n     }\n   }\n \n-  def main(args: Array[String]): Unit = {\n-    schemaInferring(100 * 1000 * 1000)\n-    perlineParsing(100 * 1000 * 1000)\n-    perlineParsingOfWideColumn(10 * 1000 * 1000)\n-    countBenchmark(10 * 1000 * 1000)\n+  override def runBenchmarkSuite(): Unit = {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "+1 for @yucai 's comment.",
    "commit": "422df479aae55b52e17eccf1f46675c9b7369ddd",
    "createdAt": "2018-10-29T17:15:51Z",
    "diffHunk": "@@ -195,23 +170,16 @@ object JSONBenchmarks extends SQLHelper {\n         ds.count()\n       }\n \n-      /*\n-      Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n-\n-      Count a dataset with 10 columns:      Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      ---------------------------------------------------------------------------------------------\n-      Select 10 columns + count()               9961 / 10006          1.0         996.1       1.0X\n-      Select 1 column + count()                  8355 / 8470          1.2         835.5       1.2X\n-      count()                                    2104 / 2156          4.8         210.4       4.7X\n-      */\n       benchmark.run()\n     }\n   }\n \n-  def main(args: Array[String]): Unit = {\n-    schemaInferring(100 * 1000 * 1000)\n-    perlineParsing(100 * 1000 * 1000)\n-    perlineParsingOfWideColumn(10 * 1000 * 1000)\n-    countBenchmark(10 * 1000 * 1000)\n+  override def runBenchmarkSuite(): Unit = {"
  }],
  "prId": 22844
}]