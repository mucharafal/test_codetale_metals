[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Change this too.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-22T01:30:55Z",
    "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStats, Statistics}\n+import org.apache.spark.sql.execution.datasources.LogicalRelation\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+trait StatisticsTest extends QueryTest with SharedSQLContext {\n+\n+  def checkColStats(\n+      df: DataFrame,\n+      expectedColStatsSeq: Seq[(String, ColumnStats)]): Unit = {\n+    val table = \"tbl\"\n+    withTable(table) {\n+      df.write.format(\"json\").saveAsTable(table)\n+      val columns = expectedColStatsSeq.map(_._1)\n+      val columnStats = spark.sessionState.computeColumnStats(table, columns)"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "``` Scala\nval df = spark.table(tableName)\n```\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-22T04:35:16Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStats, Statistics}\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.execution.datasources.LogicalRelation\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+trait StatisticsTest extends QueryTest with SharedSQLContext {\n+\n+  def checkColStats(\n+      df: DataFrame,\n+      expectedColStatsSeq: Seq[(String, ColumnStats)]): Unit = {\n+    val table = \"tbl\"\n+    withTable(table) {\n+      df.write.format(\"json\").saveAsTable(table)\n+      val columns = expectedColStatsSeq.map(_._1)\n+      val tableIdent = TableIdentifier(table, Some(\"default\"))\n+      val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n+      val columnStats =\n+        AnalyzeColumnCommand(tableIdent, columns).computeColStats(spark, relation)._2\n+      expectedColStatsSeq.foreach { expected =>\n+        assert(columnStats.contains(expected._1))\n+        checkColStats(colStats = columnStats(expected._1), expectedColStats = expected._2)\n+      }\n+    }\n+  }\n+\n+  def checkColStats(colStats: ColumnStats, expectedColStats: ColumnStats): Unit = {\n+    assert(colStats.dataType == expectedColStats.dataType)\n+    assert(colStats.numNulls == expectedColStats.numNulls)\n+    colStats.dataType match {\n+      case _: IntegralType | DateType | TimestampType =>\n+        assert(colStats.max.map(_.toString.toLong) == expectedColStats.max.map(_.toString.toLong))\n+        assert(colStats.min.map(_.toString.toLong) == expectedColStats.min.map(_.toString.toLong))\n+      case _: FractionalType =>\n+        assert(colStats.max.map(_.toString.toDouble) == expectedColStats\n+          .max.map(_.toString.toDouble))\n+        assert(colStats.min.map(_.toString.toDouble) == expectedColStats\n+          .min.map(_.toString.toDouble))\n+      case _ =>\n+        // other types don't have max and min stats\n+        assert(colStats.max.isEmpty)\n+        assert(colStats.min.isEmpty)\n+    }\n+    colStats.dataType match {\n+      case BinaryType | BooleanType => assert(colStats.ndv.isEmpty)\n+      case _ =>\n+        // ndv is an approximate value, so we make sure we have the value, and it should be\n+        // within 3*SD's of the given rsd.\n+        assert(colStats.ndv.get >= 0)\n+        if (expectedColStats.ndv.get == 0) {\n+          assert(colStats.ndv.get == 0)\n+        } else if (expectedColStats.ndv.get > 0) {\n+          val rsd = spark.sessionState.conf.ndvMaxError\n+          val error = math.abs((colStats.ndv.get / expectedColStats.ndv.get.toDouble) - 1.0d)\n+          assert(error <= rsd * 3.0d, \"Error should be within 3 std. errors.\")\n+        }\n+    }\n+    assert(colStats.avgColLen == expectedColStats.avgColLen)\n+    assert(colStats.maxColLen == expectedColStats.maxColLen)\n+    assert(colStats.numTrues == expectedColStats.numTrues)\n+    assert(colStats.numFalses == expectedColStats.numFalses)\n+  }\n+\n+  def checkTableStats(tableName: String, expectedRowCount: Option[Int]): Option[Statistics] = {\n+    val df = sql(s\"SELECT * FROM $tableName\")"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "how about `...foreach { case (field, expectedStat) =>` ? Then we use `field.name` instead of `expected._1.name`.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-10-01T12:51:45Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Statistics}\n+import org.apache.spark.sql.execution.command.{AnalyzeColumnCommand, ColumnStatStruct}\n+import org.apache.spark.sql.execution.datasources.LogicalRelation\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+trait StatisticsTest extends QueryTest with SharedSQLContext {\n+\n+  def checkColStats(\n+      df: DataFrame,\n+      expectedColStatsSeq: Seq[(StructField, ColumnStat)]): Unit = {\n+    val table = \"tbl\"\n+    withTable(table) {\n+      df.write.format(\"json\").saveAsTable(table)\n+      val columns = expectedColStatsSeq.map(_._1)\n+      val tableIdent = TableIdentifier(table, Some(\"default\"))\n+      val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n+      val columnStats =\n+        AnalyzeColumnCommand(tableIdent, columns.map(_.name)).computeColStats(spark, relation)._2\n+      expectedColStatsSeq.foreach { expected =>"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Yeah, that's better.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-10-01T16:51:50Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Statistics}\n+import org.apache.spark.sql.execution.command.{AnalyzeColumnCommand, ColumnStatStruct}\n+import org.apache.spark.sql.execution.datasources.LogicalRelation\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+trait StatisticsTest extends QueryTest with SharedSQLContext {\n+\n+  def checkColStats(\n+      df: DataFrame,\n+      expectedColStatsSeq: Seq[(StructField, ColumnStat)]): Unit = {\n+    val table = \"tbl\"\n+    withTable(table) {\n+      df.write.format(\"json\").saveAsTable(table)\n+      val columns = expectedColStatsSeq.map(_._1)\n+      val tableIdent = TableIdentifier(table, Some(\"default\"))\n+      val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n+      val columnStats =\n+        AnalyzeColumnCommand(tableIdent, columns.map(_.name)).computeColStats(spark, relation)._2\n+      expectedColStatsSeq.foreach { expected =>"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Nit:\n\n``` Scala\nval (_, columnStats) =\n  AnalyzeColumnCommand(tableIdent, columnsToAnalyze).computeColStats(spark, relation)\n```\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-10-01T17:15:04Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.plans.logical.{ColumnStat, Statistics}\n+import org.apache.spark.sql.execution.command.{AnalyzeColumnCommand, ColumnStatStruct}\n+import org.apache.spark.sql.execution.datasources.LogicalRelation\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+trait StatisticsTest extends QueryTest with SharedSQLContext {\n+\n+  def checkColStats(\n+      df: DataFrame,\n+      expectedColStatsSeq: Seq[(StructField, ColumnStat)]): Unit = {\n+    val table = \"tbl\"\n+    withTable(table) {\n+      df.write.format(\"json\").saveAsTable(table)\n+      val columns = expectedColStatsSeq.map(_._1)\n+      val tableIdent = TableIdentifier(table, Some(\"default\"))\n+      val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n+      val columnStats =\n+        AnalyzeColumnCommand(tableIdent, columns.map(_.name)).computeColStats(spark, relation)._2"
  }],
  "prId": 15090
}]