[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "This is Hive only. Thus, create a separate file for it.",
    "commit": "9c85b18c059e4ab3b4b25a5b2e414b4f0c67072f",
    "createdAt": "2018-01-19T17:27:43Z",
    "diffHunk": "@@ -82,44 +80,4 @@ class OrcHadoopFsRelationSuite extends HadoopFsRelationTest {\n       }\n     }\n   }\n-\n-  test(\"SPARK-13543: Support for specifying compression codec for ORC via option()\") {\n-    withTempPath { dir =>\n-      val path = s\"${dir.getCanonicalPath}/table1\"\n-      val df = (1 to 5).map(i => (i, (i % 2).toString)).toDF(\"a\", \"b\")\n-      df.write\n-        .option(\"compression\", \"ZlIb\")\n-        .orc(path)\n-\n-      // Check if this is compressed as ZLIB.\n-      val maybeOrcFile = new File(path).listFiles().find { f =>\n-        !f.getName.startsWith(\"_\") && f.getName.endsWith(\".zlib.orc\")\n-      }\n-      assert(maybeOrcFile.isDefined)\n-      val orcFilePath = maybeOrcFile.get.toPath.toString\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(orcFilePath).get.getCompression\n-      assert(\"ZLIB\" === expectedCompressionKind.name())\n-\n-      val copyDf = spark\n-        .read\n-        .orc(path)\n-      checkAnswer(df, copyDf)\n-    }\n-  }\n-\n-  test(\"Default compression codec is snappy for ORC compression\") {\n-    withTempPath { file =>\n-      spark.range(0, 10).write\n-        .orc(file.getCanonicalPath)\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(file.getCanonicalPath).get.getCompression\n-      assert(\"SNAPPY\" === expectedCompressionKind.name())\n-    }\n-  }\n-}\n-\n-class HiveOrcHadoopFsRelationSuite extends OrcHadoopFsRelationSuite {",
    "line": 67
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thank you!",
    "commit": "9c85b18c059e4ab3b4b25a5b2e414b4f0c67072f",
    "createdAt": "2018-01-22T02:22:50Z",
    "diffHunk": "@@ -82,44 +80,4 @@ class OrcHadoopFsRelationSuite extends HadoopFsRelationTest {\n       }\n     }\n   }\n-\n-  test(\"SPARK-13543: Support for specifying compression codec for ORC via option()\") {\n-    withTempPath { dir =>\n-      val path = s\"${dir.getCanonicalPath}/table1\"\n-      val df = (1 to 5).map(i => (i, (i % 2).toString)).toDF(\"a\", \"b\")\n-      df.write\n-        .option(\"compression\", \"ZlIb\")\n-        .orc(path)\n-\n-      // Check if this is compressed as ZLIB.\n-      val maybeOrcFile = new File(path).listFiles().find { f =>\n-        !f.getName.startsWith(\"_\") && f.getName.endsWith(\".zlib.orc\")\n-      }\n-      assert(maybeOrcFile.isDefined)\n-      val orcFilePath = maybeOrcFile.get.toPath.toString\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(orcFilePath).get.getCompression\n-      assert(\"ZLIB\" === expectedCompressionKind.name())\n-\n-      val copyDf = spark\n-        .read\n-        .orc(path)\n-      checkAnswer(df, copyDf)\n-    }\n-  }\n-\n-  test(\"Default compression codec is snappy for ORC compression\") {\n-    withTempPath { file =>\n-      spark.range(0, 10).write\n-        .orc(file.getCanonicalPath)\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(file.getCanonicalPath).get.getCompression\n-      assert(\"SNAPPY\" === expectedCompressionKind.name())\n-    }\n-  }\n-}\n-\n-class HiveOrcHadoopFsRelationSuite extends OrcHadoopFsRelationSuite {",
    "line": 67
  }],
  "prId": 20331
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`OrcFileOperator` is defined in `sql\\hive`.",
    "commit": "9c85b18c059e4ab3b4b25a5b2e414b4f0c67072f",
    "createdAt": "2018-01-19T17:28:05Z",
    "diffHunk": "@@ -82,44 +80,4 @@ class OrcHadoopFsRelationSuite extends HadoopFsRelationTest {\n       }\n     }\n   }\n-\n-  test(\"SPARK-13543: Support for specifying compression codec for ORC via option()\") {\n-    withTempPath { dir =>\n-      val path = s\"${dir.getCanonicalPath}/table1\"\n-      val df = (1 to 5).map(i => (i, (i % 2).toString)).toDF(\"a\", \"b\")\n-      df.write\n-        .option(\"compression\", \"ZlIb\")\n-        .orc(path)\n-\n-      // Check if this is compressed as ZLIB.\n-      val maybeOrcFile = new File(path).listFiles().find { f =>\n-        !f.getName.startsWith(\"_\") && f.getName.endsWith(\".zlib.orc\")\n-      }\n-      assert(maybeOrcFile.isDefined)\n-      val orcFilePath = maybeOrcFile.get.toPath.toString\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(orcFilePath).get.getCompression\n-      assert(\"ZLIB\" === expectedCompressionKind.name())\n-\n-      val copyDf = spark\n-        .read\n-        .orc(path)\n-      checkAnswer(df, copyDf)\n-    }\n-  }\n-\n-  test(\"Default compression codec is snappy for ORC compression\") {\n-    withTempPath { file =>\n-      spark.range(0, 10).write\n-        .orc(file.getCanonicalPath)\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(file.getCanonicalPath).get.getCompression",
    "line": 61
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@gatorsmile . This test case should be tested on `native` implementation, too.\r\n`HiveOrcHadoopFsRelationSuite` test coverage is only `hive` implementation.",
    "commit": "9c85b18c059e4ab3b4b25a5b2e414b4f0c67072f",
    "createdAt": "2018-01-22T02:26:36Z",
    "diffHunk": "@@ -82,44 +80,4 @@ class OrcHadoopFsRelationSuite extends HadoopFsRelationTest {\n       }\n     }\n   }\n-\n-  test(\"SPARK-13543: Support for specifying compression codec for ORC via option()\") {\n-    withTempPath { dir =>\n-      val path = s\"${dir.getCanonicalPath}/table1\"\n-      val df = (1 to 5).map(i => (i, (i % 2).toString)).toDF(\"a\", \"b\")\n-      df.write\n-        .option(\"compression\", \"ZlIb\")\n-        .orc(path)\n-\n-      // Check if this is compressed as ZLIB.\n-      val maybeOrcFile = new File(path).listFiles().find { f =>\n-        !f.getName.startsWith(\"_\") && f.getName.endsWith(\".zlib.orc\")\n-      }\n-      assert(maybeOrcFile.isDefined)\n-      val orcFilePath = maybeOrcFile.get.toPath.toString\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(orcFilePath).get.getCompression\n-      assert(\"ZLIB\" === expectedCompressionKind.name())\n-\n-      val copyDf = spark\n-        .read\n-        .orc(path)\n-      checkAnswer(df, copyDf)\n-    }\n-  }\n-\n-  test(\"Default compression codec is snappy for ORC compression\") {\n-    withTempPath { file =>\n-      spark.range(0, 10).write\n-        .orc(file.getCanonicalPath)\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(file.getCanonicalPath).get.getCompression",
    "line": 61
  }],
  "prId": 20331
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "The same here.",
    "commit": "9c85b18c059e4ab3b4b25a5b2e414b4f0c67072f",
    "createdAt": "2018-01-19T17:28:15Z",
    "diffHunk": "@@ -82,44 +80,4 @@ class OrcHadoopFsRelationSuite extends HadoopFsRelationTest {\n       }\n     }\n   }\n-\n-  test(\"SPARK-13543: Support for specifying compression codec for ORC via option()\") {\n-    withTempPath { dir =>\n-      val path = s\"${dir.getCanonicalPath}/table1\"\n-      val df = (1 to 5).map(i => (i, (i % 2).toString)).toDF(\"a\", \"b\")\n-      df.write\n-        .option(\"compression\", \"ZlIb\")\n-        .orc(path)\n-\n-      // Check if this is compressed as ZLIB.\n-      val maybeOrcFile = new File(path).listFiles().find { f =>\n-        !f.getName.startsWith(\"_\") && f.getName.endsWith(\".zlib.orc\")\n-      }\n-      assert(maybeOrcFile.isDefined)\n-      val orcFilePath = maybeOrcFile.get.toPath.toString\n-      val expectedCompressionKind =\n-        OrcFileOperator.getFileReader(orcFilePath).get.getCompression",
    "line": 46
  }],
  "prId": 20331
}]