[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "May be better to add test(s) for multiple partitions. I guess we don't need to reiterate all of tests, but just simple one with multiple partitions to ensure all RPC endpoints are working properly.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-16T14:08:53Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.next().getInt(0) == 111)\n+    assert(iter.next().getInt(0) == 222)\n+    assert(iter.next().getInt(0) == 333)\n+    assert(!iter.hasNext)\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val firstEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(firstEpoch.next().getInt(0) == 111)\n+    assert(!firstEpoch.hasNext)\n+\n+    val secondEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(secondEpoch.next().getInt(0) == 222)\n+    assert(secondEpoch.next().getInt(0) == 333)\n+    assert(!secondEpoch.hasNext)\n+  }\n+\n+  test(\"empty epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    val thirdEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(thirdEpoch.next().getInt(0) == 111)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+  }"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Added the simple one. I agree we don't need to reiterate all of them; RDD partitions being independent is pretty well enforced by the framework.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-17T03:44:39Z",
    "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.next().getInt(0) == 111)\n+    assert(iter.next().getInt(0) == 222)\n+    assert(iter.next().getInt(0) == 333)\n+    assert(!iter.hasNext)\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val firstEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(firstEpoch.next().getInt(0) == 111)\n+    assert(!firstEpoch.hasNext)\n+\n+    val secondEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(secondEpoch.next().getInt(0) == 222)\n+    assert(secondEpoch.next().getInt(0) == 333)\n+    assert(!secondEpoch.hasNext)\n+  }\n+\n+  test(\"empty epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    val thirdEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(thirdEpoch.next().getInt(0) == 111)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+  }"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "can be compressed to `assert(iter.toSeq.map(_.getInt(0)) == Seq(111, 222, 333))`",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-17T23:30:54Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    TaskContext.unset()\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.next().getInt(0) == 111)\n+    assert(iter.next().getInt(0) == 222)\n+    assert(iter.next().getInt(0) == 333)\n+    assert(!iter.hasNext)"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I would rather add data to all the partitions all at once, and try to read from all the partitions. This would test that each partition has their own distinct receivers correctly configured. Thats the real point of a multi-partition test.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-17T23:36:30Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    TaskContext.unset()\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].receiver\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.next().getInt(0) == 111)\n+    assert(iter.next().getInt(0) == 222)\n+    assert(iter.next().getInt(0) == 333)\n+    assert(!iter.hasNext)\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val firstEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(firstEpoch.next().getInt(0) == 111)\n+    assert(!firstEpoch.hasNext)\n+\n+    val secondEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(secondEpoch.next().getInt(0) == 222)\n+    assert(secondEpoch.next().getInt(0) == 333)\n+    assert(!secondEpoch.hasNext)\n+  }\n+\n+  test(\"empty epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    val thirdEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(thirdEpoch.next().getInt(0) == 111)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+  }\n+\n+  test(\"multiple partitions\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 5)\n+    for (p <- rdd.partitions) {"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: terminate the thread.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-18T22:48:17Z",
    "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  // In this unit test, we emulate that we're in the task thread where\n+  // ContinuousShuffleReadRDD.compute() will be evaluated. This requires a task context\n+  // thread local to be set.\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    TaskContext.unset()\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.toSeq.map(_.getInt(0)) == Seq(111, 222, 333))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val firstEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(firstEpoch.toSeq.map(_.getInt(0)) == Seq(111))\n+\n+    val secondEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(secondEpoch.toSeq.map(_.getInt(0)) == Seq(222, 333))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+\n+    val thirdEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(thirdEpoch.toSeq.map(_.getInt(0)) == Seq(111))\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+  }\n+\n+  test(\"multiple partitions\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 5)\n+    // Send all data before processing to ensure there's no crossover.\n+    for (p <- rdd.partitions) {\n+      val part = p.asInstanceOf[ContinuousShuffleReadPartition]\n+      // Send index for identification.\n+      part.endpoint.askSync[Unit](ReceiverRow(unsafeRow(part.index)))\n+      part.endpoint.askSync[Unit](ReceiverEpochMarker())\n+    }\n+\n+    for (p <- rdd.partitions) {\n+      val part = p.asInstanceOf[ContinuousShuffleReadPartition]\n+      val iter = rdd.compute(part, ctx)\n+      assert(iter.next().getInt(0) == part.index)\n+      assert(!iter.hasNext)\n+    }\n+  }\n+\n+  test(\"blocks waiting for new rows\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+\n+    val readRow = new Thread {\n+      override def run(): Unit = {\n+        // set the non-inheritable thread local\n+        TaskContext.setTaskContext(ctx)\n+        val epoch = rdd.compute(rdd.partitions(0), ctx)\n+        epoch.next().getInt(0)\n+      }\n+    }\n+\n+    readRow.start()\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRow.getState == Thread.State.WAITING)\n+    }\n+  }"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "with a finally.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-18T22:49:11Z",
    "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  // In this unit test, we emulate that we're in the task thread where\n+  // ContinuousShuffleReadRDD.compute() will be evaluated. This requires a task context\n+  // thread local to be set.\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    TaskContext.unset()\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.toSeq.map(_.getInt(0)) == Seq(111, 222, 333))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val firstEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(firstEpoch.toSeq.map(_.getInt(0)) == Seq(111))\n+\n+    val secondEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(secondEpoch.toSeq.map(_.getInt(0)) == Seq(222, 333))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+\n+    val thirdEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(thirdEpoch.toSeq.map(_.getInt(0)) == Seq(111))\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+  }\n+\n+  test(\"multiple partitions\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 5)\n+    // Send all data before processing to ensure there's no crossover.\n+    for (p <- rdd.partitions) {\n+      val part = p.asInstanceOf[ContinuousShuffleReadPartition]\n+      // Send index for identification.\n+      part.endpoint.askSync[Unit](ReceiverRow(unsafeRow(part.index)))\n+      part.endpoint.askSync[Unit](ReceiverEpochMarker())\n+    }\n+\n+    for (p <- rdd.partitions) {\n+      val part = p.asInstanceOf[ContinuousShuffleReadPartition]\n+      val iter = rdd.compute(part, ctx)\n+      assert(iter.next().getInt(0) == part.index)\n+      assert(!iter.hasNext)\n+    }\n+  }\n+\n+  test(\"blocks waiting for new rows\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+\n+    val readRow = new Thread {\n+      override def run(): Unit = {\n+        // set the non-inheritable thread local\n+        TaskContext.setTaskContext(ctx)\n+        val epoch = rdd.compute(rdd.partitions(0), ctx)\n+        epoch.next().getInt(0)\n+      }\n+    }\n+\n+    readRow.start()\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRow.getState == Thread.State.WAITING)\n+    }\n+  }"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: not a row. its a thread.",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-18T22:48:53Z",
    "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  // In this unit test, we emulate that we're in the task thread where\n+  // ContinuousShuffleReadRDD.compute() will be evaluated. This requires a task context\n+  // thread local to be set.\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    TaskContext.unset()\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.toSeq.map(_.getInt(0)) == Seq(111, 222, 333))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val firstEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(firstEpoch.toSeq.map(_.getInt(0)) == Seq(111))\n+\n+    val secondEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(secondEpoch.toSeq.map(_.getInt(0)) == Seq(222, 333))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+\n+    val thirdEpoch = rdd.compute(rdd.partitions(0), ctx)\n+    assert(thirdEpoch.toSeq.map(_.getInt(0)) == Seq(111))\n+\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+    assert(rdd.compute(rdd.partitions(0), ctx).isEmpty)\n+  }\n+\n+  test(\"multiple partitions\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 5)\n+    // Send all data before processing to ensure there's no crossover.\n+    for (p <- rdd.partitions) {\n+      val part = p.asInstanceOf[ContinuousShuffleReadPartition]\n+      // Send index for identification.\n+      part.endpoint.askSync[Unit](ReceiverRow(unsafeRow(part.index)))\n+      part.endpoint.askSync[Unit](ReceiverEpochMarker())\n+    }\n+\n+    for (p <- rdd.partitions) {\n+      val part = p.asInstanceOf[ContinuousShuffleReadPartition]\n+      val iter = rdd.compute(part, ctx)\n+      assert(iter.next().getInt(0) == part.index)\n+      assert(!iter.hasNext)\n+    }\n+  }\n+\n+  test(\"blocks waiting for new rows\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+\n+    val readRow = new Thread {"
  }],
  "prId": 21337
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Would be super NICE if there was a function that allowed this to be \r\n`send(endpoint, ReceiverRow(unsafeRow(111)), ReceiverEpochMarker(), ReceiverRow(unsafeRow(222)), ReceiverRow(unsafeRow(333)), ...) `",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-18T22:52:30Z",
    "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  // In this unit test, we emulate that we're in the task thread where\n+  // ContinuousShuffleReadRDD.compute() will be evaluated. This requires a task context\n+  // thread local to be set.\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    TaskContext.unset()\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.toSeq.map(_.getInt(0)) == Seq(111, 222, 333))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "simply\r\n`def send(end: ThreadsafeRPCEndpoint, messages: UnsafeRowReceiverMessage*) { ... }`",
    "commit": "00f910ea39b76a24e1e21acdf3d6a20fd7784fa9",
    "createdAt": "2018-05-18T22:53:57Z",
    "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.continuous.shuffle\n+\n+import org.apache.spark.{TaskContext, TaskContextImpl}\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleReadSuite extends StreamTest {\n+\n+  private def unsafeRow(value: Int) = {\n+    UnsafeProjection.create(Array(IntegerType : DataType))(\n+      new GenericInternalRow(Array(value: Any)))\n+  }\n+\n+  // In this unit test, we emulate that we're in the task thread where\n+  // ContinuousShuffleReadRDD.compute() will be evaluated. This requires a task context\n+  // thread local to be set.\n+  var ctx: TaskContextImpl = _\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    ctx = TaskContext.empty()\n+    TaskContext.setTaskContext(ctx)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    ctx.markTaskCompleted(None)\n+    TaskContext.unset()\n+    ctx = null\n+    super.afterEach()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[UnsafeRowReceiver].stopped.get())\n+    }\n+  }\n+\n+  test(\"one epoch\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+\n+    val iter = rdd.compute(rdd.partitions(0), ctx)\n+    assert(iter.toSeq.map(_.getInt(0)) == Seq(111, 222, 333))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(111)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(222)))\n+    endpoint.askSync[Unit](ReceiverRow(unsafeRow(333)))\n+    endpoint.askSync[Unit](ReceiverEpochMarker())"
  }],
  "prId": 21337
}]