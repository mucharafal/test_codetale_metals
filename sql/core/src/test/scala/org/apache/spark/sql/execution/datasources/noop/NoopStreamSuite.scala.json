[{
  "comments": [{
    "author": {
      "login": "MaxGekk"
    },
    "body": "Can you check that all inputs were consumed?",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-23T22:27:54Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Maybe below code would just work?\r\n\r\n```\r\ntestStream(query) {\r\n  AddData(input, 1, 2, 3),\r\n  CheckNewAnswer()\r\n}\r\n```",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-24T08:42:33Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Already considered but that uses MemorySink so can't be used.\r\nWithout exactly consuming data it's not possible what exactly consumed. The whole point here is to skip.",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-24T13:45:14Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah right my bad. Without having some modification on no-op sink I wonder we can check it, and if we modify the sink it ends up having logic which is only for testing purpose.\r\n\r\nBest effort would be putting some data in couple of batches, and see whether the query goes through the batches properly (verify via AssertOnQuery, or even just ensure `testStream` ends within timeout).",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-24T14:01:39Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I still think it's good best effort.\r\n```\r\n  /**\r\n   * Blocks until all available data in the source has been processed and committed to the sink.\r\n   ...\r\n   */\r\n  def processAllAvailable(): Unit\r\n```\r\nThe only potential what I see is to wrap it with `failAfter(streamingTimeout) {...}`.",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-24T15:04:08Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "@gaborgsomogyi we can use the streaming query progress to check, such as `eventually(...) { query.recentProgress.map(_.numInputRows).sum > 0 }`.",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-24T19:20:58Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yep, that's a good point. Changing...",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-24T19:30:13Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()"
  }],
  "prId": 23631
}, {
  "comments": [{
    "author": {
      "login": "MaxGekk"
    },
    "body": "The same as above. It would be nice to make sure that `noop` materializes all rows. Can you add such check?",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-23T22:29:19Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()\n+    } finally {\n+      query.stop()\n+    }\n+  }\n+\n+  test(\"continuous\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"numPartitions\", \"1\")\n+      .option(\"rowsPerSecond\", \"5\")\n+      .load()\n+      .select('value)\n+\n+    val query = input.writeStream.format(\"noop\").trigger(Trigger.Continuous(200)).start()\n+    assert(query.isActive)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Same applies here.",
    "commit": "642276497577e7db2211655987162349eecf8b1a",
    "createdAt": "2019-01-24T13:48:19Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.noop\n+\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+\n+class NoopStreamSuite extends StreamTest {\n+  import testImplicits._\n+\n+  test(\"microbatch\") {\n+    val input = MemoryStream[Int]\n+    val query = input.toDF().writeStream.format(\"noop\").start()\n+    assert(query.isActive)\n+    try {\n+      input.addData(1, 2, 3)\n+      query.processAllAvailable()\n+    } finally {\n+      query.stop()\n+    }\n+  }\n+\n+  test(\"continuous\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"numPartitions\", \"1\")\n+      .option(\"rowsPerSecond\", \"5\")\n+      .load()\n+      .select('value)\n+\n+    val query = input.writeStream.format(\"noop\").trigger(Trigger.Continuous(200)).start()\n+    assert(query.isActive)"
  }],
  "prId": 23631
}]