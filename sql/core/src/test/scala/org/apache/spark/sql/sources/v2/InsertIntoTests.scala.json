[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we move it to `InsertIntoTests`? It's not used in `InsertIntoSQLTests`.",
    "commit": "f1d86fffdafaac99a040ecd718f9e16aa49d164c",
    "createdAt": "2019-08-26T13:29:45Z",
    "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.internal.SQLConf.{PARTITION_OVERWRITE_MODE, PartitionOverwriteMode}\n+import org.apache.spark.sql.test.SharedSparkSession\n+\n+abstract class InsertIntoTests(\n+    override protected val supportsDynamicOverwrite: Boolean,\n+    override protected val includeSQLTests: Boolean) extends InsertIntoSQLTests {\n+\n+  import testImplicits._\n+\n+  test(\"insertInto: append\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    doInsert(t1, df)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"insertInto: append by position\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+\n+    doInsert(t1, dfr)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"insertInto: append partitioned table\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df)\n+      verifyTable(t1, df)\n+    }\n+  }\n+\n+  test(\"insertInto: overwrite non-partitioned table\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val df2 = Seq((4L, \"d\"), (5L, \"e\"), (6L, \"f\")).toDF(\"id\", \"data\")\n+    doInsert(t1, df)\n+    doInsert(t1, df2, SaveMode.Overwrite)\n+    verifyTable(t1, df2)\n+  }\n+\n+  test(\"insertInto: overwrite partitioned table in static mode\") {\n+    withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.STATIC.toString) {\n+      val t1 = s\"${catalogAndNamespace}tbl\"\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      doInsert(t1, init)\n+\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df, SaveMode.Overwrite)\n+      verifyTable(t1, df)\n+    }\n+  }\n+\n+\n+  test(\"insertInto: overwrite partitioned table in static mode by position\") {\n+    withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.STATIC.toString) {\n+      val t1 = s\"${catalogAndNamespace}tbl\"\n+      withTable(t1) {\n+        sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+        val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+        doInsert(t1, init)\n+\n+        val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+        doInsert(t1, dfr, SaveMode.Overwrite)\n+\n+        val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+        verifyTable(t1, df)\n+      }\n+    }\n+  }\n+\n+  test(\"insertInto: fails when missing a column\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string, missing string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val exc = intercept[AnalysisException] {\n+      doInsert(t1, df)\n+    }\n+\n+    verifyTable(t1, Seq.empty[(Long, String, String)].toDF(\"id\", \"data\", \"missing\"))\n+    val tableName = if (catalogAndNamespace.isEmpty) s\"default.$t1\" else t1\n+    assert(exc.getMessage.contains(s\"Cannot write to '$tableName', not enough data columns\"))\n+  }\n+\n+  test(\"insertInto: fails when an extra column is present\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+      val df = Seq((1L, \"a\", \"mango\")).toDF(\"id\", \"data\", \"fruit\")\n+      val exc = intercept[AnalysisException] {\n+        doInsert(t1, df)\n+      }\n+\n+      verifyTable(t1, Seq.empty[(Long, String)].toDF(\"id\", \"data\"))\n+      val tableName = if (catalogAndNamespace.isEmpty) s\"default.$t1\" else t1\n+      assert(exc.getMessage.contains(s\"Cannot write to '$tableName', too many data columns\"))\n+    }\n+  }\n+\n+  dynamicOverwriteTest(\"insertInto: overwrite partitioned table in dynamic mode\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      doInsert(t1, init)\n+\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df, SaveMode.Overwrite)\n+\n+      verifyTable(t1, df.union(sql(\"SELECT 4L, 'keep'\")))\n+    }\n+  }\n+\n+  dynamicOverwriteTest(\"insertInto: overwrite partitioned table in dynamic mode by position\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      doInsert(t1, init)\n+\n+      val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+      doInsert(t1, dfr, SaveMode.Overwrite)\n+\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      verifyTable(t1, df)\n+    }\n+  }\n+}\n+\n+private[v2] trait InsertIntoSQLTests extends QueryTest with SharedSparkSession with BeforeAndAfter {\n+\n+  import testImplicits._\n+\n+  protected def doInsert(tableName: String, insert: DataFrame, mode: SaveMode = null): Unit"
  }],
  "prId": 25507
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Thanks for the clarification! Now I understand. `InsertIntoSQLOnlyTests` tests features that are only available in SQL, like the PARTITION clause.",
    "commit": "f1d86fffdafaac99a040ecd718f9e16aa49d164c",
    "createdAt": "2019-08-27T02:14:07Z",
    "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.internal.SQLConf.{PARTITION_OVERWRITE_MODE, PartitionOverwriteMode}\n+import org.apache.spark.sql.test.SharedSparkSession\n+\n+/**\n+ * A collection of \"INSERT INTO\" tests that can be run through the SQL or DataFrameWriter APIs.\n+ * Extending test suites can implement the `doInsert` method to run the insert through either\n+ * API.",
    "line": 29
  }],
  "prId": 25507
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Patterns like this break IntelliJ's test runner because it can't detect blocks that are tests. That means debugging from within the IDE and running a single test by name is difficult. It may also break running single tests on the command-line too, I'm not sure about that.\r\n\r\nA better pattern is to use `assume` instead. That will cancel a test if the assumption (in this case, `supportsDynamicOverwrite`) is false.",
    "commit": "f1d86fffdafaac99a040ecd718f9e16aa49d164c",
    "createdAt": "2019-08-29T21:14:22Z",
    "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.internal.SQLConf.{PARTITION_OVERWRITE_MODE, PartitionOverwriteMode}\n+import org.apache.spark.sql.test.SharedSparkSession\n+\n+/**\n+ * A collection of \"INSERT INTO\" tests that can be run through the SQL or DataFrameWriter APIs.\n+ * Extending test suites can implement the `doInsert` method to run the insert through either\n+ * API.\n+ *\n+ * @param supportsDynamicOverwrite Whether the Table implementations used in the test suite support\n+ *                                 dynamic partition overwrites. If they do, we will check for the\n+ *                                 success of the operations. If not, then we will check that we\n+ *                                 failed with the right error message.\n+ * @param includeSQLOnlyTests Certain INSERT INTO behavior can be achieved purely through SQL, e.g.\n+ *                            static or dynamic partition overwrites. This flag should be set to\n+ *                            true if we would like to test these cases.\n+ */\n+abstract class InsertIntoTests(\n+    override protected val supportsDynamicOverwrite: Boolean,\n+    override protected val includeSQLOnlyTests: Boolean) extends InsertIntoSQLOnlyTests {\n+\n+  import testImplicits._\n+\n+  /**\n+   * Insert data into a table using the insertInto statement. Implementations can be in SQL\n+   * (\"INSERT\") or using the DataFrameWriter (`df.write.insertInto`).\n+   */\n+  protected def doInsert(tableName: String, insert: DataFrame, mode: SaveMode = null): Unit\n+\n+  test(\"insertInto: append\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    doInsert(t1, df)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"insertInto: append by position\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+\n+    doInsert(t1, dfr)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"insertInto: append partitioned table\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df)\n+      verifyTable(t1, df)\n+    }\n+  }\n+\n+  test(\"insertInto: overwrite non-partitioned table\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val df2 = Seq((4L, \"d\"), (5L, \"e\"), (6L, \"f\")).toDF(\"id\", \"data\")\n+    doInsert(t1, df)\n+    doInsert(t1, df2, SaveMode.Overwrite)\n+    verifyTable(t1, df2)\n+  }\n+\n+  test(\"insertInto: overwrite partitioned table in static mode\") {\n+    withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.STATIC.toString) {\n+      val t1 = s\"${catalogAndNamespace}tbl\"\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      doInsert(t1, init)\n+\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df, SaveMode.Overwrite)\n+      verifyTable(t1, df)\n+    }\n+  }\n+\n+\n+  test(\"insertInto: overwrite partitioned table in static mode by position\") {\n+    withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.STATIC.toString) {\n+      val t1 = s\"${catalogAndNamespace}tbl\"\n+      withTable(t1) {\n+        sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+        val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+        doInsert(t1, init)\n+\n+        val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+        doInsert(t1, dfr, SaveMode.Overwrite)\n+\n+        val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+        verifyTable(t1, df)\n+      }\n+    }\n+  }\n+\n+  test(\"insertInto: fails when missing a column\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string, missing string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val exc = intercept[AnalysisException] {\n+      doInsert(t1, df)\n+    }\n+\n+    verifyTable(t1, Seq.empty[(Long, String, String)].toDF(\"id\", \"data\", \"missing\"))\n+    val tableName = if (catalogAndNamespace.isEmpty) s\"default.$t1\" else t1\n+    assert(exc.getMessage.contains(s\"Cannot write to '$tableName', not enough data columns\"))\n+  }\n+\n+  test(\"insertInto: fails when an extra column is present\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+      val df = Seq((1L, \"a\", \"mango\")).toDF(\"id\", \"data\", \"fruit\")\n+      val exc = intercept[AnalysisException] {\n+        doInsert(t1, df)\n+      }\n+\n+      verifyTable(t1, Seq.empty[(Long, String)].toDF(\"id\", \"data\"))\n+      val tableName = if (catalogAndNamespace.isEmpty) s\"default.$t1\" else t1\n+      assert(exc.getMessage.contains(s\"Cannot write to '$tableName', too many data columns\"))\n+    }\n+  }\n+\n+  dynamicOverwriteTest(\"insertInto: overwrite partitioned table in dynamic mode\") {",
    "line": 148
  }],
  "prId": 25507
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Same here. It would be better to use assume.",
    "commit": "f1d86fffdafaac99a040ecd718f9e16aa49d164c",
    "createdAt": "2019-08-29T21:15:00Z",
    "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.internal.SQLConf.{PARTITION_OVERWRITE_MODE, PartitionOverwriteMode}\n+import org.apache.spark.sql.test.SharedSparkSession\n+\n+/**\n+ * A collection of \"INSERT INTO\" tests that can be run through the SQL or DataFrameWriter APIs.\n+ * Extending test suites can implement the `doInsert` method to run the insert through either\n+ * API.\n+ *\n+ * @param supportsDynamicOverwrite Whether the Table implementations used in the test suite support\n+ *                                 dynamic partition overwrites. If they do, we will check for the\n+ *                                 success of the operations. If not, then we will check that we\n+ *                                 failed with the right error message.\n+ * @param includeSQLOnlyTests Certain INSERT INTO behavior can be achieved purely through SQL, e.g.\n+ *                            static or dynamic partition overwrites. This flag should be set to\n+ *                            true if we would like to test these cases.\n+ */\n+abstract class InsertIntoTests(\n+    override protected val supportsDynamicOverwrite: Boolean,\n+    override protected val includeSQLOnlyTests: Boolean) extends InsertIntoSQLOnlyTests {\n+\n+  import testImplicits._\n+\n+  /**\n+   * Insert data into a table using the insertInto statement. Implementations can be in SQL\n+   * (\"INSERT\") or using the DataFrameWriter (`df.write.insertInto`).\n+   */\n+  protected def doInsert(tableName: String, insert: DataFrame, mode: SaveMode = null): Unit\n+\n+  test(\"insertInto: append\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    doInsert(t1, df)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"insertInto: append by position\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+\n+    doInsert(t1, dfr)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"insertInto: append partitioned table\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df)\n+      verifyTable(t1, df)\n+    }\n+  }\n+\n+  test(\"insertInto: overwrite non-partitioned table\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val df2 = Seq((4L, \"d\"), (5L, \"e\"), (6L, \"f\")).toDF(\"id\", \"data\")\n+    doInsert(t1, df)\n+    doInsert(t1, df2, SaveMode.Overwrite)\n+    verifyTable(t1, df2)\n+  }\n+\n+  test(\"insertInto: overwrite partitioned table in static mode\") {\n+    withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.STATIC.toString) {\n+      val t1 = s\"${catalogAndNamespace}tbl\"\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      doInsert(t1, init)\n+\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df, SaveMode.Overwrite)\n+      verifyTable(t1, df)\n+    }\n+  }\n+\n+\n+  test(\"insertInto: overwrite partitioned table in static mode by position\") {\n+    withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.STATIC.toString) {\n+      val t1 = s\"${catalogAndNamespace}tbl\"\n+      withTable(t1) {\n+        sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+        val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+        doInsert(t1, init)\n+\n+        val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+        doInsert(t1, dfr, SaveMode.Overwrite)\n+\n+        val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+        verifyTable(t1, df)\n+      }\n+    }\n+  }\n+\n+  test(\"insertInto: fails when missing a column\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    sql(s\"CREATE TABLE $t1 (id bigint, data string, missing string) USING $v2Format\")\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    val exc = intercept[AnalysisException] {\n+      doInsert(t1, df)\n+    }\n+\n+    verifyTable(t1, Seq.empty[(Long, String, String)].toDF(\"id\", \"data\", \"missing\"))\n+    val tableName = if (catalogAndNamespace.isEmpty) s\"default.$t1\" else t1\n+    assert(exc.getMessage.contains(s\"Cannot write to '$tableName', not enough data columns\"))\n+  }\n+\n+  test(\"insertInto: fails when an extra column is present\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+      val df = Seq((1L, \"a\", \"mango\")).toDF(\"id\", \"data\", \"fruit\")\n+      val exc = intercept[AnalysisException] {\n+        doInsert(t1, df)\n+      }\n+\n+      verifyTable(t1, Seq.empty[(Long, String)].toDF(\"id\", \"data\"))\n+      val tableName = if (catalogAndNamespace.isEmpty) s\"default.$t1\" else t1\n+      assert(exc.getMessage.contains(s\"Cannot write to '$tableName', too many data columns\"))\n+    }\n+  }\n+\n+  dynamicOverwriteTest(\"insertInto: overwrite partitioned table in dynamic mode\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      doInsert(t1, init)\n+\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+      doInsert(t1, df, SaveMode.Overwrite)\n+\n+      verifyTable(t1, df.union(sql(\"SELECT 4L, 'keep'\")))\n+    }\n+  }\n+\n+  dynamicOverwriteTest(\"insertInto: overwrite partitioned table in dynamic mode by position\") {\n+    val t1 = s\"${catalogAndNamespace}tbl\"\n+    withTable(t1) {\n+      sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format PARTITIONED BY (id)\")\n+      val init = Seq((2L, \"dummy\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      doInsert(t1, init)\n+\n+      val dfr = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"data\", \"id\")\n+      doInsert(t1, dfr, SaveMode.Overwrite)\n+\n+      val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"), (4L, \"keep\")).toDF(\"id\", \"data\")\n+      verifyTable(t1, df)\n+    }\n+  }\n+}\n+\n+private[v2] trait InsertIntoSQLOnlyTests\n+  extends QueryTest\n+  with SharedSparkSession\n+  with BeforeAndAfter {\n+\n+  import testImplicits._\n+\n+  /** Check that the results in `tableName` match the `expected` DataFrame. */\n+  protected def verifyTable(tableName: String, expected: DataFrame): Unit\n+\n+  protected val v2Format: String\n+  protected val catalogAndNamespace: String\n+\n+  /**\n+   * Whether dynamic partition overwrites are supported by the `Table` definitions used in the\n+   * test suites. Tables that leverage the V1 Write interface do not support dynamic partition\n+   * overwrites.\n+   */\n+  protected val supportsDynamicOverwrite: Boolean\n+\n+  /** Whether to include the SQL specific tests in this trait within the extending test suite. */\n+  protected val includeSQLOnlyTests: Boolean\n+\n+  private def withTableAndData(tableName: String)(testFn: String => Unit): Unit = {\n+    withTable(tableName) {\n+      val viewName = \"tmp_view\"\n+      val df = spark.createDataFrame(Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"))).toDF(\"id\", \"data\")\n+      df.createOrReplaceTempView(viewName)\n+      withTempView(viewName) {\n+        testFn(viewName)\n+      }\n+    }\n+  }\n+\n+  protected def dynamicOverwriteTest(testName: String)(f: => Unit): Unit = {\n+    test(testName) {\n+      try {\n+        withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.DYNAMIC.toString) {\n+          f\n+        }\n+        if (!supportsDynamicOverwrite) {\n+          fail(\"Expected failure from test, because the table doesn't support dynamic overwrites\")\n+        }\n+      } catch {\n+        case a: AnalysisException if !supportsDynamicOverwrite =>\n+          assert(a.getMessage.contains(\"Table does not support dynamic overwrite\"))\n+      }\n+    }\n+  }\n+\n+  if (includeSQLOnlyTests) {",
    "line": 228
  }],
  "prId": 25507
}]