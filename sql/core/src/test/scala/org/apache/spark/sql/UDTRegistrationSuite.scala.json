[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I don't see why we need this setup to unit test UDTRegistration. I would test the following:\n1. default values (see my previous comments)\n2. register a class\n3. query a registered class\n4. query a unregistered class\n\nWe do need to test new `VectorUDT` and `MatrixUDT`. That should be in a unit test under MLlib, not SQL, which we already covered in this PR.\n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-22T18:45:35Z",
    "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import scala.beans.{BeanInfo, BeanProperty}\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.CatalystTypeConverters\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.GenericMutableRow\n+import org.apache.spark.sql.catalyst.util.{ArrayData, GenericArrayData}\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetTest\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+private[sql] trait DVector extends Serializable\n+\n+private[sql] class MyDVector(val data: Array[Double]) extends DVector with Serializable {\n+  override def equals(other: Any): Boolean = other match {\n+    case v: MyDVector =>\n+      java.util.Arrays.equals(this.data, v.data)\n+    case _ => false\n+  }\n+}\n+\n+private[sql] class MyDVector2(val data: Array[Double]) extends DVector with Serializable {\n+  override def equals(other: Any): Boolean = other match {\n+    case v: MyDVector2 =>\n+      java.util.Arrays.equals(this.data, v.data)\n+    case _ => false\n+  }\n+}\n+\n+object MyDVector {\n+  def unapply(dv: MyDVector): Option[Array[Double]] = Some(dv.data)\n+}\n+\n+object MyDVector2 {\n+  def unapply(dv: MyDVector2): Option[Array[Double]] = Some(dv.data)\n+}\n+\n+@BeanInfo\n+private[sql] case class MyPoint(\n+    @BeanProperty label: Double,\n+    @BeanProperty features: DVector)\n+\n+private[sql] class TestUserClass {\n+}\n+\n+private[sql] class NonUserDefinedType {\n+}\n+\n+private[sql] class MyDVectorUDT extends UserDefinedType[DVector] {\n+\n+  override def sqlType: DataType = {\n+    // type: 0 = MyDVector, 1 = MyDVector2\n+    StructType(Seq(\n+      StructField(\"type\", ByteType, nullable = false),\n+      StructField(\"values\", ArrayType(DoubleType, containsNull = false), nullable = false)))\n+  }\n+\n+  override def serialize(features: DVector): InternalRow = {\n+    val row = new GenericMutableRow(2)\n+    features match {\n+      case MyDVector(data) =>\n+        row.setByte(0, 0)\n+        row.update(1, new GenericArrayData(data.map(_.asInstanceOf[Any])))\n+      case MyDVector2(data) =>\n+        row.setByte(0, 1)\n+        row.update(1, new GenericArrayData(data.map(_.asInstanceOf[Any])))\n+    }\n+    row\n+  }\n+\n+  override def deserialize(datum: Any): DVector = {\n+    datum match {\n+      case row: InternalRow =>\n+        require(row.numFields == 2, \"MyDVectorUDT.deserialize given row with length \" +\n+          s\"${row.numFields} but requires length == 2\")\n+        val tpe = row.getByte(0)\n+        tpe match {\n+          case 0 =>\n+            val values = row.getArray(1).toDoubleArray()\n+            new MyDVector(values)\n+          case 1 =>\n+            val values = row.getArray(1).toDoubleArray()\n+            new MyDVector2(values)\n+        }\n+    }\n+  }\n+\n+  override def userClass: Class[DVector] = classOf[DVector]\n+\n+  private[spark] override def asNullable: MyDVectorUDT = this\n+\n+  override def equals(other: Any): Boolean = other match {\n+    case _: MyDVectorUDT => true\n+    case _ => false\n+  }\n+}"
  }],
  "prId": 12259
}]