[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: name \"stream execution metadata\" does not convey anything. maybe \"json parsing\"",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-23T21:38:10Z",
    "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.execution.streaming.StreamExecutionMetadata\n+\n+class StreamExecutionMetadataSuite extends SparkFunSuite {\n+\n+  test(\"stream execution metadata\") {"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "add some docs saying why this query was chosen",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-23T23:20:19Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"ensure consistent results across batch executions\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    def startQuery: StreamingQuery = {"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "can you be more specific about what this test. e.g. \"metadata is recovered from log when query is restarted\"",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-23T23:21:41Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"ensure consistent results across batch executions\") {"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "increase the time added. this assumes that the query will be done before 5 to 10 seconds. that may not be true. probably add a minute.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-23T23:27:16Z",
    "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"ensure consistent results across batch executions\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 5000L"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: extra line.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T09:41:48Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L\n+    val q = startQuery\n+    ms.addData(t1, t2)\n+    q.processAllAvailable()\n+\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q.stop()\n+    Thread.sleep(60L * 1000L + 5000L) // Expire t1 and t2\n+    assert(t1 < clock.getTimeMillis())\n+    assert(t2 < clock.getTimeMillis())\n+\n+    spark.sql(s\"drop table $tableName\")\n+\n+    // verify table is dropped\n+    intercept[AnalysisException](spark.table(tableName).collect())\n+    val q2 = startQuery\n+    q2.processAllAvailable()\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q2.stop()\n+"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: functions that have sideeffects (like starting a thread), usually have `()` at the end, and is used with the `()`. for example, `val q = startQuery()`",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T09:50:46Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "what does this comment mean?",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T09:52:05Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "add a comment explaining how the test works.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T09:52:42Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This test will now takes 60 seconds! I think I didnt quite understand the test earlier, but now I do. I think the earlier 5 second was closer to being fine. Okay, lets just use 10 seconds. And instead of sleep, use `eventually` to check the conditions `t2 < clock.getTimeMillis()`. this would make the system sleep no more than that is necessary.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T09:57:02Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L\n+    val q = startQuery\n+    ms.addData(t1, t2)\n+    q.processAllAvailable()\n+\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q.stop()\n+    Thread.sleep(60L * 1000L + 5000L) // Expire t1 and t2"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Why not use `StreamManualClock` to avoid the indeterministic system clock.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T18:48:34Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L\n+    val q = startQuery\n+    ms.addData(t1, t2)\n+    q.processAllAvailable()\n+\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q.stop()\n+    Thread.sleep(60L * 1000L + 5000L) // Expire t1 and t2"
  }, {
    "author": {
      "login": "tcondie"
    },
    "body": "@zsxwing I don't see an obvious way to pass a StreamManualClock in DataStreamWriter.start(). Should I be taking an entirely different approach?",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T19:12:02Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L\n+    val q = startQuery\n+    ms.addData(t1, t2)\n+    q.processAllAvailable()\n+\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q.stop()\n+    Thread.sleep(60L * 1000L + 5000L) // Expire t1 and t2"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Actually we can remove this test completely. Rather the existing current_time and current_date tests can be extended to test recovery. No point having all this extra code and thread.sleeps thats going to be flaky. ",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T19:28:40Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L\n+    val q = startQuery\n+    ms.addData(t1, t2)\n+    q.processAllAvailable()\n+\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q.stop()\n+    Thread.sleep(60L * 1000L + 5000L) // Expire t1 and t2"
  }, {
    "author": {
      "login": "tcondie"
    },
    "body": "I should also say that I'm not too concerned by the indeterministic system clock issues since the batchTimestamp is recorded prior to running the query. Therefore as long as the query gets planned within 10 seconds, we're good.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T20:10:35Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L\n+    val q = startQuery\n+    ms.addData(t1, t2)\n+    q.processAllAvailable()\n+\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q.stop()\n+    Thread.sleep(60L * 1000L + 5000L) // Expire t1 and t2"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "i think you can use `spark.catalog,tableExists`",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T09:59:08Z",
    "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {\n+    import testImplicits._\n+    val clock = new SystemClock()\n+    val ms = new MemoryStream[Long](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"complete\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    val tableName = \"test\"\n+    // Query that prunes timestamps less than current_timestamp, making\n+    // it easy to use for ensuring that a batch is re-processed with the\n+    // timestamp used when it was first processed.\n+    def startQuery: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .where('a >= current_timestamp().cast(\"long\"))\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(tableName)\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val t1 = clock.getTimeMillis() + 60L * 1000L\n+    val t2 = clock.getTimeMillis() + 60L * 1000L + 1000L\n+    val q = startQuery\n+    ms.addData(t1, t2)\n+    q.processAllAvailable()\n+\n+    checkAnswer(\n+      spark.table(tableName),\n+      Seq(Row(t1, 1), Row(t2, 1))\n+    )\n+\n+    q.stop()\n+    Thread.sleep(60L * 1000L + 5000L) // Expire t1 and t2\n+    assert(t1 < clock.getTimeMillis())\n+    assert(t2 < clock.getTimeMillis())\n+\n+    spark.sql(s\"drop table $tableName\")\n+\n+    // verify table is dropped\n+    intercept[AnalysisException](spark.table(tableName).collect())"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "I just tried to write a deterministic test using manual lock:\r\n```\r\n  test(\"metadata is recovered from log when query is restarted\") {\r\n    import testImplicits._\r\n    val clock = new StreamManualClock()\r\n    val inputData = MemoryStream[Long]\r\n    val df = inputData.toDF()\r\n      .select(current_timestamp().cast(\"long\").as[Long])\r\n    testStream(df)(\r\n      StartStream(trigger = ProcessingTime(\"1 second\"), triggerClock = clock),\r\n      AssertOnQuery { _ =>\r\n        // Make sure the clock is waiting. Otherwise, the batch time may be 0 or 1.\r\n        eventually(timeout(streamingTimeout)) {\r\n          assert(clock.isStreamWaitingAt(0L))\r\n        }\r\n        true\r\n      },\r\n      AddData(inputData, 1L), // Trigger one output row\r\n      AdvanceManualClock(1000L),\r\n      CheckLastBatch(1L),\r\n      AddData(inputData, 1L), // Trigger one output row\r\n      AdvanceManualClock(1000L),\r\n      CheckLastBatch(2L),\r\n      StopStream,\r\n      AssertOnQuery { q => // clear the sink\r\n        q.sink.asInstanceOf[MemorySink].clear()\r\n        true\r\n      },\r\n      StartStream(trigger = ProcessingTime(\"1 second\"), triggerClock = clock),\r\n      CheckLastBatch(2L),\r\n      AddData(inputData, 1L), // Trigger one output row\r\n      AdvanceManualClock(1000L),\r\n      CheckLastBatch(3L)\r\n    )\r\n  }\r\n```\r\nYou can use it to replace yours.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T21:57:36Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.io.File\n+\n+import org.scalatest.concurrent.Eventually\n+import org.scalatest.concurrent.PatienceConfiguration.Timeout\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamExecutionMetadata}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{SystemClock, Utils}\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir =\n+    Utils.createTempDir(namePrefix = \"streaming.metadata\").getCanonicalPath\n+\n+  test(\"stream execution metadata\") {\n+    assert(StreamExecutionMetadata(0, 0) ===\n+      StreamExecutionMetadata(\"\"\"{}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 0) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchWatermarkMs\":1}\"\"\"))\n+    assert(StreamExecutionMetadata(0, 2) ===\n+      StreamExecutionMetadata(\"\"\"{\"batchTimestampMs\":2}\"\"\"))\n+    assert(StreamExecutionMetadata(1, 2) ===\n+      StreamExecutionMetadata(\n+        \"\"\"{\"batchWatermarkMs\":1,\"batchTimestampMs\":2}\"\"\"))\n+  }\n+\n+  test(\"metadata is recovered from log when query is restarted\") {"
  }],
  "prId": 15949
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: not used.",
    "commit": "0ea579695050ae9c1dd7f68cc8df2dc7fbfc833e",
    "createdAt": "2016-11-28T22:59:50Z",
    "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.apache.spark.sql.execution.streaming.StreamExecutionMetadata\n+import org.apache.spark.util.Utils\n+\n+class StreamExecutionMetadataSuite extends StreamTest {\n+\n+  private def newMetadataDir ="
  }],
  "prId": 15949
}]