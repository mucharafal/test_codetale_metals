[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "IIRC, in DS v1, `saveAsTable` fails if the table exists, but the table provider is different from the one specified in `df.write.format`. Do we have this check in the v2 code path?",
    "commit": "a70e72676da6442a45e3a358c734b6f161c615d0",
    "createdAt": "2019-08-13T12:28:46Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest}\n+import org.apache.spark.sql.catalog.v2.Identifier\n+import org.apache.spark.sql.catalog.v2.expressions.Transform\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException\n+import org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class DataSourceV2DataFrameSessionCatalogSuite\n+  extends QueryTest\n+  with SharedSQLContext\n+  with BeforeAndAfter {\n+  import testImplicits._\n+\n+  private val v2Format = classOf[InMemoryTableProvider].getName\n+\n+  before {\n+    spark.conf.set(SQLConf.V2_SESSION_CATALOG.key, classOf[TestV2SessionCatalog].getName)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    super.afterEach()\n+    spark.catalog(\"session\").asInstanceOf[TestV2SessionCatalog].clearTables()\n+  }\n+\n+  private def verifyTable(tableName: String, expected: DataFrame): Unit = {\n+    checkAnswer(spark.table(tableName), expected)\n+    checkAnswer(sql(s\"SELECT * FROM $tableName\"), expected)\n+    checkAnswer(sql(s\"TABLE $tableName\"), expected)\n+  }\n+\n+  test(\"saveAsTable and v2 table - table doesn't exist\") {\n+    val t1 = \"tbl\"\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    df.write.format(v2Format).saveAsTable(t1)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"saveAsTable: v2 table - table exists\") {\n+    val t1 = \"tbl\"\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    spark.sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    intercept[TableAlreadyExistsException] {\n+      df.select(\"id\", \"data\").write.format(v2Format).saveAsTable(t1)\n+    }\n+    df.write.format(v2Format).mode(\"append\").saveAsTable(t1)\n+    verifyTable(t1, df)\n+\n+    // Check that appends are by name\n+    df.select('data, 'id).write.format(v2Format).mode(\"append\").saveAsTable(t1)",
    "line": 114
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "I'll add a test",
    "commit": "a70e72676da6442a45e3a358c734b6f161c615d0",
    "createdAt": "2019-08-14T04:14:41Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest}\n+import org.apache.spark.sql.catalog.v2.Identifier\n+import org.apache.spark.sql.catalog.v2.expressions.Transform\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException\n+import org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class DataSourceV2DataFrameSessionCatalogSuite\n+  extends QueryTest\n+  with SharedSQLContext\n+  with BeforeAndAfter {\n+  import testImplicits._\n+\n+  private val v2Format = classOf[InMemoryTableProvider].getName\n+\n+  before {\n+    spark.conf.set(SQLConf.V2_SESSION_CATALOG.key, classOf[TestV2SessionCatalog].getName)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    super.afterEach()\n+    spark.catalog(\"session\").asInstanceOf[TestV2SessionCatalog].clearTables()\n+  }\n+\n+  private def verifyTable(tableName: String, expected: DataFrame): Unit = {\n+    checkAnswer(spark.table(tableName), expected)\n+    checkAnswer(sql(s\"SELECT * FROM $tableName\"), expected)\n+    checkAnswer(sql(s\"TABLE $tableName\"), expected)\n+  }\n+\n+  test(\"saveAsTable and v2 table - table doesn't exist\") {\n+    val t1 = \"tbl\"\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    df.write.format(v2Format).saveAsTable(t1)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"saveAsTable: v2 table - table exists\") {\n+    val t1 = \"tbl\"\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    spark.sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    intercept[TableAlreadyExistsException] {\n+      df.select(\"id\", \"data\").write.format(v2Format).saveAsTable(t1)\n+    }\n+    df.write.format(v2Format).mode(\"append\").saveAsTable(t1)\n+    verifyTable(t1, df)\n+\n+    // Check that appends are by name\n+    df.select('data, 'id).write.format(v2Format).mode(\"append\").saveAsTable(t1)",
    "line": 114
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "Since the provider isn't necessarily exposed by the table API, I'm not sure if such a check is required/possible.",
    "commit": "a70e72676da6442a45e3a358c734b6f161c615d0",
    "createdAt": "2019-08-14T04:41:05Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.JavaConverters._\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest}\n+import org.apache.spark.sql.catalog.v2.Identifier\n+import org.apache.spark.sql.catalog.v2.expressions.Transform\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException\n+import org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class DataSourceV2DataFrameSessionCatalogSuite\n+  extends QueryTest\n+  with SharedSQLContext\n+  with BeforeAndAfter {\n+  import testImplicits._\n+\n+  private val v2Format = classOf[InMemoryTableProvider].getName\n+\n+  before {\n+    spark.conf.set(SQLConf.V2_SESSION_CATALOG.key, classOf[TestV2SessionCatalog].getName)\n+  }\n+\n+  override def afterEach(): Unit = {\n+    super.afterEach()\n+    spark.catalog(\"session\").asInstanceOf[TestV2SessionCatalog].clearTables()\n+  }\n+\n+  private def verifyTable(tableName: String, expected: DataFrame): Unit = {\n+    checkAnswer(spark.table(tableName), expected)\n+    checkAnswer(sql(s\"SELECT * FROM $tableName\"), expected)\n+    checkAnswer(sql(s\"TABLE $tableName\"), expected)\n+  }\n+\n+  test(\"saveAsTable and v2 table - table doesn't exist\") {\n+    val t1 = \"tbl\"\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    df.write.format(v2Format).saveAsTable(t1)\n+    verifyTable(t1, df)\n+  }\n+\n+  test(\"saveAsTable: v2 table - table exists\") {\n+    val t1 = \"tbl\"\n+    val df = Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\")).toDF(\"id\", \"data\")\n+    spark.sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format\")\n+    intercept[TableAlreadyExistsException] {\n+      df.select(\"id\", \"data\").write.format(v2Format).saveAsTable(t1)\n+    }\n+    df.write.format(v2Format).mode(\"append\").saveAsTable(t1)\n+    verifyTable(t1, df)\n+\n+    // Check that appends are by name\n+    df.select('data, 'id).write.format(v2Format).mode(\"append\").saveAsTable(t1)",
    "line": 114
  }],
  "prId": 25402
}]