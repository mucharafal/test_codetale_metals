[{
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "To make the results easy-to-compare, how about benchmarking the 4 cases for each types? e.g.,\r\n```\r\nIntel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\r\nOutput Single Int Column                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------\r\nCSV  ...\r\nJSON  ...\r\nParquet ...\r\nORC ...\r\n```",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-29T03:26:22Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Single Int Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writeIntString(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Int and String Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writePartition(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(p INT, id INT) using $format PARTITIONED BY (p)\")\n+    benchmark.addCase(\"Output Partitions\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as id,\" +\n+        s\" cast(id % 2 as INT) as p from $tempTable\")\n+    }\n+  }\n+\n+  def writeBucket(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 INT) using $format CLUSTERED BY (c2) INTO 2 BUCKETS\")\n+    benchmark.addCase(\"Output Buckets\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as INT) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+    val tableInt = \"tableInt\"\n+    val tableIntString = \"tableIntString\"\n+    val tablePartition = \"tablePartition\"\n+    val tableBucket = \"tableBucket\"\n+    // If the\n+    val formats: Seq[String] = if (args.isEmpty) {\n+      Seq(\"Parquet\", \"ORC\", \"JSON\", \"CSV\")\n+    } else {\n+      args\n+    }\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+    Parquet writer benchmark:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Output Single Int Column                      6054 / 6070          2.6         384.9       1.0X\n+    Output Int and String Column                  5784 / 5800          2.7         367.8       1.0X\n+    Output Partitions                             3891 / 3904          4.0         247.4       1.6X\n+    Output Buckets                                5446 / 5729          2.9         346.2       1.1X\n+"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "In my opinion, the write benchmark can be used to measure the performance of before/after code changes. The code changes can be about certain data source, e.g., Parquet.\r\nIn such case, we can run this benchmark with specified argument to get straight forward results.\r\n\r\nTo me, comparing different data sources over each workload is less meaningful. I know we already did that in read benchmark. \r\nLet's discuss about this and make them consistent.",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-29T04:32:34Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Single Int Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writeIntString(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Int and String Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writePartition(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(p INT, id INT) using $format PARTITIONED BY (p)\")\n+    benchmark.addCase(\"Output Partitions\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as id,\" +\n+        s\" cast(id % 2 as INT) as p from $tempTable\")\n+    }\n+  }\n+\n+  def writeBucket(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 INT) using $format CLUSTERED BY (c2) INTO 2 BUCKETS\")\n+    benchmark.addCase(\"Output Buckets\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as INT) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+    val tableInt = \"tableInt\"\n+    val tableIntString = \"tableIntString\"\n+    val tablePartition = \"tablePartition\"\n+    val tableBucket = \"tableBucket\"\n+    // If the\n+    val formats: Seq[String] = if (args.isEmpty) {\n+      Seq(\"Parquet\", \"ORC\", \"JSON\", \"CSV\")\n+    } else {\n+      args\n+    }\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+    Parquet writer benchmark:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Output Single Int Column                      6054 / 6070          2.6         384.9       1.0X\n+    Output Int and String Column                  5784 / 5800          2.7         367.8       1.0X\n+    Output Partitions                             3891 / 3904          4.0         247.4       1.6X\n+    Output Buckets                                5446 / 5729          2.9         346.2       1.1X\n+"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "+1, I think it's more useful to compare different types of a certain data source, instead of between data sources.",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-29T12:58:27Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Single Int Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writeIntString(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Int and String Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writePartition(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(p INT, id INT) using $format PARTITIONED BY (p)\")\n+    benchmark.addCase(\"Output Partitions\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as id,\" +\n+        s\" cast(id % 2 as INT) as p from $tempTable\")\n+    }\n+  }\n+\n+  def writeBucket(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 INT) using $format CLUSTERED BY (c2) INTO 2 BUCKETS\")\n+    benchmark.addCase(\"Output Buckets\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as INT) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+    val tableInt = \"tableInt\"\n+    val tableIntString = \"tableIntString\"\n+    val tablePartition = \"tablePartition\"\n+    val tableBucket = \"tableBucket\"\n+    // If the\n+    val formats: Seq[String] = if (args.isEmpty) {\n+      Seq(\"Parquet\", \"ORC\", \"JSON\", \"CSV\")\n+    } else {\n+      args\n+    }\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+    Parquet writer benchmark:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Output Single Int Column                      6054 / 6070          2.6         384.9       1.0X\n+    Output Int and String Column                  5784 / 5800          2.7         367.8       1.0X\n+    Output Partitions                             3891 / 3904          4.0         247.4       1.6X\n+    Output Buckets                                5446 / 5729          2.9         346.2       1.1X\n+"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "ok",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-30T01:34:37Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Single Int Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writeIntString(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Int and String Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writePartition(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(p INT, id INT) using $format PARTITIONED BY (p)\")\n+    benchmark.addCase(\"Output Partitions\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as id,\" +\n+        s\" cast(id % 2 as INT) as p from $tempTable\")\n+    }\n+  }\n+\n+  def writeBucket(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 INT) using $format CLUSTERED BY (c2) INTO 2 BUCKETS\")\n+    benchmark.addCase(\"Output Buckets\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as INT) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+    val tableInt = \"tableInt\"\n+    val tableIntString = \"tableIntString\"\n+    val tablePartition = \"tablePartition\"\n+    val tableBucket = \"tableBucket\"\n+    // If the\n+    val formats: Seq[String] = if (args.isEmpty) {\n+      Seq(\"Parquet\", \"ORC\", \"JSON\", \"CSV\")\n+    } else {\n+      args\n+    }\n+    /*\n+    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n+    Parquet writer benchmark:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ------------------------------------------------------------------------------------------------\n+    Output Single Int Column                      6054 / 6070          2.6         384.9       1.0X\n+    Output Int and String Column                  5784 / 5800          2.7         367.8       1.0X\n+    Output Partitions                             3891 / 3904          4.0         247.4       1.6X\n+    Output Buckets                                5446 / 5729          2.9         346.2       1.1X\n+"
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "Here I am not sure if we need to compare all numeric types: ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-29T04:40:23Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think int and double should be good.",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-29T12:54:55Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")"
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: we can use multiline string to format the SQL\r\n```\r\ns\"\"\"\r\n  |INSERT OVERWRITE TABLE $table\r\n  |SELECT ...\r\n\"\"\".stripMargin\r\n```",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-29T12:56:25Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Single Int Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +"
  }],
  "prId": 21409
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "?",
    "commit": "e90fa00e8963eb985bdd30d9a262c61f6ca1ce61",
    "createdAt": "2018-05-29T12:56:41Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure data source write performance.\n+ * By default it measures 4 data source format: Parquet, ORC, JSON, CSV:\n+ *  spark-submit --class <this class> <spark sql test jar>\n+ * To measure specified formats, run it with arguments:\n+ *  spark-submit --class <this class> <spark sql test jar> format1 [format2] [...]\n+ */\n+object DataSourceWriteBenchmark {\n+  val conf = new SparkConf()\n+    .setAppName(\"DataSourceWriteBenchmark\")\n+    .setIfMissing(\"spark.master\", \"local[1]\")\n+    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+    .set(\"spark.sql.orc.compression.codec\", \"snappy\")\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  // Set default configs. Individual cases will change them if necessary.\n+  spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+\n+  val tempTable = \"temp\"\n+  val numRows = 1024 * 1024 * 15\n+\n+  def withTempTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally tableNames.foreach(spark.catalog.dropTempView)\n+  }\n+\n+  def withTable(tableNames: String*)(f: => Unit): Unit = {\n+    try f finally {\n+      tableNames.foreach { name =>\n+        spark.sql(s\"DROP TABLE IF EXISTS $name\")\n+      }\n+    }\n+  }\n+\n+  def writeInt(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Single Int Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writeIntString(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 STRING) using $format\")\n+    benchmark.addCase(\"Output Int and String Column\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as STRING) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def writePartition(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(p INT, id INT) using $format PARTITIONED BY (p)\")\n+    benchmark.addCase(\"Output Partitions\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as id,\" +\n+        s\" cast(id % 2 as INT) as p from $tempTable\")\n+    }\n+  }\n+\n+  def writeBucket(table: String, format: String, benchmark: Benchmark): Unit = {\n+    spark.sql(s\"create table $table(c1 INT, c2 INT) using $format CLUSTERED BY (c2) INTO 2 BUCKETS\")\n+    benchmark.addCase(\"Output Buckets\") { _ =>\n+      spark.sql(s\"INSERT overwrite table $table select cast(id as INT) as \" +\n+        s\"c1, cast(id as INT) as c2 from $tempTable\")\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+    val tableInt = \"tableInt\"\n+    val tableIntString = \"tableIntString\"\n+    val tablePartition = \"tablePartition\"\n+    val tableBucket = \"tableBucket\"\n+    // If the"
  }],
  "prId": 21409
}]