[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "How about making this the EnsureStatefulOpPartitioningSuite?\r\nThis pattern is followed by many other optimization rules (PropagateEmptyRelationSuite, CollapseProjectSuite...)",
    "commit": "4eb7f4f6df3f2d5ae831bf15715651598e52c3e6",
    "createdAt": "2017-09-19T01:14:13Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.util.UUID\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{SparkPlan, SparkPlanTest, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{Exchange, ShuffleExchange}\n+import org.apache.spark.sql.execution.streaming.{IncrementalExecution, OffsetSeqMetadata, StatefulOperator, StatefulOperatorStateInfo}\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class IncrementalExecutionRulesSuite extends SparkPlanTest with SharedSQLContext {"
  }],
  "prId": 19196
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This can be within the above class. Then it wont pollute the general namespace.",
    "commit": "4eb7f4f6df3f2d5ae831bf15715651598e52c3e6",
    "createdAt": "2017-09-19T01:14:45Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.util.UUID\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{SparkPlan, SparkPlanTest, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{Exchange, ShuffleExchange}\n+import org.apache.spark.sql.execution.streaming.{IncrementalExecution, OffsetSeqMetadata, StatefulOperator, StatefulOperatorStateInfo}\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class IncrementalExecutionRulesSuite extends SparkPlanTest with SharedSQLContext {\n+\n+  import testImplicits._\n+  super.beforeAll()\n+\n+  private val baseDf = Seq((1, \"A\"), (2, \"b\")).toDF(\"num\", \"char\")\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution generates Exchange with HashPartitioning\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution with coalesce(1) generates Exchange with HashPartitioning\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples generates Exchange with SinglePartition\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => AllTuples,\n+    keys => SinglePartition,\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples with coalesce(1) doesn't need Exchange\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => AllTuples,\n+    keys => SinglePartition,\n+    expectShuffle = false)\n+\n+  private def testEnsureStatefulOpPartitioning(\n+      testName: String,\n+      inputPlan: SparkPlan,\n+      requiredDistribution: Seq[Attribute] => Distribution,\n+      expectedPartitioning: Seq[Attribute] => Partitioning,\n+      expectShuffle: Boolean): Unit = {\n+    test(\"EnsureStatefulOpPartitioning - \" + testName) {\n+      val operator = TestOperator(inputPlan, requiredDistribution(inputPlan.output.take(1)))\n+      val executed = executePlan(operator, OutputMode.Complete())\n+      if (expectShuffle) {\n+        val exchange = executed.children.find(_.isInstanceOf[Exchange])\n+        if (exchange.isEmpty) {\n+          fail(s\"Was expecting an exchange but didn't get one in:\\n$executed\")\n+        }\n+        assert(exchange.get ===\n+          ShuffleExchange(expectedPartitioning(inputPlan.output.take(1)), inputPlan),\n+          s\"Exchange didn't have expected properties:\\n${exchange.get}\")\n+      } else {\n+        assert(!executed.children.exists(_.isInstanceOf[Exchange]),\n+          s\"Unexpected exchange found in:\\n$executed\")\n+      }\n+    }\n+  }\n+\n+  private def executePlan(\n+      p: SparkPlan,\n+      outputMode: OutputMode = OutputMode.Append()): SparkPlan = {\n+    val execution = new IncrementalExecution(\n+      spark,\n+      null,\n+      OutputMode.Complete(),\n+      \"chk\",\n+      UUID.randomUUID(),\n+      0L,\n+      OffsetSeqMetadata()) {\n+      override lazy val sparkPlan: SparkPlan = p transform {\n+        case plan: SparkPlan =>\n+          val inputMap = plan.children.flatMap(_.output).map(a => (a.name, a)).toMap\n+          plan transformExpressions {\n+            case UnresolvedAttribute(Seq(u)) =>\n+              inputMap.getOrElse(u,\n+                sys.error(s\"Invalid Test: Cannot resolve $u given input $inputMap\"))\n+          }\n+      }\n+    }\n+    execution.executedPlan\n+  }\n+}\n+\n+case class TestOperator("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Name it as TestStatefulOperator to make it more specific than just \"test operator\". And add docs saying what is it used for.",
    "commit": "4eb7f4f6df3f2d5ae831bf15715651598e52c3e6",
    "createdAt": "2017-09-19T01:15:30Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.util.UUID\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{SparkPlan, SparkPlanTest, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{Exchange, ShuffleExchange}\n+import org.apache.spark.sql.execution.streaming.{IncrementalExecution, OffsetSeqMetadata, StatefulOperator, StatefulOperatorStateInfo}\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class IncrementalExecutionRulesSuite extends SparkPlanTest with SharedSQLContext {\n+\n+  import testImplicits._\n+  super.beforeAll()\n+\n+  private val baseDf = Seq((1, \"A\"), (2, \"b\")).toDF(\"num\", \"char\")\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution generates Exchange with HashPartitioning\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution with coalesce(1) generates Exchange with HashPartitioning\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples generates Exchange with SinglePartition\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => AllTuples,\n+    keys => SinglePartition,\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples with coalesce(1) doesn't need Exchange\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => AllTuples,\n+    keys => SinglePartition,\n+    expectShuffle = false)\n+\n+  private def testEnsureStatefulOpPartitioning(\n+      testName: String,\n+      inputPlan: SparkPlan,\n+      requiredDistribution: Seq[Attribute] => Distribution,\n+      expectedPartitioning: Seq[Attribute] => Partitioning,\n+      expectShuffle: Boolean): Unit = {\n+    test(\"EnsureStatefulOpPartitioning - \" + testName) {\n+      val operator = TestOperator(inputPlan, requiredDistribution(inputPlan.output.take(1)))\n+      val executed = executePlan(operator, OutputMode.Complete())\n+      if (expectShuffle) {\n+        val exchange = executed.children.find(_.isInstanceOf[Exchange])\n+        if (exchange.isEmpty) {\n+          fail(s\"Was expecting an exchange but didn't get one in:\\n$executed\")\n+        }\n+        assert(exchange.get ===\n+          ShuffleExchange(expectedPartitioning(inputPlan.output.take(1)), inputPlan),\n+          s\"Exchange didn't have expected properties:\\n${exchange.get}\")\n+      } else {\n+        assert(!executed.children.exists(_.isInstanceOf[Exchange]),\n+          s\"Unexpected exchange found in:\\n$executed\")\n+      }\n+    }\n+  }\n+\n+  private def executePlan(\n+      p: SparkPlan,\n+      outputMode: OutputMode = OutputMode.Append()): SparkPlan = {\n+    val execution = new IncrementalExecution(\n+      spark,\n+      null,\n+      OutputMode.Complete(),\n+      \"chk\",\n+      UUID.randomUUID(),\n+      0L,\n+      OffsetSeqMetadata()) {\n+      override lazy val sparkPlan: SparkPlan = p transform {\n+        case plan: SparkPlan =>\n+          val inputMap = plan.children.flatMap(_.output).map(a => (a.name, a)).toMap\n+          plan transformExpressions {\n+            case UnresolvedAttribute(Seq(u)) =>\n+              inputMap.getOrElse(u,\n+                sys.error(s\"Invalid Test: Cannot resolve $u given input $inputMap\"))\n+          }\n+      }\n+    }\n+    execution.executedPlan\n+  }\n+}\n+\n+case class TestOperator("
  }],
  "prId": 19196
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: add some docs specifying what does it test.",
    "commit": "4eb7f4f6df3f2d5ae831bf15715651598e52c3e6",
    "createdAt": "2017-09-19T01:16:24Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.util.UUID\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{SparkPlan, SparkPlanTest, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{Exchange, ShuffleExchange}\n+import org.apache.spark.sql.execution.streaming.{IncrementalExecution, OffsetSeqMetadata, StatefulOperator, StatefulOperatorStateInfo}\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class IncrementalExecutionRulesSuite extends SparkPlanTest with SharedSQLContext {\n+\n+  import testImplicits._\n+  super.beforeAll()\n+\n+  private val baseDf = Seq((1, \"A\"), (2, \"b\")).toDF(\"num\", \"char\")\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution generates Exchange with HashPartitioning\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution with coalesce(1) generates Exchange with HashPartitioning\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples generates Exchange with SinglePartition\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => AllTuples,\n+    keys => SinglePartition,\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples with coalesce(1) doesn't need Exchange\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => AllTuples,\n+    keys => SinglePartition,\n+    expectShuffle = false)\n+\n+  private def testEnsureStatefulOpPartitioning("
  }],
  "prId": 19196
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: add `requiredDistribution = ` and `expectedPartitioning = ` for greater readability",
    "commit": "4eb7f4f6df3f2d5ae831bf15715651598e52c3e6",
    "createdAt": "2017-09-19T01:17:25Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import java.util.UUID\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.physical._\n+import org.apache.spark.sql.execution.{SparkPlan, SparkPlanTest, UnaryExecNode}\n+import org.apache.spark.sql.execution.exchange.{Exchange, ShuffleExchange}\n+import org.apache.spark.sql.execution.streaming.{IncrementalExecution, OffsetSeqMetadata, StatefulOperator, StatefulOperatorStateInfo}\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class IncrementalExecutionRulesSuite extends SparkPlanTest with SharedSQLContext {\n+\n+  import testImplicits._\n+  super.beforeAll()\n+\n+  private val baseDf = Seq((1, \"A\"), (2, \"b\")).toDF(\"num\", \"char\")\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution generates Exchange with HashPartitioning\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"ClusteredDistribution with coalesce(1) generates Exchange with HashPartitioning\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => ClusteredDistribution(keys),\n+    keys => HashPartitioning(keys, spark.sessionState.conf.numShufflePartitions),\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples generates Exchange with SinglePartition\",\n+    baseDf.queryExecution.sparkPlan,\n+    keys => AllTuples,\n+    keys => SinglePartition,\n+    expectShuffle = true)\n+\n+  testEnsureStatefulOpPartitioning(\n+    \"AllTuples with coalesce(1) doesn't need Exchange\",\n+    baseDf.coalesce(1).queryExecution.sparkPlan,\n+    keys => AllTuples,"
  }],
  "prId": 19196
}]