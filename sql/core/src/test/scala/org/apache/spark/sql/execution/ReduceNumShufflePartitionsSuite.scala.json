[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `fragment` -> `query stage`",
    "commit": "2e087785d754dfabc84b333fffcf98c39d2fd497",
    "createdAt": "2019-03-15T09:15:30Z",
    "diffHunk": "@@ -463,38 +465,111 @@ class ExchangeCoordinatorSuite extends SparkFunSuite with BeforeAndAfterAll {\n \n         // Then, let's look at the number of post-shuffle partitions estimated\n         // by the ExchangeCoordinator.\n-        val exchanges = join.queryExecution.executedPlan.collect {\n-          case e: ShuffleExchangeExec => e\n+        val finalPlan = join.queryExecution.executedPlan\n+          .asInstanceOf[AdaptiveSparkPlanExec].finalPlan\n+        val shuffleReaders = finalPlan.collect {\n+          case reader: CoalescedShuffleReaderExec => reader\n         }\n-        assert(exchanges.length === 3)\n+        assert(shuffleReaders.length === 2)\n         minNumPostShufflePartitions match {\n           case Some(numPartitions) =>\n-            exchanges.foreach {\n-              case e: ShuffleExchangeExec =>\n-                assert(e.coordinator.isDefined)\n-                assert(e.outputPartitioning.numPartitions === 5)\n-              case o =>\n+            shuffleReaders.foreach { reader =>\n+              assert(reader.outputPartitioning.numPartitions === numPartitions)\n             }\n \n           case None =>\n-            assert(exchanges.forall(_.coordinator.isDefined))\n-            assert(exchanges.map(_.outputPartitioning.numPartitions).toSet === Set(5, 3))\n+            shuffleReaders.foreach { reader =>\n+              assert(reader.outputPartitioning.numPartitions === 3)\n+            }\n         }\n       }\n \n-      withSparkSession(test, 6144, minNumPostShufflePartitions)\n+      withSparkSession(test, 12000, minNumPostShufflePartitions)\n+    }\n+\n+    test(s\"determining the number of reducers: plan already partitioned$testNameNote\") {\n+      val test: SparkSession => Unit = { spark: SparkSession =>\n+        try {\n+          spark.range(1000).write.bucketBy(30, \"id\").saveAsTable(\"t\")\n+          // `df1` is hash partitioned by `id`.\n+          val df1 = spark.read.table(\"t\")\n+          val df2 =\n+            spark\n+              .range(0, 1000, 1, numInputPartitions)\n+              .selectExpr(\"id % 500 as key2\", \"id as value2\")\n+\n+          val join = df1.join(df2, col(\"id\") === col(\"key2\")).select(col(\"id\"), col(\"value2\"))\n+\n+          // Check the answer first.\n+          val expectedAnswer = spark.range(0, 500).selectExpr(\"id % 500\", \"id as value\")\n+            .union(spark.range(500, 1000).selectExpr(\"id % 500\", \"id as value\"))\n+          checkAnswer(\n+            join,\n+            expectedAnswer.collect())\n+\n+          // Then, let's make sure we do not reduce number of ppst shuffle partitions.\n+          val finalPlan = join.queryExecution.executedPlan\n+            .asInstanceOf[AdaptiveSparkPlanExec].finalPlan\n+          val shuffleReaders = finalPlan.collect {\n+            case reader: CoalescedShuffleReaderExec => reader\n+          }\n+          assert(shuffleReaders.length === 0)\n+        } finally {\n+          spark.sql(\"drop table t\")\n+        }\n+      }\n+      withSparkSession(test, 12000, minNumPostShufflePartitions)\n     }\n   }\n \n   test(\"SPARK-24705 adaptive query execution works correctly when exchange reuse enabled\") {\n     val test = { spark: SparkSession =>\n       spark.sql(\"SET spark.sql.exchange.reuse=true\")\n       val df = spark.range(1).selectExpr(\"id AS key\", \"id AS value\")\n+\n+      // test case 1: a fragment has 3 child fragments but they are the same fragment."
  }],
  "prId": 20303
}]