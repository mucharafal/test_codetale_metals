[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Looks we can put this inside the test .. and probably the name starting lower case.",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T00:35:40Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\""
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "maybe  `import testImplicits._`",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T00:37:56Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "not a big deal at all but maybe `applyConversion` -> `int96TimestampConversion`?",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T00:39:12Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaFile), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { applyConversion =>"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "seems leading `s` can be removed.",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T00:47:24Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaFile), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { applyConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, applyConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val read = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(read.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (applyConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  TimeZone.getTimeZone(\"UTC\"),\n+                  TimeZone.getDefault()))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map {\n+              _.toString()\n+            }.sorted.toArray\n+            val actual = read.map {\n+              _.getTimestamp(0).toString()\n+            }.sorted\n+            withClue(s\"applyConversion = $applyConversion; vectorized = $vectorized\") {\n+              assert(fullExpectations === actual)\n+\n+              // Now test that the behavior is still correct even with a filter which could get\n+              // pushed down into parquet.  We don't need extra handling for pushed down\n+              // predicates because (a) in ParquetFilters, we ignore TimestampType and (b) parquet\n+              // does not read statistics from int96 fields, as they are unsigned.  See\n+              // scalastyle:off line.size.limit\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L419\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L348\n+              // scalastyle:on line.size.limit\n+              //\n+              // Just to be defensive in case anything ever changes in parquet, this test checks\n+              // the assumption on column stats, and also the end-to-end behavior.\n+\n+              val hadoopConf = sparkContext.hadoopConfiguration\n+              val fs = FileSystem.get(hadoopConf)\n+              val parts = fs.listStatus(new Path(tableDir.getAbsolutePath), new PathFilter {\n+                override def accept(path: Path): Boolean = !path.getName.startsWith(\"_\")\n+              })\n+              // grab the meta data from the parquet file.  The next section of asserts just make\n+              // sure the test is configured correctly.\n+              assert(parts.size == 2)\n+              parts.map { part =>\n+                val oneFooter =\n+                  ParquetFileReader.readFooter(hadoopConf, part.getPath, NO_FILTER)\n+                assert(oneFooter.getFileMetaData.getSchema.getColumns.size === 1)\n+                assert(oneFooter.getFileMetaData.getSchema.getColumns.get(0).getType() ===\n+                  PrimitiveTypeName.INT96)\n+                val oneBlockMeta = oneFooter.getBlocks().get(0)\n+                val oneBlockColumnMeta = oneBlockMeta.getColumns().get(0)\n+                val columnStats = oneBlockColumnMeta.getStatistics\n+                // This is the important assert.  Column stats are written, but they are ignored\n+                // when the data is read back as mentioned above, b/c int96 is unsigned.  This\n+                // assert makes sure this holds even if we change parquet versions (if eg. there\n+                // were ever statistics even on unsigned columns).\n+                assert(columnStats.isEmpty)\n+              }\n+\n+              // These queries should return the entire dataset with the conversion applied,\n+              // but if the predicates were applied to the raw values in parquet, they would\n+              // incorrectly filter data out.\n+              val query = spark.read.parquet(tableDir.getAbsolutePath)\n+                .where(s\"ts > '2001-01-01 01:00:00'\")"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "maybe `read` to .. `readBack` .. ?",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T00:48:22Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaFile), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { applyConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, applyConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val read = spark.read.parquet(tableDir.getAbsolutePath).collect()"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "seems `schema ` is not used.",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T00:48:58Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Alternatively, we could do this as below maybe:\r\n\r\n```scala\r\nval hadoopConf = sparkContext.hadoopConfiguration\r\nval footers = readAllFootersWithoutSummaryFiles(\r\n  new Path(tableDir.getAbsolutePath), hadoopConf)\r\nfooters.map { footer =>\r\n  val metadata = footer.getParquetMetadata\r\n  ...\r\n```",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T01:09:20Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaFile), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { applyConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, applyConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val read = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(read.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (applyConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  TimeZone.getTimeZone(\"UTC\"),\n+                  TimeZone.getDefault()))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map {\n+              _.toString()\n+            }.sorted.toArray\n+            val actual = read.map {\n+              _.getTimestamp(0).toString()\n+            }.sorted\n+            withClue(s\"applyConversion = $applyConversion; vectorized = $vectorized\") {\n+              assert(fullExpectations === actual)\n+\n+              // Now test that the behavior is still correct even with a filter which could get\n+              // pushed down into parquet.  We don't need extra handling for pushed down\n+              // predicates because (a) in ParquetFilters, we ignore TimestampType and (b) parquet\n+              // does not read statistics from int96 fields, as they are unsigned.  See\n+              // scalastyle:off line.size.limit\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L419\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L348\n+              // scalastyle:on line.size.limit\n+              //\n+              // Just to be defensive in case anything ever changes in parquet, this test checks\n+              // the assumption on column stats, and also the end-to-end behavior.\n+\n+              val hadoopConf = sparkContext.hadoopConfiguration\n+              val fs = FileSystem.get(hadoopConf)\n+              val parts = fs.listStatus(new Path(tableDir.getAbsolutePath), new PathFilter {\n+                override def accept(path: Path): Boolean = !path.getName.startsWith(\"_\")\n+              })\n+              // grab the meta data from the parquet file.  The next section of asserts just make\n+              // sure the test is configured correctly.\n+              assert(parts.size == 2)\n+              parts.map { part =>\n+                val oneFooter ="
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I don't have any particular preference, but just for curiosity, is there any reason to prefer that version instead?",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T06:10:40Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaFile), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { applyConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, applyConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val read = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(read.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (applyConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  TimeZone.getTimeZone(\"UTC\"),\n+                  TimeZone.getDefault()))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map {\n+              _.toString()\n+            }.sorted.toArray\n+            val actual = read.map {\n+              _.getTimestamp(0).toString()\n+            }.sorted\n+            withClue(s\"applyConversion = $applyConversion; vectorized = $vectorized\") {\n+              assert(fullExpectations === actual)\n+\n+              // Now test that the behavior is still correct even with a filter which could get\n+              // pushed down into parquet.  We don't need extra handling for pushed down\n+              // predicates because (a) in ParquetFilters, we ignore TimestampType and (b) parquet\n+              // does not read statistics from int96 fields, as they are unsigned.  See\n+              // scalastyle:off line.size.limit\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L419\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L348\n+              // scalastyle:on line.size.limit\n+              //\n+              // Just to be defensive in case anything ever changes in parquet, this test checks\n+              // the assumption on column stats, and also the end-to-end behavior.\n+\n+              val hadoopConf = sparkContext.hadoopConfiguration\n+              val fs = FileSystem.get(hadoopConf)\n+              val parts = fs.listStatus(new Path(tableDir.getAbsolutePath), new PathFilter {\n+                override def accept(path: Path): Boolean = !path.getName.startsWith(\"_\")\n+              })\n+              // grab the meta data from the parquet file.  The next section of asserts just make\n+              // sure the test is configured correctly.\n+              assert(parts.size == 2)\n+              parts.map { part =>\n+                val oneFooter ="
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I just found it shorter. I am fine.",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T09:21:16Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaFile), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { applyConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, applyConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val read = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(read.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (applyConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  TimeZone.getTimeZone(\"UTC\"),\n+                  TimeZone.getDefault()))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map {\n+              _.toString()\n+            }.sorted.toArray\n+            val actual = read.map {\n+              _.getTimestamp(0).toString()\n+            }.sorted\n+            withClue(s\"applyConversion = $applyConversion; vectorized = $vectorized\") {\n+              assert(fullExpectations === actual)\n+\n+              // Now test that the behavior is still correct even with a filter which could get\n+              // pushed down into parquet.  We don't need extra handling for pushed down\n+              // predicates because (a) in ParquetFilters, we ignore TimestampType and (b) parquet\n+              // does not read statistics from int96 fields, as they are unsigned.  See\n+              // scalastyle:off line.size.limit\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L419\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L348\n+              // scalastyle:on line.size.limit\n+              //\n+              // Just to be defensive in case anything ever changes in parquet, this test checks\n+              // the assumption on column stats, and also the end-to-end behavior.\n+\n+              val hadoopConf = sparkContext.hadoopConfiguration\n+              val fs = FileSystem.get(hadoopConf)\n+              val parts = fs.listStatus(new Path(tableDir.getAbsolutePath), new PathFilter {\n+                override def accept(path: Path): Boolean = !path.getName.startsWith(\"_\")\n+              })\n+              // grab the meta data from the parquet file.  The next section of asserts just make\n+              // sure the test is configured correctly.\n+              assert(parts.size == 2)\n+              parts.map { part =>\n+                val oneFooter ="
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "This should be session local timezone?",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T03:48:56Z",
    "diffHunk": "@@ -87,4 +96,113 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  val ImpalaFile = \"test-data/impala_timestamp.parq\"\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+\n+    // here's the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaFile = Thread.currentThread().getContextClassLoader.getResource(ImpalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      val s = spark\n+      import s.implicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      val schema = df.schema\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaFile), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { applyConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, applyConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val read = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(read.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (applyConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  TimeZone.getTimeZone(\"UTC\"),\n+                  TimeZone.getDefault()))"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `.map(java.sql.Timestamp.valueOf)`",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T09:10:34Z",
    "diffHunk": "@@ -87,4 +95,107 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+    val impalaFile = \"test-data/impala_timestamp.parq\"\n+\n+    // here are the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`map` -> `foreach`",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T09:15:18Z",
    "diffHunk": "@@ -87,4 +95,107 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+    val impalaFile = \"test-data/impala_timestamp.parq\"\n+\n+    // here are the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaPath = Thread.currentThread().getContextClassLoader.getResource(impalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      import testImplicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaPath), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { int96TimestampConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, int96TimestampConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val readBack = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(readBack.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (int96TimestampConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  DateTimeUtils.TimeZoneUTC,\n+                  DateTimeUtils.getTimeZone(conf.sessionLocalTimeZone)))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map(_.toString).sorted.toArray\n+            val actual = readBack.map(_.getTimestamp(0).toString).sorted\n+            withClue(s\"applyConversion = $int96TimestampConversion; vectorized = $vectorized\") {\n+              assert(fullExpectations === actual)\n+\n+              // Now test that the behavior is still correct even with a filter which could get\n+              // pushed down into parquet.  We don't need extra handling for pushed down\n+              // predicates because (a) in ParquetFilters, we ignore TimestampType and (b) parquet\n+              // does not read statistics from int96 fields, as they are unsigned.  See\n+              // scalastyle:off line.size.limit\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L419\n+              // https://github.com/apache/parquet-mr/blob/2fd62ee4d524c270764e9b91dca72e5cf1a005b7/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L348\n+              // scalastyle:on line.size.limit\n+              //\n+              // Just to be defensive in case anything ever changes in parquet, this test checks\n+              // the assumption on column stats, and also the end-to-end behavior.\n+\n+              val hadoopConf = sparkContext.hadoopConfiguration\n+              val fs = FileSystem.get(hadoopConf)\n+              val parts = fs.listStatus(new Path(tableDir.getAbsolutePath), new PathFilter {\n+                override def accept(path: Path): Boolean = !path.getName.startsWith(\"_\")\n+              })\n+              // grab the meta data from the parquet file.  The next section of asserts just make\n+              // sure the test is configured correctly.\n+              assert(parts.size == 2)\n+              parts.map { part =>"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "to be future proof, let's explicitly set `PARQUET_OUTPUT_TIMESTAMP_TYPE=INT96`",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-17T09:16:26Z",
    "diffHunk": "@@ -87,4 +95,107 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+    val impalaFile = \"test-data/impala_timestamp.parq\"\n+\n+    // here are the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+    val impalaPath = Thread.currentThread().getContextClassLoader.getResource(impalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      import testImplicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaPath), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { int96TimestampConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, int96TimestampConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())"
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "henryr"
    },
    "body": "how about adding an assertion or comment to ParquetFilters that it would be unsafe to add timestamp support?",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-18T06:22:32Z",
    "diffHunk": "@@ -87,4 +95,109 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+    val impalaFile = \"test-data/impala_timestamp.parq\"\n+\n+    // here are the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map(java.sql.Timestamp.valueOf)\n+    val impalaPath = Thread.currentThread().getContextClassLoader.getResource(impalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      import testImplicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaPath), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { int96TimestampConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key,\n+                SQLConf.ParquetOutputTimestampType.INT96.toString),\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, int96TimestampConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val readBack = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(readBack.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (int96TimestampConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  DateTimeUtils.TimeZoneUTC,\n+                  DateTimeUtils.getTimeZone(conf.sessionLocalTimeZone)))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map(_.toString).sorted.toArray\n+            val actual = readBack.map(_.getTimestamp(0).toString).sorted\n+            withClue(s\"applyConversion = $int96TimestampConversion; vectorized = $vectorized\") {\n+              assert(fullExpectations === actual)\n+\n+              // Now test that the behavior is still correct even with a filter which could get\n+              // pushed down into parquet.  We don't need extra handling for pushed down\n+              // predicates because (a) in ParquetFilters, we ignore TimestampType and (b) parquet",
    "line": 78
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I'd prefer not to, but don't feel very strongly.\r\n\r\nI don't think it makes sense to add an assert in `ParquetFilters`, just because of the way the code is structured -- its based on pattern matching in a partial function.  And I think a comment there is just likely to get out of date / get ignored as there isn't a great place for that.  But like this, the test will fail and there will be an explanation right here explaining why.",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-20T17:21:45Z",
    "diffHunk": "@@ -87,4 +95,109 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+    val impalaFile = \"test-data/impala_timestamp.parq\"\n+\n+    // here are the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map(java.sql.Timestamp.valueOf)\n+    val impalaPath = Thread.currentThread().getContextClassLoader.getResource(impalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      import testImplicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaPath), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { int96TimestampConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key,\n+                SQLConf.ParquetOutputTimestampType.INT96.toString),\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, int96TimestampConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val readBack = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(readBack.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (int96TimestampConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  DateTimeUtils.TimeZoneUTC,\n+                  DateTimeUtils.getTimeZone(conf.sessionLocalTimeZone)))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map(_.toString).sorted.toArray\n+            val actual = readBack.map(_.getTimestamp(0).toString).sorted\n+            withClue(s\"applyConversion = $int96TimestampConversion; vectorized = $vectorized\") {\n+              assert(fullExpectations === actual)\n+\n+              // Now test that the behavior is still correct even with a filter which could get\n+              // pushed down into parquet.  We don't need extra handling for pushed down\n+              // predicates because (a) in ParquetFilters, we ignore TimestampType and (b) parquet",
    "line": 78
  }],
  "prId": 19769
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: use `int96TimestampConversion =` instead of `applyConversion =` too?",
    "commit": "1ea75c0a8f2c5fed33b2a6d6102ad1d8bdf73906",
    "createdAt": "2017-11-20T06:18:34Z",
    "diffHunk": "@@ -87,4 +95,109 @@ class ParquetInteroperabilitySuite extends ParquetCompatibilityTest with SharedS\n           Row(Seq(2, 3))))\n     }\n   }\n+\n+  test(\"parquet timestamp conversion\") {\n+    // Make a table with one parquet file written by impala, and one parquet file written by spark.\n+    // We should only adjust the timestamps in the impala file, and only if the conf is set\n+    val impalaFile = \"test-data/impala_timestamp.parq\"\n+\n+    // here are the timestamps in the impala file, as they were saved by impala\n+    val impalaFileData =\n+      Seq(\n+        \"2001-01-01 01:01:01\",\n+        \"2002-02-02 02:02:02\",\n+        \"2003-03-03 03:03:03\"\n+      ).map(java.sql.Timestamp.valueOf)\n+    val impalaPath = Thread.currentThread().getContextClassLoader.getResource(impalaFile)\n+      .toURI.getPath\n+    withTempPath { tableDir =>\n+      val ts = Seq(\n+        \"2004-04-04 04:04:04\",\n+        \"2005-05-05 05:05:05\",\n+        \"2006-06-06 06:06:06\"\n+      ).map { s => java.sql.Timestamp.valueOf(s) }\n+      import testImplicits._\n+      // match the column names of the file from impala\n+      val df = spark.createDataset(ts).toDF().repartition(1).withColumnRenamed(\"value\", \"ts\")\n+      df.write.parquet(tableDir.getAbsolutePath)\n+      FileUtils.copyFile(new File(impalaPath), new File(tableDir, \"part-00001.parq\"))\n+\n+      Seq(false, true).foreach { int96TimestampConversion =>\n+        Seq(false, true).foreach { vectorized =>\n+          withSQLConf(\n+              (SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key,\n+                SQLConf.ParquetOutputTimestampType.INT96.toString),\n+              (SQLConf.PARQUET_INT96_TIMESTAMP_CONVERSION.key, int96TimestampConversion.toString()),\n+              (SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, vectorized.toString())\n+          ) {\n+            val readBack = spark.read.parquet(tableDir.getAbsolutePath).collect()\n+            assert(readBack.size === 6)\n+            // if we apply the conversion, we'll get the \"right\" values, as saved by impala in the\n+            // original file.  Otherwise, they're off by the local timezone offset, set to\n+            // America/Los_Angeles in tests\n+            val impalaExpectations = if (int96TimestampConversion) {\n+              impalaFileData\n+            } else {\n+              impalaFileData.map { ts =>\n+                DateTimeUtils.toJavaTimestamp(DateTimeUtils.convertTz(\n+                  DateTimeUtils.fromJavaTimestamp(ts),\n+                  DateTimeUtils.TimeZoneUTC,\n+                  DateTimeUtils.getTimeZone(conf.sessionLocalTimeZone)))\n+              }\n+            }\n+            val fullExpectations = (ts ++ impalaExpectations).map(_.toString).sorted.toArray\n+            val actual = readBack.map(_.getTimestamp(0).toString).sorted\n+            withClue(s\"applyConversion = $int96TimestampConversion; vectorized = $vectorized\") {"
  }],
  "prId": 19769
}]