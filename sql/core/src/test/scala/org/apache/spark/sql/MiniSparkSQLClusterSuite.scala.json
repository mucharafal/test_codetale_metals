[{
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Ideally, we don't necessary to create a special unit test for the bug fixing, however, there are some other issues, which probably requires re-creating the SparkContext with different SparkConf.\nFor example: https://issues.apache.org/jira/browse/SPARK-10474\n",
    "commit": "e8b27b5515251c856b7711c0c253e3b92e354fab",
    "createdAt": "2015-09-09T02:25:34Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.{SparkFunSuite, SparkContext, SparkConf}\n+\n+class MiniSparkSQLClusterSuite extends SparkFunSuite {"
  }],
  "prId": 8635
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "This test is rather brittle. Can you write a more fine-grained test for `ExternalSorter`, one that asserts `numSpills > 0` to make sure it's actually covering the merge code path? I would add it in `ExternalSorterSuite` itself instead of creating a whole new file that doesn't add much value.\n",
    "commit": "e8b27b5515251c856b7711c0c253e3b92e354fab",
    "createdAt": "2015-09-09T02:29:55Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.{SparkFunSuite, SparkContext, SparkConf}\n+\n+class MiniSparkSQLClusterSuite extends SparkFunSuite {\n+  /**\n+   * Create a spark context with specified configuration.\n+   */\n+  protected def withSparkConf(sparkConfs: (String, String)*)(f: (SQLContext) => Unit): Unit = {\n+    val (keys, values) = sparkConfs.unzip\n+    val conf = new SparkConf()\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"testing\")\n+\n+    (keys, values).zipped.foreach(conf.set)\n+    val sc = new SparkContext(conf)\n+\n+    try f(createSQLContext(sc)) finally {\n+      sc.stop()\n+    }\n+  }\n+\n+  /**\n+   * Sets all SQL configurations specified in `pairs`, and then calls `f`\n+   */\n+  protected def withSQLConf(pairs: (String, String)*)\n+     (f: SQLContext => Unit)\n+     (sqlContext: SQLContext): Unit = {\n+    val (keys, values) = pairs.unzip\n+\n+    (keys, values).zipped.foreach(sqlContext.conf.setConfString)\n+\n+    f(sqlContext)\n+  }\n+\n+  protected def createSQLContext(sc: SparkContext): SQLContext = {\n+    new SQLContext(sc)\n+  }\n+\n+  test(\"SPARK-10466 mapside external sorting for UnsafeRow\") {\n+    withSparkConf(\n+      (\"spark.shuffle.sort.bypassMergeThreshold\", \"1\"),\n+      (\"spark.shuffle.memoryFraction\", \"0.005\")) {\n+      withSQLConf(SQLConf.SHUFFLE_PARTITIONS.key -> \"2\") { sqlContext =>\n+        import sqlContext.implicits._\n+        sqlContext.sparkContext.parallelize(1 to 10000000, 5) // hopefully big enough for data spill"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "also, if your test is finer-grained then you won't need to set the SQL shuffle partitions anymore, since your test will explicitly control whether to bypass the threshold or not.\n",
    "commit": "e8b27b5515251c856b7711c0c253e3b92e354fab",
    "createdAt": "2015-09-09T02:31:47Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.{SparkFunSuite, SparkContext, SparkConf}\n+\n+class MiniSparkSQLClusterSuite extends SparkFunSuite {\n+  /**\n+   * Create a spark context with specified configuration.\n+   */\n+  protected def withSparkConf(sparkConfs: (String, String)*)(f: (SQLContext) => Unit): Unit = {\n+    val (keys, values) = sparkConfs.unzip\n+    val conf = new SparkConf()\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"testing\")\n+\n+    (keys, values).zipped.foreach(conf.set)\n+    val sc = new SparkContext(conf)\n+\n+    try f(createSQLContext(sc)) finally {\n+      sc.stop()\n+    }\n+  }\n+\n+  /**\n+   * Sets all SQL configurations specified in `pairs`, and then calls `f`\n+   */\n+  protected def withSQLConf(pairs: (String, String)*)\n+     (f: SQLContext => Unit)\n+     (sqlContext: SQLContext): Unit = {\n+    val (keys, values) = pairs.unzip\n+\n+    (keys, values).zipped.foreach(sqlContext.conf.setConfString)\n+\n+    f(sqlContext)\n+  }\n+\n+  protected def createSQLContext(sc: SparkContext): SQLContext = {\n+    new SQLContext(sc)\n+  }\n+\n+  test(\"SPARK-10466 mapside external sorting for UnsafeRow\") {\n+    withSparkConf(\n+      (\"spark.shuffle.sort.bypassMergeThreshold\", \"1\"),\n+      (\"spark.shuffle.memoryFraction\", \"0.005\")) {\n+      withSQLConf(SQLConf.SHUFFLE_PARTITIONS.key -> \"2\") { sqlContext =>\n+        import sqlContext.implicits._\n+        sqlContext.sparkContext.parallelize(1 to 10000000, 5) // hopefully big enough for data spill"
  }],
  "prId": 8635
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Exception will be thrown here if we didn't change the `UnsafeRowSerializer` as above.\n",
    "commit": "e8b27b5515251c856b7711c0c253e3b92e354fab",
    "createdAt": "2015-09-09T11:31:03Z",
    "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import org.apache.spark.executor.{ShuffleWriteMetrics, TaskMetrics}\n+import org.apache.spark.serializer.SerializerInstance\n+import org.apache.spark.sql.catalyst.{InternalRow, CatalystTypeConverters}\n+import org.apache.spark.sql.catalyst.expressions.{UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.execution.UnsafeRowSerializer\n+import org.apache.spark.sql.types.{IntegerType, DataType}\n+import org.apache.spark.storage._\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+import org.apache.spark.util.collection.ExternalSorter\n+import org.mockito.Answers.RETURNS_SMART_NULLS\n+import org.mockito.Matchers._\n+import org.mockito.Mockito._\n+import org.mockito.invocation.InvocationOnMock\n+import org.mockito.stubbing.Answer\n+import org.mockito.{MockitoAnnotations, Mock}\n+import org.scalatest.BeforeAndAfterEach\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+\n+class MiniSparkSQLClusterSuite extends SparkFunSuite with BeforeAndAfterEach {\n+\n+  @Mock(answer = RETURNS_SMART_NULLS) private var blockManager: BlockManager = _\n+  @Mock(answer = RETURNS_SMART_NULLS) private var diskBlockManager: DiskBlockManager = _\n+  @Mock(answer = RETURNS_SMART_NULLS) private var taskContext: TaskContext = _\n+\n+  private var taskMetrics: TaskMetrics = _\n+  private var shuffleWriteMetrics: ShuffleWriteMetrics = _\n+  private var tempDir: File = _\n+  private var outputFile: File = _\n+  private val temporaryFilesCreated: mutable.Buffer[File] = new ArrayBuffer[File]()\n+  private val blockIdToFileMap: mutable.Map[BlockId, File] = new mutable.HashMap[BlockId, File]\n+  private val shuffleBlockId: ShuffleBlockId = new ShuffleBlockId(0, 0, 0)\n+\n+  override def beforeEach(): Unit = {\n+    tempDir = Utils.createTempDir()\n+    outputFile = File.createTempFile(\"shuffle\", null, tempDir)\n+    shuffleWriteMetrics = new ShuffleWriteMetrics\n+    taskMetrics = new TaskMetrics\n+    taskMetrics.shuffleWriteMetrics = Some(shuffleWriteMetrics)\n+    MockitoAnnotations.initMocks(this)\n+    when(taskContext.taskMetrics()).thenReturn(taskMetrics)\n+    import InternalAccumulator._\n+    when(taskContext.internalMetricsToAccumulators).thenReturn(\n+      Map(\n+        PEAK_EXECUTION_MEMORY ->\n+        new Accumulator(\n+          0L, AccumulatorParam.LongAccumulatorParam, Some(PEAK_EXECUTION_MEMORY), internal = true)))\n+    when(blockManager.diskBlockManager).thenReturn(diskBlockManager)\n+    when(blockManager.getDiskWriter(\n+      any[BlockId],\n+      any[File],\n+      any[SerializerInstance],\n+      anyInt(),\n+      any[ShuffleWriteMetrics]\n+    )).thenAnswer(new Answer[DiskBlockObjectWriter] {\n+      override def answer(invocation: InvocationOnMock): DiskBlockObjectWriter = {\n+        val args = invocation.getArguments\n+        new DiskBlockObjectWriter(\n+          args(0).asInstanceOf[BlockId],\n+          args(1).asInstanceOf[File],\n+          args(2).asInstanceOf[SerializerInstance],\n+          args(3).asInstanceOf[Int],\n+          compressStream = identity,\n+          syncWrites = false,\n+          args(4).asInstanceOf[ShuffleWriteMetrics]\n+        )\n+      }\n+    })\n+    when(diskBlockManager.createTempShuffleBlock()).thenAnswer(\n+      new Answer[(TempShuffleBlockId, File)] {\n+        override def answer(invocation: InvocationOnMock): (TempShuffleBlockId, File) = {\n+          val blockId = new TempShuffleBlockId(UUID.randomUUID)\n+          val file = File.createTempFile(blockId.toString, null, tempDir)\n+          blockIdToFileMap.put(blockId, file)\n+          temporaryFilesCreated.append(file)\n+          (blockId, file)\n+        }\n+      })\n+    when(diskBlockManager.getFile(any[BlockId])).thenAnswer(\n+      new Answer[File] {\n+        override def answer(invocation: InvocationOnMock): File = {\n+          blockIdToFileMap.get(invocation.getArguments.head.asInstanceOf[BlockId]).get\n+        }\n+      })\n+  }\n+\n+  override def afterEach(): Unit = {\n+    Utils.deleteRecursively(tempDir)\n+    blockIdToFileMap.clear()\n+    temporaryFilesCreated.clear()\n+  }\n+\n+  private def toUnsafeRow(row: Row, schema: Array[DataType]): UnsafeRow = {\n+    val internalRow = CatalystTypeConverters.convertToCatalyst(row).asInstanceOf[InternalRow]\n+    val converter = UnsafeProjection.create(schema)\n+    converter.apply(internalRow)\n+  }\n+\n+  /**\n+   * Create a spark context with specified configuration, and the calls `f`\n+   */\n+  protected def withSparkConf(sparkConfs: (String, String)*)(f: (SparkContext) => Unit): Unit = {\n+    val (keys, values) = sparkConfs.unzip\n+    val conf = new SparkConf(true)\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"testing\")\n+\n+    var sc: SparkContext = null\n+    val env = SparkEnv.get\n+    Utils.tryWithSafeFinally {\n+      (keys, values).zipped.foreach(conf.set)\n+      sc = new SparkContext(conf)\n+      f(sc)\n+    } {\n+      sc.stop()\n+      SparkEnv.set(env)\n+    }\n+  }\n+\n+  test(\"SPARK-10466 external sorting for UnsafeRow with spilling\") {\n+    withSparkConf(\n+      (\"spark.shuffle.sort.bypassMergeThreshold\", \"0\"),\n+      (\"spark.shuffle.spill.initialMemoryThreshold\", \"1024\"), // 1k\n+      (\"spark.shuffle.memoryFraction\", \"0.0001\")) { sc =>\n+\n+      // create unsafe rows\n+      val count = 10000\n+      val unsafeRowIt = (1 to count).iterator.map { i =>\n+        (i, toUnsafeRow(Row(i, i, i), Array(IntegerType, IntegerType, IntegerType)))\n+      }\n+\n+      val serializer = new UnsafeRowSerializer(3)\n+      val sorter = new ExternalSorter[Int, UnsafeRow, UnsafeRow](\n+        None, None, Some(implicitly[Ordering[Int]]), Some(serializer))\n+      sorter.insertAll(unsafeRowIt)\n+      // Make sure it spilled\n+      assert(sc.env.blockManager.diskBlockManager.getAllFiles().length > 0)\n+\n+      assert(sorter.writePartitionedFile(shuffleBlockId, taskContext, outputFile).sum > 0)"
  }],
  "prId": 8635
}]