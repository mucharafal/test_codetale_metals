[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "All benchmarks share the same structure for now, but it will vary according to the upcoming PRs (like additional SQL statement and configurations).",
    "commit": "5cf507d7b742cd79959977fdd9fdc4cfe69c02ca",
    "createdAt": "2019-02-21T19:10:20Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.internal.SQLConf\n+\n+/**\n+ * Synthetic benchmark for nested schema pruning performance.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <sql core test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result:\n+ *      SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/NestedSchemaPruningBenchmark-results.txt\".\n+ * }}}\n+ */\n+object NestedSchemaPruningBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  private val N = 1000000\n+  private val numIters = 10\n+\n+  // We use `col1 BIGINT, col2 STRUCT<_1: BIGINT, _2: STRING>` as a test schema.\n+  // col1 and col2._1 is used for comparision. col2._2 mimics the burden for the other columns\n+  private val df = spark\n+    .range(N * 10)\n+    .sample(false, 0.1)\n+    .map(x => (x, (x, s\"$x\" * 100)))\n+    .toDF(\"col1\", \"col2\")\n+\n+  private def addCase(benchmark: Benchmark, name: String, sql: String): Unit = {\n+    benchmark.addCase(name) { _ =>\n+      spark.sql(sql).write.format(\"noop\").save()\n+    }\n+  }\n+\n+  private def selectBenchmark(numRows: Int, numIters: Int): Unit = {\n+    withTempPath { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      Seq(1, 2).foreach { i =>\n+        df.write.parquet(path + s\"/$i\")\n+        spark.read.parquet(path + s\"/$i\").createOrReplaceTempView(s\"t$i\")\n+      }\n+\n+      val benchmark = new Benchmark(s\"Selection\", numRows, numIters, output = output)\n+\n+      addCase(benchmark, \"Top-level column\", \"SELECT col1 FROM (SELECT col1 FROM t1)\")\n+      addCase(benchmark, \"Nested column\", \"SELECT col2._1 FROM (SELECT col2 FROM t2)\")\n+\n+      benchmark.run()\n+    }\n+  }\n+\n+  private def limitBenchmark(numRows: Int, numIters: Int): Unit = {\n+    withTempPath { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      Seq(1, 2).foreach { i =>\n+        df.write.parquet(path + s\"/$i\")\n+        spark.read.parquet(path + s\"/$i\").createOrReplaceTempView(s\"t$i\")\n+      }\n+\n+      val benchmark = new Benchmark(s\"Limiting\", numRows, numIters, output = output)\n+\n+      addCase(benchmark, \"Top-level column\",\n+        s\"SELECT col1 FROM (SELECT col1 FROM t1 LIMIT ${Int.MaxValue})\")\n+      addCase(benchmark, \"Nested column\",\n+        s\"SELECT col2._1 FROM (SELECT col2 FROM t2 LIMIT ${Int.MaxValue})\")\n+\n+      benchmark.run()\n+    }\n+  }\n+\n+  private def repartitionBenchmark(numRows: Int, numIters: Int): Unit = {\n+    withTempPath { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      Seq(1, 2).foreach { i =>\n+        df.write.parquet(path + s\"/$i\")\n+        spark.read.parquet(path + s\"/$i\").createOrReplaceTempView(s\"t$i\")\n+      }\n+\n+      val benchmark = new Benchmark(s\"Repartitioning\", numRows, numIters, output = output)\n+\n+      addCase(benchmark, \"Top-level column\",\n+        s\"SELECT col1 FROM (SELECT /*+ REPARTITION(1) */ col1 FROM t1)\")\n+      addCase(benchmark, \"Nested column\",\n+        s\"SELECT col2._1 FROM (SELECT /*+ REPARTITION(1) */ col2 FROM t2)\")\n+\n+      benchmark.run()\n+    }\n+  }\n+\n+  private def repartitionByExprBenchmark(numRows: Int, numIters: Int): Unit = {\n+    withTempPath { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      Seq(1, 2).foreach { i =>\n+        df.write.parquet(path + s\"/$i\")\n+        spark.read.parquet(path + s\"/$i\").createOrReplaceTempView(s\"t$i\")\n+      }\n+\n+      val benchmark = new Benchmark(s\"Repartitioning by exprs\", numRows, numIters, output = output)\n+\n+      addCase(benchmark, \"Top-level column\",\n+        s\"SELECT col1 FROM (SELECT col1 FROM t1 DISTRIBUTE BY col1)\")\n+      addCase(benchmark, \"Nested column\",\n+        s\"SELECT col2._1 FROM (SELECT col2 FROM t2 DISTRIBUTE BY col2._1)\")\n+\n+      benchmark.run()\n+    }\n+  }\n+\n+  private def sortBenchmark(numRows: Int, numIters: Int): Unit = {\n+    withTempPath { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      Seq(1, 2).foreach { i =>\n+        df.write.parquet(path + s\"/$i\")\n+        spark.read.parquet(path + s\"/$i\").createOrReplaceTempView(s\"t$i\")\n+      }\n+\n+      val benchmark = new Benchmark(s\"Sorting\", numRows, numIters, output = output)\n+\n+      addCase(benchmark, \"Top-level column\", \"SELECT col1 FROM t1 ORDER BY col1\")\n+      addCase(benchmark, \"Nested column\", \"SELECT col2._1 FROM t2 ORDER BY col2._1\")\n+\n+      benchmark.run()\n+    }\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    runBenchmark(s\"Nested Schema Pruning Benchmark\") {\n+      withSQLConf (SQLConf.NESTED_SCHEMA_PRUNING_ENABLED.key -> \"true\") {\n+        selectBenchmark (N, numIters)\n+        limitBenchmark (N, numIters)\n+        repartitionBenchmark (N, numIters)\n+        repartitionByExprBenchmark (N, numIters)\n+        sortBenchmark (N, numIters)",
    "line": 159
  }],
  "prId": 23862
}]