[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "createTempDir\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-16T17:19:49Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import scala.util.Random\n+\n+import org.scalatest.{BeforeAndAfter, BeforeAndAfterAll}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.LocalSparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.sql.catalyst.util.quietly\n+import org.apache.spark.util.Utils\n+\n+class StateStoreRDDSuite extends SparkFunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+\n+  private val conf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getCanonicalName)\n+  private var tempDir = Files.createTempDirectory(\"StateStoreRDDSuite\").toString\n+\n+  import StateStoreCoordinatorSuite._\n+  import StateStoreSuite._\n+\n+  after {\n+    StateStore.stop()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    Utils.deleteRecursively(new File(tempDir))\n+  }\n+\n+  test(\"versioning and immutability\") {\n+    quietly {\n+      withSpark(new SparkContext(conf)) { sc =>\n+        val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "The root `tempDir` is cleaned up after all tests. See comments below to understand the reason behind this design.\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-21T21:16:31Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import scala.util.Random\n+\n+import org.scalatest.{BeforeAndAfter, BeforeAndAfterAll}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.LocalSparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.sql.catalyst.util.quietly\n+import org.apache.spark.util.Utils\n+\n+class StateStoreRDDSuite extends SparkFunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+\n+  private val conf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getCanonicalName)\n+  private var tempDir = Files.createTempDirectory(\"StateStoreRDDSuite\").toString\n+\n+  import StateStoreCoordinatorSuite._\n+  import StateStoreSuite._\n+\n+  after {\n+    StateStore.stop()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    Utils.deleteRecursively(new File(tempDir))\n+  }\n+\n+  test(\"versioning and immutability\") {\n+    quietly {\n+      withSpark(new SparkContext(conf)) { sc =>\n+        val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString"
  }],
  "prId": 11645
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "createTempDir\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-16T17:19:55Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import scala.util.Random\n+\n+import org.scalatest.{BeforeAndAfter, BeforeAndAfterAll}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.LocalSparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.sql.catalyst.util.quietly\n+import org.apache.spark.util.Utils\n+\n+class StateStoreRDDSuite extends SparkFunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+\n+  private val conf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getCanonicalName)\n+  private var tempDir = Files.createTempDirectory(\"StateStoreRDDSuite\").toString\n+\n+  import StateStoreCoordinatorSuite._\n+  import StateStoreSuite._\n+\n+  after {\n+    StateStore.stop()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    Utils.deleteRecursively(new File(tempDir))\n+  }\n+\n+  test(\"versioning and immutability\") {\n+    quietly {\n+      withSpark(new SparkContext(conf)) { sc =>\n+        val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString\n+        val increment = (store: StateStore, iter: Iterator[String]) => {\n+          iter.foreach { s =>\n+            store.update(\n+              wrapKey(s), oldRow => {\n+                val oldValue = oldRow.map(unwrapValue).getOrElse(0)\n+                wrapValue(oldValue + 1)\n+              })\n+          }\n+          store.commit()\n+          store.iterator().map(unwrapKeyValue)\n+        }\n+        val opId = 0\n+        val rdd1 = makeRDD(sc, Seq(\"a\", \"b\", \"a\"))\n+          .withStateStores(increment, opId, storeVersion = 0, path)\n+        assert(rdd1.collect().toSet === Set(\"a\" -> 2, \"b\" -> 1))\n+\n+        // Generate next version of stores\n+        val rdd2 = makeRDD(sc, Seq(\"a\", \"c\"))\n+          .withStateStores(increment, opId, storeVersion = 1, path)\n+        assert(rdd2.collect().toSet === Set(\"a\" -> 3, \"b\" -> 1, \"c\" -> 1))\n+\n+        // Make sure the previous RDD still has the same data.\n+        assert(rdd1.collect().toSet === Set(\"a\" -> 2, \"b\" -> 1))\n+      }\n+    }\n+  }\n+\n+  test(\"recovering from files\") {\n+    quietly {\n+      val opId = 0\n+      val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString\n+\n+      def makeStoreRDD(\n+          sc: SparkContext,\n+          seq: Seq[String],\n+          storeVersion: Int): RDD[(String, Int)] = {\n+        makeRDD(sc, Seq(\"a\")).withStateStores(increment, opId, storeVersion, path)\n+      }\n+\n+      // Generate RDDs and state store data\n+      withSpark(new SparkContext(conf)) { sc =>\n+        for (i <- 1 to 20) {\n+          require(makeStoreRDD(sc, Seq(\"a\"), i - 1).collect().toSet === Set(\"a\" -> i))\n+        }\n+      }\n+\n+      // With a new context, try using the earlier state store data\n+      withSpark(new SparkContext(conf)) { sc =>\n+        assert(makeStoreRDD(sc, Seq(\"a\"), 20).collect().toSet === Set(\"a\" -> 21))\n+      }\n+    }\n+  }\n+\n+  test(\"preferred locations using StateStoreCoordinator\") {\n+    val opId = 0\n+    val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString"
  }],
  "prId": 11645
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "createTempDir\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-16T17:19:59Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import scala.util.Random\n+\n+import org.scalatest.{BeforeAndAfter, BeforeAndAfterAll}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.LocalSparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.sql.catalyst.util.quietly\n+import org.apache.spark.util.Utils\n+\n+class StateStoreRDDSuite extends SparkFunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+\n+  private val conf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getCanonicalName)\n+  private var tempDir = Files.createTempDirectory(\"StateStoreRDDSuite\").toString\n+\n+  import StateStoreCoordinatorSuite._\n+  import StateStoreSuite._\n+\n+  after {\n+    StateStore.stop()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    Utils.deleteRecursively(new File(tempDir))\n+  }\n+\n+  test(\"versioning and immutability\") {\n+    quietly {\n+      withSpark(new SparkContext(conf)) { sc =>\n+        val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString\n+        val increment = (store: StateStore, iter: Iterator[String]) => {\n+          iter.foreach { s =>\n+            store.update(\n+              wrapKey(s), oldRow => {\n+                val oldValue = oldRow.map(unwrapValue).getOrElse(0)\n+                wrapValue(oldValue + 1)\n+              })\n+          }\n+          store.commit()\n+          store.iterator().map(unwrapKeyValue)\n+        }\n+        val opId = 0\n+        val rdd1 = makeRDD(sc, Seq(\"a\", \"b\", \"a\"))\n+          .withStateStores(increment, opId, storeVersion = 0, path)\n+        assert(rdd1.collect().toSet === Set(\"a\" -> 2, \"b\" -> 1))\n+\n+        // Generate next version of stores\n+        val rdd2 = makeRDD(sc, Seq(\"a\", \"c\"))\n+          .withStateStores(increment, opId, storeVersion = 1, path)\n+        assert(rdd2.collect().toSet === Set(\"a\" -> 3, \"b\" -> 1, \"c\" -> 1))\n+\n+        // Make sure the previous RDD still has the same data.\n+        assert(rdd1.collect().toSet === Set(\"a\" -> 2, \"b\" -> 1))\n+      }\n+    }\n+  }\n+\n+  test(\"recovering from files\") {\n+    quietly {\n+      val opId = 0\n+      val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString",
    "line": 89
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Dirs created with `Utils.createTempDir` gets deleted when SparkContext is shutdown, so cannot be used for this test. Rather the root `tempDir` gets deleted after all the tests in `afterAll()`\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-16T18:16:05Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming.state\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import scala.util.Random\n+\n+import org.scalatest.{BeforeAndAfter, BeforeAndAfterAll}\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.LocalSparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.sql.catalyst.util.quietly\n+import org.apache.spark.util.Utils\n+\n+class StateStoreRDDSuite extends SparkFunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+\n+  private val conf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getCanonicalName)\n+  private var tempDir = Files.createTempDirectory(\"StateStoreRDDSuite\").toString\n+\n+  import StateStoreCoordinatorSuite._\n+  import StateStoreSuite._\n+\n+  after {\n+    StateStore.stop()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    Utils.deleteRecursively(new File(tempDir))\n+  }\n+\n+  test(\"versioning and immutability\") {\n+    quietly {\n+      withSpark(new SparkContext(conf)) { sc =>\n+        val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString\n+        val increment = (store: StateStore, iter: Iterator[String]) => {\n+          iter.foreach { s =>\n+            store.update(\n+              wrapKey(s), oldRow => {\n+                val oldValue = oldRow.map(unwrapValue).getOrElse(0)\n+                wrapValue(oldValue + 1)\n+              })\n+          }\n+          store.commit()\n+          store.iterator().map(unwrapKeyValue)\n+        }\n+        val opId = 0\n+        val rdd1 = makeRDD(sc, Seq(\"a\", \"b\", \"a\"))\n+          .withStateStores(increment, opId, storeVersion = 0, path)\n+        assert(rdd1.collect().toSet === Set(\"a\" -> 2, \"b\" -> 1))\n+\n+        // Generate next version of stores\n+        val rdd2 = makeRDD(sc, Seq(\"a\", \"c\"))\n+          .withStateStores(increment, opId, storeVersion = 1, path)\n+        assert(rdd2.collect().toSet === Set(\"a\" -> 3, \"b\" -> 1, \"c\" -> 1))\n+\n+        // Make sure the previous RDD still has the same data.\n+        assert(rdd1.collect().toSet === Set(\"a\" -> 2, \"b\" -> 1))\n+      }\n+    }\n+  }\n+\n+  test(\"recovering from files\") {\n+    quietly {\n+      val opId = 0\n+      val path = Utils.createDirectory(tempDir, Random.nextString(10)).toString",
    "line": 89
  }],
  "prId": 11645
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: should clone `sparkConf`\n",
    "commit": "70cc7b11c0d502d4a741ccdcab5e7aa006b5b311",
    "createdAt": "2016-03-23T00:43:54Z",
    "diffHunk": "@@ -142,6 +142,38 @@ class StateStoreRDDSuite extends SparkFunSuite with BeforeAndAfter with BeforeAn\n     }\n   }\n \n+  test(\"distributed test\") {\n+    quietly {\n+      withSpark(new SparkContext(sparkConf.setMaster(\"local-cluster[2, 1, 1024]\"))) { sc =>"
  }],
  "prId": 11645
}]