[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can do something like\r\n```\r\ndt match {\r\n case BooleanType => reader.getBoolean(rowid)\r\n case IntegerType => ...\r\n ...\r\n}\r\n```\r\nThen the caller side doesn't need to pass in a `get`",
    "commit": "b85dc231d05f5e1a1a3d8b0bcbc778b85d83c533",
    "createdAt": "2017-07-26T04:38:45Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.arrow\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.execution.vectorized.ArrowColumnVector\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ArrowWriterSuite extends SparkFunSuite {\n+\n+  test(\"simple\") {\n+    def check(dt: DataType, data: Seq[Any], get: (ArrowColumnVector, Int) => Any): Unit = {\n+      val schema = new StructType().add(\"value\", dt, nullable = true)\n+      val writer = ArrowWriter.create(schema)\n+      assert(writer.schema === schema)\n+\n+      data.foreach { datum =>\n+        writer.write(InternalRow(datum))\n+      }\n+      writer.finish()\n+\n+      val reader = new ArrowColumnVector(writer.root.getFieldVectors().get(0))\n+      data.zipWithIndex.foreach {\n+        case (null, rowId) => assert(reader.isNullAt(rowId))\n+        case (datum, rowId) => assert(get(reader, rowId) === datum)"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "Thanks, I'll update it.",
    "commit": "b85dc231d05f5e1a1a3d8b0bcbc778b85d83c533",
    "createdAt": "2017-07-26T05:51:59Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.arrow\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.execution.vectorized.ArrowColumnVector\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ArrowWriterSuite extends SparkFunSuite {\n+\n+  test(\"simple\") {\n+    def check(dt: DataType, data: Seq[Any], get: (ArrowColumnVector, Int) => Any): Unit = {\n+      val schema = new StructType().add(\"value\", dt, nullable = true)\n+      val writer = ArrowWriter.create(schema)\n+      assert(writer.schema === schema)\n+\n+      data.foreach { datum =>\n+        writer.write(InternalRow(datum))\n+      }\n+      writer.finish()\n+\n+      val reader = new ArrowColumnVector(writer.root.getFieldVectors().get(0))\n+      data.zipWithIndex.foreach {\n+        case (null, rowId) => assert(reader.isNullAt(rowId))\n+        case (datum, rowId) => assert(get(reader, rowId) === datum)"
  }],
  "prId": 18655
}]