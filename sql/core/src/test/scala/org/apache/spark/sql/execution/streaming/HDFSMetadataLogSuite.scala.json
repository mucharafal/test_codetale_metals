[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "This is using a fake scheme.",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T04:41:51Z",
    "diffHunk": "@@ -88,14 +88,14 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n       s\"fs.$scheme.impl\",\n       classOf[FakeFileSystem].getName)\n     withTempDir { temp =>\n-      val metadataLog = new HDFSMetadataLog[String](spark, s\"$scheme://$temp\")\n+      val metadataLog = new HDFSMetadataLog[String](spark, s\"$scheme://${temp.toURI.getPath}\")",
    "line": 5
  }],
  "prId": 16335
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "We should close when it emits an exception. Otherwise, it is possible to suppress the actual assertion error due to the exception in removing the path, that makes the debug harder.",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T04:52:49Z",
    "diffHunk": "@@ -209,14 +209,20 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    if (Utils.isWindows) {\n+      fm.open(path).close()\n+      fm.delete(path)\n+      fm.delete(path) // should not throw exception\n+    } else {\n+      Utils.tryWithResource(fm.open(path)) { _ =>"
  }],
  "prId": 16335
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Windows holds an exclusive lock so it does not allow to remove a file when it is open. So, I re-wrote the test cases for WIndows.\r\n\r\ncc @tdas, do you mind if I ask to see whether these tests on Windows are fine?",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T04:53:27Z",
    "diffHunk": "@@ -209,14 +209,20 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    if (Utils.isWindows) {\n+      fm.open(path).close()\n+      fm.delete(path)\n+      fm.delete(path) // should not throw exception"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Why does windows need two deletes? It seems funny we'd need an entirely different path to delete the file just here and just on Windows?",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T09:17:19Z",
    "diffHunk": "@@ -209,14 +209,20 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    if (Utils.isWindows) {\n+      fm.open(path).close()\n+      fm.delete(path)\n+      fm.delete(path) // should not throw exception"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I thought the tests are intended to delete twice (opened file). The first one removes it and the other one does nothing without any exception. The original test deletes twice. (Actually , that's why I cc'ed him to check his intention).",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T09:45:18Z",
    "diffHunk": "@@ -209,14 +209,20 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    if (Utils.isWindows) {\n+      fm.open(path).close()\n+      fm.delete(path)\n+      fm.delete(path) // should not throw exception"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, BTW, I think I should add assert after the first deletion. ",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T09:55:55Z",
    "diffHunk": "@@ -209,14 +209,20 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    if (Utils.isWindows) {\n+      fm.open(path).close()\n+      fm.delete(path)\n+      fm.delete(path) // should not throw exception"
  }],
  "prId": 16335
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "cc @tdas, could I ask if these tests follow your intention please?",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T10:27:39Z",
    "diffHunk": "@@ -209,14 +209,26 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    def handleResource(f: => Unit): Unit = {\n+      if (Utils.isWindows) {\n+        // Windows does not allow deleting opened files.\n+        fm.open(path).close()\n+        f\n+      } else {\n+        Utils.tryWithResource(fm.open(path)) { _ =>\n+          f\n+        }\n+      }\n+    }\n+\n+    handleResource {\n+      fm.delete(path)\n+      assert(!fm.exists(path))\n+      intercept[IOException] {\n+        fm.open(path)\n+      }\n+      fm.delete(path) // should not throw exception"
  }],
  "prId": 16335
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Should we not try to close open file handles in any event, everywhere? It doesn't feel like this must be specific to Windows, even if it only is a problem on Windows.",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T11:19:02Z",
    "diffHunk": "@@ -209,14 +209,24 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    def closeOnWindows[R <: Closeable](res: => R)(f: => Unit): Unit = {\n+      if (Utils.isWindows) {\n+        // Windows does not allow deleting opened files.\n+        res.close()"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, I just checked the history by the blame button. I think it was just a mistake. Let me try to test just `fm.open(path).close()` once.",
    "commit": "54f0b7733a77f90c83be6deb555e9178637f6203",
    "createdAt": "2016-12-19T11:37:35Z",
    "diffHunk": "@@ -209,14 +209,24 @@ class HDFSMetadataLogSuite extends SparkFunSuite with SharedSQLContext {\n     }\n \n     // Open and delete\n-    val f1 = fm.open(path)\n-    fm.delete(path)\n-    assert(!fm.exists(path))\n-    intercept[IOException] {\n-      fm.open(path)\n+    def closeOnWindows[R <: Closeable](res: => R)(f: => Unit): Unit = {\n+      if (Utils.isWindows) {\n+        // Windows does not allow deleting opened files.\n+        res.close()"
  }],
  "prId": 16335
}]