[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`FileDataSourceV2FallbackSuite `",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T02:23:16Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {"
  }],
  "prId": 21123
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `val df = spark.range(10)`",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T02:24:58Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {\n+  class DummyFileWriter extends DataSourceWriter {\n+    override def createWriterFactory(): DataWriterFactory[Row] = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def commit(messages: Array[WriterCommitMessage]): Unit = {}\n+\n+    override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+  }\n+\n+  private val dummyParquetReaderV2 = classOf[DummyReadOnlyFileDataSourceV2].getName\n+  private val dummyParquetWriterV2 = classOf[DummyWriteOnlyFileDataSourceV2].getName\n+  private val simpleFileDataSourceV2 = classOf[SimpleFileDataSourceV2].getName\n+  private val parquetV1 = classOf[ParquetFileFormat].getCanonicalName\n+\n+  test(\"Fall back to v1 when writing to file with read only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()"
  }],
  "prId": 21123
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "this check should be put just below `df.write.format(dummyParquetWriterV2).save(path)`",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T02:27:05Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {\n+  class DummyFileWriter extends DataSourceWriter {\n+    override def createWriterFactory(): DataWriterFactory[Row] = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def commit(messages: Array[WriterCommitMessage]): Unit = {}\n+\n+    override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+  }\n+\n+  private val dummyParquetReaderV2 = classOf[DummyReadOnlyFileDataSourceV2].getName\n+  private val dummyParquetWriterV2 = classOf[DummyWriteOnlyFileDataSourceV2].getName\n+  private val simpleFileDataSourceV2 = classOf[SimpleFileDataSourceV2].getName\n+  private val parquetV1 = classOf[ParquetFileFormat].getCanonicalName\n+\n+  test(\"Fall back to v1 when writing to file with read only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      // Writing file should fall back to v1 and succeed.\n+      df.write.format(dummyParquetReaderV2).save(path)\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+\n+      // Dummy File reader should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        spark.read.format(dummyParquetReaderV2).load(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file reader\"))\n+    }\n+  }\n+\n+  test(\"Fall back to v1 when reading file with write only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      // Dummy File writer should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file writer\"))\n+\n+      df.write.format(parquetV1).save(path)\n+      // Reading file should fall back to v1 and succeed.\n+      checkAnswer(spark.read.format(dummyParquetWriterV2).load(path), df)\n+    }\n+  }\n+\n+  test(\"Fall back read path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      df.write.format(parquetV1).save(path)\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,parquet,bar\") {\n+        // Reading file should fall back to v1 and succeed.\n+        checkAnswer(spark.read.format(dummyParquetReaderV2).load(path), df)\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,bar\") {\n+        // Dummy File reader should fail as DISABLED_V2_FILE_DATA_SOURCE_READERS doesn't include it.\n+        val exception = intercept[AnalysisException] {\n+          spark.read.format(dummyParquetReaderV2).load(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file reader\"))\n+      }\n+    }\n+  }\n+\n+  test(\"Fall back write path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,bar\") {\n+        // Dummy File writer should fail as expected.\n+        val exception = intercept[AnalysisException] {\n+          df.write.format(dummyParquetWriterV2).save(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file writer\"))\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,parquet,bar\") {\n+        // Writing file should fall back to v1 and succeed.\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)"
  }],
  "prId": 21123
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: `val data = (100 until 105).map(i => (i, -i)).toDF(\"i\", \"j\")`",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T02:28:46Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {\n+  class DummyFileWriter extends DataSourceWriter {\n+    override def createWriterFactory(): DataWriterFactory[Row] = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def commit(messages: Array[WriterCommitMessage]): Unit = {}\n+\n+    override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+  }\n+\n+  private val dummyParquetReaderV2 = classOf[DummyReadOnlyFileDataSourceV2].getName\n+  private val dummyParquetWriterV2 = classOf[DummyWriteOnlyFileDataSourceV2].getName\n+  private val simpleFileDataSourceV2 = classOf[SimpleFileDataSourceV2].getName\n+  private val parquetV1 = classOf[ParquetFileFormat].getCanonicalName\n+\n+  test(\"Fall back to v1 when writing to file with read only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      // Writing file should fall back to v1 and succeed.\n+      df.write.format(dummyParquetReaderV2).save(path)\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+\n+      // Dummy File reader should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        spark.read.format(dummyParquetReaderV2).load(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file reader\"))\n+    }\n+  }\n+\n+  test(\"Fall back to v1 when reading file with write only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      // Dummy File writer should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file writer\"))\n+\n+      df.write.format(parquetV1).save(path)\n+      // Reading file should fall back to v1 and succeed.\n+      checkAnswer(spark.read.format(dummyParquetWriterV2).load(path), df)\n+    }\n+  }\n+\n+  test(\"Fall back read path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      df.write.format(parquetV1).save(path)\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,parquet,bar\") {\n+        // Reading file should fall back to v1 and succeed.\n+        checkAnswer(spark.read.format(dummyParquetReaderV2).load(path), df)\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,bar\") {\n+        // Dummy File reader should fail as DISABLED_V2_FILE_DATA_SOURCE_READERS doesn't include it.\n+        val exception = intercept[AnalysisException] {\n+          spark.read.format(dummyParquetReaderV2).load(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file reader\"))\n+      }\n+    }\n+  }\n+\n+  test(\"Fall back write path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,bar\") {\n+        // Dummy File writer should fail as expected.\n+        val exception = intercept[AnalysisException] {\n+          df.write.format(dummyParquetWriterV2).save(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file writer\"))\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,parquet,bar\") {\n+        // Writing file should fall back to v1 and succeed.\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+    }\n+  }\n+\n+  test(\"InsertIntoTable: Fall back to V1\") {\n+    val data = (100 until 105).map(i => (i, -i))"
  }],
  "prId": 21123
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`xxx.write.parquet(...)`, then we don't need the `parquetV1 `",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T02:29:14Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {\n+  class DummyFileWriter extends DataSourceWriter {\n+    override def createWriterFactory(): DataWriterFactory[Row] = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def commit(messages: Array[WriterCommitMessage]): Unit = {}\n+\n+    override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+  }\n+\n+  private val dummyParquetReaderV2 = classOf[DummyReadOnlyFileDataSourceV2].getName\n+  private val dummyParquetWriterV2 = classOf[DummyWriteOnlyFileDataSourceV2].getName\n+  private val simpleFileDataSourceV2 = classOf[SimpleFileDataSourceV2].getName\n+  private val parquetV1 = classOf[ParquetFileFormat].getCanonicalName\n+\n+  test(\"Fall back to v1 when writing to file with read only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      // Writing file should fall back to v1 and succeed.\n+      df.write.format(dummyParquetReaderV2).save(path)\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+\n+      // Dummy File reader should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        spark.read.format(dummyParquetReaderV2).load(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file reader\"))\n+    }\n+  }\n+\n+  test(\"Fall back to v1 when reading file with write only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      // Dummy File writer should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file writer\"))\n+\n+      df.write.format(parquetV1).save(path)\n+      // Reading file should fall back to v1 and succeed.\n+      checkAnswer(spark.read.format(dummyParquetWriterV2).load(path), df)\n+    }\n+  }\n+\n+  test(\"Fall back read path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      df.write.format(parquetV1).save(path)\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,parquet,bar\") {\n+        // Reading file should fall back to v1 and succeed.\n+        checkAnswer(spark.read.format(dummyParquetReaderV2).load(path), df)\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,bar\") {\n+        // Dummy File reader should fail as DISABLED_V2_FILE_DATA_SOURCE_READERS doesn't include it.\n+        val exception = intercept[AnalysisException] {\n+          spark.read.format(dummyParquetReaderV2).load(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file reader\"))\n+      }\n+    }\n+  }\n+\n+  test(\"Fall back write path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,bar\") {\n+        // Dummy File writer should fail as expected.\n+        val exception = intercept[AnalysisException] {\n+          df.write.format(dummyParquetWriterV2).save(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file writer\"))\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,parquet,bar\") {\n+        // Writing file should fall back to v1 and succeed.\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+    }\n+  }\n+\n+  test(\"InsertIntoTable: Fall back to V1\") {\n+    val data = (100 until 105).map(i => (i, -i))\n+    val data2 = (5 until 10).map(i => (i, -i))\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      withTempView(\"tmp\", \"tbl\") {\n+        spark.createDataFrame(data).toDF(\"i\", \"j\").createOrReplaceTempView(\"tmp\")\n+        spark.createDataFrame(data2).toDF(\"i\", \"j\").write.format(parquetV1).save(path)"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Make sense. I was trying to make sure it is using V1 format, which seems unnecessary.",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T05:51:14Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {\n+  class DummyFileWriter extends DataSourceWriter {\n+    override def createWriterFactory(): DataWriterFactory[Row] = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def commit(messages: Array[WriterCommitMessage]): Unit = {}\n+\n+    override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+  }\n+\n+  private val dummyParquetReaderV2 = classOf[DummyReadOnlyFileDataSourceV2].getName\n+  private val dummyParquetWriterV2 = classOf[DummyWriteOnlyFileDataSourceV2].getName\n+  private val simpleFileDataSourceV2 = classOf[SimpleFileDataSourceV2].getName\n+  private val parquetV1 = classOf[ParquetFileFormat].getCanonicalName\n+\n+  test(\"Fall back to v1 when writing to file with read only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      // Writing file should fall back to v1 and succeed.\n+      df.write.format(dummyParquetReaderV2).save(path)\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+\n+      // Dummy File reader should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        spark.read.format(dummyParquetReaderV2).load(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file reader\"))\n+    }\n+  }\n+\n+  test(\"Fall back to v1 when reading file with write only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      // Dummy File writer should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file writer\"))\n+\n+      df.write.format(parquetV1).save(path)\n+      // Reading file should fall back to v1 and succeed.\n+      checkAnswer(spark.read.format(dummyParquetWriterV2).load(path), df)\n+    }\n+  }\n+\n+  test(\"Fall back read path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      df.write.format(parquetV1).save(path)\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,parquet,bar\") {\n+        // Reading file should fall back to v1 and succeed.\n+        checkAnswer(spark.read.format(dummyParquetReaderV2).load(path), df)\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,bar\") {\n+        // Dummy File reader should fail as DISABLED_V2_FILE_DATA_SOURCE_READERS doesn't include it.\n+        val exception = intercept[AnalysisException] {\n+          spark.read.format(dummyParquetReaderV2).load(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file reader\"))\n+      }\n+    }\n+  }\n+\n+  test(\"Fall back write path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,bar\") {\n+        // Dummy File writer should fail as expected.\n+        val exception = intercept[AnalysisException] {\n+          df.write.format(dummyParquetWriterV2).save(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file writer\"))\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,parquet,bar\") {\n+        // Writing file should fall back to v1 and succeed.\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+    }\n+  }\n+\n+  test(\"InsertIntoTable: Fall back to V1\") {\n+    val data = (100 until 105).map(i => (i, -i))\n+    val data2 = (5 until 10).map(i => (i, -i))\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      withTempView(\"tmp\", \"tbl\") {\n+        spark.createDataFrame(data).toDF(\"i\", \"j\").createOrReplaceTempView(\"tmp\")\n+        spark.createDataFrame(data2).toDF(\"i\", \"j\").write.format(parquetV1).save(path)"
  }],
  "prId": 21123
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can we use `dummyParquetWriterV2` here?",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T02:29:53Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {\n+  class DummyFileWriter extends DataSourceWriter {\n+    override def createWriterFactory(): DataWriterFactory[Row] = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def commit(messages: Array[WriterCommitMessage]): Unit = {}\n+\n+    override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+  }\n+\n+  private val dummyParquetReaderV2 = classOf[DummyReadOnlyFileDataSourceV2].getName\n+  private val dummyParquetWriterV2 = classOf[DummyWriteOnlyFileDataSourceV2].getName\n+  private val simpleFileDataSourceV2 = classOf[SimpleFileDataSourceV2].getName\n+  private val parquetV1 = classOf[ParquetFileFormat].getCanonicalName\n+\n+  test(\"Fall back to v1 when writing to file with read only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      // Writing file should fall back to v1 and succeed.\n+      df.write.format(dummyParquetReaderV2).save(path)\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+\n+      // Dummy File reader should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        spark.read.format(dummyParquetReaderV2).load(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file reader\"))\n+    }\n+  }\n+\n+  test(\"Fall back to v1 when reading file with write only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      // Dummy File writer should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file writer\"))\n+\n+      df.write.format(parquetV1).save(path)\n+      // Reading file should fall back to v1 and succeed.\n+      checkAnswer(spark.read.format(dummyParquetWriterV2).load(path), df)\n+    }\n+  }\n+\n+  test(\"Fall back read path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      df.write.format(parquetV1).save(path)\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,parquet,bar\") {\n+        // Reading file should fall back to v1 and succeed.\n+        checkAnswer(spark.read.format(dummyParquetReaderV2).load(path), df)\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,bar\") {\n+        // Dummy File reader should fail as DISABLED_V2_FILE_DATA_SOURCE_READERS doesn't include it.\n+        val exception = intercept[AnalysisException] {\n+          spark.read.format(dummyParquetReaderV2).load(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file reader\"))\n+      }\n+    }\n+  }\n+\n+  test(\"Fall back write path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,bar\") {\n+        // Dummy File writer should fail as expected.\n+        val exception = intercept[AnalysisException] {\n+          df.write.format(dummyParquetWriterV2).save(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file writer\"))\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,parquet,bar\") {\n+        // Writing file should fall back to v1 and succeed.\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+    }\n+  }\n+\n+  test(\"InsertIntoTable: Fall back to V1\") {\n+    val data = (100 until 105).map(i => (i, -i))\n+    val data2 = (5 until 10).map(i => (i, -i))\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      withTempView(\"tmp\", \"tbl\") {\n+        spark.createDataFrame(data).toDF(\"i\", \"j\").createOrReplaceTempView(\"tmp\")\n+        spark.createDataFrame(data2).toDF(\"i\", \"j\").write.format(parquetV1).save(path)\n+        // Create temporary view with FileDataSourceV2\n+        spark.read.format(simpleFileDataSourceV2).load(path).createOrReplaceTempView(\"tbl\")"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "No, the data source of `tbl` would be V1 format. Thus the rule can't be applied.",
    "commit": "5ed7854c98a226e5f31bd212b80546d60fda65c8",
    "createdAt": "2018-04-24T05:49:06Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util.{List => JList, Optional}\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row, SaveMode}\n+import org.apache.spark.sql.execution.datasources.FileFormat\n+import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetTest}\n+import org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}\n+import org.apache.spark.sql.sources.v2.writer.{DataSourceWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.StructType\n+\n+class DummyReadOnlyFileDataSourceV2 extends FileDataSourceV2 with ReadSupport {\n+  class DummyFileReader extends DataSourceReader {\n+    override def readSchema(): StructType = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def createDataReaderFactories(): JList[DataReaderFactory[Row]] =\n+      java.util.Arrays.asList()\n+  }\n+\n+  override def createReader(options: DataSourceOptions): DataSourceReader = {\n+    throw new AnalysisException(\"Dummy file reader\")\n+  }\n+\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class DummyWriteOnlyFileDataSourceV2 extends FileDataSourceV2 with WriteSupport {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+\n+  override def createWriter(\n+      jobId: String,\n+      schema: StructType,\n+      mode: SaveMode,\n+      options: DataSourceOptions): Optional[DataSourceWriter] = {\n+    throw new AnalysisException(\"Dummy file writer\")\n+  }\n+}\n+\n+class SimpleFileDataSourceV2 extends SimpleDataSourceV2 with FileDataSourceV2 {\n+  override def fallBackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n+\n+  override def shortName(): String = \"parquet\"\n+}\n+\n+class FileDataSourceV2Suite extends QueryTest with ParquetTest with SharedSQLContext {\n+  class DummyFileWriter extends DataSourceWriter {\n+    override def createWriterFactory(): DataWriterFactory[Row] = {\n+      throw new AnalysisException(\"hehe\")\n+    }\n+\n+    override def commit(messages: Array[WriterCommitMessage]): Unit = {}\n+\n+    override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+  }\n+\n+  private val dummyParquetReaderV2 = classOf[DummyReadOnlyFileDataSourceV2].getName\n+  private val dummyParquetWriterV2 = classOf[DummyWriteOnlyFileDataSourceV2].getName\n+  private val simpleFileDataSourceV2 = classOf[SimpleFileDataSourceV2].getName\n+  private val parquetV1 = classOf[ParquetFileFormat].getCanonicalName\n+\n+  test(\"Fall back to v1 when writing to file with read only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      // Writing file should fall back to v1 and succeed.\n+      df.write.format(dummyParquetReaderV2).save(path)\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+\n+      // Dummy File reader should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        spark.read.format(dummyParquetReaderV2).load(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file reader\"))\n+    }\n+  }\n+\n+  test(\"Fall back to v1 when reading file with write only FileDataSourceV2\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      // Dummy File writer should fail as expected.\n+      val exception = intercept[AnalysisException] {\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+      assert(exception.message.equals(\"Dummy file writer\"))\n+\n+      df.write.format(parquetV1).save(path)\n+      // Reading file should fall back to v1 and succeed.\n+      checkAnswer(spark.read.format(dummyParquetWriterV2).load(path), df)\n+    }\n+  }\n+\n+  test(\"Fall back read path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      df.write.format(parquetV1).save(path)\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,parquet,bar\") {\n+        // Reading file should fall back to v1 and succeed.\n+        checkAnswer(spark.read.format(dummyParquetReaderV2).load(path), df)\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_READERS.key -> \"foo,bar\") {\n+        // Dummy File reader should fail as DISABLED_V2_FILE_DATA_SOURCE_READERS doesn't include it.\n+        val exception = intercept[AnalysisException] {\n+          spark.read.format(dummyParquetReaderV2).load(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file reader\"))\n+      }\n+    }\n+  }\n+\n+  test(\"Fall back write path to v1 with configuration DISABLED_V2_FILE_DATA_SOURCE_READERS\") {\n+    val df = spark.range(1, 10).toDF()\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,bar\") {\n+        // Dummy File writer should fail as expected.\n+        val exception = intercept[AnalysisException] {\n+          df.write.format(dummyParquetWriterV2).save(path)\n+        }\n+        assert(exception.message.equals(\"Dummy file writer\"))\n+      }\n+\n+      withSQLConf(SQLConf.DISABLED_V2_FILE_DATA_SOURCE_WRITERS.key -> \"foo,parquet,bar\") {\n+        // Writing file should fall back to v1 and succeed.\n+        df.write.format(dummyParquetWriterV2).save(path)\n+      }\n+\n+      // Validate write result with [[ParquetFileFormat]].\n+      checkAnswer(spark.read.format(parquetV1).load(path), df)\n+    }\n+  }\n+\n+  test(\"InsertIntoTable: Fall back to V1\") {\n+    val data = (100 until 105).map(i => (i, -i))\n+    val data2 = (5 until 10).map(i => (i, -i))\n+    withTempPath { file =>\n+      val path = file.getCanonicalPath\n+      withTempView(\"tmp\", \"tbl\") {\n+        spark.createDataFrame(data).toDF(\"i\", \"j\").createOrReplaceTempView(\"tmp\")\n+        spark.createDataFrame(data2).toDF(\"i\", \"j\").write.format(parquetV1).save(path)\n+        // Create temporary view with FileDataSourceV2\n+        spark.read.format(simpleFileDataSourceV2).load(path).createOrReplaceTempView(\"tbl\")"
  }],
  "prId": 21123
}]