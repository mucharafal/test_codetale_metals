[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "why this change? I think we will have a default session timezone?",
    "commit": "ae6397d50a7c8d946c022d481d57788360097756",
    "createdAt": "2017-02-15T21:25:42Z",
    "diffHunk": "@@ -19,11 +19,15 @@ package org.apache.spark.sql.sources\n \n import org.apache.spark.SparkFunSuite\n import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n import org.apache.spark.sql.execution.datasources.DataSource\n \n class ResolvedDataSourceSuite extends SparkFunSuite {\n   private def getProvidingClass(name: String): Class[_] =\n-    DataSource(sparkSession = null, className = name).providingClass\n+    DataSource(\n+      sparkSession = null,\n+      className = name,\n+      options = Map(\"timeZone\" -> DateTimeUtils.defaultTimeZone().getID)).providingClass",
    "line": 13
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "Unfortunately, we can't use the default session timezone because sparkSession is null here..",
    "commit": "ae6397d50a7c8d946c022d481d57788360097756",
    "createdAt": "2017-02-21T05:05:21Z",
    "diffHunk": "@@ -19,11 +19,15 @@ package org.apache.spark.sql.sources\n \n import org.apache.spark.SparkFunSuite\n import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n import org.apache.spark.sql.execution.datasources.DataSource\n \n class ResolvedDataSourceSuite extends SparkFunSuite {\n   private def getProvidingClass(name: String): Class[_] =\n-    DataSource(sparkSession = null, className = name).providingClass\n+    DataSource(\n+      sparkSession = null,\n+      className = name,\n+      options = Map(\"timeZone\" -> DateTimeUtils.defaultTimeZone().getID)).providingClass",
    "line": 13
  }],
  "prId": 16750
}]