[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "`Make sure we set those...` (no need \"at here\")\n",
    "commit": "26c42912fee12ea1327e0380f84e14375e590038",
    "createdAt": "2015-09-04T18:51:14Z",
    "diffHunk": "@@ -31,13 +31,24 @@ private[sql] class TestSQLContext(sc: SparkContext) extends SQLContext(sc) { sel\n       new SparkConf().set(\"spark.sql.testkey\", \"true\")))\n   }\n \n-  // Use fewer partitions to speed up testing\n+  // At here, we make sure we set those test specific confs correctly when we create"
  }],
  "prId": 8602
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "can you add a comment here: `// Make sure we start with the default test configs even after clear`\n",
    "commit": "26c42912fee12ea1327e0380f84e14375e590038",
    "createdAt": "2015-09-04T18:52:20Z",
    "diffHunk": "@@ -31,13 +31,24 @@ private[sql] class TestSQLContext(sc: SparkContext) extends SQLContext(sc) { sel\n       new SparkConf().set(\"spark.sql.testkey\", \"true\")))\n   }\n \n-  // Use fewer partitions to speed up testing\n+  // At here, we make sure we set those test specific confs correctly when we create\n+  // the SQLConf as well as when we call clear.\n   protected[sql] override def createSession(): SQLSession = new this.SQLSession()\n \n-  /** A special [[SQLSession]] that uses fewer shuffle partitions than normal. */\n   protected[sql] class SQLSession extends super.SQLSession {\n     protected[sql] override lazy val conf: SQLConf = new SQLConf {\n-      override def numShufflePartitions: Int = this.getConf(SQLConf.SHUFFLE_PARTITIONS, 5)\n+\n+      TestSQLContext.overrideConfs.map {\n+        case (key, value) => setConfString(key, value)\n+      }\n+\n+      override def clear(): Unit = {\n+        super.clear()\n+\n+        TestSQLContext.overrideConfs.map {"
  }],
  "prId": 8602
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this can be replaced with a simple call to `clear()` instead of duplicating the code\n",
    "commit": "26c42912fee12ea1327e0380f84e14375e590038",
    "createdAt": "2015-09-04T18:52:38Z",
    "diffHunk": "@@ -31,13 +31,24 @@ private[sql] class TestSQLContext(sc: SparkContext) extends SQLContext(sc) { sel\n       new SparkConf().set(\"spark.sql.testkey\", \"true\")))\n   }\n \n-  // Use fewer partitions to speed up testing\n+  // At here, we make sure we set those test specific confs correctly when we create\n+  // the SQLConf as well as when we call clear.\n   protected[sql] override def createSession(): SQLSession = new this.SQLSession()\n \n-  /** A special [[SQLSession]] that uses fewer shuffle partitions than normal. */\n   protected[sql] class SQLSession extends super.SQLSession {\n     protected[sql] override lazy val conf: SQLConf = new SQLConf {\n-      override def numShufflePartitions: Int = this.getConf(SQLConf.SHUFFLE_PARTITIONS, 5)\n+\n+      TestSQLContext.overrideConfs.map {\n+        case (key, value) => setConfString(key, value)\n+      }"
  }],
  "prId": 8602
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "This comment is still relevant, please add back\n",
    "commit": "26c42912fee12ea1327e0380f84e14375e590038",
    "createdAt": "2015-09-04T18:52:59Z",
    "diffHunk": "@@ -31,13 +31,24 @@ private[sql] class TestSQLContext(sc: SparkContext) extends SQLContext(sc) { sel\n       new SparkConf().set(\"spark.sql.testkey\", \"true\")))\n   }\n \n-  // Use fewer partitions to speed up testing\n+  // At here, we make sure we set those test specific confs correctly when we create\n+  // the SQLConf as well as when we call clear.\n   protected[sql] override def createSession(): SQLSession = new this.SQLSession()\n \n-  /** A special [[SQLSession]] that uses fewer shuffle partitions than normal. */"
  }],
  "prId": 8602
}]