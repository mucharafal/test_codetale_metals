[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @tejasapatil .\r\n`checkIfValueExists`?",
    "commit": "23acc3ff82bc885b295d069bd865129427d3c59e",
    "createdAt": "2017-02-13T18:40:15Z",
    "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.ConcurrentModificationException\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark._\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+class ExternalAppendOnlyUnsafeRowArraySuite extends SparkFunSuite with LocalSparkContext {\n+  private val random = new java.util.Random()\n+\n+  private def createSparkConf(): SparkConf = {\n+    val conf = new SparkConf(false)\n+    // Make the Java serializer write a reset instruction (TC_RESET) after each object to test\n+    // for a bug we had with bytes written past the last object in a batch (SPARK-2792)\n+    conf.set(\"spark.serializer.objectStreamReset\", \"1\")\n+    conf.set(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\")\n+    conf\n+  }\n+\n+  private def withExternalArray(spillThreshold: Int)\n+                               (f: ExternalAppendOnlyUnsafeRowArray => Unit): Unit = {\n+    sc = new SparkContext(\"local\", \"test\", createSparkConf())\n+    val taskContext = MemoryTestingUtils.fakeTaskContext(SparkEnv.get)\n+    TaskContext.setTaskContext(taskContext)\n+    val array = new ExternalAppendOnlyUnsafeRowArray(spillThreshold)\n+\n+    try f(array) finally {\n+      array.clear()\n+      sc.stop()\n+    }\n+  }\n+\n+  private def insertRow(array: ExternalAppendOnlyUnsafeRowArray): Long = {\n+    val valueInserted = random.nextLong()\n+\n+    val row = new UnsafeRow(1)\n+    row.pointTo(new Array[Byte](64), 16)\n+    row.setLong(0, valueInserted)\n+    array.add(row)\n+    valueInserted\n+  }\n+\n+  private def checkIfValueExits(iterator: Iterator[UnsafeRow], expectedValue: Long): Unit = {"
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "fixed the typo",
    "commit": "23acc3ff82bc885b295d069bd865129427d3c59e",
    "createdAt": "2017-02-13T21:15:19Z",
    "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution\n+\n+import java.util.ConcurrentModificationException\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark._\n+import org.apache.spark.memory.MemoryTestingUtils\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n+\n+class ExternalAppendOnlyUnsafeRowArraySuite extends SparkFunSuite with LocalSparkContext {\n+  private val random = new java.util.Random()\n+\n+  private def createSparkConf(): SparkConf = {\n+    val conf = new SparkConf(false)\n+    // Make the Java serializer write a reset instruction (TC_RESET) after each object to test\n+    // for a bug we had with bytes written past the last object in a batch (SPARK-2792)\n+    conf.set(\"spark.serializer.objectStreamReset\", \"1\")\n+    conf.set(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\")\n+    conf\n+  }\n+\n+  private def withExternalArray(spillThreshold: Int)\n+                               (f: ExternalAppendOnlyUnsafeRowArray => Unit): Unit = {\n+    sc = new SparkContext(\"local\", \"test\", createSparkConf())\n+    val taskContext = MemoryTestingUtils.fakeTaskContext(SparkEnv.get)\n+    TaskContext.setTaskContext(taskContext)\n+    val array = new ExternalAppendOnlyUnsafeRowArray(spillThreshold)\n+\n+    try f(array) finally {\n+      array.clear()\n+      sc.stop()\n+    }\n+  }\n+\n+  private def insertRow(array: ExternalAppendOnlyUnsafeRowArray): Long = {\n+    val valueInserted = random.nextLong()\n+\n+    val row = new UnsafeRow(1)\n+    row.pointTo(new Array[Byte](64), 16)\n+    row.setLong(0, valueInserted)\n+    array.add(row)\n+    valueInserted\n+  }\n+\n+  private def checkIfValueExits(iterator: Iterator[UnsafeRow], expectedValue: Long): Unit = {"
  }],
  "prId": 16909
}]