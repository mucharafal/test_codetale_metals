[{
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Could you make these benchmarks bigger? I think you are mainly measuring all sorts of initialization slow downs here.\n",
    "commit": "3e9c79081d7f07067c5772b5d14f64da7ed06c55",
    "createdAt": "2016-09-19T21:05:26Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution.benchmark\r\n+\r\n+import org.apache.spark.util.Benchmark\r\n+\r\n+/**\r\n+ * Benchmark to measure performance for broadcast hash join with duplicated keys.\r\n+ * To run this:\r\n+ *  build/sbt \"sql/test-only *benchmark.BroadcastHashJoinBenchmark\"\r\n+ *\r\n+ * Benchmarks in this file are skipped in normal builds.\r\n+ */\r\n+class BroadcastHashJoinBenchmark extends BenchmarkBase {\r\n+\r\n+  ignore(\"BroadcastHashJoin w duplicated keys\") {\r\n+    val N = 1 << 16\r\n+    val M = 1 << 8\r\n+\r\n+    val benchmark = new Benchmark(\"BroadcastHashJoin w duplicated keys\", N)\r\n+    sparkSession.range(N).selectExpr(\r\n+      \"(id % 1024) as sk\",\r\n+      \"(id / 2) as sv1\",\r\n+      \"(id / 3) as sv2\",\r\n+      \"(id / 4) as sv3\",\r\n+      \"(id / 5) as sv4\",\r\n+      \"(id / 6) as sv5\")\r\n+      .createOrReplaceTempView(\"src\")\r\n+\r\n+    sparkSession.range(M).selectExpr(\r\n+      \"(id % 5) as dk\",\r\n+      \"(id / 2) as dv1\",\r\n+      \"(id / 3) as dv2\")\r\n+      .createOrReplaceTempView(\"dim\")\r\n+\r\n+    def f(): Unit = sparkSession.sql(\r\n+      \"select sk, sv1, sv2, sv3, sv4, sv5, dk, dv1, dv2 from src join dim on sk = dk\").collect()\r\n+\r\n+    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.addCase(s\"codegen = T avoid = F\", numIters = 3) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\r\n+      sparkSession.conf.set(\"spark.sql.codegen.useInBenchmark\", value = true)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.addCase(s\"codegen = T avoid = T\", numIters = 3) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\r\n+      sparkSession.conf.set(\"spark.sql.codegen.useInBenchmark\", value = false)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.run()\r\n+\r\n+    /*\r\n+    Java HotSpot(TM) 64-Bit Server VM 1.8.0_65-b17 on Mac OS X 10.11.6\r\n+    Intel(R) Core(TM) i5-5287U CPU @ 2.90GHz\r\n+\r\n+    BroadcastHashJoin w duplicated keys:     Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n+    ------------------------------------------------------------------------------------------------\r\n+    codegen = F                                    226 /  344          0.3        3447.1       1.0X\r\n+    codegen = T avoid = F                          168 /  173          0.4        2565.2       1.3X\r"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "Check this blog post, for some context on these figures: https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\n",
    "commit": "3e9c79081d7f07067c5772b5d14f64da7ed06c55",
    "createdAt": "2016-09-19T21:07:05Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution.benchmark\r\n+\r\n+import org.apache.spark.util.Benchmark\r\n+\r\n+/**\r\n+ * Benchmark to measure performance for broadcast hash join with duplicated keys.\r\n+ * To run this:\r\n+ *  build/sbt \"sql/test-only *benchmark.BroadcastHashJoinBenchmark\"\r\n+ *\r\n+ * Benchmarks in this file are skipped in normal builds.\r\n+ */\r\n+class BroadcastHashJoinBenchmark extends BenchmarkBase {\r\n+\r\n+  ignore(\"BroadcastHashJoin w duplicated keys\") {\r\n+    val N = 1 << 16\r\n+    val M = 1 << 8\r\n+\r\n+    val benchmark = new Benchmark(\"BroadcastHashJoin w duplicated keys\", N)\r\n+    sparkSession.range(N).selectExpr(\r\n+      \"(id % 1024) as sk\",\r\n+      \"(id / 2) as sv1\",\r\n+      \"(id / 3) as sv2\",\r\n+      \"(id / 4) as sv3\",\r\n+      \"(id / 5) as sv4\",\r\n+      \"(id / 6) as sv5\")\r\n+      .createOrReplaceTempView(\"src\")\r\n+\r\n+    sparkSession.range(M).selectExpr(\r\n+      \"(id % 5) as dk\",\r\n+      \"(id / 2) as dv1\",\r\n+      \"(id / 3) as dv2\")\r\n+      .createOrReplaceTempView(\"dim\")\r\n+\r\n+    def f(): Unit = sparkSession.sql(\r\n+      \"select sk, sv1, sv2, sv3, sv4, sv5, dk, dv1, dv2 from src join dim on sk = dk\").collect()\r\n+\r\n+    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.addCase(s\"codegen = T avoid = F\", numIters = 3) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\r\n+      sparkSession.conf.set(\"spark.sql.codegen.useInBenchmark\", value = true)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.addCase(s\"codegen = T avoid = T\", numIters = 3) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\r\n+      sparkSession.conf.set(\"spark.sql.codegen.useInBenchmark\", value = false)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.run()\r\n+\r\n+    /*\r\n+    Java HotSpot(TM) 64-Bit Server VM 1.8.0_65-b17 on Mac OS X 10.11.6\r\n+    Intel(R) Core(TM) i5-5287U CPU @ 2.90GHz\r\n+\r\n+    BroadcastHashJoin w duplicated keys:     Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n+    ------------------------------------------------------------------------------------------------\r\n+    codegen = F                                    226 /  344          0.3        3447.1       1.0X\r\n+    codegen = T avoid = F                          168 /  173          0.4        2565.2       1.3X\r"
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "OKï¼Œmaybe now it is big enough\n",
    "commit": "3e9c79081d7f07067c5772b5d14f64da7ed06c55",
    "createdAt": "2016-09-20T15:20:59Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.execution.benchmark\r\n+\r\n+import org.apache.spark.util.Benchmark\r\n+\r\n+/**\r\n+ * Benchmark to measure performance for broadcast hash join with duplicated keys.\r\n+ * To run this:\r\n+ *  build/sbt \"sql/test-only *benchmark.BroadcastHashJoinBenchmark\"\r\n+ *\r\n+ * Benchmarks in this file are skipped in normal builds.\r\n+ */\r\n+class BroadcastHashJoinBenchmark extends BenchmarkBase {\r\n+\r\n+  ignore(\"BroadcastHashJoin w duplicated keys\") {\r\n+    val N = 1 << 16\r\n+    val M = 1 << 8\r\n+\r\n+    val benchmark = new Benchmark(\"BroadcastHashJoin w duplicated keys\", N)\r\n+    sparkSession.range(N).selectExpr(\r\n+      \"(id % 1024) as sk\",\r\n+      \"(id / 2) as sv1\",\r\n+      \"(id / 3) as sv2\",\r\n+      \"(id / 4) as sv3\",\r\n+      \"(id / 5) as sv4\",\r\n+      \"(id / 6) as sv5\")\r\n+      .createOrReplaceTempView(\"src\")\r\n+\r\n+    sparkSession.range(M).selectExpr(\r\n+      \"(id % 5) as dk\",\r\n+      \"(id / 2) as dv1\",\r\n+      \"(id / 3) as dv2\")\r\n+      .createOrReplaceTempView(\"dim\")\r\n+\r\n+    def f(): Unit = sparkSession.sql(\r\n+      \"select sk, sv1, sv2, sv3, sv4, sv5, dk, dv1, dv2 from src join dim on sk = dk\").collect()\r\n+\r\n+    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.addCase(s\"codegen = T avoid = F\", numIters = 3) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\r\n+      sparkSession.conf.set(\"spark.sql.codegen.useInBenchmark\", value = true)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.addCase(s\"codegen = T avoid = T\", numIters = 3) { iter =>\r\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\r\n+      sparkSession.conf.set(\"spark.sql.codegen.useInBenchmark\", value = false)\r\n+      f()\r\n+    }\r\n+\r\n+    benchmark.run()\r\n+\r\n+    /*\r\n+    Java HotSpot(TM) 64-Bit Server VM 1.8.0_65-b17 on Mac OS X 10.11.6\r\n+    Intel(R) Core(TM) i5-5287U CPU @ 2.90GHz\r\n+\r\n+    BroadcastHashJoin w duplicated keys:     Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n+    ------------------------------------------------------------------------------------------------\r\n+    codegen = F                                    226 /  344          0.3        3447.1       1.0X\r\n+    codegen = T avoid = F                          168 /  173          0.4        2565.2       1.3X\r"
  }],
  "prId": 15071
}]