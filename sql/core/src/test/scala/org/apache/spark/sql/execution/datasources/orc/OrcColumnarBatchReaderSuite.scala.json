[{
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "Here we can see the result columns is pruned.",
    "commit": "5ed34d8621cbd644df8b9cc0e8d7b5b27aa3a2ad",
    "createdAt": "2018-12-28T07:51:22Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc\n+\n+import org.apache.orc.TypeDescription\n+\n+import org.apache.spark.sql.QueryTest\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.vectorized.{OnHeapColumnVector, WritableColumnVector}\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+import org.apache.spark.sql.types.{StructField, StructType}\n+import org.apache.spark.unsafe.types.UTF8String.fromString\n+\n+class OrcColumnarBatchReaderSuite extends QueryTest with SQLTestUtils with SharedSQLContext {\n+  private val dataSchema = StructType.fromDDL(\"col1 int, col2 int\")\n+  private val partitionSchema = StructType.fromDDL(\"p1 string, p2 string\")\n+  private val partitionValues = InternalRow(fromString(\"partValue1\"), fromString(\"partValue2\"))\n+  private val orcFileSchemaList = Seq(\n+    \"struct<col1:int,col2:int>\", \"struct<col1:int,col2:int,p1:string,p2:string>\",\n+    \"struct<col1:int,col2:int,p1:string>\", \"struct<col1:int,col2:int,p2:string>\")\n+  orcFileSchemaList.foreach { case schema =>\n+    val orcFileSchema = TypeDescription.fromString(schema)\n+\n+    val isConstant = classOf[WritableColumnVector].getDeclaredField(\"isConstant\")\n+    isConstant.setAccessible(true)\n+\n+    def getReader(\n+        requestedDataColIds: Array[Int],\n+        requestedPartitionColIds: Array[Int],\n+        resultFields: Array[StructField]): OrcColumnarBatchReader = {\n+      val reader = new OrcColumnarBatchReader(false, false, 4096)\n+      reader.initBatch(\n+        orcFileSchema,\n+        resultFields,\n+        requestedDataColIds,\n+        requestedPartitionColIds,\n+        partitionValues)\n+      reader\n+    }\n+\n+    test(s\"all partitions are requested: $schema\") {\n+      val requestedDataColIds = Array(0, 1, 0, 0)\n+      val requestedPartitionColIds = Array(-1, -1, 0, 1)\n+      val reader = getReader(requestedDataColIds, requestedPartitionColIds,\n+        dataSchema.fields ++ partitionSchema.fields)\n+      assert(reader.requestedDataColIds === Array(0, 1, -1, -1))\n+    }\n+\n+    test(s\"initBatch should initialize requested partition columns only: $schema\") {\n+      val requestedDataColIds = Array(0, -1) // only `col1` is requested, `col2` doesn't exist\n+      val requestedPartitionColIds = Array(-1, 0) // only `p1` is requested\n+      val reader = getReader(requestedDataColIds, requestedPartitionColIds,\n+        Array(dataSchema.fields(0), partitionSchema.fields(0)))\n+      val batch = reader.columnarBatch\n+      assert(batch.numCols() === 2)",
    "line": 70
  }],
  "prId": 23387
}]