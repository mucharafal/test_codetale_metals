[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "```\r\n    val options =\r\n      s\"\"\"\r\n        |OPTIONS('path'='${rootDir.toURI.toString.stripSuffix(\"/\")}/$tableName',\r\n        |'parquet.compression'='$compressionCodec')\r\n       \"\"\".stripMargin\r\n    val partitionCreate = if (isPartitioned) \"PARTITIONED BY (p)\" else \"\"\r\n    sql(\r\n      s\"\"\"\r\n        |CREATE TABLE $tableName USING Parquet $options $partitionCreate\r\n        |AS SELECT 1 AS col1, 2 AS p\r\n      \"\"\".stripMargin)\r\n```",
    "commit": "1a8c654805656d3e143c2e63355c7b6365dac471",
    "createdAt": "2018-01-05T14:21:00Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.parquet\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.parquet.hadoop.ParquetOutputFormat\n+\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class ParquetCompressionCodecPrecedenceSuite extends ParquetTest with SharedSQLContext {\n+  test(\"Test `spark.sql.parquet.compression.codec` config\") {\n+    Seq(\"NONE\", \"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"LZO\").foreach { c =>\n+      withSQLConf(SQLConf.PARQUET_COMPRESSION.key -> c) {\n+        val expected = if (c == \"NONE\") \"UNCOMPRESSED\" else c\n+        val option = new ParquetOptions(Map.empty[String, String], spark.sessionState.conf)\n+        assert(option.compressionCodecClassName == expected)\n+      }\n+    }\n+  }\n+\n+  test(\"[SPARK-21786] Test Acquiring 'compressionCodecClassName' for parquet in right order.\") {\n+    // When \"compression\" is configured, it should be the first choice.\n+    withSQLConf(SQLConf.PARQUET_COMPRESSION.key -> \"snappy\") {\n+      val props = Map(\"compression\" -> \"uncompressed\", ParquetOutputFormat.COMPRESSION -> \"gzip\")\n+      val option = new ParquetOptions(props, spark.sessionState.conf)\n+      assert(option.compressionCodecClassName == \"UNCOMPRESSED\")\n+    }\n+\n+    // When \"compression\" is not configured, \"parquet.compression\" should be the preferred choice.\n+    withSQLConf(SQLConf.PARQUET_COMPRESSION.key -> \"snappy\") {\n+      val props = Map(ParquetOutputFormat.COMPRESSION -> \"gzip\")\n+      val option = new ParquetOptions(props, spark.sessionState.conf)\n+      assert(option.compressionCodecClassName == \"GZIP\")\n+    }\n+\n+    // When both \"compression\" and \"parquet.compression\" are not configured,\n+    // spark.sql.parquet.compression.codec should be the right choice.\n+    withSQLConf(SQLConf.PARQUET_COMPRESSION.key -> \"snappy\") {\n+      val props = Map.empty[String, String]\n+      val option = new ParquetOptions(props, spark.sessionState.conf)\n+      assert(option.compressionCodecClassName == \"SNAPPY\")\n+    }\n+  }\n+\n+  private def getTableCompressionCodec(path: String): Seq[String] = {\n+    val hadoopConf = spark.sessionState.newHadoopConf()\n+    val codecs = for {\n+      footer <- readAllFootersWithoutSummaryFiles(new Path(path), hadoopConf)\n+      block <- footer.getParquetMetadata.getBlocks.asScala\n+      column <- block.getColumns.asScala\n+    } yield column.getCodec.name()\n+    codecs.distinct\n+  }\n+\n+  private def createTableWithCompression(\n+      tableName: String,\n+      isPartitioned: Boolean,\n+      compressionCodec: String,\n+      rootDir: File): Unit = {\n+    val options =\n+      s\"\"\"OPTIONS('path'='${rootDir.toURI.toString.stripSuffix(\"/\")}/$tableName',\n+         |'parquet.compression'='$compressionCodec')\"\"\".stripMargin\n+    val partitionCreate = if (isPartitioned) \"PARTITIONED BY (p)\" else \"\"\n+    sql(s\"\"\"CREATE TABLE $tableName USING Parquet $options $partitionCreate\n+    |as select 1 as col1, 2 as p\"\"\".stripMargin)"
  }],
  "prId": 20076
}]