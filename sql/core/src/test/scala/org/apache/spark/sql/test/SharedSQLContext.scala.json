[{
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Why do we still need this?",
    "commit": "6c0b0d569ae1d779fd9253da0c7e97d12634063c",
    "createdAt": "2017-10-24T15:19:24Z",
    "diffHunk": "@@ -17,86 +17,8 @@\n \n package org.apache.spark.sql.test\n \n-import scala.concurrent.duration._\n-\n-import org.scalatest.BeforeAndAfterEach\n-import org.scalatest.concurrent.Eventually\n-\n-import org.apache.spark.{DebugFilesystem, SparkConf}\n-import org.apache.spark.sql.{SparkSession, SQLContext}\n-import org.apache.spark.sql.internal.SQLConf\n-\n-/**\n- * Helper trait for SQL test suites where all tests share a single [[TestSparkSession]].\n- */\n-trait SharedSQLContext extends SQLTestUtils with BeforeAndAfterEach with Eventually {\n-\n-  protected def sparkConf = {\n-    new SparkConf()\n-      .set(\"spark.hadoop.fs.file.impl\", classOf[DebugFilesystem].getName)\n-      .set(\"spark.unsafe.exceptionOnMemoryLeak\", \"true\")\n-      .set(SQLConf.CODEGEN_FALLBACK.key, \"false\")\n-  }\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   *\n-   * By default, the underlying [[org.apache.spark.SparkContext]] will be run in local\n-   * mode with the default test configurations.\n-   */\n-  private var _spark: TestSparkSession = null\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   */\n-  protected implicit def spark: SparkSession = _spark\n-\n-  /**\n-   * The [[TestSQLContext]] to use for all tests in this suite.\n-   */\n-  protected implicit def sqlContext: SQLContext = _spark.sqlContext\n-\n-  protected def createSparkSession: TestSparkSession = {\n-    new TestSparkSession(sparkConf)\n-  }\n-\n-  /**\n-   * Initialize the [[TestSparkSession]].\n-   */\n+trait SharedSQLContext extends SQLTestUtils with SharedSparkSession {\n   protected override def beforeAll(): Unit = {"
  }, {
    "author": {
      "login": "nkronenfeld"
    },
    "body": "It's still used throughout the unit tests.\r\nI had noticed another issue related to that ([SPARK-15037]), but that is marked resolved without having gotten rid of this.\r\nNote that SharedSQLContext is to SharedSessionContext as PlanTest is to PlanTestBase - it extends FunSuite.",
    "commit": "6c0b0d569ae1d779fd9253da0c7e97d12634063c",
    "createdAt": "2017-10-25T15:05:03Z",
    "diffHunk": "@@ -17,86 +17,8 @@\n \n package org.apache.spark.sql.test\n \n-import scala.concurrent.duration._\n-\n-import org.scalatest.BeforeAndAfterEach\n-import org.scalatest.concurrent.Eventually\n-\n-import org.apache.spark.{DebugFilesystem, SparkConf}\n-import org.apache.spark.sql.{SparkSession, SQLContext}\n-import org.apache.spark.sql.internal.SQLConf\n-\n-/**\n- * Helper trait for SQL test suites where all tests share a single [[TestSparkSession]].\n- */\n-trait SharedSQLContext extends SQLTestUtils with BeforeAndAfterEach with Eventually {\n-\n-  protected def sparkConf = {\n-    new SparkConf()\n-      .set(\"spark.hadoop.fs.file.impl\", classOf[DebugFilesystem].getName)\n-      .set(\"spark.unsafe.exceptionOnMemoryLeak\", \"true\")\n-      .set(SQLConf.CODEGEN_FALLBACK.key, \"false\")\n-  }\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   *\n-   * By default, the underlying [[org.apache.spark.SparkContext]] will be run in local\n-   * mode with the default test configurations.\n-   */\n-  private var _spark: TestSparkSession = null\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   */\n-  protected implicit def spark: SparkSession = _spark\n-\n-  /**\n-   * The [[TestSQLContext]] to use for all tests in this suite.\n-   */\n-  protected implicit def sqlContext: SQLContext = _spark.sqlContext\n-\n-  protected def createSparkSession: TestSparkSession = {\n-    new TestSparkSession(sparkConf)\n-  }\n-\n-  /**\n-   * Initialize the [[TestSparkSession]].\n-   */\n+trait SharedSQLContext extends SQLTestUtils with SharedSparkSession {\n   protected override def beforeAll(): Unit = {"
  }],
  "prId": 19529
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "If this `beforeAll` is just calling `super.beforeAll`, why do we still need to override it? ",
    "commit": "6c0b0d569ae1d779fd9253da0c7e97d12634063c",
    "createdAt": "2017-10-25T20:30:27Z",
    "diffHunk": "@@ -17,86 +17,8 @@\n \n package org.apache.spark.sql.test\n \n-import scala.concurrent.duration._\n-\n-import org.scalatest.BeforeAndAfterEach\n-import org.scalatest.concurrent.Eventually\n-\n-import org.apache.spark.{DebugFilesystem, SparkConf}\n-import org.apache.spark.sql.{SparkSession, SQLContext}\n-import org.apache.spark.sql.internal.SQLConf\n-\n-/**\n- * Helper trait for SQL test suites where all tests share a single [[TestSparkSession]].\n- */\n-trait SharedSQLContext extends SQLTestUtils with BeforeAndAfterEach with Eventually {\n-\n-  protected def sparkConf = {\n-    new SparkConf()\n-      .set(\"spark.hadoop.fs.file.impl\", classOf[DebugFilesystem].getName)\n-      .set(\"spark.unsafe.exceptionOnMemoryLeak\", \"true\")\n-      .set(SQLConf.CODEGEN_FALLBACK.key, \"false\")\n-  }\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   *\n-   * By default, the underlying [[org.apache.spark.SparkContext]] will be run in local\n-   * mode with the default test configurations.\n-   */\n-  private var _spark: TestSparkSession = null\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   */\n-  protected implicit def spark: SparkSession = _spark\n-\n-  /**\n-   * The [[TestSQLContext]] to use for all tests in this suite.\n-   */\n-  protected implicit def sqlContext: SQLContext = _spark.sqlContext\n-\n-  protected def createSparkSession: TestSparkSession = {\n-    new TestSparkSession(sparkConf)\n-  }\n-\n-  /**\n-   * Initialize the [[TestSparkSession]].\n-   */\n+trait SharedSQLContext extends SQLTestUtils with SharedSparkSession {\n   protected override def beforeAll(): Unit = {\n-    SparkSession.sqlListener.set(null)\n-    if (_spark == null) {\n-      _spark = createSparkSession\n-    }\n-    // Ensure we have initialized the context before calling parent code\n     super.beforeAll()"
  }, {
    "author": {
      "login": "nkronenfeld"
    },
    "body": "we don't.  Removed.",
    "commit": "6c0b0d569ae1d779fd9253da0c7e97d12634063c",
    "createdAt": "2017-10-26T03:25:04Z",
    "diffHunk": "@@ -17,86 +17,8 @@\n \n package org.apache.spark.sql.test\n \n-import scala.concurrent.duration._\n-\n-import org.scalatest.BeforeAndAfterEach\n-import org.scalatest.concurrent.Eventually\n-\n-import org.apache.spark.{DebugFilesystem, SparkConf}\n-import org.apache.spark.sql.{SparkSession, SQLContext}\n-import org.apache.spark.sql.internal.SQLConf\n-\n-/**\n- * Helper trait for SQL test suites where all tests share a single [[TestSparkSession]].\n- */\n-trait SharedSQLContext extends SQLTestUtils with BeforeAndAfterEach with Eventually {\n-\n-  protected def sparkConf = {\n-    new SparkConf()\n-      .set(\"spark.hadoop.fs.file.impl\", classOf[DebugFilesystem].getName)\n-      .set(\"spark.unsafe.exceptionOnMemoryLeak\", \"true\")\n-      .set(SQLConf.CODEGEN_FALLBACK.key, \"false\")\n-  }\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   *\n-   * By default, the underlying [[org.apache.spark.SparkContext]] will be run in local\n-   * mode with the default test configurations.\n-   */\n-  private var _spark: TestSparkSession = null\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   */\n-  protected implicit def spark: SparkSession = _spark\n-\n-  /**\n-   * The [[TestSQLContext]] to use for all tests in this suite.\n-   */\n-  protected implicit def sqlContext: SQLContext = _spark.sqlContext\n-\n-  protected def createSparkSession: TestSparkSession = {\n-    new TestSparkSession(sparkConf)\n-  }\n-\n-  /**\n-   * Initialize the [[TestSparkSession]].\n-   */\n+trait SharedSQLContext extends SQLTestUtils with SharedSparkSession {\n   protected override def beforeAll(): Unit = {\n-    SparkSession.sqlListener.set(null)\n-    if (_spark == null) {\n-      _spark = createSparkSession\n-    }\n-    // Ensure we have initialized the context before calling parent code\n     super.beforeAll()"
  }],
  "prId": 19529
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Does this work? Basically, we are just creating a trait. \r\n```Scala\r\ntrait SharedSQLContext extends SQLTestUtils with SharedSparkSession\r\n```\r\n\r\nCould we just move this line to  `SharedSparkSession.scala`?\r\n",
    "commit": "6c0b0d569ae1d779fd9253da0c7e97d12634063c",
    "createdAt": "2017-10-25T20:34:23Z",
    "diffHunk": "@@ -17,86 +17,8 @@\n \n package org.apache.spark.sql.test\n \n-import scala.concurrent.duration._\n-\n-import org.scalatest.BeforeAndAfterEach\n-import org.scalatest.concurrent.Eventually\n-\n-import org.apache.spark.{DebugFilesystem, SparkConf}\n-import org.apache.spark.sql.{SparkSession, SQLContext}\n-import org.apache.spark.sql.internal.SQLConf\n-\n-/**\n- * Helper trait for SQL test suites where all tests share a single [[TestSparkSession]].\n- */\n-trait SharedSQLContext extends SQLTestUtils with BeforeAndAfterEach with Eventually {\n-\n-  protected def sparkConf = {\n-    new SparkConf()\n-      .set(\"spark.hadoop.fs.file.impl\", classOf[DebugFilesystem].getName)\n-      .set(\"spark.unsafe.exceptionOnMemoryLeak\", \"true\")\n-      .set(SQLConf.CODEGEN_FALLBACK.key, \"false\")\n-  }\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   *\n-   * By default, the underlying [[org.apache.spark.SparkContext]] will be run in local\n-   * mode with the default test configurations.\n-   */\n-  private var _spark: TestSparkSession = null\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   */\n-  protected implicit def spark: SparkSession = _spark\n-\n-  /**\n-   * The [[TestSQLContext]] to use for all tests in this suite.\n-   */\n-  protected implicit def sqlContext: SQLContext = _spark.sqlContext\n-\n-  protected def createSparkSession: TestSparkSession = {\n-    new TestSparkSession(sparkConf)\n-  }\n-\n-  /**\n-   * Initialize the [[TestSparkSession]].\n-   */\n+trait SharedSQLContext extends SQLTestUtils with SharedSparkSession {"
  }, {
    "author": {
      "login": "nkronenfeld"
    },
    "body": "We could... that would more fit the pattern of what we've done now for PlanTest/PlanTestBase and SQLTestUtils/SQLTestUtilsBase.\r\n\r\nI hesitated in this case just because the two are such conceptually different concepts, and the idea is that both would actually get used - SharedSQLContext in internal tests, SharedSparkSession in external tests.",
    "commit": "6c0b0d569ae1d779fd9253da0c7e97d12634063c",
    "createdAt": "2017-10-26T03:26:42Z",
    "diffHunk": "@@ -17,86 +17,8 @@\n \n package org.apache.spark.sql.test\n \n-import scala.concurrent.duration._\n-\n-import org.scalatest.BeforeAndAfterEach\n-import org.scalatest.concurrent.Eventually\n-\n-import org.apache.spark.{DebugFilesystem, SparkConf}\n-import org.apache.spark.sql.{SparkSession, SQLContext}\n-import org.apache.spark.sql.internal.SQLConf\n-\n-/**\n- * Helper trait for SQL test suites where all tests share a single [[TestSparkSession]].\n- */\n-trait SharedSQLContext extends SQLTestUtils with BeforeAndAfterEach with Eventually {\n-\n-  protected def sparkConf = {\n-    new SparkConf()\n-      .set(\"spark.hadoop.fs.file.impl\", classOf[DebugFilesystem].getName)\n-      .set(\"spark.unsafe.exceptionOnMemoryLeak\", \"true\")\n-      .set(SQLConf.CODEGEN_FALLBACK.key, \"false\")\n-  }\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   *\n-   * By default, the underlying [[org.apache.spark.SparkContext]] will be run in local\n-   * mode with the default test configurations.\n-   */\n-  private var _spark: TestSparkSession = null\n-\n-  /**\n-   * The [[TestSparkSession]] to use for all tests in this suite.\n-   */\n-  protected implicit def spark: SparkSession = _spark\n-\n-  /**\n-   * The [[TestSQLContext]] to use for all tests in this suite.\n-   */\n-  protected implicit def sqlContext: SQLContext = _spark.sqlContext\n-\n-  protected def createSparkSession: TestSparkSession = {\n-    new TestSparkSession(sparkConf)\n-  }\n-\n-  /**\n-   * Initialize the [[TestSparkSession]].\n-   */\n+trait SharedSQLContext extends SQLTestUtils with SharedSparkSession {"
  }],
  "prId": 19529
}]