[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Since this is `ParquetPartitionDiscoverySuite`, `parquet` is more proper than `load`.",
    "commit": "2975aff63a4a22ed60bded5011ee4ae8169cf987",
    "createdAt": "2018-03-01T19:29:33Z",
    "diffHunk": "@@ -739,15 +739,15 @@ class ParquetPartitionDiscoverySuite extends QueryTest with ParquetTest with Sha\n     withTempPath { dir =>\n       df.write.format(\"parquet\").partitionBy(partitionColumns.map(_.name): _*).save(dir.toString)\n       val fields = schema.map(f => Column(f.name).cast(f.dataType))\n-      checkAnswer(spark.read.load(dir.toString).select(fields: _*), row)\n+      checkAnswer(spark.read.parquet(dir.toString).select(fields: _*), row)"
  }],
  "prId": 20705
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Since this is `ParquetPartitionDiscoverySuite`, the test cases' assumption is legitimate.",
    "commit": "2975aff63a4a22ed60bded5011ee4ae8169cf987",
    "createdAt": "2018-03-03T05:51:56Z",
    "diffHunk": "@@ -57,6 +57,16 @@ class ParquetPartitionDiscoverySuite extends QueryTest with ParquetTest with Sha\n   val timeZone = TimeZone.getDefault()\n   val timeZoneId = timeZone.getID\n \n+  protected override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    spark.conf.set(SQLConf.DEFAULT_DATA_SOURCE_NAME.key, \"parquet\")",
    "line": 6
  }],
  "prId": 20705
}]