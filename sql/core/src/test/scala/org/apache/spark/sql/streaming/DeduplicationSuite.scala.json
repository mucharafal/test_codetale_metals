[{
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "I know the semantics are the same for `Append` and `Update` but just so that no one breaks it in the future, should we wrap these tests with:\r\n`Seq(Append, Update).foreach { mode =>`",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-17T19:19:23Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StreamTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication - complete\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Complete)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"a\", \"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication - append/update\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with columns - complete\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates(\"_1\")\n+\n+    testStream(result, Complete)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\" -> 2), // Dropped\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"a\" -> 1, \"b\" -> 1),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with columns - append/update\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)("
  }],
  "prId": 16970
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "These methods seems to be common across StreamingAggregationSuite, MapGroupsWithStateSuite and this one. Can you make a trait?",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-18T01:03:18Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StreamTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with columns\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\" -> 2), // Dropped\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"multiple deduplications\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, \"a\" -> 2), // Dropped from the second `dropDuplicates`\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(0L, 1L)),\n+\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  test(\"deduplication with watermark\") {\n+    val inputData = MemoryStream[Int]\n+    val result = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .select($\"eventTime\".cast(\"long\").as[Long])\n+\n+    testStream(result, Append)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(10 to 15: _*),\n+      assertNumStateRows(total = 6, updated = 6),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(25),\n+      assertNumStateRows(total = 7, updated = 1),\n+\n+      AddData(inputData, 25), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 45), // Advance watermark to 35 seconds\n+      CheckLastBatch(45),\n+      assertNumStateRows(total = 2, updated = 1),\n+\n+      AddData(inputData, 45), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0)\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - append\") {\n+    val inputData = MemoryStream[Int]\n+    val windowedAggregation = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .groupBy(window($\"eventTime\", \"5 seconds\") as 'window)\n+      .agg(count(\"*\") as 'count)\n+      .select($\"window\".getField(\"start\").cast(\"long\").as[Long], $\"count\".as[Long])\n+\n+    testStream(windowedAggregation)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) (2 windows)\n+      // states in deduplication is 10 to 15\n+      assertNumStateRows(total = Seq(2L, 6L), updated = Seq(2L, 6L)),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) and [25, 30) (3 windows)\n+      // states in deduplication is 10 to 15 and 25\n+      assertNumStateRows(total = Seq(3L, 7L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 25), // Emit items less than watermark and drop their state\n+      CheckLastBatch((10 -> 5)), // 5 items (10 to 14) after deduplication\n+      // states in aggregation in [15, 20) and [25, 30) (2 windows, note aggregation uses the end of\n+      // window to evict items, so [15, 20) is still in the state store)\n+      // states in deduplication is 25\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 40), // Advance watermark to 30 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [15, 20), [25, 30) and [40, 45)\n+      // states in deduplication is 25 and 40,\n+      assertNumStateRows(total = Seq(3L, 2L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 40), // Emit items less than watermark and drop their state\n+      CheckLastBatch((15 -> 1), (25 -> 1)),\n+        // states in aggregation in [40, 45)\n+      // states in deduplication is 40,\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(0L, 0L))\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - update\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS()\n+      .dropDuplicates()\n+      .groupBy($\"_1\")\n+      .agg(sum($\"_2\"))\n+      .as[(String, Long)]\n+\n+    testStream(result, Update)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1L),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+      AddData(inputData, \"a\" -> 1), // Dropped\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(0L, 0L)),\n+      AddData(inputData, \"a\" -> 2),\n+      CheckLastBatch(\"a\" -> 3L),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(1L, 1L)),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1L),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - complete\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS()\n+      .dropDuplicates()\n+      .groupBy($\"_1\")\n+      .agg(sum($\"_2\"))\n+      .as[(String, Long)]\n+\n+    testStream(result, Complete)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1L),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+      AddData(inputData, \"a\" -> 1), // Dropped\n+      CheckLastBatch(\"a\" -> 1L),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(0L, 0L)),\n+      AddData(inputData, \"a\" -> 2),\n+      CheckLastBatch(\"a\" -> 3L),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(1L, 1L)),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"a\" -> 3L, \"b\" -> 1L),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  private def assertNumStateRows(total: Seq[Long], updated: Seq[Long]): AssertOnQuery ="
  }],
  "prId": 16970
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: deduplication with all columns",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-18T01:14:31Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StreamTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication\") {"
  }],
  "prId": 16970
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: deduplication with some columns",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-18T01:14:52Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StreamTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with columns\") {"
  }],
  "prId": 16970
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: update mode",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-18T01:15:17Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StreamTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with columns\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\" -> 2), // Dropped\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"multiple deduplications\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, \"a\" -> 2), // Dropped from the second `dropDuplicates`\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(0L, 1L)),\n+\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  test(\"deduplication with watermark\") {\n+    val inputData = MemoryStream[Int]\n+    val result = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .select($\"eventTime\".cast(\"long\").as[Long])\n+\n+    testStream(result, Append)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(10 to 15: _*),\n+      assertNumStateRows(total = 6, updated = 6),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(25),\n+      assertNumStateRows(total = 7, updated = 1),\n+\n+      AddData(inputData, 25), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 45), // Advance watermark to 35 seconds\n+      CheckLastBatch(45),\n+      assertNumStateRows(total = 2, updated = 1),\n+\n+      AddData(inputData, 45), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0)\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - append\") {\n+    val inputData = MemoryStream[Int]\n+    val windowedAggregation = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .groupBy(window($\"eventTime\", \"5 seconds\") as 'window)\n+      .agg(count(\"*\") as 'count)\n+      .select($\"window\".getField(\"start\").cast(\"long\").as[Long], $\"count\".as[Long])\n+\n+    testStream(windowedAggregation)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) (2 windows)\n+      // states in deduplication is 10 to 15\n+      assertNumStateRows(total = Seq(2L, 6L), updated = Seq(2L, 6L)),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) and [25, 30) (3 windows)\n+      // states in deduplication is 10 to 15 and 25\n+      assertNumStateRows(total = Seq(3L, 7L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 25), // Emit items less than watermark and drop their state\n+      CheckLastBatch((10 -> 5)), // 5 items (10 to 14) after deduplication\n+      // states in aggregation in [15, 20) and [25, 30) (2 windows, note aggregation uses the end of\n+      // window to evict items, so [15, 20) is still in the state store)\n+      // states in deduplication is 25\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 40), // Advance watermark to 30 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [15, 20), [25, 30) and [40, 45)\n+      // states in deduplication is 25 and 40,\n+      assertNumStateRows(total = Seq(3L, 2L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 40), // Emit items less than watermark and drop their state\n+      CheckLastBatch((15 -> 1), (25 -> 1)),\n+        // states in aggregation in [40, 45)\n+      // states in deduplication is 40,\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(0L, 0L))\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - update\") {"
  }],
  "prId": 16970
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: why not name the columns!?",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-18T01:15:40Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StreamTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with columns\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\" -> 2), // Dropped\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"multiple deduplications\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, \"a\" -> 2), // Dropped from the second `dropDuplicates`\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(0L, 1L)),\n+\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  test(\"deduplication with watermark\") {\n+    val inputData = MemoryStream[Int]\n+    val result = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .select($\"eventTime\".cast(\"long\").as[Long])\n+\n+    testStream(result, Append)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(10 to 15: _*),\n+      assertNumStateRows(total = 6, updated = 6),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(25),\n+      assertNumStateRows(total = 7, updated = 1),\n+\n+      AddData(inputData, 25), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 45), // Advance watermark to 35 seconds\n+      CheckLastBatch(45),\n+      assertNumStateRows(total = 2, updated = 1),\n+\n+      AddData(inputData, 45), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0)\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - append\") {\n+    val inputData = MemoryStream[Int]\n+    val windowedAggregation = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .groupBy(window($\"eventTime\", \"5 seconds\") as 'window)\n+      .agg(count(\"*\") as 'count)\n+      .select($\"window\".getField(\"start\").cast(\"long\").as[Long], $\"count\".as[Long])\n+\n+    testStream(windowedAggregation)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) (2 windows)\n+      // states in deduplication is 10 to 15\n+      assertNumStateRows(total = Seq(2L, 6L), updated = Seq(2L, 6L)),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) and [25, 30) (3 windows)\n+      // states in deduplication is 10 to 15 and 25\n+      assertNumStateRows(total = Seq(3L, 7L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 25), // Emit items less than watermark and drop their state\n+      CheckLastBatch((10 -> 5)), // 5 items (10 to 14) after deduplication\n+      // states in aggregation in [15, 20) and [25, 30) (2 windows, note aggregation uses the end of\n+      // window to evict items, so [15, 20) is still in the state store)\n+      // states in deduplication is 25\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 40), // Advance watermark to 30 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [15, 20), [25, 30) and [40, 45)\n+      // states in deduplication is 25 and 40,\n+      assertNumStateRows(total = Seq(3L, 2L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 40), // Emit items less than watermark and drop their state\n+      CheckLastBatch((15 -> 1), (25 -> 1)),\n+        // states in aggregation in [40, 45)\n+      // states in deduplication is 40,\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(0L, 0L))\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - update\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS()\n+      .dropDuplicates()\n+      .groupBy($\"_1\")"
  }],
  "prId": 16970
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: same as above.",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-18T01:16:03Z",
    "diffHunk": "@@ -0,0 +1,235 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StreamTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with columns\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\" -> 2), // Dropped\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"multiple deduplications\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, \"a\" -> 2), // Dropped from the second `dropDuplicates`\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(0L, 1L)),\n+\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  test(\"deduplication with watermark\") {\n+    val inputData = MemoryStream[Int]\n+    val result = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .select($\"eventTime\".cast(\"long\").as[Long])\n+\n+    testStream(result, Append)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(10 to 15: _*),\n+      assertNumStateRows(total = 6, updated = 6),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(25),\n+      assertNumStateRows(total = 7, updated = 1),\n+\n+      AddData(inputData, 25), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 45), // Advance watermark to 35 seconds\n+      CheckLastBatch(45),\n+      assertNumStateRows(total = 2, updated = 1),\n+\n+      AddData(inputData, 45), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0)\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - append\") {\n+    val inputData = MemoryStream[Int]\n+    val windowedAggregation = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .groupBy(window($\"eventTime\", \"5 seconds\") as 'window)\n+      .agg(count(\"*\") as 'count)\n+      .select($\"window\".getField(\"start\").cast(\"long\").as[Long], $\"count\".as[Long])\n+\n+    testStream(windowedAggregation)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) (2 windows)\n+      // states in deduplication is 10 to 15\n+      assertNumStateRows(total = Seq(2L, 6L), updated = Seq(2L, 6L)),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) and [25, 30) (3 windows)\n+      // states in deduplication is 10 to 15 and 25\n+      assertNumStateRows(total = Seq(3L, 7L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 25), // Emit items less than watermark and drop their state\n+      CheckLastBatch((10 -> 5)), // 5 items (10 to 14) after deduplication\n+      // states in aggregation in [15, 20) and [25, 30) (2 windows, note aggregation uses the end of\n+      // window to evict items, so [15, 20) is still in the state store)\n+      // states in deduplication is 25\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 40), // Advance watermark to 30 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [15, 20), [25, 30) and [40, 45)\n+      // states in deduplication is 25 and 40,\n+      assertNumStateRows(total = Seq(3L, 2L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 40), // Emit items less than watermark and drop their state\n+      CheckLastBatch((15 -> 1), (25 -> 1)),\n+        // states in aggregation in [40, 45)\n+      // states in deduplication is 40,\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(0L, 0L))\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - update\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS()\n+      .dropDuplicates()\n+      .groupBy($\"_1\")\n+      .agg(sum($\"_2\"))\n+      .as[(String, Long)]\n+\n+    testStream(result, Update)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1L),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+      AddData(inputData, \"a\" -> 1), // Dropped\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(0L, 0L)),\n+      AddData(inputData, \"a\" -> 2),\n+      CheckLastBatch(\"a\" -> 3L),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(1L, 1L)),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1L),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - complete\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS()\n+      .dropDuplicates()\n+      .groupBy($\"_1\")"
  }],
  "prId": 16970
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: indent.",
    "commit": "d0b7b77e345b275d58ba5582f6acde86a80cb3da",
    "createdAt": "2017-02-22T04:07:52Z",
    "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming\n+\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.execution.streaming.state.StateStore\n+import org.apache.spark.sql.functions._\n+\n+class DeduplicationSuite extends StateStoreMetricsTest with BeforeAndAfterAll {\n+\n+  import testImplicits._\n+\n+  override def afterAll(): Unit = {\n+    super.afterAll()\n+    StateStore.stop()\n+  }\n+\n+  test(\"deduplication with all columns\") {\n+    val inputData = MemoryStream[String]\n+    val result = inputData.toDS().dropDuplicates()\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(\"a\"),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\"),\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\"),\n+      CheckLastBatch(\"b\"),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"deduplication with some columns\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = 1, updated = 1),\n+      AddData(inputData, \"a\" -> 2), // Dropped\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = 2, updated = 1)\n+    )\n+  }\n+\n+  test(\"multiple deduplications\") {\n+    val inputData = MemoryStream[(String, Int)]\n+    val result = inputData.toDS().dropDuplicates().dropDuplicates(\"_1\")\n+\n+    testStream(result, Append)(\n+      AddData(inputData, \"a\" -> 1),\n+      CheckLastBatch(\"a\" -> 1),\n+      assertNumStateRows(total = Seq(1L, 1L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, \"a\" -> 2), // Dropped from the second `dropDuplicates`\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(1L, 2L), updated = Seq(0L, 1L)),\n+\n+      AddData(inputData, \"b\" -> 1),\n+      CheckLastBatch(\"b\" -> 1),\n+      assertNumStateRows(total = Seq(2L, 3L), updated = Seq(1L, 1L))\n+    )\n+  }\n+\n+  test(\"deduplication with watermark\") {\n+    val inputData = MemoryStream[Int]\n+    val result = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .select($\"eventTime\".cast(\"long\").as[Long])\n+\n+    testStream(result, Append)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(10 to 15: _*),\n+      assertNumStateRows(total = 6, updated = 6),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(25),\n+      assertNumStateRows(total = 7, updated = 1),\n+\n+      AddData(inputData, 25), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0),\n+\n+      AddData(inputData, 45), // Advance watermark to 35 seconds\n+      CheckLastBatch(45),\n+      assertNumStateRows(total = 2, updated = 1),\n+\n+      AddData(inputData, 45), // Drop states less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = 1, updated = 0)\n+    )\n+  }\n+\n+  test(\"deduplication with aggregation - append mode\") {\n+    val inputData = MemoryStream[Int]\n+    val windowedAggregation = inputData.toDS()\n+      .withColumn(\"eventTime\", $\"value\".cast(\"timestamp\"))\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .dropDuplicates()\n+      .withWatermark(\"eventTime\", \"10 seconds\")\n+      .groupBy(window($\"eventTime\", \"5 seconds\") as 'window)\n+      .agg(count(\"*\") as 'count)\n+      .select($\"window\".getField(\"start\").cast(\"long\").as[Long], $\"count\".as[Long])\n+\n+    testStream(windowedAggregation)(\n+      AddData(inputData, (1 to 5).flatMap(_ => (10 to 15)): _*),\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) (2 windows)\n+      // states in deduplication is 10 to 15\n+      assertNumStateRows(total = Seq(2L, 6L), updated = Seq(2L, 6L)),\n+\n+      AddData(inputData, 25), // Advance watermark to 15 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [10, 14), [15, 20) and [25, 30) (3 windows)\n+      // states in deduplication is 10 to 15 and 25\n+      assertNumStateRows(total = Seq(3L, 7L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 25), // Emit items less than watermark and drop their state\n+      CheckLastBatch((10 -> 5)), // 5 items (10 to 14) after deduplication\n+      // states in aggregation in [15, 20) and [25, 30) (2 windows, note aggregation uses the end of\n+      // window to evict items, so [15, 20) is still in the state store)\n+      // states in deduplication is 25\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 10), // Should not emit anything as data less than watermark\n+      CheckLastBatch(),\n+      assertNumStateRows(total = Seq(2L, 1L), updated = Seq(0L, 0L)),\n+\n+      AddData(inputData, 40), // Advance watermark to 30 seconds\n+      CheckLastBatch(),\n+      // states in aggregation in [15, 20), [25, 30) and [40, 45)\n+      // states in deduplication is 25 and 40,\n+      assertNumStateRows(total = Seq(3L, 2L), updated = Seq(1L, 1L)),\n+\n+      AddData(inputData, 40), // Emit items less than watermark and drop their state\n+      CheckLastBatch((15 -> 1), (25 -> 1)),\n+        // states in aggregation in [40, 45)"
  }],
  "prId": 16970
}]