[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Discussed offline. Merged these tests into the earlier test suite. Name the combined one appropriately.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-25T22:46:31Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.streaming.continuous.shuffle\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{HashPartitioner, Partition, TaskContext, TaskContextImpl}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.execution.streaming.continuous.shuffle.{ContinuousShuffleReadPartition, ContinuousShuffleReadRDD, ContinuousShuffleWriteRDD, UnsafeRowWriter}\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.types.{DataType, IntegerType}\n+\n+class ContinuousShuffleSuite extends StreamTest {"
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: i generally put the simplest test first (likely to be the reader tests since they dont depend on writer) and the more complex, e2e-ish tests later (writers since they needs readers).",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-29T18:34:28Z",
    "diffHunk": "@@ -40,22 +60,129 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     messages.foreach(endpoint.askSync[Unit](_))\n   }\n \n-  // In this unit test, we emulate that we're in the task thread where\n-  // ContinuousShuffleReadRDD.compute() will be evaluated. This requires a task context\n-  // thread local to be set.\n-  var ctx: TaskContextImpl = _\n+  private def readRDDEndpoint(rdd: ContinuousShuffleReadRDD) = {\n+    rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+  }\n \n-  override def beforeEach(): Unit = {\n-    super.beforeEach()\n-    ctx = TaskContext.empty()\n-    TaskContext.setTaskContext(ctx)\n+  private def readEpoch(rdd: ContinuousShuffleReadRDD) = {\n+    rdd.compute(rdd.partitions(0), ctx).toSeq.map(_.getInt(0))\n   }\n \n-  override def afterEach(): Unit = {\n-    ctx.markTaskCompleted(None)\n-    TaskContext.unset()\n-    ctx = null\n-    super.afterEach()\n+  test(\"one epoch\") {"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Reordered.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-29T22:43:59Z",
    "diffHunk": "@@ -40,22 +60,129 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     messages.foreach(endpoint.askSync[Unit](_))\n   }\n \n-  // In this unit test, we emulate that we're in the task thread where\n-  // ContinuousShuffleReadRDD.compute() will be evaluated. This requires a task context\n-  // thread local to be set.\n-  var ctx: TaskContextImpl = _\n+  private def readRDDEndpoint(rdd: ContinuousShuffleReadRDD) = {\n+    rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+  }\n \n-  override def beforeEach(): Unit = {\n-    super.beforeEach()\n-    ctx = TaskContext.empty()\n-    TaskContext.setTaskContext(ctx)\n+  private def readEpoch(rdd: ContinuousShuffleReadRDD) = {\n+    rdd.compute(rdd.partitions(0), ctx).toSeq.map(_.getInt(0))\n   }\n \n-  override def afterEach(): Unit = {\n-    ctx.markTaskCompleted(None)\n-    TaskContext.unset()\n-    ctx = null\n-    super.afterEach()\n+  test(\"one epoch\") {"
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Just curious: is there a reason to rearrange functions, this and below twos? Looks like they're same except changing this function to `implicit`.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-29T23:31:07Z",
    "diffHunk": "@@ -58,39 +46,29 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     super.afterEach()\n   }\n \n-  test(\"receiver stopped with row last\") {\n-    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n-    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n-    send(\n-      endpoint,\n-      ReceiverEpochMarker(0),\n-      ReceiverRow(0, unsafeRow(111))\n-    )\n+  private implicit def unsafeRow(value: Int) = {",
    "line": 45
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "And where it leverages the `implicit` attribute of this method? I'm not sure it is really needed, but I'm review on Github page so I might be missing here.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T02:49:36Z",
    "diffHunk": "@@ -58,39 +46,29 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     super.afterEach()\n   }\n \n-  test(\"receiver stopped with row last\") {\n-    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n-    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n-    send(\n-      endpoint,\n-      ReceiverEpochMarker(0),\n-      ReceiverRow(0, unsafeRow(111))\n-    )\n+  private implicit def unsafeRow(value: Int) = {",
    "line": 45
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "writer.write(Iterator(1, 2, 3)) and such leverages the implicit.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-31T22:19:00Z",
    "diffHunk": "@@ -58,39 +46,29 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     super.afterEach()\n   }\n \n-  test(\"receiver stopped with row last\") {\n-    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n-    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n-    send(\n-      endpoint,\n-      ReceiverEpochMarker(0),\n-      ReceiverRow(0, unsafeRow(111))\n-    )\n+  private implicit def unsafeRow(value: Int) = {",
    "line": 45
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "there isnt a test where a RPCContinuousShuffleWriter writes to multiple reader endpoints.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T00:43:56Z",
    "diffHunk": "@@ -288,4 +267,153 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     val thirdEpoch = rdd.compute(rdd.partitions(0), ctx).map(_.getUTF8String(0).toString).toSet\n     assert(thirdEpoch == Set(\"writer1-row1\", \"writer2-row0\"))\n   }\n+\n+  test(\"one epoch\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+    writer.write(Iterator(4, 5, 6))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+    assert(readEpoch(reader) == Seq(4, 5, 6))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator())\n+    writer.write(Iterator(1, 2))\n+    writer.write(Iterator())\n+    writer.write(Iterator())\n+    writer.write(Iterator(3, 4))\n+    writer.write(Iterator())\n+\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(1, 2))\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(3, 4))\n+    assert(readEpoch(reader) == Seq())\n+  }\n+\n+  test(\"blocks waiting for writer\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readRowThread = new Thread {\n+      override def run(): Unit = {\n+        assert(readerEpoch.toSeq.map(_.getInt(0)) == Seq(1))\n+      }\n+    }\n+    readRowThread.start()\n+\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRowThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    // Once we write the epoch the thread should stop waiting and succeed.\n+    writer.write(Iterator(1))\n+    readRowThread.join()\n+  }\n+\n+  test(\"multiple writer partitions\") {\n+    val numWriterPartitions = 3\n+\n+    val reader = new ContinuousShuffleReadRDD(\n+      sparkContext, numPartitions = 1, numShuffleWriters = numWriterPartitions)\n+    val writers = (0 until 3).map { idx =>\n+      new RPCContinuousShuffleWriter(idx, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+    }\n+\n+    writers(0).write(Iterator(1, 4, 7))\n+    writers(1).write(Iterator(2, 5))\n+    writers(2).write(Iterator(3, 6))\n+\n+    writers(0).write(Iterator(4, 7, 10))\n+    writers(1).write(Iterator(5, 8))\n+    writers(2).write(Iterator(6, 9))\n+\n+    // Since there are multiple asynchronous writers, the original row sequencing is not guaranteed.\n+    // The epochs should be deterministically preserved, however.\n+    assert(readEpoch(reader).toSet == Seq(1, 2, 3, 4, 5, 6, 7).toSet)\n+    assert(readEpoch(reader).toSet == Seq(4, 5, 6, 7, 8, 9, 10).toSet)\n+  }\n+\n+  test(\"reader epoch only ends when all writer partitions write it\") {\n+    val numWriterPartitions = 3\n+\n+    val reader = new ContinuousShuffleReadRDD(\n+      sparkContext, numPartitions = 1, numShuffleWriters = numWriterPartitions)\n+    val writers = (0 until 3).map { idx =>\n+      new RPCContinuousShuffleWriter(idx, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+    }\n+\n+    writers(1).write(Iterator())\n+    writers(2).write(Iterator())\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readEpochMarkerThread = new Thread {\n+      override def run(): Unit = {\n+        assert(!readerEpoch.hasNext)\n+      }\n+    }\n+\n+    readEpochMarkerThread.start()\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readEpochMarkerThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    writers(0).write(Iterator())\n+    readEpochMarkerThread.join()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    send(\n+      endpoint,\n+      ReceiverEpochMarker(0),\n+      ReceiverRow(0, unsafeRow(111))\n+    )\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[RPCContinuousShuffleReader].stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    send(\n+      endpoint,\n+      ReceiverRow(0, unsafeRow(111)),\n+      ReceiverEpochMarker(0)\n+    )\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[RPCContinuousShuffleReader].stopped.get())\n+    }\n+  }",
    "line": 301
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "Discussed offline and above - this is a deliberate limitation of the PR.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-31T22:33:06Z",
    "diffHunk": "@@ -288,4 +267,153 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     val thirdEpoch = rdd.compute(rdd.partitions(0), ctx).map(_.getUTF8String(0).toString).toSet\n     assert(thirdEpoch == Set(\"writer1-row1\", \"writer2-row0\"))\n   }\n+\n+  test(\"one epoch\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+    writer.write(Iterator(4, 5, 6))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+    assert(readEpoch(reader) == Seq(4, 5, 6))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator())\n+    writer.write(Iterator(1, 2))\n+    writer.write(Iterator())\n+    writer.write(Iterator())\n+    writer.write(Iterator(3, 4))\n+    writer.write(Iterator())\n+\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(1, 2))\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(3, 4))\n+    assert(readEpoch(reader) == Seq())\n+  }\n+\n+  test(\"blocks waiting for writer\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readRowThread = new Thread {\n+      override def run(): Unit = {\n+        assert(readerEpoch.toSeq.map(_.getInt(0)) == Seq(1))\n+      }\n+    }\n+    readRowThread.start()\n+\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRowThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    // Once we write the epoch the thread should stop waiting and succeed.\n+    writer.write(Iterator(1))\n+    readRowThread.join()\n+  }\n+\n+  test(\"multiple writer partitions\") {\n+    val numWriterPartitions = 3\n+\n+    val reader = new ContinuousShuffleReadRDD(\n+      sparkContext, numPartitions = 1, numShuffleWriters = numWriterPartitions)\n+    val writers = (0 until 3).map { idx =>\n+      new RPCContinuousShuffleWriter(idx, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+    }\n+\n+    writers(0).write(Iterator(1, 4, 7))\n+    writers(1).write(Iterator(2, 5))\n+    writers(2).write(Iterator(3, 6))\n+\n+    writers(0).write(Iterator(4, 7, 10))\n+    writers(1).write(Iterator(5, 8))\n+    writers(2).write(Iterator(6, 9))\n+\n+    // Since there are multiple asynchronous writers, the original row sequencing is not guaranteed.\n+    // The epochs should be deterministically preserved, however.\n+    assert(readEpoch(reader).toSet == Seq(1, 2, 3, 4, 5, 6, 7).toSet)\n+    assert(readEpoch(reader).toSet == Seq(4, 5, 6, 7, 8, 9, 10).toSet)\n+  }\n+\n+  test(\"reader epoch only ends when all writer partitions write it\") {\n+    val numWriterPartitions = 3\n+\n+    val reader = new ContinuousShuffleReadRDD(\n+      sparkContext, numPartitions = 1, numShuffleWriters = numWriterPartitions)\n+    val writers = (0 until 3).map { idx =>\n+      new RPCContinuousShuffleWriter(idx, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+    }\n+\n+    writers(1).write(Iterator())\n+    writers(2).write(Iterator())\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readEpochMarkerThread = new Thread {\n+      override def run(): Unit = {\n+        assert(!readerEpoch.hasNext)\n+      }\n+    }\n+\n+    readEpochMarkerThread.start()\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readEpochMarkerThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    writers(0).write(Iterator())\n+    readEpochMarkerThread.join()\n+  }\n+\n+  test(\"receiver stopped with row last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    send(\n+      endpoint,\n+      ReceiverEpochMarker(0),\n+      ReceiverRow(0, unsafeRow(111))\n+    )\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[RPCContinuousShuffleReader].stopped.get())\n+    }\n+  }\n+\n+  test(\"receiver stopped with marker last\") {\n+    val rdd = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val endpoint = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].endpoint\n+    send(\n+      endpoint,\n+      ReceiverRow(0, unsafeRow(111)),\n+      ReceiverEpochMarker(0)\n+    )\n+\n+    ctx.markTaskCompleted(None)\n+    val receiver = rdd.partitions(0).asInstanceOf[ContinuousShuffleReadPartition].reader\n+    eventually(timeout(streamingTimeout)) {\n+      assert(receiver.asInstanceOf[RPCContinuousShuffleReader].stopped.get())\n+    }\n+  }",
    "line": 301
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Would we want to have another test which covers out-of-order epoch between writers (if that's valid case for us), or rely on the test in ContinuousShuffleReadRDD?",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-30T02:47:00Z",
    "diffHunk": "@@ -288,4 +267,153 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     val thirdEpoch = rdd.compute(rdd.partitions(0), ctx).map(_.getUTF8String(0).toString).toSet\n     assert(thirdEpoch == Set(\"writer1-row1\", \"writer2-row0\"))\n   }\n+\n+  test(\"one epoch\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+    writer.write(Iterator(4, 5, 6))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+    assert(readEpoch(reader) == Seq(4, 5, 6))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator())\n+    writer.write(Iterator(1, 2))\n+    writer.write(Iterator())\n+    writer.write(Iterator())\n+    writer.write(Iterator(3, 4))\n+    writer.write(Iterator())\n+\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(1, 2))\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(3, 4))\n+    assert(readEpoch(reader) == Seq())\n+  }\n+\n+  test(\"blocks waiting for writer\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readRowThread = new Thread {\n+      override def run(): Unit = {\n+        assert(readerEpoch.toSeq.map(_.getInt(0)) == Seq(1))\n+      }\n+    }\n+    readRowThread.start()\n+\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRowThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    // Once we write the epoch the thread should stop waiting and succeed.\n+    writer.write(Iterator(1))\n+    readRowThread.join()\n+  }\n+\n+  test(\"multiple writer partitions\") {"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I think \"reader epoch only ends when all writer partitions write it\" is a sufficient test for that.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-05-31T22:20:14Z",
    "diffHunk": "@@ -288,4 +267,153 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     val thirdEpoch = rdd.compute(rdd.partitions(0), ctx).map(_.getUTF8String(0).toString).toSet\n     assert(thirdEpoch == Set(\"writer1-row1\", \"writer2-row0\"))\n   }\n+\n+  test(\"one epoch\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+    writer.write(Iterator(4, 5, 6))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+    assert(readEpoch(reader) == Seq(4, 5, 6))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator())\n+    writer.write(Iterator(1, 2))\n+    writer.write(Iterator())\n+    writer.write(Iterator())\n+    writer.write(Iterator(3, 4))\n+    writer.write(Iterator())\n+\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(1, 2))\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(3, 4))\n+    assert(readEpoch(reader) == Seq())\n+  }\n+\n+  test(\"blocks waiting for writer\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readRowThread = new Thread {\n+      override def run(): Unit = {\n+        assert(readerEpoch.toSeq.map(_.getInt(0)) == Seq(1))\n+      }\n+    }\n+    readRowThread.start()\n+\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRowThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    // Once we write the epoch the thread should stop waiting and succeed.\n+    writer.write(Iterator(1))\n+    readRowThread.join()\n+  }\n+\n+  test(\"multiple writer partitions\") {"
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: it's better to add a timeout here, such as `readRowThread.join(streamingTimeout.toMillis)`. Without a timeout, if there is a bug causing this hang, we will need to wait until the jenkins build timeout, which is much longer.",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-06-12T23:03:48Z",
    "diffHunk": "@@ -288,4 +264,153 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     val thirdEpoch = rdd.compute(rdd.partitions(0), ctx).map(_.getUTF8String(0).toString).toSet\n     assert(thirdEpoch == Set(\"writer1-row1\", \"writer2-row0\"))\n   }\n+\n+  test(\"one epoch\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+    writer.write(Iterator(4, 5, 6))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+    assert(readEpoch(reader) == Seq(4, 5, 6))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator())\n+    writer.write(Iterator(1, 2))\n+    writer.write(Iterator())\n+    writer.write(Iterator())\n+    writer.write(Iterator(3, 4))\n+    writer.write(Iterator())\n+\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(1, 2))\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(3, 4))\n+    assert(readEpoch(reader) == Seq())\n+  }\n+\n+  test(\"blocks waiting for writer\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readRowThread = new Thread {\n+      override def run(): Unit = {\n+        assert(readerEpoch.toSeq.map(_.getInt(0)) == Seq(1))\n+      }\n+    }\n+    readRowThread.start()\n+\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRowThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    // Once we write the epoch the thread should stop waiting and succeed.\n+    writer.write(Iterator(1))\n+    readRowThread.join()"
  }],
  "prId": 21428
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "ditto",
    "commit": "cff37c45f084d50a0844fbe8481565f6a9985302",
    "createdAt": "2018-06-12T23:04:08Z",
    "diffHunk": "@@ -288,4 +264,153 @@ class ContinuousShuffleReadSuite extends StreamTest {\n     val thirdEpoch = rdd.compute(rdd.partitions(0), ctx).map(_.getUTF8String(0).toString).toSet\n     assert(thirdEpoch == Set(\"writer1-row1\", \"writer2-row0\"))\n   }\n+\n+  test(\"one epoch\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+  }\n+\n+  test(\"multiple epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator(1, 2, 3))\n+    writer.write(Iterator(4, 5, 6))\n+\n+    assert(readEpoch(reader) == Seq(1, 2, 3))\n+    assert(readEpoch(reader) == Seq(4, 5, 6))\n+  }\n+\n+  test(\"empty epochs\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    writer.write(Iterator())\n+    writer.write(Iterator(1, 2))\n+    writer.write(Iterator())\n+    writer.write(Iterator())\n+    writer.write(Iterator(3, 4))\n+    writer.write(Iterator())\n+\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(1, 2))\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq())\n+    assert(readEpoch(reader) == Seq(3, 4))\n+    assert(readEpoch(reader) == Seq())\n+  }\n+\n+  test(\"blocks waiting for writer\") {\n+    val reader = new ContinuousShuffleReadRDD(sparkContext, numPartitions = 1)\n+    val writer = new RPCContinuousShuffleWriter(\n+      0, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readRowThread = new Thread {\n+      override def run(): Unit = {\n+        assert(readerEpoch.toSeq.map(_.getInt(0)) == Seq(1))\n+      }\n+    }\n+    readRowThread.start()\n+\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readRowThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    // Once we write the epoch the thread should stop waiting and succeed.\n+    writer.write(Iterator(1))\n+    readRowThread.join()\n+  }\n+\n+  test(\"multiple writer partitions\") {\n+    val numWriterPartitions = 3\n+\n+    val reader = new ContinuousShuffleReadRDD(\n+      sparkContext, numPartitions = 1, numShuffleWriters = numWriterPartitions)\n+    val writers = (0 until 3).map { idx =>\n+      new RPCContinuousShuffleWriter(idx, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+    }\n+\n+    writers(0).write(Iterator(1, 4, 7))\n+    writers(1).write(Iterator(2, 5))\n+    writers(2).write(Iterator(3, 6))\n+\n+    writers(0).write(Iterator(4, 7, 10))\n+    writers(1).write(Iterator(5, 8))\n+    writers(2).write(Iterator(6, 9))\n+\n+    // Since there are multiple asynchronous writers, the original row sequencing is not guaranteed.\n+    // The epochs should be deterministically preserved, however.\n+    assert(readEpoch(reader).toSet == Seq(1, 2, 3, 4, 5, 6, 7).toSet)\n+    assert(readEpoch(reader).toSet == Seq(4, 5, 6, 7, 8, 9, 10).toSet)\n+  }\n+\n+  test(\"reader epoch only ends when all writer partitions write it\") {\n+    val numWriterPartitions = 3\n+\n+    val reader = new ContinuousShuffleReadRDD(\n+      sparkContext, numPartitions = 1, numShuffleWriters = numWriterPartitions)\n+    val writers = (0 until 3).map { idx =>\n+      new RPCContinuousShuffleWriter(idx, new HashPartitioner(1), Array(readRDDEndpoint(reader)))\n+    }\n+\n+    writers(1).write(Iterator())\n+    writers(2).write(Iterator())\n+\n+    val readerEpoch = reader.compute(reader.partitions(0), ctx)\n+\n+    val readEpochMarkerThread = new Thread {\n+      override def run(): Unit = {\n+        assert(!readerEpoch.hasNext)\n+      }\n+    }\n+\n+    readEpochMarkerThread.start()\n+    eventually(timeout(streamingTimeout)) {\n+      assert(readEpochMarkerThread.getState == Thread.State.TIMED_WAITING)\n+    }\n+\n+    writers(0).write(Iterator())\n+    readEpochMarkerThread.join()"
  }],
  "prId": 21428
}]