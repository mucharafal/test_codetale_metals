[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Why are these tests commented out?\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-01T18:49:57Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.hive.ql.io.orc.CompressionKind\r\n+import org.apache.spark.sql.{SQLConf, SchemaRDD, TestData, QueryTest}\r\n+import org.apache.spark.sql.test.TestSQLContext\r\n+import org.scalatest.{BeforeAndAfterAll, FunSuiteLike}\r\n+import org.apache.spark.util.Utils\r\n+import org.apache.spark.sql.catalyst.util.getTempFilePath\r\n+import org.apache.spark.sql.test.TestSQLContext._\r\n+\r\n+import java.io.File\r\n+\r\n+case class TestRDDEntry(key: Int, value: String)\r\n+\r\n+case class NullReflectData(\r\n+    intField: java.lang.Integer,\r\n+    longField: java.lang.Long,\r\n+    floatField: java.lang.Float,\r\n+    doubleField: java.lang.Double,\r\n+    booleanField: java.lang.Boolean)\r\n+\r\n+case class OptionalReflectData(\r\n+    intField: Option[Int],\r\n+    longField: Option[Long],\r\n+    floatField: Option[Float],\r\n+    doubleField: Option[Double],\r\n+    booleanField: Option[Boolean])\r\n+\r\n+case class Nested(i: Int, s: String)\r\n+\r\n+case class Data(array: Seq[Int], nested: Nested)\r\n+\r\n+case class AllDataTypes(\r\n+    stringField: String,\r\n+    intField: Int,\r\n+    longField: Long,\r\n+    floatField: Float,\r\n+    doubleField: Double,\r\n+    shortField: Short,\r\n+    byteField: Byte,\r\n+    booleanField: Boolean)\r\n+\r\n+case class AllDataTypesWithNonPrimitiveType(\r\n+    stringField: String,\r\n+    intField: Int,\r\n+    longField: Long,\r\n+    floatField: Float,\r\n+    doubleField: Double,\r\n+    shortField: Short,\r\n+    byteField: Byte,\r\n+    booleanField: Boolean,\r\n+    array: Seq[Int],\r\n+    arrayContainsNull: Seq[Option[Int]],\r\n+    map: Map[Int, Long],\r\n+    mapValueContainsNull: Map[Int, Option[Long]],\r\n+    data: Data)\r\n+\r\n+case class BinaryData(binaryData: Array[Byte])\r\n+\r\n+class OrcQuerySuite extends QueryTest with FunSuiteLike with BeforeAndAfterAll {\r\n+  TestData // Load test data tables.\r\n+\r\n+  var testRDD: SchemaRDD = null\r\n+  test(\"Read/Write All Types\") {\r\n+    val tempDir = getTempFilePath(\"orcTest\").getCanonicalPath\r\n+    val range = (0 to 255)\r\n+    val data = sparkContext.parallelize(range)\r\n+      .map(x => AllDataTypes(s\"$x\", x, x.toLong, x.toFloat, x.toDouble, x.toShort, x.toByte, x % 2 == 0))\r\n+\r\n+    data.saveAsOrcFile(tempDir)\r\n+\r\n+    checkAnswer(\r\n+      orcFile(tempDir),\r\n+      data.toSchemaRDD.collect().toSeq)\r\n+\r\n+    Utils.deleteRecursively(new File(tempDir))\r\n+\r\n+  }\r\n+\r\n+  test(\"Compression options for writing to a Orcfile\") {\r\n+    val defaultOrcCompressionCodec = TestSQLContext.orcCompressionCodec\r\n+    //TODO: support other compress codec\r\n+    val file = getTempFilePath(\"orcTest\")\r\n+    val path = file.toString\r\n+    val rdd = TestSQLContext.sparkContext.parallelize((1 to 100))\r\n+      .map(i => TestRDDEntry(i, s\"val_$i\"))\r\n+\r\n+    // test default compression codec, now only support zlib\r\n+    rdd.saveAsOrcFile(path)\r\n+    var actualCodec = OrcFileOperator.readMetaData(new Path(path)).getCompression.name\r\n+    assert(actualCodec == TestSQLContext.orcCompressionCodec.toUpperCase)\r\n+\r\n+    /**\r"
  }, {
    "author": {
      "login": "scwf"
    },
    "body": "now only support zlib, i will remove this\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-02T03:27:51Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.orc\r\n+\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.hadoop.hive.ql.io.orc.CompressionKind\r\n+import org.apache.spark.sql.{SQLConf, SchemaRDD, TestData, QueryTest}\r\n+import org.apache.spark.sql.test.TestSQLContext\r\n+import org.scalatest.{BeforeAndAfterAll, FunSuiteLike}\r\n+import org.apache.spark.util.Utils\r\n+import org.apache.spark.sql.catalyst.util.getTempFilePath\r\n+import org.apache.spark.sql.test.TestSQLContext._\r\n+\r\n+import java.io.File\r\n+\r\n+case class TestRDDEntry(key: Int, value: String)\r\n+\r\n+case class NullReflectData(\r\n+    intField: java.lang.Integer,\r\n+    longField: java.lang.Long,\r\n+    floatField: java.lang.Float,\r\n+    doubleField: java.lang.Double,\r\n+    booleanField: java.lang.Boolean)\r\n+\r\n+case class OptionalReflectData(\r\n+    intField: Option[Int],\r\n+    longField: Option[Long],\r\n+    floatField: Option[Float],\r\n+    doubleField: Option[Double],\r\n+    booleanField: Option[Boolean])\r\n+\r\n+case class Nested(i: Int, s: String)\r\n+\r\n+case class Data(array: Seq[Int], nested: Nested)\r\n+\r\n+case class AllDataTypes(\r\n+    stringField: String,\r\n+    intField: Int,\r\n+    longField: Long,\r\n+    floatField: Float,\r\n+    doubleField: Double,\r\n+    shortField: Short,\r\n+    byteField: Byte,\r\n+    booleanField: Boolean)\r\n+\r\n+case class AllDataTypesWithNonPrimitiveType(\r\n+    stringField: String,\r\n+    intField: Int,\r\n+    longField: Long,\r\n+    floatField: Float,\r\n+    doubleField: Double,\r\n+    shortField: Short,\r\n+    byteField: Byte,\r\n+    booleanField: Boolean,\r\n+    array: Seq[Int],\r\n+    arrayContainsNull: Seq[Option[Int]],\r\n+    map: Map[Int, Long],\r\n+    mapValueContainsNull: Map[Int, Option[Long]],\r\n+    data: Data)\r\n+\r\n+case class BinaryData(binaryData: Array[Byte])\r\n+\r\n+class OrcQuerySuite extends QueryTest with FunSuiteLike with BeforeAndAfterAll {\r\n+  TestData // Load test data tables.\r\n+\r\n+  var testRDD: SchemaRDD = null\r\n+  test(\"Read/Write All Types\") {\r\n+    val tempDir = getTempFilePath(\"orcTest\").getCanonicalPath\r\n+    val range = (0 to 255)\r\n+    val data = sparkContext.parallelize(range)\r\n+      .map(x => AllDataTypes(s\"$x\", x, x.toLong, x.toFloat, x.toDouble, x.toShort, x.toByte, x % 2 == 0))\r\n+\r\n+    data.saveAsOrcFile(tempDir)\r\n+\r\n+    checkAnswer(\r\n+      orcFile(tempDir),\r\n+      data.toSchemaRDD.collect().toSeq)\r\n+\r\n+    Utils.deleteRecursively(new File(tempDir))\r\n+\r\n+  }\r\n+\r\n+  test(\"Compression options for writing to a Orcfile\") {\r\n+    val defaultOrcCompressionCodec = TestSQLContext.orcCompressionCodec\r\n+    //TODO: support other compress codec\r\n+    val file = getTempFilePath(\"orcTest\")\r\n+    val path = file.toString\r\n+    val rdd = TestSQLContext.sparkContext.parallelize((1 to 100))\r\n+      .map(i => TestRDDEntry(i, s\"val_$i\"))\r\n+\r\n+    // test default compression codec, now only support zlib\r\n+    rdd.saveAsOrcFile(path)\r\n+    var actualCodec = OrcFileOperator.readMetaData(new Path(path)).getCompression.name\r\n+    assert(actualCodec == TestSQLContext.orcCompressionCodec.toUpperCase)\r\n+\r\n+    /**\r"
  }],
  "prId": 2576
}]