[{
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "why is this?",
    "commit": "53a65fb0d2ab0b8f9d641967d55c7c03e66fe787",
    "createdAt": "2017-06-09T18:46:01Z",
    "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.ManualClock\n+\n+class RateSourceSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  case class AdvanceRateManualClock(seconds: Long) extends AddData {\n+    override def addData(query: Option[StreamExecution]): (Source, Offset) = {\n+      assert(query.nonEmpty)\n+      val rateSource = query.get.logicalPlan.collect {\n+        case StreamingExecutionRelation(source, _) if source.isInstanceOf[RateStreamSource] =>\n+          source.asInstanceOf[RateStreamSource]\n+      }.head\n+      rateSource.clock.asInstanceOf[ManualClock].advance(TimeUnit.SECONDS.toMillis(seconds))\n+      (rateSource, rateSource.getOffset.get)\n+    }\n+  }\n+\n+  test(\"basic\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"tuplesPerSecond\", \"10\")\n+      .option(\"useManualClock\", \"true\")\n+      .load()\n+    testStream(input)(\n+      AdvanceRateManualClock(seconds = 1),\n+      CheckLastBatch((0 until 10).map(v => new java.sql.Timestamp(v * 100L) -> v): _*),\n+      StopStream,\n+      StartStream(),\n+      // Advance 2 seconds because creating a new RateSource will also create a new ManualClock\n+      AdvanceRateManualClock(seconds = 2),\n+      CheckLastBatch((10 until 20).map(v => new java.sql.Timestamp(v * 100L) -> v): _*)\n+    )\n+  }\n+\n+  test(\"uniform distribution of event timestamps: tuplesPerSecond > 1000\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"tuplesPerSecond\", \"1500\")\n+      .option(\"useManualClock\", \"true\")\n+      .load()\n+      .as[(java.sql.Timestamp, Long)]\n+      .map(v => (v._1.getTime, v._2))\n+    val expectedAnswer =\n+      (0 until 1000).map(v => (v / 2, v)) ++ // Two values share the same timestamp."
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Because there are 1000 timestamps in one second but we have 1500 values.",
    "commit": "53a65fb0d2ab0b8f9d641967d55c7c03e66fe787",
    "createdAt": "2017-06-09T18:59:35Z",
    "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.ManualClock\n+\n+class RateSourceSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  case class AdvanceRateManualClock(seconds: Long) extends AddData {\n+    override def addData(query: Option[StreamExecution]): (Source, Offset) = {\n+      assert(query.nonEmpty)\n+      val rateSource = query.get.logicalPlan.collect {\n+        case StreamingExecutionRelation(source, _) if source.isInstanceOf[RateStreamSource] =>\n+          source.asInstanceOf[RateStreamSource]\n+      }.head\n+      rateSource.clock.asInstanceOf[ManualClock].advance(TimeUnit.SECONDS.toMillis(seconds))\n+      (rateSource, rateSource.getOffset.get)\n+    }\n+  }\n+\n+  test(\"basic\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"tuplesPerSecond\", \"10\")\n+      .option(\"useManualClock\", \"true\")\n+      .load()\n+    testStream(input)(\n+      AdvanceRateManualClock(seconds = 1),\n+      CheckLastBatch((0 until 10).map(v => new java.sql.Timestamp(v * 100L) -> v): _*),\n+      StopStream,\n+      StartStream(),\n+      // Advance 2 seconds because creating a new RateSource will also create a new ManualClock\n+      AdvanceRateManualClock(seconds = 2),\n+      CheckLastBatch((10 until 20).map(v => new java.sql.Timestamp(v * 100L) -> v): _*)\n+    )\n+  }\n+\n+  test(\"uniform distribution of event timestamps: tuplesPerSecond > 1000\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"tuplesPerSecond\", \"1500\")\n+      .option(\"useManualClock\", \"true\")\n+      .load()\n+      .as[(java.sql.Timestamp, Long)]\n+      .map(v => (v._1.getTime, v._2))\n+    val expectedAnswer =\n+      (0 until 1000).map(v => (v / 2, v)) ++ // Two values share the same timestamp."
  }],
  "prId": 18199
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "would be nice to add one test where `rampUpTimeSeconds = 0`",
    "commit": "53a65fb0d2ab0b8f9d641967d55c7c03e66fe787",
    "createdAt": "2017-06-09T23:39:11Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.TimeUnit\n+\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.streaming.{StreamingQueryException, StreamTest}\n+import org.apache.spark.util.ManualClock\n+\n+class RateSourceSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  case class AdvanceRateManualClock(seconds: Long) extends AddData {\n+    override def addData(query: Option[StreamExecution]): (Source, Offset) = {\n+      assert(query.nonEmpty)\n+      val rateSource = query.get.logicalPlan.collect {\n+        case StreamingExecutionRelation(source, _) if source.isInstanceOf[RateStreamSource] =>\n+          source.asInstanceOf[RateStreamSource]\n+      }.head\n+      rateSource.clock.asInstanceOf[ManualClock].advance(TimeUnit.SECONDS.toMillis(seconds))\n+      (rateSource, rateSource.getOffset.get)\n+    }\n+  }\n+\n+  test(\"basic\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"rowsPerSecond\", \"10\")\n+      .option(\"useManualClock\", \"true\")\n+      .load()\n+    testStream(input)(\n+      AdvanceRateManualClock(seconds = 1),\n+      CheckLastBatch((0 until 10).map(v => new java.sql.Timestamp(v * 100L) -> v): _*),\n+      StopStream,\n+      StartStream(),\n+      // Advance 2 seconds because creating a new RateSource will also create a new ManualClock\n+      AdvanceRateManualClock(seconds = 2),\n+      CheckLastBatch((10 until 20).map(v => new java.sql.Timestamp(v * 100L) -> v): _*)\n+    )\n+  }\n+\n+  test(\"uniform distribution of event timestamps\") {\n+    val input = spark.readStream\n+      .format(\"rate\")\n+      .option(\"rowsPerSecond\", \"1500\")\n+      .option(\"useManualClock\", \"true\")\n+      .load()\n+      .as[(java.sql.Timestamp, Long)]\n+      .map(v => (v._1.getTime, v._2))\n+    val expectedAnswer = (0 until 1500).map { v =>\n+      (math.round(v * (1000.0 / 1500)), v)\n+    }\n+    testStream(input)(\n+      AdvanceRateManualClock(seconds = 1),\n+      CheckLastBatch(expectedAnswer: _*)\n+    )\n+  }\n+\n+  test(\"valueAtSecond\") {\n+    import RateStreamSource._\n+\n+    assert(valueAtSecond(seconds = 0, rowsPerSecond = 5, rampUpTimeSeconds = 2) === 0)"
  }],
  "prId": 18199
}]