[{
  "comments": [{
    "author": {
      "login": "wangyum"
    },
    "body": "Function name should be `explode`.",
    "commit": "623998bfb7596f92c603e5be378d23b0e3a3b1ea",
    "createdAt": "2018-09-20T15:47:23Z",
    "diffHunk": "@@ -17,251 +17,154 @@\n \n package org.apache.spark.sql.execution.benchmark\n \n-import org.apache.spark.util.Benchmark\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.util.{Benchmark, BenchmarkBase => FileBenchmarkBase}\n \n /**\n  * Benchmark to measure whole stage codegen performance.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.MiscBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * 1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ * 2. build/sbt \"sql/test:runMain <this class>\"\n+ * 3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *    Results will be written to \"benchmarks/MiscBenchmark-results.txt\".\n  */\n-class MiscBenchmark extends BenchmarkBase {\n-\n-  ignore(\"filter & aggregate without group\") {\n-    val N = 500L << 22\n-    runBenchmark(\"range/filter/sum\", N) {\n-      sparkSession.range(N).filter(\"(id & 1) = 1\").groupBy().sum().collect()\n+object MiscBenchmark extends FileBenchmarkBase {\n+\n+  lazy val sparkSession = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"microbenchmark\")\n+    .config(\"spark.sql.shuffle.partitions\", 1)\n+    .config(\"spark.sql.autoBroadcastJoinThreshold\", 1)\n+    .getOrCreate()\n+\n+  /** Runs function `f` with whole stage codegen on and off. */\n+  def runMiscBenchmark(name: String, cardinality: Long)(f: => Unit): Unit = {\n+    val benchmark = new Benchmark(name, cardinality, output = output)\n+\n+    benchmark.addCase(s\"$name wholestage off\", numIters = 2) { iter =>\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\n+      f\n     }\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Mac OS X 10.11\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    range/filter/sum:                        Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    range/filter/sum codegen=false              30663 / 31216         68.4          14.6       1.0X\n-    range/filter/sum codegen=true                 2399 / 2409        874.1           1.1      12.8X\n-    */\n-  }\n \n-  ignore(\"range/limit/sum\") {\n-    val N = 500L << 20\n-    runBenchmark(\"range/limit/sum\", N) {\n-      sparkSession.range(N).limit(1000000).groupBy().sum().collect()\n+    benchmark.addCase(s\"$name wholestage on\", numIters = 5) { iter =>\n+      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n+      f\n     }\n-    /*\n-    Westmere E56xx/L56xx/X56xx (Nehalem-C)\n-    range/limit/sum:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    -------------------------------------------------------------------------------------------\n-    range/limit/sum codegen=false             609 /  672        861.6           1.2       1.0X\n-    range/limit/sum codegen=true              561 /  621        935.3           1.1       1.1X\n-    */\n-  }\n \n-  ignore(\"sample\") {\n-    val N = 500 << 18\n-    runBenchmark(\"sample with replacement\", N) {\n-      sparkSession.range(N).sample(withReplacement = true, 0.01).groupBy().sum().collect()\n-    }\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Mac OS X 10.11\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    sample with replacement:                 Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    sample with replacement codegen=false         7073 / 7227         18.5          54.0       1.0X\n-    sample with replacement codegen=true          5199 / 5203         25.2          39.7       1.4X\n-    */\n-\n-    runBenchmark(\"sample without replacement\", N) {\n-      sparkSession.range(N).sample(withReplacement = false, 0.01).groupBy().sum().collect()\n-    }\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Mac OS X 10.11\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    sample without replacement:              Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    sample without replacement codegen=false      1508 / 1529         86.9          11.5       1.0X\n-    sample without replacement codegen=true        644 /  662        203.5           4.9       2.3X\n-    */\n-  }\n-\n-  ignore(\"collect\") {\n-    val N = 1 << 20\n-\n-    val benchmark = new Benchmark(\"collect\", N)\n-    benchmark.addCase(\"collect 1 million\") { iter =>\n-      sparkSession.range(N).collect()\n-    }\n-    benchmark.addCase(\"collect 2 millions\") { iter =>\n-      sparkSession.range(N * 2).collect()\n-    }\n-    benchmark.addCase(\"collect 4 millions\") { iter =>\n-      sparkSession.range(N * 4).collect()\n-    }\n     benchmark.run()\n-\n-    /*\n-    Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-    collect:                            Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    -------------------------------------------------------------------------------------------\n-    collect 1 million                         439 /  654          2.4         418.7       1.0X\n-    collect 2 millions                        961 / 1907          1.1         916.4       0.5X\n-    collect 4 millions                       3193 / 3895          0.3        3044.7       0.1X\n-     */\n   }\n \n-  ignore(\"collect limit\") {\n-    val N = 1 << 20\n-\n-    val benchmark = new Benchmark(\"collect limit\", N)\n-    benchmark.addCase(\"collect limit 1 million\") { iter =>\n-      sparkSession.range(N * 4).limit(N).collect()\n-    }\n-    benchmark.addCase(\"collect limit 2 millions\") { iter =>\n-      sparkSession.range(N * 4).limit(N * 2).collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"filter & aggregate without group\") {\n+      val N = 500L << 22\n+      runMiscBenchmark(\"range/filter/sum\", N) {\n+        sparkSession.range(N).filter(\"(id & 1) = 1\").groupBy().sum().collect()\n+      }\n     }\n-    benchmark.run()\n \n-    /*\n-    model name      : Westmere E56xx/L56xx/X56xx (Nehalem-C)\n-    collect limit:                      Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    -------------------------------------------------------------------------------------------\n-    collect limit 1 million                   833 / 1284          1.3         794.4       1.0X\n-    collect limit 2 millions                 3348 / 4005          0.3        3193.3       0.2X\n-     */\n-  }\n-\n-  ignore(\"generate explode\") {\n-    val N = 1 << 24\n-    runBenchmark(\"generate explode array\", N) {\n-      val df = sparkSession.range(N).selectExpr(\n-        \"id as key\",\n-        \"array(rand(), rand(), rand(), rand(), rand()) as values\")\n-      df.selectExpr(\"key\", \"explode(values) value\").count()\n+    runBenchmark(\"range/limit/sum\") {\n+      val N = 500L << 20\n+      runMiscBenchmark(\"range/limit/sum\", N) {\n+        sparkSession.range(N).limit(1000000).groupBy().sum().collect()\n+      }\n     }\n \n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.11.6\n-    Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\n-\n-    generate explode array:                  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    generate explode array wholestage off         6920 / 7129          2.4         412.5       1.0X\n-    generate explode array wholestage on           623 /  646         26.9          37.1      11.1X\n-     */\n+    runBenchmark(\"sample\") {\n+      val N = 500 << 18\n+      runMiscBenchmark(\"sample with replacement\", N) {\n+        sparkSession.range(N).sample(withReplacement = true, 0.01).groupBy().sum().collect()\n+      }\n \n-    runBenchmark(\"generate explode map\", N) {\n-      val df = sparkSession.range(N).selectExpr(\n-        \"id as key\",\n-        \"map('a', rand(), 'b', rand(), 'c', rand(), 'd', rand(), 'e', rand()) pairs\")\n-      df.selectExpr(\"key\", \"explode(pairs) as (k, v)\").count()\n+      runMiscBenchmark(\"sample without replacement\", N) {\n+        sparkSession.range(N).sample(withReplacement = false, 0.01).groupBy().sum().collect()\n+      }\n     }\n \n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.11.6\n-    Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\n-\n-    generate explode map:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    generate explode map wholestage off         11978 / 11993          1.4         714.0       1.0X\n-    generate explode map wholestage on             866 /  919         19.4          51.6      13.8X\n-     */\n-\n-    runBenchmark(\"generate posexplode array\", N) {\n-      val df = sparkSession.range(N).selectExpr(\n-        \"id as key\",\n-        \"array(rand(), rand(), rand(), rand(), rand()) as values\")\n-      df.selectExpr(\"key\", \"posexplode(values) as (idx, value)\").count()\n+    runBenchmark(\"collect\") {\n+      val N = 1 << 20\n+\n+      val benchmark = new Benchmark(\"collect\", N, output = output)\n+      benchmark.addCase(\"collect 1 million\") { iter =>\n+        sparkSession.range(N).collect()\n+      }\n+      benchmark.addCase(\"collect 2 millions\") { iter =>\n+        sparkSession.range(N * 2).collect()\n+      }\n+      benchmark.addCase(\"collect 4 millions\") { iter =>\n+        sparkSession.range(N * 4).collect()\n+      }\n+      benchmark.run()\n     }\n \n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.11.6\n-    Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\n-\n-    generate posexplode array:               Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    generate posexplode array wholestage off      7502 / 7513          2.2         447.1       1.0X\n-    generate posexplode array wholestage on        617 /  623         27.2          36.8      12.2X\n-     */\n-\n-    runBenchmark(\"generate inline array\", N) {\n-      val df = sparkSession.range(N).selectExpr(\n-        \"id as key\",\n-        \"array((rand(), rand()), (rand(), rand()), (rand(), 0.0d)) as values\")\n-      df.selectExpr(\"key\", \"inline(values) as (r1, r2)\").count()\n+    runBenchmark(\"collect limit\") {\n+      val N = 1 << 20\n+\n+      val benchmark = new Benchmark(\"collect limit\", N, output = output)\n+      benchmark.addCase(\"collect limit 1 million\") { iter =>\n+        sparkSession.range(N * 4).limit(N).collect()\n+      }\n+      benchmark.addCase(\"collect limit 2 millions\") { iter =>\n+        sparkSession.range(N * 4).limit(N * 2).collect()\n+      }\n+      benchmark.run()\n     }\n \n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.11.6\n-    Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\n-\n-    generate inline array:                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    generate inline array wholestage off          6901 / 6928          2.4         411.3       1.0X\n-    generate inline array wholestage on           1001 / 1010         16.8          59.7       6.9X\n-     */\n-\n-    val M = 60000\n-    runBenchmark(\"generate big struct array\", M) {\n-      import sparkSession.implicits._\n-      val df = sparkSession.sparkContext.parallelize(Seq((\"1\",\n-        Array.fill(M)({\n-          val i = math.random\n-          (i.toString, (i + 1).toString, (i + 2).toString, (i + 3).toString)\n-        })))).toDF(\"col\", \"arr\")\n-\n-      df.selectExpr(\"*\", \"expode(arr) as arr_col\")\n-        .select(\"col\", \"arr_col.*\").count\n+    runBenchmark(\"generate explode\") {\n+      val N = 1 << 24\n+      runMiscBenchmark(\"generate explode array\", N) {\n+        val df = sparkSession.range(N).selectExpr(\n+          \"id as key\",\n+          \"array(rand(), rand(), rand(), rand(), rand()) as values\")\n+        df.selectExpr(\"key\", \"explode(values) value\").count()\n+      }\n+\n+      runMiscBenchmark(\"generate explode map\", N) {\n+        val df = sparkSession.range(N).selectExpr(\n+          \"id as key\",\n+          \"map('a', rand(), 'b', rand(), 'c', rand(), 'd', rand(), 'e', rand()) pairs\")\n+        df.selectExpr(\"key\", \"explode(pairs) as (k, v)\").count()\n+      }\n+\n+      runMiscBenchmark(\"generate posexplode array\", N) {\n+        val df = sparkSession.range(N).selectExpr(\n+          \"id as key\",\n+          \"array(rand(), rand(), rand(), rand(), rand()) as values\")\n+        df.selectExpr(\"key\", \"posexplode(values) as (idx, value)\").count()\n+      }\n+\n+      runMiscBenchmark(\"generate inline array\", N) {\n+        val df = sparkSession.range(N).selectExpr(\n+          \"id as key\",\n+          \"array((rand(), rand()), (rand(), rand()), (rand(), 0.0d)) as values\")\n+        df.selectExpr(\"key\", \"inline(values) as (r1, r2)\").count()\n+      }\n+\n+      val M = 60000\n+      runMiscBenchmark(\"generate big struct array\", M) {\n+        import sparkSession.implicits._\n+        val df = sparkSession.sparkContext.parallelize(Seq((\"1\",\n+          Array.fill(M)({\n+            val i = math.random\n+            (i.toString, (i + 1).toString, (i + 2).toString, (i + 3).toString)\n+          })))).toDF(\"col\", \"arr\")\n+\n+        df.selectExpr(\"*\", \"explode(arr) as arr_col\")",
    "line": 245
  }],
  "prId": 22500
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "This refactoring introduces a long function body at `runBenchmarkSuite`. In general, it's not a better direction.\r\n\r\nCould you map each `ignore` function  into an independent function and make `runBenchmarkSuite()` invoke a series of those functions?",
    "commit": "623998bfb7596f92c603e5be378d23b0e3a3b1ea",
    "createdAt": "2018-10-02T23:05:43Z",
    "diffHunk": "@@ -21,247 +21,125 @@ import org.apache.spark.benchmark.Benchmark\n \n /**\n  * Benchmark to measure whole stage codegen performance.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.MiscBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * 1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ * 2. build/sbt \"sql/test:runMain <this class>\"\n+ * 3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *    Results will be written to \"benchmarks/MiscBenchmark-results.txt\".\n  */\n-class MiscBenchmark extends BenchmarkWithCodegen {\n-\n-  ignore(\"filter & aggregate without group\") {",
    "line": 19
  }],
  "prId": 22500
}]