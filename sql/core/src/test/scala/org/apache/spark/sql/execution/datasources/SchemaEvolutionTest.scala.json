[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "nit: `` `byte` `` -> `byte` or the opposite for consistency with the same instances. ",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-20T12:19:34Z",
    "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |\n+ *\n+ * This aims to provide an explicit test coverage for schema evolution on file-based data sources.\n+ * Since a file format has its own coverage of schema evolution, we need a test suite\n+ * for each file-based data source with corresponding supported test case traits.\n+ *\n+ * The following is a hierarchy of test traits.\n+ *\n+ *   SchemaEvolutionTest\n+ *     -> AddColumnEvolutionTest\n+ *     -> RemoveColumnEvolutionTest\n+ *     -> ChangePositionEvolutionTest\n+ *     -> BooleanTypeEvolutionTest\n+ *     -> IntegralTypeEvolutionTest\n+ *     -> ToDoubleTypeEvolutionTest\n+ *     -> ToDecimalTypeEvolutionTest\n+ */\n+\n+trait SchemaEvolutionTest extends QueryTest with SQLTestUtils with SharedSQLContext {\n+  val format: String\n+  val options: Map[String, String] = Map.empty[String, String]\n+}\n+\n+/**\n+ * Add column.\n+ * This test suite assumes that the missing column should be `null`.\n+ */\n+trait AddColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"append column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq(\"a\", \"b\").toDF(\"col1\")\n+      val df2 = df1.withColumn(\"col2\", lit(\"x\"))\n+      val df3 = df2.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+      val dir3 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+      df3.write.mode(\"overwrite\").format(format).options(options).save(dir3)\n+\n+      val df = spark.read\n+        .schema(df3.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"a\", null, null, \"one\"),\n+        Row(\"b\", null, null, \"one\"),\n+        Row(\"a\", \"x\", null, \"two\"),\n+        Row(\"b\", \"x\", null, \"two\"),\n+        Row(\"a\", \"x\", \"y\", \"three\"),\n+        Row(\"b\", \"x\", \"y\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Remove column.\n+ * This test suite is identical with AddColumnEvolutionTest,\n+ * but this test suite ensures that the schema and result are truncated to the given schema.\n+ */\n+trait RemoveColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"remove column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\")).toDF(\"col1\", \"col2\")\n+      val df2 = df1.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=two\"\n+      val dir2 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(df1.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"1\", \"a\", \"two\"),\n+        Row(\"2\", \"b\", \"two\"),\n+        Row(\"1\", \"a\", \"three\"),\n+        Row(\"2\", \"b\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Change column positions.\n+ * This suite assumes that all data set have the same number of columns.\n+ */\n+trait ChangePositionEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"change column position\") {\n+    withTempDir { dir =>\n+      // val path = dir.getCanonicalPath\n+      val path = \"/tmp/change\"\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\"), (\"3\", \"c\")).toDF(\"col1\", \"col2\")\n+      val df2 = Seq((\"d\", \"4\"), (\"e\", \"5\"), (\"f\", \"6\")).toDF(\"col2\", \"col1\")\n+      val unionDF = df1.unionByName(df2)\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(unionDF.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+        .select(\"col1\", \"col2\")\n+\n+      checkAnswer(df, unionDF)\n+    }\n+  }\n+}\n+\n+trait BooleanTypeEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"boolean to byte/short/int/long\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val values = (1 to 10).map(_ % 2)\n+      val booleanDF = (1 to 10).map(_ % 2 == 1).toDF(\"col1\")\n+      val byteDF = values.map(_.toByte).toDF(\"col1\")\n+      val shortDF = values.map(_.toShort).toDF(\"col1\")\n+      val intDF = values.toDF(\"col1\")\n+      val longDF = values.map(_.toLong).toDF(\"col1\")\n+\n+      booleanDF.write.mode(\"overwrite\").format(format).options(options).save(path)\n+\n+      val df1 = spark.read\n+        .schema(\"col1 byte\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df1, byteDF)\n+\n+      val df2 = spark.read\n+        .schema(\"col1 short\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df2, shortDF)\n+\n+      val df3 = spark.read\n+        .schema(\"col1 int\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df3, intDF)\n+\n+      val df4 = spark.read\n+        .schema(\"col1 long\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df4, longDF)\n+    }\n+  }\n+}\n+\n+trait IntegralTypeEvolutionTest extends SchemaEvolutionTest {\n+\n+  import testImplicits._\n+\n+  test(\"change column type from `byte` to `short/int/long`\") {"
  }],
  "prId": 20208
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Here seems many tests have some duplicated codes .. can we maybe do such as something like as below?\r\n\r\n```scala\r\nSeq(byteDF, ...).zip(\"byte\").foreach { case (df, t) =>\r\n  test(s\"boolean to $t\") {\r\n    spark.read\r\n      .schema(\"col1 long\")\r\n      .format(format)\r\n      .options(options)\r\n      .load(path)\r\n    checkAnswer(df4, longDF)\r\n  }\r\n}\r\n```\r\n\r\nI am fine with any idea to deal with this duplication.\r\n",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-20T12:20:40Z",
    "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |\n+ *\n+ * This aims to provide an explicit test coverage for schema evolution on file-based data sources.\n+ * Since a file format has its own coverage of schema evolution, we need a test suite\n+ * for each file-based data source with corresponding supported test case traits.\n+ *\n+ * The following is a hierarchy of test traits.\n+ *\n+ *   SchemaEvolutionTest\n+ *     -> AddColumnEvolutionTest\n+ *     -> RemoveColumnEvolutionTest\n+ *     -> ChangePositionEvolutionTest\n+ *     -> BooleanTypeEvolutionTest\n+ *     -> IntegralTypeEvolutionTest\n+ *     -> ToDoubleTypeEvolutionTest\n+ *     -> ToDecimalTypeEvolutionTest\n+ */\n+\n+trait SchemaEvolutionTest extends QueryTest with SQLTestUtils with SharedSQLContext {\n+  val format: String\n+  val options: Map[String, String] = Map.empty[String, String]\n+}\n+\n+/**\n+ * Add column.\n+ * This test suite assumes that the missing column should be `null`.\n+ */\n+trait AddColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"append column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq(\"a\", \"b\").toDF(\"col1\")\n+      val df2 = df1.withColumn(\"col2\", lit(\"x\"))\n+      val df3 = df2.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+      val dir3 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+      df3.write.mode(\"overwrite\").format(format).options(options).save(dir3)\n+\n+      val df = spark.read\n+        .schema(df3.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"a\", null, null, \"one\"),\n+        Row(\"b\", null, null, \"one\"),\n+        Row(\"a\", \"x\", null, \"two\"),\n+        Row(\"b\", \"x\", null, \"two\"),\n+        Row(\"a\", \"x\", \"y\", \"three\"),\n+        Row(\"b\", \"x\", \"y\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Remove column.\n+ * This test suite is identical with AddColumnEvolutionTest,\n+ * but this test suite ensures that the schema and result are truncated to the given schema.\n+ */\n+trait RemoveColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"remove column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\")).toDF(\"col1\", \"col2\")\n+      val df2 = df1.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=two\"\n+      val dir2 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(df1.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"1\", \"a\", \"two\"),\n+        Row(\"2\", \"b\", \"two\"),\n+        Row(\"1\", \"a\", \"three\"),\n+        Row(\"2\", \"b\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Change column positions.\n+ * This suite assumes that all data set have the same number of columns.\n+ */\n+trait ChangePositionEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"change column position\") {\n+    withTempDir { dir =>\n+      // val path = dir.getCanonicalPath\n+      val path = \"/tmp/change\"\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\"), (\"3\", \"c\")).toDF(\"col1\", \"col2\")\n+      val df2 = Seq((\"d\", \"4\"), (\"e\", \"5\"), (\"f\", \"6\")).toDF(\"col2\", \"col1\")\n+      val unionDF = df1.unionByName(df2)\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(unionDF.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+        .select(\"col1\", \"col2\")\n+\n+      checkAnswer(df, unionDF)\n+    }\n+  }\n+}\n+\n+trait BooleanTypeEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"boolean to byte/short/int/long\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val values = (1 to 10).map(_ % 2)\n+      val booleanDF = (1 to 10).map(_ % 2 == 1).toDF(\"col1\")\n+      val byteDF = values.map(_.toByte).toDF(\"col1\")\n+      val shortDF = values.map(_.toShort).toDF(\"col1\")\n+      val intDF = values.toDF(\"col1\")\n+      val longDF = values.map(_.toLong).toDF(\"col1\")\n+\n+      booleanDF.write.mode(\"overwrite\").format(format).options(options).save(path)\n+\n+      val df1 = spark.read\n+        .schema(\"col1 byte\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df1, byteDF)\n+\n+      val df2 = spark.read\n+        .schema(\"col1 short\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df2, shortDF)\n+\n+      val df3 = spark.read\n+        .schema(\"col1 int\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df3, intDF)\n+\n+      val df4 = spark.read\n+        .schema(\"col1 long\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df4, longDF)\n+    }\n+  }\n+}\n+\n+trait IntegralTypeEvolutionTest extends SchemaEvolutionTest {\n+\n+  import testImplicits._\n+\n+  test(\"change column type from `byte` to `short/int/long`\") {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "~Ur, for this, when we put the variables (byteDF, ...) outside of `test` functions, it seems to cause SQLContext errors.~ Never mind. I handled that as lazy variables.",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-20T16:11:15Z",
    "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |\n+ *\n+ * This aims to provide an explicit test coverage for schema evolution on file-based data sources.\n+ * Since a file format has its own coverage of schema evolution, we need a test suite\n+ * for each file-based data source with corresponding supported test case traits.\n+ *\n+ * The following is a hierarchy of test traits.\n+ *\n+ *   SchemaEvolutionTest\n+ *     -> AddColumnEvolutionTest\n+ *     -> RemoveColumnEvolutionTest\n+ *     -> ChangePositionEvolutionTest\n+ *     -> BooleanTypeEvolutionTest\n+ *     -> IntegralTypeEvolutionTest\n+ *     -> ToDoubleTypeEvolutionTest\n+ *     -> ToDecimalTypeEvolutionTest\n+ */\n+\n+trait SchemaEvolutionTest extends QueryTest with SQLTestUtils with SharedSQLContext {\n+  val format: String\n+  val options: Map[String, String] = Map.empty[String, String]\n+}\n+\n+/**\n+ * Add column.\n+ * This test suite assumes that the missing column should be `null`.\n+ */\n+trait AddColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"append column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq(\"a\", \"b\").toDF(\"col1\")\n+      val df2 = df1.withColumn(\"col2\", lit(\"x\"))\n+      val df3 = df2.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+      val dir3 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+      df3.write.mode(\"overwrite\").format(format).options(options).save(dir3)\n+\n+      val df = spark.read\n+        .schema(df3.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"a\", null, null, \"one\"),\n+        Row(\"b\", null, null, \"one\"),\n+        Row(\"a\", \"x\", null, \"two\"),\n+        Row(\"b\", \"x\", null, \"two\"),\n+        Row(\"a\", \"x\", \"y\", \"three\"),\n+        Row(\"b\", \"x\", \"y\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Remove column.\n+ * This test suite is identical with AddColumnEvolutionTest,\n+ * but this test suite ensures that the schema and result are truncated to the given schema.\n+ */\n+trait RemoveColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"remove column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\")).toDF(\"col1\", \"col2\")\n+      val df2 = df1.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=two\"\n+      val dir2 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(df1.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"1\", \"a\", \"two\"),\n+        Row(\"2\", \"b\", \"two\"),\n+        Row(\"1\", \"a\", \"three\"),\n+        Row(\"2\", \"b\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Change column positions.\n+ * This suite assumes that all data set have the same number of columns.\n+ */\n+trait ChangePositionEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"change column position\") {\n+    withTempDir { dir =>\n+      // val path = dir.getCanonicalPath\n+      val path = \"/tmp/change\"\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\"), (\"3\", \"c\")).toDF(\"col1\", \"col2\")\n+      val df2 = Seq((\"d\", \"4\"), (\"e\", \"5\"), (\"f\", \"6\")).toDF(\"col2\", \"col1\")\n+      val unionDF = df1.unionByName(df2)\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(unionDF.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+        .select(\"col1\", \"col2\")\n+\n+      checkAnswer(df, unionDF)\n+    }\n+  }\n+}\n+\n+trait BooleanTypeEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"boolean to byte/short/int/long\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val values = (1 to 10).map(_ % 2)\n+      val booleanDF = (1 to 10).map(_ % 2 == 1).toDF(\"col1\")\n+      val byteDF = values.map(_.toByte).toDF(\"col1\")\n+      val shortDF = values.map(_.toShort).toDF(\"col1\")\n+      val intDF = values.toDF(\"col1\")\n+      val longDF = values.map(_.toLong).toDF(\"col1\")\n+\n+      booleanDF.write.mode(\"overwrite\").format(format).options(options).save(path)\n+\n+      val df1 = spark.read\n+        .schema(\"col1 byte\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df1, byteDF)\n+\n+      val df2 = spark.read\n+        .schema(\"col1 short\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df2, shortDF)\n+\n+      val df3 = spark.read\n+        .schema(\"col1 int\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df3, intDF)\n+\n+      val df4 = spark.read\n+        .schema(\"col1 long\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df4, longDF)\n+    }\n+  }\n+}\n+\n+trait IntegralTypeEvolutionTest extends SchemaEvolutionTest {\n+\n+  import testImplicits._\n+\n+  test(\"change column type from `byte` to `short/int/long`\") {"
  }],
  "prId": 20208
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think we can do `withTempPath`.",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-20T12:21:44Z",
    "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |\n+ *\n+ * This aims to provide an explicit test coverage for schema evolution on file-based data sources.\n+ * Since a file format has its own coverage of schema evolution, we need a test suite\n+ * for each file-based data source with corresponding supported test case traits.\n+ *\n+ * The following is a hierarchy of test traits.\n+ *\n+ *   SchemaEvolutionTest\n+ *     -> AddColumnEvolutionTest\n+ *     -> RemoveColumnEvolutionTest\n+ *     -> ChangePositionEvolutionTest\n+ *     -> BooleanTypeEvolutionTest\n+ *     -> IntegralTypeEvolutionTest\n+ *     -> ToDoubleTypeEvolutionTest\n+ *     -> ToDecimalTypeEvolutionTest\n+ */\n+\n+trait SchemaEvolutionTest extends QueryTest with SQLTestUtils with SharedSQLContext {\n+  val format: String\n+  val options: Map[String, String] = Map.empty[String, String]\n+}\n+\n+/**\n+ * Add column.\n+ * This test suite assumes that the missing column should be `null`.\n+ */\n+trait AddColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"append column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq(\"a\", \"b\").toDF(\"col1\")\n+      val df2 = df1.withColumn(\"col2\", lit(\"x\"))\n+      val df3 = df2.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+      val dir3 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+      df3.write.mode(\"overwrite\").format(format).options(options).save(dir3)\n+\n+      val df = spark.read\n+        .schema(df3.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"a\", null, null, \"one\"),\n+        Row(\"b\", null, null, \"one\"),\n+        Row(\"a\", \"x\", null, \"two\"),\n+        Row(\"b\", \"x\", null, \"two\"),\n+        Row(\"a\", \"x\", \"y\", \"three\"),\n+        Row(\"b\", \"x\", \"y\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Remove column.\n+ * This test suite is identical with AddColumnEvolutionTest,\n+ * but this test suite ensures that the schema and result are truncated to the given schema.\n+ */\n+trait RemoveColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"remove column at the end\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\")).toDF(\"col1\", \"col2\")\n+      val df2 = df1.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=two\"\n+      val dir2 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(df1.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+\n+      checkAnswer(df, Seq(\n+        Row(\"1\", \"a\", \"two\"),\n+        Row(\"2\", \"b\", \"two\"),\n+        Row(\"1\", \"a\", \"three\"),\n+        Row(\"2\", \"b\", \"three\")))\n+    }\n+  }\n+}\n+\n+/**\n+ * Change column positions.\n+ * This suite assumes that all data set have the same number of columns.\n+ */\n+trait ChangePositionEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"change column position\") {\n+    withTempDir { dir =>\n+      // val path = dir.getCanonicalPath\n+      val path = \"/tmp/change\"\n+\n+      val df1 = Seq((\"1\", \"a\"), (\"2\", \"b\"), (\"3\", \"c\")).toDF(\"col1\", \"col2\")\n+      val df2 = Seq((\"d\", \"4\"), (\"e\", \"5\"), (\"f\", \"6\")).toDF(\"col2\", \"col1\")\n+      val unionDF = df1.unionByName(df2)\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+\n+      df1.write.mode(\"overwrite\").format(format).options(options).save(dir1)\n+      df2.write.mode(\"overwrite\").format(format).options(options).save(dir2)\n+\n+      val df = spark.read\n+        .schema(unionDF.schema)\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+        .select(\"col1\", \"col2\")\n+\n+      checkAnswer(df, unionDF)\n+    }\n+  }\n+}\n+\n+trait BooleanTypeEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"boolean to byte/short/int/long\") {\n+    withTempDir { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val values = (1 to 10).map(_ % 2)\n+      val booleanDF = (1 to 10).map(_ % 2 == 1).toDF(\"col1\")\n+      val byteDF = values.map(_.toByte).toDF(\"col1\")\n+      val shortDF = values.map(_.toShort).toDF(\"col1\")\n+      val intDF = values.toDF(\"col1\")\n+      val longDF = values.map(_.toLong).toDF(\"col1\")\n+\n+      booleanDF.write.mode(\"overwrite\").format(format).options(options).save(path)\n+\n+      val df1 = spark.read\n+        .schema(\"col1 byte\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df1, byteDF)\n+\n+      val df2 = spark.read\n+        .schema(\"col1 short\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df2, shortDF)\n+\n+      val df3 = spark.read\n+        .schema(\"col1 int\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df3, intDF)\n+\n+      val df4 = spark.read\n+        .schema(\"col1 long\")\n+        .format(format)\n+        .options(options)\n+        .load(path)\n+      checkAnswer(df4, longDF)\n+    }\n+  }\n+}\n+\n+trait IntegralTypeEvolutionTest extends SchemaEvolutionTest {\n+\n+  import testImplicits._\n+\n+  test(\"change column type from `byte` to `short/int/long`\") {\n+    withTempDir { dir =>"
  }],
  "prId": 20208
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Shall we leave the number given above in this comment like `(case 1.)`.",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-20T12:31:48Z",
    "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |\n+ *\n+ * This aims to provide an explicit test coverage for schema evolution on file-based data sources.\n+ * Since a file format has its own coverage of schema evolution, we need a test suite\n+ * for each file-based data source with corresponding supported test case traits.\n+ *\n+ * The following is a hierarchy of test traits.\n+ *\n+ *   SchemaEvolutionTest\n+ *     -> AddColumnEvolutionTest\n+ *     -> RemoveColumnEvolutionTest\n+ *     -> ChangePositionEvolutionTest\n+ *     -> BooleanTypeEvolutionTest\n+ *     -> IntegralTypeEvolutionTest\n+ *     -> ToDoubleTypeEvolutionTest\n+ *     -> ToDecimalTypeEvolutionTest\n+ */\n+\n+trait SchemaEvolutionTest extends QueryTest with SQLTestUtils with SharedSQLContext {\n+  val format: String\n+  val options: Map[String, String] = Map.empty[String, String]\n+}\n+\n+/**\n+ * Add column."
  }],
  "prId": 20208
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@dongjoon-hyun, how do we guarantee schema change in Parquet and ORC?\r\n\r\nI thought we (roughly) randomly pick up a file, read its footer and then use it. So, I was thinking we don't properly support this. It makes sense to Parquet with `mergeSchema` tho.\r\n\r\nI think it's not even guaranteed in CSV too because we will rely on its header from one file. ",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-22T01:40:31Z",
    "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Correct, and this is not about schema merging.\r\nThe final correct schema is given by users (or Hive).\r\nIn this PR, all schema is given by users, but for Hive tables, we uses the Hive Metastore Schema.",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-22T01:44:45Z",
    "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ohaaa, the schema is explicitly set here. Sorry, I missed it.",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-01-22T02:11:43Z",
    "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |"
  }],
  "prId": 20208
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@gatorsmile . Please see this. This is not about **schema inferencing**.",
    "commit": "a7064ac0bc7b56ffffa3e322f31bda8a45bd9517",
    "createdAt": "2018-03-19T20:46:12Z",
    "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources\n+\n+import java.io.File\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+/**\n+ * Schema can evolve in several ways and the followings are supported in file-based data sources.\n+ *\n+ *   1. Add a column\n+ *   2. Remove a column\n+ *   3. Change a column position\n+ *   4. Change a column type\n+ *\n+ * Here, we consider safe evolution without data loss. For example, data type evolution should be\n+ * from small types to larger types like `int`-to-`long`, not vice versa.\n+ *\n+ * So far, file-based data sources have schema evolution coverages like the followings.\n+ *\n+ *   | File Format  | Coverage     | Note                                                   |\n+ *   | ------------ | ------------ | ------------------------------------------------------ |\n+ *   | TEXT         | N/A          | Schema consists of a single string column.             |\n+ *   | CSV          | 1, 2, 4      |                                                        |\n+ *   | JSON         | 1, 2, 3, 4   |                                                        |\n+ *   | ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |\n+ *   | PARQUET      | 1, 2, 3      |                                                        |\n+ *\n+ * This aims to provide an explicit test coverage for schema evolution on file-based data sources.\n+ * Since a file format has its own coverage of schema evolution, we need a test suite\n+ * for each file-based data source with corresponding supported test case traits.\n+ *\n+ * The following is a hierarchy of test traits.\n+ *\n+ *   SchemaEvolutionTest\n+ *     -> AddColumnEvolutionTest\n+ *     -> RemoveColumnEvolutionTest\n+ *     -> ChangePositionEvolutionTest\n+ *     -> BooleanTypeEvolutionTest\n+ *     -> IntegralTypeEvolutionTest\n+ *     -> ToDoubleTypeEvolutionTest\n+ *     -> ToDecimalTypeEvolutionTest\n+ */\n+\n+trait SchemaEvolutionTest extends QueryTest with SQLTestUtils with SharedSQLContext {\n+  val format: String\n+  val options: Map[String, String] = Map.empty[String, String]\n+}\n+\n+/**\n+ * Add column (Case 1).\n+ * This test suite assumes that the missing column should be `null`.\n+ */\n+trait AddColumnEvolutionTest extends SchemaEvolutionTest {\n+  import testImplicits._\n+\n+  test(\"append column at the end\") {\n+    withTempPath { dir =>\n+      val path = dir.getCanonicalPath\n+\n+      val df1 = Seq(\"a\", \"b\").toDF(\"col1\")\n+      val df2 = df1.withColumn(\"col2\", lit(\"x\"))\n+      val df3 = df2.withColumn(\"col3\", lit(\"y\"))\n+\n+      val dir1 = s\"$path${File.separator}part=one\"\n+      val dir2 = s\"$path${File.separator}part=two\"\n+      val dir3 = s\"$path${File.separator}part=three\"\n+\n+      df1.write.format(format).options(options).save(dir1)\n+      df2.write.format(format).options(options).save(dir2)\n+      df3.write.format(format).options(options).save(dir3)\n+\n+      val df = spark.read\n+        .schema(df3.schema)"
  }],
  "prId": 20208
}]