[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This test should also check that the memory sink data is recovered to be the same as before failure. \n",
    "commit": "52ea3f170a9f4602f4b8e5e762680e53766e88cc",
    "createdAt": "2016-11-08T02:40:50Z",
    "diffHunk": "@@ -467,4 +468,37 @@ class DataStreamReaderWriterSuite extends StreamTest with BeforeAndAfter {\n     val sq = df.writeStream.format(\"console\").start()\n     sq.stop()\n   }\n+\n+  test(\"MemorySink can recover from a checkpoint in Complete Mode\") {\n+    val df = spark.readStream\n+      .format(\"org.apache.spark.sql.streaming.test\")\n+      .load(\"/test\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"offsets\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    // no exception here\n+    val q = df\n+      .groupBy(\"a\")\n+      .count()\n+      .writeStream\n+      .format(\"memory\")\n+      .queryName(\"test\")\n+      .option(\"checkpointLocation\", checkpointLoc)\n+      .outputMode(\"complete\")\n+      .start()\n+"
  }],
  "prId": 15801
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: change to startQuery\n",
    "commit": "52ea3f170a9f4602f4b8e5e762680e53766e88cc",
    "createdAt": "2016-11-10T01:46:41Z",
    "diffHunk": "@@ -467,4 +468,54 @@ class DataStreamReaderWriterSuite extends StreamTest with BeforeAndAfter {\n     val sq = df.writeStream.format(\"console\").start()\n     sq.stop()\n   }\n+\n+  test(\"MemorySink can recover from a checkpoint in Complete Mode\") {\n+    import testImplicits._\n+    val ms = new MemoryStream[Int](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"offsets\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    def startStream: StreamingQuery = {"
  }],
  "prId": 15801
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "its not clear if this table was regenerated by the query, or just the same table that existed before when the query was stopped earlier. so it would be good it if before restarting you remove the table, or rewrite the table to something different (and verify that), so that we are sure that the table is actually being regenerated.\n",
    "commit": "52ea3f170a9f4602f4b8e5e762680e53766e88cc",
    "createdAt": "2016-11-10T01:49:21Z",
    "diffHunk": "@@ -467,4 +468,54 @@ class DataStreamReaderWriterSuite extends StreamTest with BeforeAndAfter {\n     val sq = df.writeStream.format(\"console\").start()\n     sq.stop()\n   }\n+\n+  test(\"MemorySink can recover from a checkpoint in Complete Mode\") {\n+    import testImplicits._\n+    val ms = new MemoryStream[Int](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"offsets\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    def startStream: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(\"test\")\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val q = startStream\n+    ms.addData(0, 1)\n+    q.processAllAvailable()\n+    q.stop()\n+\n+    checkAnswer(\n+      spark.table(\"test\"),\n+      Seq(Row(0, 1), Row(1, 1))\n+    )\n+\n+    val q2 = startStream\n+    q2.processAllAvailable()\n+    checkAnswer(\n+      spark.table(\"test\"),\n+      Seq(Row(0, 1), Row(1, 1))\n+    )"
  }],
  "prId": 15801
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I will also add more data to make sure that the table is being updated as well after restart.\n",
    "commit": "52ea3f170a9f4602f4b8e5e762680e53766e88cc",
    "createdAt": "2016-11-10T01:49:51Z",
    "diffHunk": "@@ -467,4 +468,54 @@ class DataStreamReaderWriterSuite extends StreamTest with BeforeAndAfter {\n     val sq = df.writeStream.format(\"console\").start()\n     sq.stop()\n   }\n+\n+  test(\"MemorySink can recover from a checkpoint in Complete Mode\") {\n+    import testImplicits._\n+    val ms = new MemoryStream[Int](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"offsets\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    def startStream: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(\"test\")\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val q = startStream\n+    ms.addData(0, 1)\n+    q.processAllAvailable()\n+    q.stop()\n+\n+    checkAnswer(\n+      spark.table(\"test\"),\n+      Seq(Row(0, 1), Row(1, 1))\n+    )\n+\n+    val q2 = startStream\n+    q2.processAllAvailable()\n+    checkAnswer(\n+      spark.table(\"test\"),\n+      Seq(Row(0, 1), Row(1, 1))\n+    )\n+\n+    q2.stop()"
  }],
  "prId": 15801
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Split this into a different test, as this is not complete mode as this test says.\n",
    "commit": "52ea3f170a9f4602f4b8e5e762680e53766e88cc",
    "createdAt": "2016-11-10T01:51:00Z",
    "diffHunk": "@@ -467,4 +468,54 @@ class DataStreamReaderWriterSuite extends StreamTest with BeforeAndAfter {\n     val sq = df.writeStream.format(\"console\").start()\n     sq.stop()\n   }\n+\n+  test(\"MemorySink can recover from a checkpoint in Complete Mode\") {\n+    import testImplicits._\n+    val ms = new MemoryStream[Int](0, sqlContext)\n+    val df = ms.toDF().toDF(\"a\")\n+    val checkpointLoc = newMetadataDir\n+    val checkpointDir = new File(checkpointLoc, \"offsets\")\n+    checkpointDir.mkdirs()\n+    assert(checkpointDir.exists())\n+    def startStream: StreamingQuery = {\n+      df.groupBy(\"a\")\n+        .count()\n+        .writeStream\n+        .format(\"memory\")\n+        .queryName(\"test\")\n+        .option(\"checkpointLocation\", checkpointLoc)\n+        .outputMode(\"complete\")\n+        .start()\n+    }\n+    // no exception here\n+    val q = startStream\n+    ms.addData(0, 1)\n+    q.processAllAvailable()\n+    q.stop()\n+\n+    checkAnswer(\n+      spark.table(\"test\"),\n+      Seq(Row(0, 1), Row(1, 1))\n+    )\n+\n+    val q2 = startStream\n+    q2.processAllAvailable()\n+    checkAnswer(\n+      spark.table(\"test\"),\n+      Seq(Row(0, 1), Row(1, 1))\n+    )\n+\n+    q2.stop()\n+\n+    val e = intercept[AnalysisException] {"
  }],
  "prId": 15801
}]