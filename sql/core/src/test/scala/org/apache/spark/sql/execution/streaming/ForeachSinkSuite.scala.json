[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "test failures will not stop the query. might lead to cascading failures. adding a `after  {  ... }` to stop all queries may be a good idea.\n",
    "commit": "8cb7aa5593e0d845eaa92799f43792c5c9f1c02a",
    "createdAt": "2016-06-09T23:41:28Z",
    "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.ConcurrentLinkedQueue\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class ForeachSinkSuite extends StreamTest with SharedSQLContext {\n+\n+  import testImplicits._\n+\n+  test(\"foreach\") {\n+    ForeachWriterEvent.clear()\n+    withTempDir { checkpointDir =>"
  }],
  "prId": 13342
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: foreach with error\n",
    "commit": "8cb7aa5593e0d845eaa92799f43792c5c9f1c02a",
    "createdAt": "2016-06-09T23:42:27Z",
    "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.ConcurrentLinkedQueue\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class ForeachSinkSuite extends StreamTest with SharedSQLContext {\n+\n+  import testImplicits._\n+\n+  test(\"foreach\") {\n+    ForeachWriterEvent.clear()\n+    withTempDir { checkpointDir =>\n+      val input = MemoryStream[Int]\n+      val query = input.toDS().repartition(2).write\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .foreach(new ForeachWriter[Int] {\n+\n+          private val events = mutable.ArrayBuffer[ForeachWriterEvent.Event]()\n+\n+          override def open(partitionId: Long, version: Long): Boolean = {\n+            events += ForeachWriterEvent.Open(partition = partitionId, version = version)\n+            true\n+          }\n+\n+          override def process(value: Int): Unit = {\n+            events += ForeachWriterEvent.Process(value)\n+          }\n+\n+          override def close(errorOrNull: Throwable): Unit = {\n+            events += ForeachWriterEvent.Close(error = Option(errorOrNull))\n+            ForeachWriterEvent.addEvents(events)\n+          }\n+        })\n+      input.addData(1, 2, 3, 4)\n+      query.processAllAvailable()\n+\n+      val expectedEventsForPartition0 = Seq(\n+        ForeachWriterEvent.Open(partition = 0, version = 0),\n+        ForeachWriterEvent.Process(value = 1),\n+        ForeachWriterEvent.Process(value = 3),\n+        ForeachWriterEvent.Close(None)\n+      )\n+      val expectedEventsForPartition1 = Seq(\n+        ForeachWriterEvent.Open(partition = 1, version = 0),\n+        ForeachWriterEvent.Process(value = 2),\n+        ForeachWriterEvent.Process(value = 4),\n+        ForeachWriterEvent.Close(None)\n+      )\n+\n+      val allEvents = ForeachWriterEvent.allEvents()\n+      assert(allEvents.size === 2)\n+      assert {\n+        allEvents === Seq(expectedEventsForPartition0, expectedEventsForPartition1) ||\n+          allEvents === Seq(expectedEventsForPartition1, expectedEventsForPartition0)\n+      }\n+      query.stop()\n+    }\n+  }\n+\n+  test(\"foreach error\") {"
  }],
  "prId": 13342
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Not used anywhere. \nCant this writer code be deduped?\n",
    "commit": "8cb7aa5593e0d845eaa92799f43792c5c9f1c02a",
    "createdAt": "2016-06-09T23:43:45Z",
    "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.ConcurrentLinkedQueue\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class ForeachSinkSuite extends StreamTest with SharedSQLContext {\n+\n+  import testImplicits._\n+\n+  test(\"foreach\") {\n+    ForeachWriterEvent.clear()\n+    withTempDir { checkpointDir =>\n+      val input = MemoryStream[Int]\n+      val query = input.toDS().repartition(2).write\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .foreach(new ForeachWriter[Int] {\n+\n+          private val events = mutable.ArrayBuffer[ForeachWriterEvent.Event]()\n+\n+          override def open(partitionId: Long, version: Long): Boolean = {\n+            events += ForeachWriterEvent.Open(partition = partitionId, version = version)\n+            true\n+          }\n+\n+          override def process(value: Int): Unit = {\n+            events += ForeachWriterEvent.Process(value)\n+          }\n+\n+          override def close(errorOrNull: Throwable): Unit = {\n+            events += ForeachWriterEvent.Close(error = Option(errorOrNull))\n+            ForeachWriterEvent.addEvents(events)\n+          }\n+        })\n+      input.addData(1, 2, 3, 4)\n+      query.processAllAvailable()\n+\n+      val expectedEventsForPartition0 = Seq(\n+        ForeachWriterEvent.Open(partition = 0, version = 0),\n+        ForeachWriterEvent.Process(value = 1),\n+        ForeachWriterEvent.Process(value = 3),\n+        ForeachWriterEvent.Close(None)\n+      )\n+      val expectedEventsForPartition1 = Seq(\n+        ForeachWriterEvent.Open(partition = 1, version = 0),\n+        ForeachWriterEvent.Process(value = 2),\n+        ForeachWriterEvent.Process(value = 4),\n+        ForeachWriterEvent.Close(None)\n+      )\n+\n+      val allEvents = ForeachWriterEvent.allEvents()\n+      assert(allEvents.size === 2)\n+      assert {\n+        allEvents === Seq(expectedEventsForPartition0, expectedEventsForPartition1) ||\n+          allEvents === Seq(expectedEventsForPartition1, expectedEventsForPartition0)\n+      }\n+      query.stop()\n+    }\n+  }\n+\n+  test(\"foreach error\") {\n+    ForeachWriterEvent.clear()\n+    withTempDir { checkpointDir =>\n+      val input = MemoryStream[Int]\n+      val query = input.toDS().repartition(1).write\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .foreach(new ForeachWriter[Int] {\n+\n+          private val events = mutable.ArrayBuffer[ForeachWriterEvent.Event]()\n+\n+          private var currentPartitionId = -1L"
  }],
  "prId": 13342
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: ForeachWriterEvent --> ForeachSinkSuite\n",
    "commit": "8cb7aa5593e0d845eaa92799f43792c5c9f1c02a",
    "createdAt": "2016-06-09T23:44:03Z",
    "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.streaming\n+\n+import java.util.concurrent.ConcurrentLinkedQueue\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.sql.ForeachWriter\n+import org.apache.spark.sql.streaming.StreamTest\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class ForeachSinkSuite extends StreamTest with SharedSQLContext {\n+\n+  import testImplicits._\n+\n+  test(\"foreach\") {\n+    ForeachWriterEvent.clear()\n+    withTempDir { checkpointDir =>\n+      val input = MemoryStream[Int]\n+      val query = input.toDS().repartition(2).write\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .foreach(new ForeachWriter[Int] {\n+\n+          private val events = mutable.ArrayBuffer[ForeachWriterEvent.Event]()\n+\n+          override def open(partitionId: Long, version: Long): Boolean = {\n+            events += ForeachWriterEvent.Open(partition = partitionId, version = version)\n+            true\n+          }\n+\n+          override def process(value: Int): Unit = {\n+            events += ForeachWriterEvent.Process(value)\n+          }\n+\n+          override def close(errorOrNull: Throwable): Unit = {\n+            events += ForeachWriterEvent.Close(error = Option(errorOrNull))\n+            ForeachWriterEvent.addEvents(events)\n+          }\n+        })\n+      input.addData(1, 2, 3, 4)\n+      query.processAllAvailable()\n+\n+      val expectedEventsForPartition0 = Seq(\n+        ForeachWriterEvent.Open(partition = 0, version = 0),\n+        ForeachWriterEvent.Process(value = 1),\n+        ForeachWriterEvent.Process(value = 3),\n+        ForeachWriterEvent.Close(None)\n+      )\n+      val expectedEventsForPartition1 = Seq(\n+        ForeachWriterEvent.Open(partition = 1, version = 0),\n+        ForeachWriterEvent.Process(value = 2),\n+        ForeachWriterEvent.Process(value = 4),\n+        ForeachWriterEvent.Close(None)\n+      )\n+\n+      val allEvents = ForeachWriterEvent.allEvents()\n+      assert(allEvents.size === 2)\n+      assert {\n+        allEvents === Seq(expectedEventsForPartition0, expectedEventsForPartition1) ||\n+          allEvents === Seq(expectedEventsForPartition1, expectedEventsForPartition0)\n+      }\n+      query.stop()\n+    }\n+  }\n+\n+  test(\"foreach error\") {\n+    ForeachWriterEvent.clear()\n+    withTempDir { checkpointDir =>\n+      val input = MemoryStream[Int]\n+      val query = input.toDS().repartition(1).write\n+        .option(\"checkpointLocation\", checkpointDir.getCanonicalPath)\n+        .foreach(new ForeachWriter[Int] {\n+\n+          private val events = mutable.ArrayBuffer[ForeachWriterEvent.Event]()\n+\n+          private var currentPartitionId = -1L\n+\n+          override def open(partitionId: Long, version: Long): Boolean = {\n+            currentPartitionId = partitionId\n+            events += ForeachWriterEvent.Open(partition = partitionId, version = version)\n+            true\n+          }\n+\n+          override def process(value: Int): Unit = {\n+            events += ForeachWriterEvent.Process(value)\n+            throw new RuntimeException(\"error\")\n+          }\n+\n+          override def close(errorOrNull: Throwable): Unit = {\n+            events += ForeachWriterEvent.Close(error = Option(errorOrNull))\n+            ForeachWriterEvent.addEvents(events)\n+          }\n+        })\n+      input.addData(1, 2, 3, 4)\n+      query.processAllAvailable()\n+\n+      val allEvents = ForeachWriterEvent.allEvents()\n+      assert(allEvents.size === 1)\n+      assert(allEvents(0)(0) === ForeachWriterEvent.Open(partition = 0, version = 0))\n+      assert(allEvents(0)(1) ===  ForeachWriterEvent.Process(value = 1))\n+      val errorEvent = allEvents(0)(2).asInstanceOf[ForeachWriterEvent.Close]\n+      assert(errorEvent.error.get.isInstanceOf[RuntimeException])\n+      assert(errorEvent.error.get.getMessage === \"error\")\n+      query.stop()\n+    }\n+  }\n+}\n+\n+/** A global object to collect events in the executor side */\n+object ForeachWriterEvent {"
  }],
  "prId": 13342
}]