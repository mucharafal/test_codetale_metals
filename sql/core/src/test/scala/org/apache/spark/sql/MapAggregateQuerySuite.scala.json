[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Can you use `def createDataFrame(rowRDD: RDD[Row], schema: StructType)`, instead of using implicit schema inference? \n\nWe need an explicit schema definition when we verifying different data types. \n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-29T18:37:57Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+\n+class MapAggregateQuerySuite extends QueryTest with SharedSQLContext {\n+  import testImplicits._\n+\n+  private val table = \"map_aggregate_test\"\n+  private val col1 = \"col1\"\n+  private val col2 = \"col2\"\n+  private def query(numBins: Int): DataFrame = {\n+    sql(s\"SELECT map_aggregate($col1, $numBins), map_aggregate($col2, $numBins) FROM $table\")\n+  }\n+\n+  test(\"null handling\") {\n+    withTempView(table) {\n+      val schema = StructType(Seq(StructField(col1, StringType), StructField(col2, DoubleType)))\n+      // Empty input row\n+      val rdd1 = spark.sparkContext.parallelize(Seq(Row(null, null)))\n+      spark.createDataFrame(rdd1, schema).createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 2), Row(Map.empty, Map.empty))\n+\n+      // Add some non-empty row\n+      val rdd2 = spark.sparkContext.parallelize(Seq(Row(null, 3.0D), Row(\"a\", null)))\n+      spark.createDataFrame(rdd2, schema).createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 2), Row(Map((\"a\", 1)), Map((3.0D, 1))))\n+    }\n+  }\n+\n+  test(\"returns empty result when ndv exceeds numBins\") {\n+    withTempView(table) {\n+      spark.sparkContext.makeRDD(\n+        Seq((\"a\", 4), (\"d\", 2), (\"c\", 4), (\"b\", 1), (\"a\", 3), (\"a\", 2)), 2).toDF(col1, col2)\n+        .createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 4), Row(\n+        Map((\"a\", 3), (\"b\", 1), (\"c\", 1), (\"d\", 1)),\n+        Map((1.0D, 1), (2.0D, 2), (3.0D, 1), (4.0D, 2))))\n+      // One partial exceeds numBins during update()\n+      checkAnswer(query(numBins = 2), Row(Map.empty, Map.empty))\n+      // Exceeding numBins during merge()\n+      checkAnswer(query(numBins = 3), Row(Map.empty, Map.empty))\n+    }\n+  }\n+\n+  test(\"multiple columns of different types\") {\n+    def queryMultiColumns(numBins: Int): DataFrame = {\n+      sql(\n+        s\"\"\"\n+           |SELECT\n+           |  map_aggregate(c1, $numBins),\n+           |  map_aggregate(c2, $numBins),\n+           |  map_aggregate(c3, $numBins),\n+           |  map_aggregate(c4, $numBins),\n+           |  map_aggregate(c5, $numBins),\n+           |  map_aggregate(c6, $numBins),\n+           |  map_aggregate(c7, $numBins),\n+           |  map_aggregate(c8, $numBins),\n+           |  map_aggregate(c9, $numBins),\n+           |  map_aggregate(c10, $numBins)\n+           |FROM $table\n+        \"\"\".stripMargin)\n+    }\n+\n+    val ints = Seq(5, 3, 1)\n+    val doubles = Seq(1.0D, 3.0D, 5.0D)\n+    val dates = Seq(\"1970-01-01\", \"1970-02-02\", \"1970-03-03\")\n+    val timestamps = Seq(\"1970-01-01 00:00:00\", \"1970-01-01 00:00:05\", \"1970-01-01 00:00:10\")\n+    val strings = Seq(\"a\", \"bb\", \"ccc\")\n+\n+    val data = ints.indices.map { i =>\n+      (ints(i).toByte,\n+        ints(i).toShort,\n+        ints(i),\n+        ints(i).toLong,\n+        doubles(i).toFloat,\n+        doubles(i),\n+        Decimal(doubles(i)),\n+        Date.valueOf(dates(i)),\n+        Timestamp.valueOf(timestamps(i)),\n+        strings(i))\n+    }\n+    withTempView(table) {\n+      data.toDF(\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\")"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "In this example, below is the printed schema\n\n```\n+--------+--------------+-------+\n|col_name|     data_type|comment|\n+--------+--------------+-------+\n|      c1|       tinyint|   null|\n|      c2|      smallint|   null|\n|      c3|           int|   null|\n|      c4|        bigint|   null|\n|      c5|         float|   null|\n|      c6|        double|   null|\n|      c7|decimal(38,18)|   null|\n|      c8|          date|   null|\n|      c9|     timestamp|   null|\n|     c10|        string|   null|\n+--------+--------------+-------+\n```\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-29T18:39:18Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+\n+class MapAggregateQuerySuite extends QueryTest with SharedSQLContext {\n+  import testImplicits._\n+\n+  private val table = \"map_aggregate_test\"\n+  private val col1 = \"col1\"\n+  private val col2 = \"col2\"\n+  private def query(numBins: Int): DataFrame = {\n+    sql(s\"SELECT map_aggregate($col1, $numBins), map_aggregate($col2, $numBins) FROM $table\")\n+  }\n+\n+  test(\"null handling\") {\n+    withTempView(table) {\n+      val schema = StructType(Seq(StructField(col1, StringType), StructField(col2, DoubleType)))\n+      // Empty input row\n+      val rdd1 = spark.sparkContext.parallelize(Seq(Row(null, null)))\n+      spark.createDataFrame(rdd1, schema).createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 2), Row(Map.empty, Map.empty))\n+\n+      // Add some non-empty row\n+      val rdd2 = spark.sparkContext.parallelize(Seq(Row(null, 3.0D), Row(\"a\", null)))\n+      spark.createDataFrame(rdd2, schema).createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 2), Row(Map((\"a\", 1)), Map((3.0D, 1))))\n+    }\n+  }\n+\n+  test(\"returns empty result when ndv exceeds numBins\") {\n+    withTempView(table) {\n+      spark.sparkContext.makeRDD(\n+        Seq((\"a\", 4), (\"d\", 2), (\"c\", 4), (\"b\", 1), (\"a\", 3), (\"a\", 2)), 2).toDF(col1, col2)\n+        .createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 4), Row(\n+        Map((\"a\", 3), (\"b\", 1), (\"c\", 1), (\"d\", 1)),\n+        Map((1.0D, 1), (2.0D, 2), (3.0D, 1), (4.0D, 2))))\n+      // One partial exceeds numBins during update()\n+      checkAnswer(query(numBins = 2), Row(Map.empty, Map.empty))\n+      // Exceeding numBins during merge()\n+      checkAnswer(query(numBins = 3), Row(Map.empty, Map.empty))\n+    }\n+  }\n+\n+  test(\"multiple columns of different types\") {\n+    def queryMultiColumns(numBins: Int): DataFrame = {\n+      sql(\n+        s\"\"\"\n+           |SELECT\n+           |  map_aggregate(c1, $numBins),\n+           |  map_aggregate(c2, $numBins),\n+           |  map_aggregate(c3, $numBins),\n+           |  map_aggregate(c4, $numBins),\n+           |  map_aggregate(c5, $numBins),\n+           |  map_aggregate(c6, $numBins),\n+           |  map_aggregate(c7, $numBins),\n+           |  map_aggregate(c8, $numBins),\n+           |  map_aggregate(c9, $numBins),\n+           |  map_aggregate(c10, $numBins)\n+           |FROM $table\n+        \"\"\".stripMargin)\n+    }\n+\n+    val ints = Seq(5, 3, 1)\n+    val doubles = Seq(1.0D, 3.0D, 5.0D)\n+    val dates = Seq(\"1970-01-01\", \"1970-02-02\", \"1970-03-03\")\n+    val timestamps = Seq(\"1970-01-01 00:00:00\", \"1970-01-01 00:00:05\", \"1970-01-01 00:00:10\")\n+    val strings = Seq(\"a\", \"bb\", \"ccc\")\n+\n+    val data = ints.indices.map { i =>\n+      (ints(i).toByte,\n+        ints(i).toShort,\n+        ints(i),\n+        ints(i).toLong,\n+        doubles(i).toFloat,\n+        doubles(i),\n+        Decimal(doubles(i)),\n+        Date.valueOf(dates(i)),\n+        Timestamp.valueOf(timestamps(i)),\n+        strings(i))\n+    }\n+    withTempView(table) {\n+      data.toDF(\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\")"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "ok\n",
    "commit": "b7fcaf46a865a1f48ffa1377f47d8f17ffe3e445",
    "createdAt": "2016-10-30T11:09:29Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types._\n+\n+\n+class MapAggregateQuerySuite extends QueryTest with SharedSQLContext {\n+  import testImplicits._\n+\n+  private val table = \"map_aggregate_test\"\n+  private val col1 = \"col1\"\n+  private val col2 = \"col2\"\n+  private def query(numBins: Int): DataFrame = {\n+    sql(s\"SELECT map_aggregate($col1, $numBins), map_aggregate($col2, $numBins) FROM $table\")\n+  }\n+\n+  test(\"null handling\") {\n+    withTempView(table) {\n+      val schema = StructType(Seq(StructField(col1, StringType), StructField(col2, DoubleType)))\n+      // Empty input row\n+      val rdd1 = spark.sparkContext.parallelize(Seq(Row(null, null)))\n+      spark.createDataFrame(rdd1, schema).createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 2), Row(Map.empty, Map.empty))\n+\n+      // Add some non-empty row\n+      val rdd2 = spark.sparkContext.parallelize(Seq(Row(null, 3.0D), Row(\"a\", null)))\n+      spark.createDataFrame(rdd2, schema).createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 2), Row(Map((\"a\", 1)), Map((3.0D, 1))))\n+    }\n+  }\n+\n+  test(\"returns empty result when ndv exceeds numBins\") {\n+    withTempView(table) {\n+      spark.sparkContext.makeRDD(\n+        Seq((\"a\", 4), (\"d\", 2), (\"c\", 4), (\"b\", 1), (\"a\", 3), (\"a\", 2)), 2).toDF(col1, col2)\n+        .createOrReplaceTempView(table)\n+      checkAnswer(query(numBins = 4), Row(\n+        Map((\"a\", 3), (\"b\", 1), (\"c\", 1), (\"d\", 1)),\n+        Map((1.0D, 1), (2.0D, 2), (3.0D, 1), (4.0D, 2))))\n+      // One partial exceeds numBins during update()\n+      checkAnswer(query(numBins = 2), Row(Map.empty, Map.empty))\n+      // Exceeding numBins during merge()\n+      checkAnswer(query(numBins = 3), Row(Map.empty, Map.empty))\n+    }\n+  }\n+\n+  test(\"multiple columns of different types\") {\n+    def queryMultiColumns(numBins: Int): DataFrame = {\n+      sql(\n+        s\"\"\"\n+           |SELECT\n+           |  map_aggregate(c1, $numBins),\n+           |  map_aggregate(c2, $numBins),\n+           |  map_aggregate(c3, $numBins),\n+           |  map_aggregate(c4, $numBins),\n+           |  map_aggregate(c5, $numBins),\n+           |  map_aggregate(c6, $numBins),\n+           |  map_aggregate(c7, $numBins),\n+           |  map_aggregate(c8, $numBins),\n+           |  map_aggregate(c9, $numBins),\n+           |  map_aggregate(c10, $numBins)\n+           |FROM $table\n+        \"\"\".stripMargin)\n+    }\n+\n+    val ints = Seq(5, 3, 1)\n+    val doubles = Seq(1.0D, 3.0D, 5.0D)\n+    val dates = Seq(\"1970-01-01\", \"1970-02-02\", \"1970-03-03\")\n+    val timestamps = Seq(\"1970-01-01 00:00:00\", \"1970-01-01 00:00:05\", \"1970-01-01 00:00:10\")\n+    val strings = Seq(\"a\", \"bb\", \"ccc\")\n+\n+    val data = ints.indices.map { i =>\n+      (ints(i).toByte,\n+        ints(i).toShort,\n+        ints(i),\n+        ints(i).toLong,\n+        doubles(i).toFloat,\n+        doubles(i),\n+        Decimal(doubles(i)),\n+        Date.valueOf(dates(i)),\n+        Timestamp.valueOf(timestamps(i)),\n+        strings(i))\n+    }\n+    withTempView(table) {\n+      data.toDF(\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\")"
  }],
  "prId": 15637
}]