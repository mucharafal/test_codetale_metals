[{
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "we can compile the pattern only once here and in the other cases",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-25T18:21:02Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "@seancxmao can you address this comment? Thanks!",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-27T03:02:13Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "OK, I'll fix it.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-27T05:38:21Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "Fixed with a new commit. I also extracted `Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\")`.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-27T06:01:24Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)"
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "We need to put these helper functions here? That's because these functions are only used for `test(\"Sort metrics\")` now ...",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-27T03:04:59Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse time string: \" + str)\n+    }\n+  }\n+\n+  /**\n+   * Convert a size metric string to a sequence of stats, including sum, min, med and max in order,\n+   * each a tuple of (value, unit).\n+   * @param metricStr size metric string, e.g. \"\\n96.2 MB (32.1 MB, 32.1 MB, 32.1 MB)\"\n+   * @return A sequence of stats, e.g. ((96.2,MB), (32.1,MB), (32.1,MB), (32.1,MB))\n+   */\n+  protected def sizeMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToBytes)\n+  }\n+\n+  /**\n+   * Convert a timing metric string to a sequence of stats, including sum, min, med and max in\n+   * order, each a tuple of (value, unit).\n+   * @param metricStr timing metric string, e.g. \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+   * @return A sequence of stats, e.g. ((2.0,ms), (1.0,ms), (1.0,ms), (1.0,ms))\n+   */\n+  protected def timingMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToDuration)\n+  }"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "Yes, currently these functions are only used for `test(\"Sort metrics\")`. What `SQLMetricsSuite` has been checking are almost all integer number metrics (e.g. \"number of output rows\", \"records read\", ...). However we should also check non-integer metrics, such as timing metric and size metric. These metrics are in the same format of `\"total (min, med, max)\"`. These help functions could be used to check all these metrics. Please see the screenshot I posted above to see more timing or size metric examples (shuffle write, shuffle read, ...).\r\n\r\n",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-27T05:31:59Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse time string: \" + str)\n+    }\n+  }\n+\n+  /**\n+   * Convert a size metric string to a sequence of stats, including sum, min, med and max in order,\n+   * each a tuple of (value, unit).\n+   * @param metricStr size metric string, e.g. \"\\n96.2 MB (32.1 MB, 32.1 MB, 32.1 MB)\"\n+   * @return A sequence of stats, e.g. ((96.2,MB), (32.1,MB), (32.1,MB), (32.1,MB))\n+   */\n+  protected def sizeMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToBytes)\n+  }\n+\n+  /**\n+   * Convert a timing metric string to a sequence of stats, including sum, min, med and max in\n+   * order, each a tuple of (value, unit).\n+   * @param metricStr timing metric string, e.g. \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+   * @return A sequence of stats, e.g. ((2.0,ms), (1.0,ms), (1.0,ms), (1.0,ms))\n+   */\n+  protected def timingMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToDuration)\n+  }"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "I think we can actually remove all them for now. I think we can just check that the metrics are defined, since we are not really checking their values (the only one for which we are ensuring something is the peak memory...). I'd propose defining a `testSparkPlanMetricsPattern` which is basically the same as `testSparkPlanMetrics` but instead of providing a value for each metric, we pass a pattern. What do you think?",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-27T10:54:30Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse time string: \" + str)\n+    }\n+  }\n+\n+  /**\n+   * Convert a size metric string to a sequence of stats, including sum, min, med and max in order,\n+   * each a tuple of (value, unit).\n+   * @param metricStr size metric string, e.g. \"\\n96.2 MB (32.1 MB, 32.1 MB, 32.1 MB)\"\n+   * @return A sequence of stats, e.g. ((96.2,MB), (32.1,MB), (32.1,MB), (32.1,MB))\n+   */\n+  protected def sizeMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToBytes)\n+  }\n+\n+  /**\n+   * Convert a timing metric string to a sequence of stats, including sum, min, med and max in\n+   * order, each a tuple of (value, unit).\n+   * @param metricStr timing metric string, e.g. \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+   * @return A sequence of stats, e.g. ((2.0,ms), (1.0,ms), (1.0,ms), (1.0,ms))\n+   */\n+  protected def timingMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToDuration)\n+  }"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "It's a great idea to add a method similar to testSparkPlanMetrics. Let me try. I'd like to slightly change the method name to testSparkPlanMetricsWithPredicates, since we are actually passing in predicates.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-28T05:38:54Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse time string: \" + str)\n+    }\n+  }\n+\n+  /**\n+   * Convert a size metric string to a sequence of stats, including sum, min, med and max in order,\n+   * each a tuple of (value, unit).\n+   * @param metricStr size metric string, e.g. \"\\n96.2 MB (32.1 MB, 32.1 MB, 32.1 MB)\"\n+   * @return A sequence of stats, e.g. ((96.2,MB), (32.1,MB), (32.1,MB), (32.1,MB))\n+   */\n+  protected def sizeMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToBytes)\n+  }\n+\n+  /**\n+   * Convert a timing metric string to a sequence of stats, including sum, min, med and max in\n+   * order, each a tuple of (value, unit).\n+   * @param metricStr timing metric string, e.g. \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+   * @return A sequence of stats, e.g. ((2.0,ms), (1.0,ms), (1.0,ms), (1.0,ms))\n+   */\n+  protected def timingMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToDuration)\n+  }"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "As for checking metrics, checking \">= 0\" is better than just checking whether it is defined. because size or timing SQLMetric could be initialized by non-0 values, e.g. -1.\r\n\r\nhttps://github.com/apache/spark/blob/1e55f31e382cf67fd38ea8001d0b1d6b3bdcc586/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala#L109-L125",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-28T05:49:31Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse time string: \" + str)\n+    }\n+  }\n+\n+  /**\n+   * Convert a size metric string to a sequence of stats, including sum, min, med and max in order,\n+   * each a tuple of (value, unit).\n+   * @param metricStr size metric string, e.g. \"\\n96.2 MB (32.1 MB, 32.1 MB, 32.1 MB)\"\n+   * @return A sequence of stats, e.g. ((96.2,MB), (32.1,MB), (32.1,MB), (32.1,MB))\n+   */\n+  protected def sizeMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToBytes)\n+  }\n+\n+  /**\n+   * Convert a timing metric string to a sequence of stats, including sum, min, med and max in\n+   * order, each a tuple of (value, unit).\n+   * @param metricStr timing metric string, e.g. \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+   * @return A sequence of stats, e.g. ((2.0,ms), (1.0,ms), (1.0,ms), (1.0,ms))\n+   */\n+  protected def timingMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToDuration)\n+  }"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "In a new commit, I have added `SQLMetricsTestUtils#testSparkPlanMetricsWithPredicates`. In such a way, we simply need to provide a test spec in `test(\"Sort metrics\")` to make the test case declarative rather than procedural.\r\n\r\nTo simplify timing and size metric testing, I added 2 common predicates, `timingMetricAllStatsShould` and `sizeMetricAllStatsShould`. These could be used for other metrics as long as they are timing or size metrics.\r\n\r\nAnd I also modified the original `testSparkPlanMetrics` to make it a special case of `testSparkPlanMetricsWithPredicates`, where each expected metric value is converted to an equality predicate. This eliminated duplicate code as `testSparkPlanMetrics` and `testSparkPlanMetricsWithPredicates` are almost the same.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-28T10:43:49Z",
    "diffHunk": "@@ -198,6 +199,52 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  private def metricStats(metricStr: String): Seq[String] = {\n+    val sum = metricStr.substring(0, metricStr.indexOf(\"(\")).stripPrefix(\"\\n\").stripSuffix(\" \")\n+    val minMedMax = metricStr.substring(metricStr.indexOf(\"(\") + 1, metricStr.indexOf(\")\"))\n+      .split(\", \").toSeq\n+    (sum +: minMedMax)\n+  }\n+\n+  private def stringToBytes(str: String): (Float, String) = {\n+    val matcher =\n+      Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse byte string: \" + str)\n+    }\n+  }\n+\n+  private def stringToDuration(str: String): (Float, String) = {\n+    val matcher = Pattern.compile(\"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\").matcher(str)\n+    if (matcher.matches()) {\n+      (matcher.group(1).toFloat, matcher.group(3))\n+    } else {\n+      throw new NumberFormatException(\"Failed to parse time string: \" + str)\n+    }\n+  }\n+\n+  /**\n+   * Convert a size metric string to a sequence of stats, including sum, min, med and max in order,\n+   * each a tuple of (value, unit).\n+   * @param metricStr size metric string, e.g. \"\\n96.2 MB (32.1 MB, 32.1 MB, 32.1 MB)\"\n+   * @return A sequence of stats, e.g. ((96.2,MB), (32.1,MB), (32.1,MB), (32.1,MB))\n+   */\n+  protected def sizeMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToBytes)\n+  }\n+\n+  /**\n+   * Convert a timing metric string to a sequence of stats, including sum, min, med and max in\n+   * order, each a tuple of (value, unit).\n+   * @param metricStr timing metric string, e.g. \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+   * @return A sequence of stats, e.g. ((2.0,ms), (1.0,ms), (1.0,ms), (1.0,ms))\n+   */\n+  protected def timingMetricStats(metricStr: String): Seq[(Float, String)] = {\n+    metricStats(metricStr).map(stringToDuration)\n+  }"
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "nit: go to 100 chars and the next line has a bad indentation",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-28T10:52:18Z",
    "diffHunk": "@@ -185,19 +190,105 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue)\n+    )}\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is",
    "line": 35
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "Because usually metric values are numbers, so for metrics values, predicates could be more natural than regular expressions which are more suitable for text matching. For simple metric values, helper functions are not needed. However, timing and size metric values are a little complex:\r\n\r\n* timing metric value example: \"\\n96.2 MB (32.1 MB, 32.1 MB, 32.1 MB)\"\r\n* size metric value example: \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\r\n\r\nWith helper functions, we extract stats (by `timingMetricStats` or `sizeMetricStats` method), then we can apply predicates to check any stats (all stats or any single one). `timingMetricAllStatsShould` and `sizeMetricAllStatsShould` are not required, they are something like syntax sugar to eliminate boilerplate code since timing and size metrics are frequently used. If we want to check any single value (e.g sum >=0), we can provide a predicate like below:\r\n```\r\ntimingMetricStats(_)(0)._1 >= 0\r\n```\r\n\r\nBTW, may be timing and size metric values should be stored in a more structured way rather than pure text format (even with \"\\n\" in values).",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-28T15:34:29Z",
    "diffHunk": "@@ -185,19 +190,105 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue)\n+    )}\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is",
    "line": 35
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "Yes, indentation is not right. I have fixed it in the new commit.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-28T15:55:56Z",
    "diffHunk": "@@ -185,19 +190,105 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue)\n+    )}\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is",
    "line": 35
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "my point is: as of now, pattern matching is enough for what we need to check and we do not have a use case when we actually need to parse the exact values. Doing that, we can simplify this PR and reduce considerably the size of this change. So I think we should go this way. If in the future we will need something like you proposed here because we want to check the actual values, then we can introduce all the methods you are suggesting here. But as of know this can be skipped IMO.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-28T23:56:09Z",
    "diffHunk": "@@ -185,19 +190,105 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue)\n+    )}\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is",
    "line": 35
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "This does look like a load of additional code that I think duplicates some existing code in Utils? is it really necessary to make some basic assertions about metric values?",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-29T00:00:03Z",
    "diffHunk": "@@ -185,19 +190,105 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue)\n+    )}\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is",
    "line": 35
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "@mgaido91 I agree. Thanks for your detailed and clear explanation. Checking metric values do make things unnecessarily complex.\r\n\r\n@srowen As @mgaido91 said, currently it is not necessary to check metric values, pattern matching is enough, and we could eliminate these methods. As for code duplication, methods here are not duplicate with code in `Utils`. `Utils` provides a bunch of methods to do conversion between string and bytes, bytes there are of type `Long`. However bytes in metric values are of type `Float`, e.g. `96.2 MB`.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-29T02:22:27Z",
    "diffHunk": "@@ -185,19 +190,105 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue)\n+    )}\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is",
    "line": 35
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "Hi, I have switched to pattern matching and also removed unnecessary helper methods in the new commit.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-29T05:47:40Z",
    "diffHunk": "@@ -185,19 +190,105 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue)\n+    )}\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is",
    "line": 35
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Add `.r` to the end of these strings to make them a `scala.util.matching.Regex` automatically. That's more idiomatic for Scala. No need to import and use Java's `Pattern`.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T12:57:25Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\"\n+  protected val sizeMetricPattern = Pattern.compile(s\"\\\\n$bytes \\\\($bytes, $bytes, $bytes\\\\)\")"
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "`private`? or you can inline this in an initializer below:\r\n\r\n```\r\nprotected val sizeMetricPattern = {\r\n  val bytes = ...\r\n  \"s\\\\n$bytes...\".r\r\n}",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T12:58:11Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\""
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "It might be a little cleaner to iterate over (key, value) pairs here and below rather than iterate over keys then get values:\r\n\r\n```\r\nfor ((nodeId, (expectedNodeName, expectedMetricsPredicatesMap) <- expectedMetricsPredicates) {\r\n```",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:00:33Z",
    "diffHunk": "@@ -198,6 +219,32 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is\n+   * `nodeId -> (operatorName, metric name -> metric value predicate)`.\n+   */\n+  protected def testSparkPlanMetricsWithPredicates(\n+      df: DataFrame,\n+      expectedNumOfJobs: Int,\n+      expectedMetricsPredicates: Map[Long, (String, Map[String, Any => Boolean])]): Unit = {\n+    val optActualMetrics =\n+      getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetricsPredicates.keySet)\n+    optActualMetrics.foreach { actualMetrics =>\n+      assert(expectedMetricsPredicates.keySet === actualMetrics.keySet)\n+      for (nodeId <- expectedMetricsPredicates.keySet) {"
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "nit: in the next line",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:01:49Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\"\n+  protected val sizeMetricPattern = Pattern.compile(s\"\\\\n$bytes \\\\($bytes, $bytes, $bytes\\\\)\")\n+\n+  // \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+  protected val timingMetricPattern =\n+    Pattern.compile(s\"\\\\n$duration \\\\($duration, $duration, $duration\\\\)\")\n+\n+  /** Generate a function to check the specified pattern."
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "not very useful, we can remove it",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:02:09Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\"\n+  protected val sizeMetricPattern = Pattern.compile(s\"\\\\n$bytes \\\\($bytes, $bytes, $bytes\\\\)\")\n+\n+  // \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+  protected val timingMetricPattern =\n+    Pattern.compile(s\"\\\\n$duration \\\\($duration, $duration, $duration\\\\)\")\n+\n+  /** Generate a function to check the specified pattern.\n+   *\n+   * @param pattern a pattern"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "removed `checkPattern` method.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:34:02Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\"\n+  protected val sizeMetricPattern = Pattern.compile(s\"\\\\n$bytes \\\\($bytes, $bytes, $bytes\\\\)\")\n+\n+  // \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+  protected val timingMetricPattern =\n+    Pattern.compile(s\"\\\\n$duration \\\\($duration, $duration, $duration\\\\)\")\n+\n+  /** Generate a function to check the specified pattern.\n+   *\n+   * @param pattern a pattern"
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "we can update this in order to avoid code duplication and reuse `testSparkPlanMetrics`.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:03:32Z",
    "diffHunk": "@@ -198,6 +219,32 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "I have updated `testSparkPlanMetrics` to invoke `testSparkPlanMetricsWithPredicates` to avoid code duplication.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:36:02Z",
    "diffHunk": "@@ -198,6 +219,32 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }"
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "nit: indent",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:03:54Z",
    "diffHunk": "@@ -198,6 +219,32 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is\n+   * `nodeId -> (operatorName, metric name -> metric value predicate)`."
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "Fixed indentation.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:36:33Z",
    "diffHunk": "@@ -198,6 +219,32 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       }\n     }\n   }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is\n+   * `nodeId -> (operatorName, metric name -> metric value predicate)`."
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "nit: private? and maybe close to where it is used?",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:04:35Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\""
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "I have inlined this in an initializer as @srowen  suggested.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:38:21Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\""
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "ditto",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:04:46Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\""
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "I have inlined this in an initializer as @srowen  suggested.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:38:41Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\""
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "Shall we say something more here? A line which explains what this is and then `eg. ` and your example is fine IMHO.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:05:37Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\""
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "I have added more comments.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:39:03Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\""
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Is this method really needed? the only place it's used is the very specific method for testing metrics, and that always provides a regex. Just provide a map to regexes that you check against, rather than whole predicates?\r\n\r\nOr, consider _not_ compiling regexes above and keeping them as string patterns. Then, the predicate you pass is just something like `sizeMetricPattern.matches(_)`. It means compiling the regex on every check, but for this test context, that's no big deal.\r\n\r\nThat would help limit the complexity of all this",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T13:09:59Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\"\n+  protected val sizeMetricPattern = Pattern.compile(s\"\\\\n$bytes \\\\($bytes, $bytes, $bytes\\\\)\")\n+\n+  // \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+  protected val timingMetricPattern =\n+    Pattern.compile(s\"\\\\n$duration \\\\($duration, $duration, $duration\\\\)\")\n+\n+  /** Generate a function to check the specified pattern.\n+   *\n+   * @param pattern a pattern\n+   * @return a function to check the specified pattern\n+   */\n+  protected def checkPattern(pattern: Pattern): (Any => Boolean) = {"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "I'd like to take the 2nd option.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:42:17Z",
    "diffHunk": "@@ -40,6 +41,26 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n \n   protected def statusStore: SQLAppStatusStore = spark.sharedState.statusStore\n \n+  protected val bytes = \"([0-9]+(\\\\.[0-9]+)?) (EiB|PiB|TiB|GiB|MiB|KiB|B)\"\n+\n+  protected val duration = \"([0-9]+(\\\\.[0-9]+)?) (ms|s|m|h)\"\n+\n+  // \"\\n96.2 MiB (32.1 MiB, 32.1 MiB, 32.1 MiB)\"\n+  protected val sizeMetricPattern = Pattern.compile(s\"\\\\n$bytes \\\\($bytes, $bytes, $bytes\\\\)\")\n+\n+  // \"\\n2.0 ms (1.0 ms, 1.0 ms, 1.0 ms)\"\n+  protected val timingMetricPattern =\n+    Pattern.compile(s\"\\\\n$duration \\\\($duration, $duration, $duration\\\\)\")\n+\n+  /** Generate a function to check the specified pattern.\n+   *\n+   * @param pattern a pattern\n+   * @return a function to check the specified pattern\n+   */\n+  protected def checkPattern(pattern: Pattern): (Any => Boolean) = {"
  }],
  "prId": 23258
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You can use a similar iteration over the map here that avoid the keySet and get",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T16:59:40Z",
    "diffHunk": "@@ -185,15 +197,34 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue))\n+    }\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is\n+   *                                  `nodeId -> (operatorName, metric name -> metric predicate)`.\n+   */\n+  protected def testSparkPlanMetricsWithPredicates(\n+      df: DataFrame,\n+      expectedNumOfJobs: Int,\n+      expectedMetricsPredicates: Map[Long, (String, Map[String, Any => Boolean])]): Unit = {\n+    val optActualMetrics =\n+      getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetricsPredicates.keySet)\n     optActualMetrics.foreach { actualMetrics =>\n-      assert(expectedMetrics.keySet === actualMetrics.keySet)\n-      for (nodeId <- expectedMetrics.keySet) {\n-        val (expectedNodeName, expectedMetricsMap) = expectedMetrics(nodeId)\n+      assert(expectedMetricsPredicates.keySet === actualMetrics.keySet)\n+      for ((nodeId, (expectedNodeName, expectedMetricsPredicatesMap))\n+          <- expectedMetricsPredicates) {\n         val (actualNodeName, actualMetricsMap) = actualMetrics(nodeId)\n         assert(expectedNodeName === actualNodeName)\n-        for (metricName <- expectedMetricsMap.keySet) {\n-          assert(expectedMetricsMap(metricName).toString === actualMetricsMap(metricName))\n+        for (metricName <- expectedMetricsPredicatesMap.keySet) {"
  }, {
    "author": {
      "login": "seancxmao"
    },
    "body": "Changed in the new commit.",
    "commit": "5e94a3eb8bb515ee9638c7767882ddd559d7b6ac",
    "createdAt": "2018-12-30T22:43:54Z",
    "diffHunk": "@@ -185,15 +197,34 @@ trait SQLMetricsTestUtils extends SQLTestUtils {\n       df: DataFrame,\n       expectedNumOfJobs: Int,\n       expectedMetrics: Map[Long, (String, Map[String, Any])]): Unit = {\n-    val optActualMetrics = getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetrics.keySet)\n+    val expectedMetricsPredicates = expectedMetrics.mapValues { case (nodeName, nodeMetrics) =>\n+      (nodeName, nodeMetrics.mapValues(expectedMetricValue =>\n+        (actualMetricValue: Any) => expectedMetricValue.toString === actualMetricValue))\n+    }\n+    testSparkPlanMetricsWithPredicates(df, expectedNumOfJobs, expectedMetricsPredicates)\n+  }\n+\n+  /**\n+   * Call `df.collect()` and verify if the collected metrics satisfy the specified predicates.\n+   * @param df `DataFrame` to run\n+   * @param expectedNumOfJobs number of jobs that will run\n+   * @param expectedMetricsPredicates the expected metrics predicates. The format is\n+   *                                  `nodeId -> (operatorName, metric name -> metric predicate)`.\n+   */\n+  protected def testSparkPlanMetricsWithPredicates(\n+      df: DataFrame,\n+      expectedNumOfJobs: Int,\n+      expectedMetricsPredicates: Map[Long, (String, Map[String, Any => Boolean])]): Unit = {\n+    val optActualMetrics =\n+      getSparkPlanMetrics(df, expectedNumOfJobs, expectedMetricsPredicates.keySet)\n     optActualMetrics.foreach { actualMetrics =>\n-      assert(expectedMetrics.keySet === actualMetrics.keySet)\n-      for (nodeId <- expectedMetrics.keySet) {\n-        val (expectedNodeName, expectedMetricsMap) = expectedMetrics(nodeId)\n+      assert(expectedMetricsPredicates.keySet === actualMetrics.keySet)\n+      for ((nodeId, (expectedNodeName, expectedMetricsPredicatesMap))\n+          <- expectedMetricsPredicates) {\n         val (actualNodeName, actualMetricsMap) = actualMetrics(nodeId)\n         assert(expectedNodeName === actualNodeName)\n-        for (metricName <- expectedMetricsMap.keySet) {\n-          assert(expectedMetricsMap(metricName).toString === actualMetricsMap(metricName))\n+        for (metricName <- expectedMetricsPredicatesMap.keySet) {"
  }],
  "prId": 23258
}]