[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Could you remove ss_max?\n",
    "commit": "18150121df04c8f0fd39c2c2fbfbc7fc39ccbd64",
    "createdAt": "2016-05-20T02:54:42Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark.tpcds\n+\n+import java.io.File\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation\n+import org.apache.spark.sql.catalyst.expressions.SubqueryExpression\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure TPCDS query performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object TPCDSQueryBenchmark {\n+  val conf =\n+    new SparkConf()\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"test-sql-context\")\n+      .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+      .set(\"spark.sql.shuffle.partitions\", \"4\")\n+      .set(\"spark.driver.memory\", \"3g\")\n+      .set(\"spark.executor.memory\", \"3g\")\n+      .set(\"spark.sql.autoBroadcastJoinThreshold\", (20 * 1024 * 1024).toString)\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  val tables = Seq(\"catalog_page\", \"catalog_returns\", \"customer\", \"customer_address\",\n+    \"customer_demographics\", \"date_dim\", \"household_demographics\", \"inventory\", \"item\",\n+    \"promotion\", \"store\", \"store_returns\", \"catalog_sales\", \"web_sales\", \"store_sales\",\n+    \"web_returns\", \"web_site\", \"reason\", \"call_center\", \"warehouse\", \"ship_mode\", \"income_band\",\n+    \"time_dim\", \"web_page\")\n+\n+  def setupTables(dataLocation: String): Map[String, Long] = {\n+    tables.map { tableName =>\n+      spark.read.parquet(s\"$dataLocation/$tableName\").createOrReplaceTempView(tableName)\n+      tableName -> spark.table(tableName).count()\n+    }.toMap\n+  }\n+\n+  def tpcdsAll(dataLocation: String, queries: Seq[String]): Unit = {\n+    require(dataLocation.nonEmpty,\n+      \"please modify the value of dataLocation to point to your local TPCDS data\")\n+    val tableSizes = setupTables(dataLocation)\n+    spark.conf.set(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, \"true\")\n+    spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+    queries.foreach { name =>\n+      val queriesString = fileToString(new File(s\"sql/core/src/test/scala/org/apache/spark/sql/\" +\n+        s\"execution/benchmark/tpcds/queries/$name.sql\"))\n+\n+      // This is an indirect hack to estimate the size of each query's input by traversing the\n+      // logical plan and adding up the sizes of all tables that appear in the plan. Note that this\n+      // currently doesn't take WITH subqueries into account which might lead to fairly inaccurate\n+      // per-row processing time for those cases.\n+      val queryRelations = scala.collection.mutable.HashSet[String]()\n+      spark.sql(queriesString).queryExecution.logical.map {\n+        case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+          queryRelations.add(t.table)\n+        case lp: LogicalPlan =>\n+          lp.expressions.foreach { _ foreach {\n+            case subquery: SubqueryExpression =>\n+              subquery.plan.foreach {\n+                case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+                  queryRelations.add(t.table)\n+                case _ =>\n+              }\n+            case _ =>\n+          }\n+        }\n+        case _ =>\n+      }\n+      val numRows = queryRelations.map(tableSizes.getOrElse(_, 0L)).sum\n+      val benchmark = new Benchmark(\"TPCDS Snappy\", numRows, 5)\n+      benchmark.addCase(name) { i =>\n+        spark.sql(queriesString).collect()\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+\n+    // List of all TPC-DS queries\n+    val allQueries = Seq(\n+      \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\", \"q11\",\n+      \"q12\", \"q13\", \"q14a\", \"q14b\", \"q15\", \"q16\", \"q17\", \"q18\", \"q19\", \"q20\",\n+      \"q21\", \"q22\", \"q23a\", \"q23b\", \"q24a\", \"q24b\", \"q25\", \"q26\", \"q27\", \"q28\", \"q29\", \"q30\",\n+      \"q31\", \"q32\", \"q33\", \"q34\", \"q35\", \"q36\", \"q37\", \"q38\", \"q39a\", \"q39b\", \"q40\",\n+      \"q41\", \"q42\", \"q43\", \"q44\", \"q45\", \"q46\", \"q47\", \"q48\", \"q49\", \"q50\",\n+      \"q51\", \"q52\", \"q53\", \"q54\", \"q55\", \"q56\", \"q57\", \"q58\", \"q59\", \"q60\",\n+      \"q61\", \"q62\", \"q63\", \"q64\", \"q65\", \"q66\", \"q67\", \"q68\", \"q69\", \"q70\",\n+      \"q71\", \"q72\", \"q73\", \"q74\", \"q75\", \"q76\", \"q77\", \"q78\", \"q79\", \"q80\",\n+      \"q81\", \"q82\", \"q83\", \"q84\", \"q85\", \"q86\", \"q87\", \"q88\", \"q89\", \"q90\",\n+      \"q91\", \"q92\", \"q93\", \"q94\", \"q95\", \"q96\", \"q97\", \"q98\", \"q99\", \"ss_max\")"
  }, {
    "author": {
      "login": "sameeragarwal"
    },
    "body": "Reynold suggested that it might be a good idea to keep it around (https://github.com/apache/spark/pull/13188#discussion_r63973393). Let me know if you think otherwise.\n",
    "commit": "18150121df04c8f0fd39c2c2fbfbc7fc39ccbd64",
    "createdAt": "2016-05-20T05:14:38Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark.tpcds\n+\n+import java.io.File\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation\n+import org.apache.spark.sql.catalyst.expressions.SubqueryExpression\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure TPCDS query performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object TPCDSQueryBenchmark {\n+  val conf =\n+    new SparkConf()\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"test-sql-context\")\n+      .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+      .set(\"spark.sql.shuffle.partitions\", \"4\")\n+      .set(\"spark.driver.memory\", \"3g\")\n+      .set(\"spark.executor.memory\", \"3g\")\n+      .set(\"spark.sql.autoBroadcastJoinThreshold\", (20 * 1024 * 1024).toString)\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  val tables = Seq(\"catalog_page\", \"catalog_returns\", \"customer\", \"customer_address\",\n+    \"customer_demographics\", \"date_dim\", \"household_demographics\", \"inventory\", \"item\",\n+    \"promotion\", \"store\", \"store_returns\", \"catalog_sales\", \"web_sales\", \"store_sales\",\n+    \"web_returns\", \"web_site\", \"reason\", \"call_center\", \"warehouse\", \"ship_mode\", \"income_band\",\n+    \"time_dim\", \"web_page\")\n+\n+  def setupTables(dataLocation: String): Map[String, Long] = {\n+    tables.map { tableName =>\n+      spark.read.parquet(s\"$dataLocation/$tableName\").createOrReplaceTempView(tableName)\n+      tableName -> spark.table(tableName).count()\n+    }.toMap\n+  }\n+\n+  def tpcdsAll(dataLocation: String, queries: Seq[String]): Unit = {\n+    require(dataLocation.nonEmpty,\n+      \"please modify the value of dataLocation to point to your local TPCDS data\")\n+    val tableSizes = setupTables(dataLocation)\n+    spark.conf.set(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, \"true\")\n+    spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+    queries.foreach { name =>\n+      val queriesString = fileToString(new File(s\"sql/core/src/test/scala/org/apache/spark/sql/\" +\n+        s\"execution/benchmark/tpcds/queries/$name.sql\"))\n+\n+      // This is an indirect hack to estimate the size of each query's input by traversing the\n+      // logical plan and adding up the sizes of all tables that appear in the plan. Note that this\n+      // currently doesn't take WITH subqueries into account which might lead to fairly inaccurate\n+      // per-row processing time for those cases.\n+      val queryRelations = scala.collection.mutable.HashSet[String]()\n+      spark.sql(queriesString).queryExecution.logical.map {\n+        case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+          queryRelations.add(t.table)\n+        case lp: LogicalPlan =>\n+          lp.expressions.foreach { _ foreach {\n+            case subquery: SubqueryExpression =>\n+              subquery.plan.foreach {\n+                case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+                  queryRelations.add(t.table)\n+                case _ =>\n+              }\n+            case _ =>\n+          }\n+        }\n+        case _ =>\n+      }\n+      val numRows = queryRelations.map(tableSizes.getOrElse(_, 0L)).sum\n+      val benchmark = new Benchmark(\"TPCDS Snappy\", numRows, 5)\n+      benchmark.addCase(name) { i =>\n+        spark.sql(queriesString).collect()\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+\n+    // List of all TPC-DS queries\n+    val allQueries = Seq(\n+      \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\", \"q11\",\n+      \"q12\", \"q13\", \"q14a\", \"q14b\", \"q15\", \"q16\", \"q17\", \"q18\", \"q19\", \"q20\",\n+      \"q21\", \"q22\", \"q23a\", \"q23b\", \"q24a\", \"q24b\", \"q25\", \"q26\", \"q27\", \"q28\", \"q29\", \"q30\",\n+      \"q31\", \"q32\", \"q33\", \"q34\", \"q35\", \"q36\", \"q37\", \"q38\", \"q39a\", \"q39b\", \"q40\",\n+      \"q41\", \"q42\", \"q43\", \"q44\", \"q45\", \"q46\", \"q47\", \"q48\", \"q49\", \"q50\",\n+      \"q51\", \"q52\", \"q53\", \"q54\", \"q55\", \"q56\", \"q57\", \"q58\", \"q59\", \"q60\",\n+      \"q61\", \"q62\", \"q63\", \"q64\", \"q65\", \"q66\", \"q67\", \"q68\", \"q69\", \"q70\",\n+      \"q71\", \"q72\", \"q73\", \"q74\", \"q75\", \"q76\", \"q77\", \"q78\", \"q79\", \"q80\",\n+      \"q81\", \"q82\", \"q83\", \"q84\", \"q85\", \"q86\", \"q87\", \"q88\", \"q89\", \"q90\",\n+      \"q91\", \"q92\", \"q93\", \"q94\", \"q95\", \"q96\", \"q97\", \"q98\", \"q99\", \"ss_max\")"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "If we end up keeping it, we should probably have a comment saying it is not part of tpcds, but added from the impala test kit.\n",
    "commit": "18150121df04c8f0fd39c2c2fbfbc7fc39ccbd64",
    "createdAt": "2016-05-20T05:16:49Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark.tpcds\n+\n+import java.io.File\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation\n+import org.apache.spark.sql.catalyst.expressions.SubqueryExpression\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure TPCDS query performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object TPCDSQueryBenchmark {\n+  val conf =\n+    new SparkConf()\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"test-sql-context\")\n+      .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+      .set(\"spark.sql.shuffle.partitions\", \"4\")\n+      .set(\"spark.driver.memory\", \"3g\")\n+      .set(\"spark.executor.memory\", \"3g\")\n+      .set(\"spark.sql.autoBroadcastJoinThreshold\", (20 * 1024 * 1024).toString)\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  val tables = Seq(\"catalog_page\", \"catalog_returns\", \"customer\", \"customer_address\",\n+    \"customer_demographics\", \"date_dim\", \"household_demographics\", \"inventory\", \"item\",\n+    \"promotion\", \"store\", \"store_returns\", \"catalog_sales\", \"web_sales\", \"store_sales\",\n+    \"web_returns\", \"web_site\", \"reason\", \"call_center\", \"warehouse\", \"ship_mode\", \"income_band\",\n+    \"time_dim\", \"web_page\")\n+\n+  def setupTables(dataLocation: String): Map[String, Long] = {\n+    tables.map { tableName =>\n+      spark.read.parquet(s\"$dataLocation/$tableName\").createOrReplaceTempView(tableName)\n+      tableName -> spark.table(tableName).count()\n+    }.toMap\n+  }\n+\n+  def tpcdsAll(dataLocation: String, queries: Seq[String]): Unit = {\n+    require(dataLocation.nonEmpty,\n+      \"please modify the value of dataLocation to point to your local TPCDS data\")\n+    val tableSizes = setupTables(dataLocation)\n+    spark.conf.set(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, \"true\")\n+    spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+    queries.foreach { name =>\n+      val queriesString = fileToString(new File(s\"sql/core/src/test/scala/org/apache/spark/sql/\" +\n+        s\"execution/benchmark/tpcds/queries/$name.sql\"))\n+\n+      // This is an indirect hack to estimate the size of each query's input by traversing the\n+      // logical plan and adding up the sizes of all tables that appear in the plan. Note that this\n+      // currently doesn't take WITH subqueries into account which might lead to fairly inaccurate\n+      // per-row processing time for those cases.\n+      val queryRelations = scala.collection.mutable.HashSet[String]()\n+      spark.sql(queriesString).queryExecution.logical.map {\n+        case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+          queryRelations.add(t.table)\n+        case lp: LogicalPlan =>\n+          lp.expressions.foreach { _ foreach {\n+            case subquery: SubqueryExpression =>\n+              subquery.plan.foreach {\n+                case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+                  queryRelations.add(t.table)\n+                case _ =>\n+              }\n+            case _ =>\n+          }\n+        }\n+        case _ =>\n+      }\n+      val numRows = queryRelations.map(tableSizes.getOrElse(_, 0L)).sum\n+      val benchmark = new Benchmark(\"TPCDS Snappy\", numRows, 5)\n+      benchmark.addCase(name) { i =>\n+        spark.sql(queriesString).collect()\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+\n+    // List of all TPC-DS queries\n+    val allQueries = Seq(\n+      \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\", \"q11\",\n+      \"q12\", \"q13\", \"q14a\", \"q14b\", \"q15\", \"q16\", \"q17\", \"q18\", \"q19\", \"q20\",\n+      \"q21\", \"q22\", \"q23a\", \"q23b\", \"q24a\", \"q24b\", \"q25\", \"q26\", \"q27\", \"q28\", \"q29\", \"q30\",\n+      \"q31\", \"q32\", \"q33\", \"q34\", \"q35\", \"q36\", \"q37\", \"q38\", \"q39a\", \"q39b\", \"q40\",\n+      \"q41\", \"q42\", \"q43\", \"q44\", \"q45\", \"q46\", \"q47\", \"q48\", \"q49\", \"q50\",\n+      \"q51\", \"q52\", \"q53\", \"q54\", \"q55\", \"q56\", \"q57\", \"q58\", \"q59\", \"q60\",\n+      \"q61\", \"q62\", \"q63\", \"q64\", \"q65\", \"q66\", \"q67\", \"q68\", \"q69\", \"q70\",\n+      \"q71\", \"q72\", \"q73\", \"q74\", \"q75\", \"q76\", \"q77\", \"q78\", \"q79\", \"q80\",\n+      \"q81\", \"q82\", \"q83\", \"q84\", \"q85\", \"q86\", \"q87\", \"q88\", \"q89\", \"q90\",\n+      \"q91\", \"q92\", \"q93\", \"q94\", \"q95\", \"q96\", \"q97\", \"q98\", \"q99\", \"ss_max\")"
  }, {
    "author": {
      "login": "sameeragarwal"
    },
    "body": "oh, I think that's what davies meant. I'll remove `ss_max` from `allQueries` as it's already part of `commonQueries` below. Additionally, let me rename `allQueries` -> `allTpcdsQueries` and `commonQueries` -> `impalaKitQueries` so that the query sets are more obvious.\n",
    "commit": "18150121df04c8f0fd39c2c2fbfbc7fc39ccbd64",
    "createdAt": "2016-05-20T05:34:11Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark.tpcds\n+\n+import java.io.File\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation\n+import org.apache.spark.sql.catalyst.expressions.SubqueryExpression\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure TPCDS query performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object TPCDSQueryBenchmark {\n+  val conf =\n+    new SparkConf()\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"test-sql-context\")\n+      .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+      .set(\"spark.sql.shuffle.partitions\", \"4\")\n+      .set(\"spark.driver.memory\", \"3g\")\n+      .set(\"spark.executor.memory\", \"3g\")\n+      .set(\"spark.sql.autoBroadcastJoinThreshold\", (20 * 1024 * 1024).toString)\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  val tables = Seq(\"catalog_page\", \"catalog_returns\", \"customer\", \"customer_address\",\n+    \"customer_demographics\", \"date_dim\", \"household_demographics\", \"inventory\", \"item\",\n+    \"promotion\", \"store\", \"store_returns\", \"catalog_sales\", \"web_sales\", \"store_sales\",\n+    \"web_returns\", \"web_site\", \"reason\", \"call_center\", \"warehouse\", \"ship_mode\", \"income_band\",\n+    \"time_dim\", \"web_page\")\n+\n+  def setupTables(dataLocation: String): Map[String, Long] = {\n+    tables.map { tableName =>\n+      spark.read.parquet(s\"$dataLocation/$tableName\").createOrReplaceTempView(tableName)\n+      tableName -> spark.table(tableName).count()\n+    }.toMap\n+  }\n+\n+  def tpcdsAll(dataLocation: String, queries: Seq[String]): Unit = {\n+    require(dataLocation.nonEmpty,\n+      \"please modify the value of dataLocation to point to your local TPCDS data\")\n+    val tableSizes = setupTables(dataLocation)\n+    spark.conf.set(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, \"true\")\n+    spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+    queries.foreach { name =>\n+      val queriesString = fileToString(new File(s\"sql/core/src/test/scala/org/apache/spark/sql/\" +\n+        s\"execution/benchmark/tpcds/queries/$name.sql\"))\n+\n+      // This is an indirect hack to estimate the size of each query's input by traversing the\n+      // logical plan and adding up the sizes of all tables that appear in the plan. Note that this\n+      // currently doesn't take WITH subqueries into account which might lead to fairly inaccurate\n+      // per-row processing time for those cases.\n+      val queryRelations = scala.collection.mutable.HashSet[String]()\n+      spark.sql(queriesString).queryExecution.logical.map {\n+        case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+          queryRelations.add(t.table)\n+        case lp: LogicalPlan =>\n+          lp.expressions.foreach { _ foreach {\n+            case subquery: SubqueryExpression =>\n+              subquery.plan.foreach {\n+                case ur @ UnresolvedRelation(t: TableIdentifier, _) =>\n+                  queryRelations.add(t.table)\n+                case _ =>\n+              }\n+            case _ =>\n+          }\n+        }\n+        case _ =>\n+      }\n+      val numRows = queryRelations.map(tableSizes.getOrElse(_, 0L)).sum\n+      val benchmark = new Benchmark(\"TPCDS Snappy\", numRows, 5)\n+      benchmark.addCase(name) { i =>\n+        spark.sql(queriesString).collect()\n+      }\n+      benchmark.run()\n+    }\n+  }\n+\n+  def main(args: Array[String]): Unit = {\n+\n+    // List of all TPC-DS queries\n+    val allQueries = Seq(\n+      \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\", \"q11\",\n+      \"q12\", \"q13\", \"q14a\", \"q14b\", \"q15\", \"q16\", \"q17\", \"q18\", \"q19\", \"q20\",\n+      \"q21\", \"q22\", \"q23a\", \"q23b\", \"q24a\", \"q24b\", \"q25\", \"q26\", \"q27\", \"q28\", \"q29\", \"q30\",\n+      \"q31\", \"q32\", \"q33\", \"q34\", \"q35\", \"q36\", \"q37\", \"q38\", \"q39a\", \"q39b\", \"q40\",\n+      \"q41\", \"q42\", \"q43\", \"q44\", \"q45\", \"q46\", \"q47\", \"q48\", \"q49\", \"q50\",\n+      \"q51\", \"q52\", \"q53\", \"q54\", \"q55\", \"q56\", \"q57\", \"q58\", \"q59\", \"q60\",\n+      \"q61\", \"q62\", \"q63\", \"q64\", \"q65\", \"q66\", \"q67\", \"q68\", \"q69\", \"q70\",\n+      \"q71\", \"q72\", \"q73\", \"q74\", \"q75\", \"q76\", \"q77\", \"q78\", \"q79\", \"q80\",\n+      \"q81\", \"q82\", \"q83\", \"q84\", \"q85\", \"q86\", \"q87\", \"q88\", \"q89\", \"q90\",\n+      \"q91\", \"q92\", \"q93\", \"q94\", \"q95\", \"q96\", \"q97\", \"q98\", \"q99\", \"ss_max\")"
  }],
  "prId": 13188
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "one thing - these files should go into test/resources, and then we can get their path using the getresource function on the current thread's classloader.\n",
    "commit": "18150121df04c8f0fd39c2c2fbfbc7fc39ccbd64",
    "createdAt": "2016-05-20T05:17:51Z",
    "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.benchmark.tpcds\n+\n+import java.io.File\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation\n+import org.apache.spark.sql.catalyst.expressions.SubqueryExpression\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark to measure TPCDS query performance.\n+ * To run this:\n+ *  spark-submit --class <this class> --jars <spark sql test jar>\n+ */\n+object TPCDSQueryBenchmark {\n+  val conf =\n+    new SparkConf()\n+      .setMaster(\"local[1]\")\n+      .setAppName(\"test-sql-context\")\n+      .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n+      .set(\"spark.sql.shuffle.partitions\", \"4\")\n+      .set(\"spark.driver.memory\", \"3g\")\n+      .set(\"spark.executor.memory\", \"3g\")\n+      .set(\"spark.sql.autoBroadcastJoinThreshold\", (20 * 1024 * 1024).toString)\n+\n+  val spark = SparkSession.builder.config(conf).getOrCreate()\n+\n+  val tables = Seq(\"catalog_page\", \"catalog_returns\", \"customer\", \"customer_address\",\n+    \"customer_demographics\", \"date_dim\", \"household_demographics\", \"inventory\", \"item\",\n+    \"promotion\", \"store\", \"store_returns\", \"catalog_sales\", \"web_sales\", \"store_sales\",\n+    \"web_returns\", \"web_site\", \"reason\", \"call_center\", \"warehouse\", \"ship_mode\", \"income_band\",\n+    \"time_dim\", \"web_page\")\n+\n+  def setupTables(dataLocation: String): Map[String, Long] = {\n+    tables.map { tableName =>\n+      spark.read.parquet(s\"$dataLocation/$tableName\").createOrReplaceTempView(tableName)\n+      tableName -> spark.table(tableName).count()\n+    }.toMap\n+  }\n+\n+  def tpcdsAll(dataLocation: String, queries: Seq[String]): Unit = {\n+    require(dataLocation.nonEmpty,\n+      \"please modify the value of dataLocation to point to your local TPCDS data\")\n+    val tableSizes = setupTables(dataLocation)\n+    spark.conf.set(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, \"true\")\n+    spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+    queries.foreach { name =>\n+      val queriesString = fileToString(new File(s\"sql/core/src/test/scala/org/apache/spark/sql/\" +"
  }],
  "prId": 13188
}]