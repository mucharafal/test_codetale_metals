[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Here, you can just replace it by \n\n``` Scala\n        val tableIdent = TableIdentifier(table, Option(\"default\"))\n        val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n        val columnStats =\n          AnalyzeColumnCommand(tableIdent, columnsToAnalyze).computeColStats(spark, relation)._2\n```\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-22T01:30:12Z",
    "diffHunk": "@@ -0,0 +1,352 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStats\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    def assertAnalyzeColumnCommand(analyzeCommand: String, c: Class[_]) {\n+      val parsed = spark.sessionState.sqlParser.parsePlan(analyzeCommand)\n+      val operators = parsed.collect {\n+        case a: AnalyzeColumnCommand => a\n+        case o => o\n+      }\n+      assert(operators.size == 1)\n+      if (operators.head.getClass != c) {\n+        fail(\n+          s\"\"\"$analyzeCommand expected command: $c, but got ${operators.head}\n+             |parsed command:\n+             |$parsed\n+           \"\"\".stripMargin)\n+      }\n+    }\n+\n+    val table = \"table\"\n+    assertAnalyzeColumnCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[ParseException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val colName1 = \"abc\"\n+    val colName2 = \"x.yz\"\n+    val quotedColName2 = s\"`$colName2`\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table ($colName1 int, $quotedColName2 string) USING PARQUET\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == \"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ${colName1.toUpperCase}\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ${colName1.toUpperCase}.\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val columnsToAnalyze = Seq(colName2.toUpperCase, colName1, colName2)\n+        val columnStats = spark.sessionState.computeColumnStats(table, columnsToAnalyze)"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Thanks!\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-22T02:29:02Z",
    "diffHunk": "@@ -0,0 +1,352 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStats\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    def assertAnalyzeColumnCommand(analyzeCommand: String, c: Class[_]) {\n+      val parsed = spark.sessionState.sqlParser.parsePlan(analyzeCommand)\n+      val operators = parsed.collect {\n+        case a: AnalyzeColumnCommand => a\n+        case o => o\n+      }\n+      assert(operators.size == 1)\n+      if (operators.head.getClass != c) {\n+        fail(\n+          s\"\"\"$analyzeCommand expected command: $c, but got ${operators.head}\n+             |parsed command:\n+             |$parsed\n+           \"\"\".stripMargin)\n+      }\n+    }\n+\n+    val table = \"table\"\n+    assertAnalyzeColumnCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[ParseException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val colName1 = \"abc\"\n+    val colName2 = \"x.yz\"\n+    val quotedColName2 = s\"`$colName2`\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table ($colName1 int, $quotedColName2 string) USING PARQUET\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == \"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ${colName1.toUpperCase}\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ${colName1.toUpperCase}.\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val columnsToAnalyze = Seq(colName2.toUpperCase, colName1, colName2)\n+        val columnStats = spark.sessionState.computeColumnStats(table, columnsToAnalyze)"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "please follow the style in `DDLCommandSuite` to write test for parser rules\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-29T10:02:14Z",
    "diffHunk": "@@ -0,0 +1,355 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {",
    "line": 33
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "I followed the style in StatisticsSuite. I think this style is ok.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-29T17:38:20Z",
    "diffHunk": "@@ -0,0 +1,355 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {",
    "line": 33
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "you can inline this variable, i.e.`CREATE TABLE $table ($colName1 int, `$colName2` string)`\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-29T10:04:24Z",
    "diffHunk": "@@ -0,0 +1,355 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    def assertAnalyzeColumnCommand(analyzeCommand: String, c: Class[_]) {\n+      val parsed = spark.sessionState.sqlParser.parsePlan(analyzeCommand)\n+      val operators = parsed.collect {\n+        case a: AnalyzeColumnCommand => a\n+        case o => o\n+      }\n+      assert(operators.size == 1)\n+      if (operators.head.getClass != c) {\n+        fail(\n+          s\"\"\"$analyzeCommand expected command: $c, but got ${operators.head}\n+             |parsed command:\n+             |$parsed\n+           \"\"\".stripMargin)\n+      }\n+    }\n+\n+    val table = \"table\"\n+    assertAnalyzeColumnCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[ParseException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+  }\n+\n+  test(\"analyzing columns of non-atomic types is not supported\") {\n+    val tableName = \"tbl\"\n+    withTable(tableName) {\n+      Seq(ArrayData(Seq(1, 2, 3), Seq(Seq(1, 2, 3)))).toDF().write.saveAsTable(tableName)\n+      val err = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS data\")\n+      }\n+      assert(err.message.contains(\"Analyzing columns is not supported\"))\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val colName1 = \"abc\"\n+    val colName2 = \"x.yz\"\n+    val quotedColName2 = s\"`$colName2`\""
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "What this `collect` do? \n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-30T00:01:23Z",
    "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    def assertAnalyzeColumnCommand(analyzeCommand: String, c: Class[_]) {\n+      val parsed = spark.sessionState.sqlParser.parsePlan(analyzeCommand)\n+      val operators = parsed.collect {"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I don't think it's a good style, parsing a SQL string and constructing an expected logical plan and then compare them looks much clearer.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-30T00:02:38Z",
    "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    def assertAnalyzeColumnCommand(analyzeCommand: String, c: Class[_]) {\n+      val parsed = spark.sessionState.sqlParser.parsePlan(analyzeCommand)\n+      val operators = parsed.collect {\n+        case a: AnalyzeColumnCommand => a\n+        case o => o\n+      }\n+      assert(operators.size == 1)\n+      if (operators.head.getClass != c) {"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "ok, then I'll fix it\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-30T00:09:41Z",
    "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    def assertAnalyzeColumnCommand(analyzeCommand: String, c: Class[_]) {\n+      val parsed = spark.sessionState.sqlParser.parsePlan(analyzeCommand)\n+      val operators = parsed.collect {\n+        case a: AnalyzeColumnCommand => a\n+        case o => o\n+      }\n+      assert(operators.size == 1)\n+      if (operators.head.getClass != c) {"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think we should write `InternalRow(0L, values.max, ...)` directly\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-10-01T13:17:20Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val tableName = \"tbl\"\n+\n+    // we need to specify column names\n+    intercept[ParseException] {\n+      sql(s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+\n+    val analyzeSql = s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS key, value\"\n+    val parsed = spark.sessionState.sqlParser.parsePlan(analyzeSql)\n+    val expected = AnalyzeColumnCommand(TableIdentifier(tableName), Seq(\"key\", \"value\"))\n+    comparePlans(parsed, expected)\n+  }\n+\n+  test(\"analyzing columns of non-atomic types is not supported\") {\n+    val tableName = \"tbl\"\n+    withTable(tableName) {\n+      Seq(ArrayData(Seq(1, 2, 3), Seq(Seq(1, 2, 3)))).toDF().write.saveAsTable(tableName)\n+      val err = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS data\")\n+      }\n+      assert(err.message.contains(\"Analyzing columns is not supported\"))\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val colName1 = \"abc\"\n+    val colName2 = \"x.yz\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table ($colName1 int, `$colName2` string) USING PARQUET\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == \"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ${colName1.toUpperCase}\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ${colName1.toUpperCase}.\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val columnsToAnalyze = Seq(colName2.toUpperCase, colName1, colName2)\n+        val tableIdent = TableIdentifier(table, Some(\"default\"))\n+        val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n+        val columnStats =\n+          AnalyzeColumnCommand(tableIdent, columnsToAnalyze).computeColStats(spark, relation)._2\n+        assert(columnStats.contains(colName1))\n+        assert(columnStats.contains(colName2))\n+        // check deduplication\n+        assert(columnStats.size == 2)\n+        assert(!columnStats.contains(colName2.toUpperCase))\n+      }\n+    }\n+  }\n+\n+  private def getNonNullValues[T](values: Seq[Option[T]]): Seq[T] = {\n+    values.filter(_.isDefined).map(_.get)\n+  }\n+\n+  test(\"column-level statistics for integral type columns\") {\n+    val values = (0 to 5).map { i =>\n+      if (i % 2 == 0) None else Some(i)\n+    }\n+    val data = values.map { i =>\n+      (i.map(_.toByte), i.map(_.toShort), i.map(_.toInt), i.map(_.toLong))\n+    }\n+\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\", \"c4\")\n+    val nonNullValues = getNonNullValues[Int](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.max,\n+        nonNullValues.min,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for fractional type columns\") {\n+    val values: Seq[Option[Decimal]] = (0 to 5).map { i =>\n+      if (i == 0) None else Some(Decimal(i + i * 0.01))\n+    }\n+    val data = values.map { i =>\n+      (i.map(_.toFloat), i.map(_.toDouble), i)\n+    }\n+\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\")\n+    val nonNullValues = getNonNullValues[Decimal](values)\n+    val numNulls = values.count(_.isEmpty).toLong\n+    val ndv = nonNullValues.distinct.length.toLong\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = f.dataType match {\n+        case floatType: FloatType =>\n+          ColumnStat(InternalRow(numNulls, nonNullValues.max.toFloat, nonNullValues.min.toFloat,\n+            ndv))\n+        case doubleType: DoubleType =>\n+          ColumnStat(InternalRow(numNulls, nonNullValues.max.toDouble, nonNullValues.min.toDouble,\n+            ndv))\n+        case decimalType: DecimalType =>\n+          ColumnStat(InternalRow(numNulls, nonNullValues.max, nonNullValues.min, ndv))\n+      }\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for string column\") {\n+    val values = Seq(None, Some(\"a\"), Some(\"bbbb\"), Some(\"cccc\"), Some(\"\"))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[String](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.map(_.length).sum / nonNullValues.length.toDouble,\n+        nonNullValues.map(_.length).max.toLong,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for binary column\") {\n+    val values = Seq(None, Some(\"a\"), Some(\"bbbb\"), Some(\"cccc\"), Some(\"\")).map(_.map(_.getBytes))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Array[Byte]](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.map(_.length).sum / nonNullValues.length.toDouble,\n+        nonNullValues.map(_.length).max.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for boolean column\") {\n+    val values = Seq(None, Some(true), Some(false), Some(true))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Boolean](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.count(_.equals(true)).toLong,\n+        nonNullValues.count(_.equals(false)).toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for date column\") {\n+    val values = Seq(None, Some(\"1970-01-01\"), Some(\"1970-02-02\")).map(_.map(Date.valueOf))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Date](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        // Internally, DateType is represented as the number of days from 1970-01-01.\n+        nonNullValues.map(DateTimeUtils.fromJavaDate).max,\n+        nonNullValues.map(DateTimeUtils.fromJavaDate).min,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for timestamp column\") {\n+    val values = Seq(None, Some(\"1970-01-01 00:00:00\"), Some(\"1970-01-01 00:00:05\")).map { i =>\n+      i.map(Timestamp.valueOf)\n+    }\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Timestamp](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        // Internally, TimestampType is represented as the number of days from 1970-01-01\n+        nonNullValues.map(DateTimeUtils.fromJavaTimestamp).max,\n+        nonNullValues.map(DateTimeUtils.fromJavaTimestamp).min,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for null columns\") {\n+    val values = Seq(None, None)\n+    val data = values.map { i =>\n+      (i.map(_.toString), i.map(_.toString.toInt))\n+    }\n+    val df = data.toDF(\"c1\", \"c2\")\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      (f, ColumnStat(InternalRow(values.count(_.isEmpty).toLong, null, null, 0L)))\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for columns with different types\") {\n+    val intSeq = Seq(1, 2)\n+    val doubleSeq = Seq(1.01d, 2.02d)\n+    val stringSeq = Seq(\"a\", \"bb\")\n+    val binarySeq = Seq(\"a\", \"bb\").map(_.getBytes)\n+    val booleanSeq = Seq(true, false)\n+    val dateSeq = Seq(\"1970-01-01\", \"1970-02-02\").map(Date.valueOf)\n+    val timestampSeq = Seq(\"1970-01-01 00:00:00\", \"1970-01-01 00:00:05\").map(Timestamp.valueOf)\n+    val longSeq = Seq(5L, 4L)\n+\n+    val data = intSeq.indices.map { i =>\n+      (intSeq(i), doubleSeq(i), stringSeq(i), binarySeq(i), booleanSeq(i), dateSeq(i),\n+        timestampSeq(i), longSeq(i))\n+    }\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\")\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = f.dataType match {\n+        case IntegerType =>\n+          ColumnStat(InternalRow(0L, intSeq.max, intSeq.min, intSeq.distinct.length.toLong))\n+        case DoubleType =>\n+          ColumnStat(InternalRow(0L, doubleSeq.max, doubleSeq.min,\n+              doubleSeq.distinct.length.toLong))\n+        case StringType =>\n+          ColumnStat(InternalRow(0L, stringSeq.map(_.length).sum / stringSeq.length.toDouble,\n+                stringSeq.map(_.length).max.toLong, stringSeq.distinct.length.toLong))\n+        case BinaryType =>\n+          ColumnStat(InternalRow(0L, binarySeq.map(_.length).sum / binarySeq.length.toDouble,\n+                binarySeq.map(_.length).max.toLong))\n+        case BooleanType =>\n+          ColumnStat(InternalRow(0L, booleanSeq.count(_.equals(true)).toLong,\n+              booleanSeq.count(_.equals(false)).toLong))\n+        case DateType =>\n+          ColumnStat(InternalRow(0L, dateSeq.map(DateTimeUtils.fromJavaDate).max,\n+                dateSeq.map(DateTimeUtils.fromJavaDate).min, dateSeq.distinct.length.toLong))\n+        case TimestampType =>\n+          ColumnStat(InternalRow(0L, timestampSeq.map(DateTimeUtils.fromJavaTimestamp).max,\n+                timestampSeq.map(DateTimeUtils.fromJavaTimestamp).min,\n+                timestampSeq.distinct.length.toLong))\n+        case LongType =>\n+          ColumnStat(InternalRow(0L, longSeq.max, longSeq.min, longSeq.distinct.length.toLong))\n+      }\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"update table-level stats while collecting column-level stats\") {\n+    val table = \"tbl\"\n+    val tmpTable = \"tmp\"\n+    withTable(table, tmpTable) {\n+      val values = Seq(1)\n+      val df = values.toDF(\"c1\")\n+      df.write.format(\"json\").saveAsTable(tmpTable)\n+\n+      sql(s\"CREATE TABLE $table (c1 int) USING PARQUET\")\n+      sql(s\"INSERT INTO $table SELECT * FROM $tmpTable\")\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS\")\n+      checkTableStats(tableName = table, expectedRowCount = Some(values.length))\n+\n+      // update table-level stats between analyze table and analyze column commands\n+      sql(s\"INSERT INTO $table SELECT * FROM $tmpTable\")\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS c1\")\n+      val fetchedStats =\n+        checkTableStats(tableName = table, expectedRowCount = Some(values.length * 2))\n+\n+      val colStat = fetchedStats.get.colStats(\"c1\")\n+      StatisticsTest.checkColStat(\n+        dataType = IntegerType,\n+        colStat = colStat,\n+        expectedColStat = ColumnStat(InternalRow.fromSeq(\n+          Seq(0L, values.max, values.min, values.distinct.length.toLong))),"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we can just write `INSERT INTO $table SELECT 1`, thus don't need the temp table\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-10-01T13:19:25Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val tableName = \"tbl\"\n+\n+    // we need to specify column names\n+    intercept[ParseException] {\n+      sql(s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+\n+    val analyzeSql = s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS key, value\"\n+    val parsed = spark.sessionState.sqlParser.parsePlan(analyzeSql)\n+    val expected = AnalyzeColumnCommand(TableIdentifier(tableName), Seq(\"key\", \"value\"))\n+    comparePlans(parsed, expected)\n+  }\n+\n+  test(\"analyzing columns of non-atomic types is not supported\") {\n+    val tableName = \"tbl\"\n+    withTable(tableName) {\n+      Seq(ArrayData(Seq(1, 2, 3), Seq(Seq(1, 2, 3)))).toDF().write.saveAsTable(tableName)\n+      val err = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS data\")\n+      }\n+      assert(err.message.contains(\"Analyzing columns is not supported\"))\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val colName1 = \"abc\"\n+    val colName2 = \"x.yz\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table ($colName1 int, `$colName2` string) USING PARQUET\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == \"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ${colName1.toUpperCase}\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ${colName1.toUpperCase}.\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val columnsToAnalyze = Seq(colName2.toUpperCase, colName1, colName2)\n+        val tableIdent = TableIdentifier(table, Some(\"default\"))\n+        val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n+        val columnStats =\n+          AnalyzeColumnCommand(tableIdent, columnsToAnalyze).computeColStats(spark, relation)._2\n+        assert(columnStats.contains(colName1))\n+        assert(columnStats.contains(colName2))\n+        // check deduplication\n+        assert(columnStats.size == 2)\n+        assert(!columnStats.contains(colName2.toUpperCase))\n+      }\n+    }\n+  }\n+\n+  private def getNonNullValues[T](values: Seq[Option[T]]): Seq[T] = {\n+    values.filter(_.isDefined).map(_.get)\n+  }\n+\n+  test(\"column-level statistics for integral type columns\") {\n+    val values = (0 to 5).map { i =>\n+      if (i % 2 == 0) None else Some(i)\n+    }\n+    val data = values.map { i =>\n+      (i.map(_.toByte), i.map(_.toShort), i.map(_.toInt), i.map(_.toLong))\n+    }\n+\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\", \"c4\")\n+    val nonNullValues = getNonNullValues[Int](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.max,\n+        nonNullValues.min,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for fractional type columns\") {\n+    val values: Seq[Option[Decimal]] = (0 to 5).map { i =>\n+      if (i == 0) None else Some(Decimal(i + i * 0.01))\n+    }\n+    val data = values.map { i =>\n+      (i.map(_.toFloat), i.map(_.toDouble), i)\n+    }\n+\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\")\n+    val nonNullValues = getNonNullValues[Decimal](values)\n+    val numNulls = values.count(_.isEmpty).toLong\n+    val ndv = nonNullValues.distinct.length.toLong\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = f.dataType match {\n+        case floatType: FloatType =>\n+          ColumnStat(InternalRow(numNulls, nonNullValues.max.toFloat, nonNullValues.min.toFloat,\n+            ndv))\n+        case doubleType: DoubleType =>\n+          ColumnStat(InternalRow(numNulls, nonNullValues.max.toDouble, nonNullValues.min.toDouble,\n+            ndv))\n+        case decimalType: DecimalType =>\n+          ColumnStat(InternalRow(numNulls, nonNullValues.max, nonNullValues.min, ndv))\n+      }\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for string column\") {\n+    val values = Seq(None, Some(\"a\"), Some(\"bbbb\"), Some(\"cccc\"), Some(\"\"))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[String](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.map(_.length).sum / nonNullValues.length.toDouble,\n+        nonNullValues.map(_.length).max.toLong,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for binary column\") {\n+    val values = Seq(None, Some(\"a\"), Some(\"bbbb\"), Some(\"cccc\"), Some(\"\")).map(_.map(_.getBytes))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Array[Byte]](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.map(_.length).sum / nonNullValues.length.toDouble,\n+        nonNullValues.map(_.length).max.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for boolean column\") {\n+    val values = Seq(None, Some(true), Some(false), Some(true))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Boolean](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        nonNullValues.count(_.equals(true)).toLong,\n+        nonNullValues.count(_.equals(false)).toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for date column\") {\n+    val values = Seq(None, Some(\"1970-01-01\"), Some(\"1970-02-02\")).map(_.map(Date.valueOf))\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Date](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        // Internally, DateType is represented as the number of days from 1970-01-01.\n+        nonNullValues.map(DateTimeUtils.fromJavaDate).max,\n+        nonNullValues.map(DateTimeUtils.fromJavaDate).min,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for timestamp column\") {\n+    val values = Seq(None, Some(\"1970-01-01 00:00:00\"), Some(\"1970-01-01 00:00:05\")).map { i =>\n+      i.map(Timestamp.valueOf)\n+    }\n+    val df = values.toDF(\"c1\")\n+    val nonNullValues = getNonNullValues[Timestamp](values)\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = ColumnStat(InternalRow(\n+        values.count(_.isEmpty).toLong,\n+        // Internally, TimestampType is represented as the number of days from 1970-01-01\n+        nonNullValues.map(DateTimeUtils.fromJavaTimestamp).max,\n+        nonNullValues.map(DateTimeUtils.fromJavaTimestamp).min,\n+        nonNullValues.distinct.length.toLong))\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for null columns\") {\n+    val values = Seq(None, None)\n+    val data = values.map { i =>\n+      (i.map(_.toString), i.map(_.toString.toInt))\n+    }\n+    val df = data.toDF(\"c1\", \"c2\")\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      (f, ColumnStat(InternalRow(values.count(_.isEmpty).toLong, null, null, 0L)))\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"column-level statistics for columns with different types\") {\n+    val intSeq = Seq(1, 2)\n+    val doubleSeq = Seq(1.01d, 2.02d)\n+    val stringSeq = Seq(\"a\", \"bb\")\n+    val binarySeq = Seq(\"a\", \"bb\").map(_.getBytes)\n+    val booleanSeq = Seq(true, false)\n+    val dateSeq = Seq(\"1970-01-01\", \"1970-02-02\").map(Date.valueOf)\n+    val timestampSeq = Seq(\"1970-01-01 00:00:00\", \"1970-01-01 00:00:05\").map(Timestamp.valueOf)\n+    val longSeq = Seq(5L, 4L)\n+\n+    val data = intSeq.indices.map { i =>\n+      (intSeq(i), doubleSeq(i), stringSeq(i), binarySeq(i), booleanSeq(i), dateSeq(i),\n+        timestampSeq(i), longSeq(i))\n+    }\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\")\n+    val expectedColStatsSeq = df.schema.map { f =>\n+      val colStat = f.dataType match {\n+        case IntegerType =>\n+          ColumnStat(InternalRow(0L, intSeq.max, intSeq.min, intSeq.distinct.length.toLong))\n+        case DoubleType =>\n+          ColumnStat(InternalRow(0L, doubleSeq.max, doubleSeq.min,\n+              doubleSeq.distinct.length.toLong))\n+        case StringType =>\n+          ColumnStat(InternalRow(0L, stringSeq.map(_.length).sum / stringSeq.length.toDouble,\n+                stringSeq.map(_.length).max.toLong, stringSeq.distinct.length.toLong))\n+        case BinaryType =>\n+          ColumnStat(InternalRow(0L, binarySeq.map(_.length).sum / binarySeq.length.toDouble,\n+                binarySeq.map(_.length).max.toLong))\n+        case BooleanType =>\n+          ColumnStat(InternalRow(0L, booleanSeq.count(_.equals(true)).toLong,\n+              booleanSeq.count(_.equals(false)).toLong))\n+        case DateType =>\n+          ColumnStat(InternalRow(0L, dateSeq.map(DateTimeUtils.fromJavaDate).max,\n+                dateSeq.map(DateTimeUtils.fromJavaDate).min, dateSeq.distinct.length.toLong))\n+        case TimestampType =>\n+          ColumnStat(InternalRow(0L, timestampSeq.map(DateTimeUtils.fromJavaTimestamp).max,\n+                timestampSeq.map(DateTimeUtils.fromJavaTimestamp).min,\n+                timestampSeq.distinct.length.toLong))\n+        case LongType =>\n+          ColumnStat(InternalRow(0L, longSeq.max, longSeq.min, longSeq.distinct.length.toLong))\n+      }\n+      (f, colStat)\n+    }\n+    checkColStats(df, expectedColStatsSeq)\n+  }\n+\n+  test(\"update table-level stats while collecting column-level stats\") {\n+    val table = \"tbl\"\n+    val tmpTable = \"tmp\"\n+    withTable(table, tmpTable) {\n+      val values = Seq(1)\n+      val df = values.toDF(\"c1\")\n+      df.write.format(\"json\").saveAsTable(tmpTable)\n+\n+      sql(s\"CREATE TABLE $table (c1 int) USING PARQUET\")\n+      sql(s\"INSERT INTO $table SELECT * FROM $tmpTable\")"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "The same here. \n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-10-01T17:15:54Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}\n+import org.apache.spark.sql.catalyst.parser.ParseException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStat\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.test.SQLTestData.ArrayData\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val tableName = \"tbl\"\n+\n+    // we need to specify column names\n+    intercept[ParseException] {\n+      sql(s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+\n+    val analyzeSql = s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS key, value\"\n+    val parsed = spark.sessionState.sqlParser.parsePlan(analyzeSql)\n+    val expected = AnalyzeColumnCommand(TableIdentifier(tableName), Seq(\"key\", \"value\"))\n+    comparePlans(parsed, expected)\n+  }\n+\n+  test(\"analyzing columns of non-atomic types is not supported\") {\n+    val tableName = \"tbl\"\n+    withTable(tableName) {\n+      Seq(ArrayData(Seq(1, 2, 3), Seq(Seq(1, 2, 3)))).toDF().write.saveAsTable(tableName)\n+      val err = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS data\")\n+      }\n+      assert(err.message.contains(\"Analyzing columns is not supported\"))\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val colName1 = \"abc\"\n+    val colName2 = \"x.yz\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table ($colName1 int, `$colName2` string) USING PARQUET\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == \"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ${colName1.toUpperCase}\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ${colName1.toUpperCase}.\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val columnsToAnalyze = Seq(colName2.toUpperCase, colName1, colName2)\n+        val tableIdent = TableIdentifier(table, Some(\"default\"))\n+        val relation = spark.sessionState.catalog.lookupRelation(tableIdent)\n+        val columnStats =\n+          AnalyzeColumnCommand(tableIdent, columnsToAnalyze).computeColStats(spark, relation)._2"
  }],
  "prId": 15090
}]