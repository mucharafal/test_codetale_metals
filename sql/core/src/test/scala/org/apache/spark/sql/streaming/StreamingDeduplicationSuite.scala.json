[{
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "This is just removing `withSQLConf`",
    "commit": "1ed79cf3fc54c4fab60973156dd81d6a5b83f41c",
    "createdAt": "2019-06-18T11:23:52Z",
    "diffHunk": "@@ -197,32 +197,30 @@ class StreamingDeduplicationSuite extends StateStoreMetricsTest {\n   }\n \n   test(\"deduplicate with file sink\") {\n-    withSQLConf(SQLConf.USE_V1_SOURCE_READER_LIST.key -> \"parquet\") {\n-      withTempDir { output =>\n-        withTempDir { checkpointDir =>\n-          val outputPath = output.getAbsolutePath\n-          val inputData = MemoryStream[String]\n-          val result = inputData.toDS().dropDuplicates()\n-          val q = result.writeStream\n-            .format(\"parquet\")\n-            .outputMode(Append)\n-            .option(\"checkpointLocation\", checkpointDir.getPath)\n-            .start(outputPath)\n-          try {\n-            inputData.addData(\"a\")\n-            q.processAllAvailable()\n-            checkDataset(spark.read.parquet(outputPath).as[String], \"a\")\n-\n-            inputData.addData(\"a\") // Dropped\n-            q.processAllAvailable()\n-            checkDataset(spark.read.parquet(outputPath).as[String], \"a\")\n-\n-            inputData.addData(\"b\")\n-            q.processAllAvailable()\n-            checkDataset(spark.read.parquet(outputPath).as[String], \"a\", \"b\")\n-          } finally {\n-            q.stop()\n-          }\n+    withTempDir { output =>",
    "line": 30
  }],
  "prId": 24900
}]