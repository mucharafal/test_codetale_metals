[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Don't do post-processing. Just match what Hadoop fs.listStatus returns.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:58:09Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.{col, substring_index}\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+import org.apache.spark.sql.types.LongType\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+  import testImplicits._\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {\n+\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .load(filePath)\n+      .select(\n+        substring_index(col(\"status.path\"), \"/\", -1).as(\"path\"),"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "add an inline comment to indicate this is a partition column",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T00:59:18Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.{col, substring_index}\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+import org.apache.spark.sql.types.LongType\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+  import testImplicits._\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {\n+\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .load(filePath)\n+      .select(\n+        substring_index(col(\"status.path\"), \"/\", -1).as(\"path\"),\n+        col(\"status.modification_time\"),\n+        col(\"status.length\"),\n+        col(\"content\"),\n+        col(\"year\")"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should also test options.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T01:00:04Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.{col, substring_index}\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+import org.apache.spark.sql.types.LongType\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+  import testImplicits._\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It seems all files are txt. Then the filter doesn't do anything. You need a test case that the filter actually filter out some existent files.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T17:21:57Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {\n+\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", \"*.txt\")"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it necessary?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T17:22:51Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {\n+\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", \"*.txt\")\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Yes. Spark datasource interface provide path like \"file:///xxx/...\" but `fileStatus.getPath` return \"file:/xxx/...\"",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T17:28:12Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {\n+\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", \"*.txt\")\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Doesn't need to be in the for loop.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T17:24:15Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {\n+\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", \"*.txt\")\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+        val flen = fileStatus.getLen\n+        val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+        val fcontent = {\n+          val stream = fs.open(fileStatus.getPath)\n+          val content = try {\n+            ByteStreams.toByteArray(stream)\n+          } finally {\n+            Closeables.close(stream, true)\n+          }\n+          content\n+        }\n+\n+        val partitionName = dirPath.getName.split(\"=\")(1)"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should also test `write` throw an exception.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T17:25:02Z",
    "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/text-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  test(\"binary file test\") {\n+\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", \"*.txt\")\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+        val flen = fileStatus.getLen\n+        val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+        val fcontent = {\n+          val stream = fs.open(fileStatus.getPath)\n+          val content = try {\n+            ByteStreams.toByteArray(stream)\n+          } finally {\n+            Closeables.close(stream, true)\n+          }\n+          content\n+        }\n+\n+        val partitionName = dirPath.getName.split(\"=\")(1)\n+        val year = partitionName.toInt\n+        val row = Row(fpath, modificationTime, flen, fcontent, year)\n+        expectedRowSet.add(row)\n+      }\n+    }\n+\n+    val result = resultDF.collect()\n+    assert(Set(result: _*) === expectedRowSet.toSet)\n+\n+    val resultDF2 = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", \"*.a\")\n+      .load(filePath)\n+    assert(resultDF2.count() === 0)\n+  }\n+"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Why do we need to have expected count? After listStatus, can we just filter using a `GlobFilter` instance?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T18:41:08Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String, expectedCount: Int): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+          val flen = fileStatus.getLen\n+          val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+          val fcontent = {\n+            val stream = fs.open(fileStatus.getPath)\n+            val content = try {\n+              ByteStreams.toByteArray(stream)\n+            } finally {\n+              Closeables.close(stream, true)\n+            }\n+            content\n+          }\n+\n+          val row = Row(fpath, modificationTime, flen, fcontent, year)\n+          expectedRowSet.add(row)\n+        }\n+      }\n+    }\n+\n+    val result = resultDF.collect()\n+    assert(Set(result: _*) === expectedRowSet.toSet)\n+    assert(result.length === expectedCount)\n+  }\n+\n+  test(\"binary file data source test\") {\n+    testBinaryFileDataSource(pathGlobFilter = \"*.*\", expectedCount = 4)"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "oh, double check. Inside test code, I have compared datasource returned DF with GlobFilter filtering result.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T18:42:25Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String, expectedCount: Int): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+          val flen = fileStatus.getLen\n+          val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+          val fcontent = {\n+            val stream = fs.open(fileStatus.getPath)\n+            val content = try {\n+              ByteStreams.toByteArray(stream)\n+            } finally {\n+              Closeables.close(stream, true)\n+            }\n+            content\n+          }\n+\n+          val row = Row(fpath, modificationTime, flen, fcontent, year)\n+          expectedRowSet.add(row)\n+        }\n+      }\n+    }\n+\n+    val result = resultDF.collect()\n+    assert(Set(result: _*) === expectedRowSet.toSet)\n+    assert(result.length === expectedCount)\n+  }\n+\n+  test(\"binary file data source test\") {\n+    testBinaryFileDataSource(pathGlobFilter = \"*.*\", expectedCount = 4)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "not necessary",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-12T18:43:29Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String, expectedCount: Int): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+          val flen = fileStatus.getLen\n+          val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+          val fcontent = {\n+            val stream = fs.open(fileStatus.getPath)\n+            val content = try {\n+              ByteStreams.toByteArray(stream)\n+            } finally {\n+              Closeables.close(stream, true)\n+            }\n+            content\n+          }\n+\n+          val row = Row(fpath, modificationTime, flen, fcontent, year)\n+          expectedRowSet.add(row)\n+        }\n+      }\n+    }\n+\n+    val result = resultDF.collect()\n+    assert(Set(result: _*) === expectedRowSet.toSet)\n+    assert(result.length === expectedCount)\n+  }\n+\n+  test(\"binary file data source test\") {\n+    testBinaryFileDataSource(pathGlobFilter = \"*.*\", expectedCount = 4)"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Can we use `Utils.tryWithResource` here too?",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T08:31:24Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+          val flen = fileStatus.getLen\n+          val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+          val fcontent = {\n+            val stream = fs.open(fileStatus.getPath)\n+            val content = try {"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "ditto.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-15T20:05:51Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+          val flen = fileStatus.getLen\n+          val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+          val fcontent = {\n+            val stream = fs.open(fileStatus.getPath)\n+            val content = try {"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "We're in SQL module. I think we can `checkAnswer`",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T08:38:14Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")\n+          val flen = fileStatus.getLen\n+          val modificationTime = new Timestamp(fileStatus.getModificationTime)\n+\n+          val fcontent = {\n+            val stream = fs.open(fileStatus.getPath)\n+            val content = try {\n+              ByteStreams.toByteArray(stream)\n+            } finally {\n+              Closeables.close(stream, true)\n+            }\n+            content\n+          }\n+\n+          val row = Row(fpath, modificationTime, flen, fcontent, year)\n+          expectedRowSet.add(row)\n+        }\n+      }\n+    }\n+\n+    val result = resultDF.collect()\n+    assert(Set(result: _*) === expectedRowSet.toSet)"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think you want to do `toUri().getPath()` here.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T08:42:15Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "URI allows both `file:/` and `file:///`. `toUri` doesn't have the contract to produce only one form.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-15T17:32:45Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I was thinking the way, for instance,  `s\"file:///${fileStatus.getPath().toUri().getPath()}\"` so that we get rid of the concern about `file:///` or `file:/` completely, rather than replacing the string manually.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T03:24:28Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Isn't it even harder to read? This is just a test case.",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-16T04:27:37Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")\n+\n+  private lazy val fsFilePath = new Path(filePath)\n+\n+  private lazy val fs = fsFilePath.getFileSystem(sparkContext.hadoopConfiguration)\n+\n+  def testBinaryFileDataSource(pathGlobFilter: String): Unit = {\n+    val resultDF = spark.read.format(\"binaryFile\")\n+      .option(\"pathGlobFilter\", pathGlobFilter)\n+      .load(filePath)\n+      .select(\n+        col(\"status.path\"),\n+        col(\"status.modificationTime\"),\n+        col(\"status.len\"),\n+        col(\"content\"),\n+        col(\"year\") // this is a partition column\n+      )\n+\n+    val expectedRowSet = new collection.mutable.HashSet[Row]()\n+\n+    val globFilter = new GlobFilter(pathGlobFilter)\n+    for (partitionDirStatus <- fs.listStatus(fsFilePath)) {\n+      val dirPath = partitionDirStatus.getPath\n+\n+      val partitionName = dirPath.getName.split(\"=\")(1)\n+      val year = partitionName.toInt // partition column \"year\" value which is `Int` type\n+\n+      for (fileStatus <- fs.listStatus(dirPath)) {\n+        if (globFilter.accept(fileStatus.getPath)) {\n+          val fpath = fileStatus.getPath.toString.replace(\"file:/\", \"file:///\")"
  }],
  "prId": 24354
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Can we write test files only for testing time to avoid adding unnesseary binary files into our code base and make it easier to read? For instance, for binary,\r\n\r\n```scala\r\nval tempDir = Utils.createTempDir()\r\nval testFile = new File(dir, \"part-00000\")\r\nFiles.write(binary, testFile)\r\n```\r\n\r\nFor csv, text\r\n\r\n```\r\ndf.write.[csv|text](...)\r\n```",
    "commit": "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "createdAt": "2019-04-14T09:05:15Z",
    "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.binaryfile\n+\n+import java.sql.Timestamp\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.fs.{GlobFilter, Path}\n+\n+import org.apache.spark.sql.{QueryTest, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.test.{SharedSQLContext, SQLTestUtils}\n+\n+class BinaryFileSuite extends QueryTest with SharedSQLContext with SQLTestUtils {\n+\n+  private lazy val filePath = testFile(\"test-data/binaryfile-partitioned\")"
  }],
  "prId": 24354
}]