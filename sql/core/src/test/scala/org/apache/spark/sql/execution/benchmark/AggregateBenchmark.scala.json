[{
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "The `runBenchmark` here is different from the on in line 48, but they have the same name. We should have a different name.",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-22T16:29:13Z",
    "diffHunk": "@@ -34,621 +34,508 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends RunBenchmarkWithCodegen {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmark(\"agg w/o group\", N) {"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Yes. Do you have a suggested name?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-22T22:48:52Z",
    "diffHunk": "@@ -34,621 +34,508 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends RunBenchmarkWithCodegen {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmark(\"agg w/o group\", N) {"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Well I don't a good name in mind. How about make the method `runBenchmark` of `RunBenchmarkWithCodegen` overriding the one in `BenchmarkBase`?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-23T17:59:12Z",
    "diffHunk": "@@ -34,621 +34,508 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends RunBenchmarkWithCodegen {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmark(\"agg w/o group\", N) {"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Although this is not a problem of this refactoring, this test suite seems to be unhealthy because the configuration from the previous benchmark is propagated to the next benchmark.\r\n\r\nCan we fix this test suite to use `withSQLConf`?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-24T18:12:37Z",
    "diffHunk": "@@ -34,621 +34,508 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends SqlBasedBenchmark {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmarkWithCodegen(\"agg w/o group\", N) {\n+        spark.range(N).selectExpr(\"sum(id)\").collect()\n+      }\n     }\n-    /*\n-    agg w/o group:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    agg w/o group wholestage off                30136 / 31885         69.6          14.4       1.0X\n-    agg w/o group wholestage on                   1851 / 1860       1132.9           0.9      16.3X\n-     */\n-  }\n \n-  ignore(\"stat functions\") {\n-    val N = 100L << 20\n+    runBenchmark(\"stat functions\") {\n+      val N = 100L << 20\n \n-    runBenchmark(\"stddev\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n-    }\n+      runBenchmarkWithCodegen(\"stddev\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n+      }\n \n-    runBenchmark(\"kurtosis\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      runBenchmarkWithCodegen(\"kurtosis\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      }\n     }\n \n-    /*\n-    Using ImperativeAggregate (as implemented in Spark 1.6):\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                            Avg Time(ms)    Avg Rate(M/s)  Relative Rate\n-      -------------------------------------------------------------------------------\n-      stddev w/o codegen                      2019.04            10.39         1.00 X\n-      stddev w codegen                        2097.29            10.00         0.96 X\n-      kurtosis w/o codegen                    2108.99             9.94         0.96 X\n-      kurtosis w codegen                      2090.69            10.03         0.97 X\n-\n-      Using DeclarativeAggregate:\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      stddev codegen=false                     5630 / 5776         18.0          55.6       1.0X\n-      stddev codegen=true                      1259 / 1314         83.0          12.0       4.5X\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      kurtosis:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      kurtosis codegen=false                 14847 / 15084          7.0         142.9       1.0X\n-      kurtosis codegen=true                    1652 / 2124         63.0          15.9       9.0X\n-    */\n-  }\n+    runBenchmark(\"aggregate with linear keys\") {\n+      val N = 20 << 22\n \n-  ignore(\"aggregate with linear keys\") {\n-    val N = 20 << 22\n+      val benchmark = new Benchmark(\"Aggregate w keys\", N, output = output)\n \n-    val benchmark = new Benchmark(\"Aggregate w keys\", N)\n-    def f(): Unit = {\n-      sparkSession.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n-    }\n-\n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n+      def f(): Unit = {\n+        spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n+        f()\n+      }\n \n-    benchmark.run()\n+      benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n+        f()\n+      }\n \n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Mac OS X 10.11\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n+      benchmark.run()\n+    }\n \n-    Aggregate w keys:                        Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    codegen = F                                   6619 / 6780         12.7          78.9       1.0X\n-    codegen = T hashmap = F                       3935 / 4059         21.3          46.9       1.7X\n-    codegen = T hashmap = T                        897 /  971         93.5          10.7       7.4X\n-    */\n-  }\n+    runBenchmark(\"aggregate with randomized keys\") {\n+      val N = 20 << 22\n \n-  ignore(\"aggregate with randomized keys\") {\n-    val N = 20 << 22\n+      val benchmark = new Benchmark(\"Aggregate w keys\", N, output = output)\n+      spark.range(N).selectExpr(\"id\", \"floor(rand() * 10000) as k\")\n+        .createOrReplaceTempView(\"test\")\n \n-    val benchmark = new Benchmark(\"Aggregate w keys\", N)\n-    sparkSession.range(N).selectExpr(\"id\", \"floor(rand() * 10000) as k\")\n-      .createOrReplaceTempView(\"test\")\n+      def f(): Unit = spark.sql(\"select k, k, sum(id) from test group by k, k\").collect()\n \n-    def f(): Unit = sparkSession.sql(\"select k, k, sum(id) from test group by k, k\").collect()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n+      benchmark.run()\n     }\n \n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Mac OS X 10.11\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n+    runBenchmark(\"aggregate with string key\") {\n+      val N = 20 << 20\n \n-    Aggregate w keys:                        Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    codegen = F                                   7445 / 7517         11.3          88.7       1.0X\n-    codegen = T hashmap = F                       4672 / 4703         18.0          55.7       1.6X\n-    codegen = T hashmap = T                       1764 / 1958         47.6          21.0       4.2X\n-    */\n-  }\n+      val benchmark = new Benchmark(\"Aggregate w string key\", N, output = output)\n \n-  ignore(\"aggregate with string key\") {\n-    val N = 20 << 20\n+      def f(): Unit = spark.range(N).selectExpr(\"id\", \"cast(id & 1023 as string) as k\")\n+        .groupBy(\"k\").count().collect()\n \n-    val benchmark = new Benchmark(\"Aggregate w string key\", N)\n-    def f(): Unit = sparkSession.range(N).selectExpr(\"id\", \"cast(id & 1023 as string) as k\")\n-      .groupBy(\"k\").count().collect()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n+      benchmark.run()\n     }\n \n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_73-b02 on Mac OS X 10.11.4\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-    Aggregate w string key:             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    -------------------------------------------------------------------------------------------\n-    codegen = F                              3307 / 3376          6.3         157.7       1.0X\n-    codegen = T hashmap = F                  2364 / 2471          8.9         112.7       1.4X\n-    codegen = T hashmap = T                  1740 / 1841         12.0          83.0       1.9X\n-    */\n-  }\n+    runBenchmark(\"aggregate with decimal key\") {\n+      val N = 20 << 20\n \n-  ignore(\"aggregate with decimal key\") {\n-    val N = 20 << 20\n+      val benchmark = new Benchmark(\"Aggregate w decimal key\", N, output = output)\n \n-    val benchmark = new Benchmark(\"Aggregate w decimal key\", N)\n-    def f(): Unit = sparkSession.range(N).selectExpr(\"id\", \"cast(id & 65535 as decimal) as k\")\n-      .groupBy(\"k\").count().collect()\n+      def f(): Unit = spark.range(N).selectExpr(\"id\", \"cast(id & 65535 as decimal) as k\")\n+        .groupBy(\"k\").count().collect()\n \n-    benchmark.addCase(s\"codegen = F\") { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n-\n-    benchmark.addCase(s\"codegen = T hashmap = F\") { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n-\n-    benchmark.addCase(s\"codegen = T hashmap = T\") { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n-    }\n-\n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_73-b02 on Mac OS X 10.11.4\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-    Aggregate w decimal key:             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    -------------------------------------------------------------------------------------------\n-    codegen = F                              2756 / 2817          7.6         131.4       1.0X\n-    codegen = T hashmap = F                  1580 / 1647         13.3          75.4       1.7X\n-    codegen = T hashmap = T                   641 /  662         32.7          30.6       4.3X\n-    */\n-  }\n+      benchmark.addCase(s\"codegen = F\") { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n+        f()\n+      }\n \n-  ignore(\"aggregate with multiple key types\") {\n-    val N = 20 << 20\n-\n-    val benchmark = new Benchmark(\"Aggregate w multiple keys\", N)\n-    def f(): Unit = sparkSession.range(N)\n-      .selectExpr(\n-        \"id\",\n-        \"(id & 1023) as k1\",\n-        \"cast(id & 1023 as string) as k2\",\n-        \"cast(id & 1023 as int) as k3\",\n-        \"cast(id & 1023 as double) as k4\",\n-        \"cast(id & 1023 as float) as k5\",\n-        \"id > 1023 as k6\")\n-      .groupBy(\"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\")\n-      .sum()\n-      .collect()\n-\n-    benchmark.addCase(s\"codegen = F\") { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = F\") { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\") { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = T\") { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n+        f()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = T\") { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n+      benchmark.run()\n     }\n \n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_73-b02 on Mac OS X 10.11.4\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-    Aggregate w decimal key:             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    -------------------------------------------------------------------------------------------\n-    codegen = F                              5885 / 6091          3.6         280.6       1.0X\n-    codegen = T hashmap = F                  3625 / 4009          5.8         172.8       1.6X\n-    codegen = T hashmap = T                  3204 / 3271          6.5         152.8       1.8X\n-    */\n-  }\n+    runBenchmark(\"aggregate with multiple key types\") {\n+      val N = 20 << 20\n \n-  ignore(\"max function bytecode size of wholestagecodegen\") {\n-    val N = 20 << 15\n-\n-    val benchmark = new Benchmark(\"max function bytecode size\", N)\n-    def f(): Unit = sparkSession.range(N)\n-      .selectExpr(\n-        \"id\",\n-        \"(id & 1023) as k1\",\n-        \"cast(id & 1023 as double) as k2\",\n-        \"cast(id & 1023 as int) as k3\",\n-        \"case when id > 100 and id <= 200 then 1 else 0 end as v1\",\n-        \"case when id > 200 and id <= 300 then 1 else 0 end as v2\",\n-        \"case when id > 300 and id <= 400 then 1 else 0 end as v3\",\n-        \"case when id > 400 and id <= 500 then 1 else 0 end as v4\",\n-        \"case when id > 500 and id <= 600 then 1 else 0 end as v5\",\n-        \"case when id > 600 and id <= 700 then 1 else 0 end as v6\",\n-        \"case when id > 700 and id <= 800 then 1 else 0 end as v7\",\n-        \"case when id > 800 and id <= 900 then 1 else 0 end as v8\",\n-        \"case when id > 900 and id <= 1000 then 1 else 0 end as v9\",\n-        \"case when id > 1000 and id <= 1100 then 1 else 0 end as v10\",\n-        \"case when id > 1100 and id <= 1200 then 1 else 0 end as v11\",\n-        \"case when id > 1200 and id <= 1300 then 1 else 0 end as v12\",\n-        \"case when id > 1300 and id <= 1400 then 1 else 0 end as v13\",\n-        \"case when id > 1400 and id <= 1500 then 1 else 0 end as v14\",\n-        \"case when id > 1500 and id <= 1600 then 1 else 0 end as v15\",\n-        \"case when id > 1600 and id <= 1700 then 1 else 0 end as v16\",\n-        \"case when id > 1700 and id <= 1800 then 1 else 0 end as v17\",\n-        \"case when id > 1800 and id <= 1900 then 1 else 0 end as v18\")\n-      .groupBy(\"k1\", \"k2\", \"k3\")\n-      .sum()\n-      .collect()\n-\n-    benchmark.addCase(\"codegen = F\") { iter =>\n-      sparkSession.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"false\")\n-      f()\n-    }\n+      val benchmark = new Benchmark(\"Aggregate w multiple keys\", N, output = output)\n \n-    benchmark.addCase(\"codegen = T hugeMethodLimit = 10000\") { iter =>\n-      sparkSession.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n-      sparkSession.conf.set(SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key, \"10000\")\n-      f()\n-    }\n+      def f(): Unit = spark.range(N)\n+        .selectExpr(\n+          \"id\",\n+          \"(id & 1023) as k1\",\n+          \"cast(id & 1023 as string) as k2\",\n+          \"cast(id & 1023 as int) as k3\",\n+          \"cast(id & 1023 as double) as k4\",\n+          \"cast(id & 1023 as float) as k5\",\n+          \"id > 1023 as k6\")\n+        .groupBy(\"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\")\n+        .sum()\n+        .collect()\n \n-    benchmark.addCase(\"codegen = T hugeMethodLimit = 1500\") { iter =>\n-      sparkSession.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n-      sparkSession.conf.set(SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key, \"1500\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = F\") { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n+        f()\n+      }\n \n-    benchmark.run()\n+      benchmark.addCase(s\"codegen = T hashmap = F\") { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n+        f()\n+      }\n \n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_31-b13 on Mac OS X 10.10.2\n-    Intel(R) Core(TM) i7-4578U CPU @ 3.00GHz\n+      benchmark.addCase(s\"codegen = T hashmap = T\") { iter =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n+        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n+        f()\n+      }\n \n-    max function bytecode size:              Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    codegen = F                                    709 /  803          0.9        1082.1       1.0X\n-    codegen = T hugeMethodLimit = 10000           3485 / 3548          0.2        5317.7       0.2X\n-    codegen = T hugeMethodLimit = 1500             636 /  701          1.0         969.9       1.1X\n-     */\n-  }\n+      benchmark.run()\n+    }\n+\n+    runBenchmark(\"max function bytecode size of wholestagecodegen\") {\n+      val N = 20 << 15\n+\n+      val benchmark = new Benchmark(\"max function bytecode size\", N, output = output)\n+\n+      def f(): Unit = spark.range(N)\n+        .selectExpr(\n+          \"id\",\n+          \"(id & 1023) as k1\",\n+          \"cast(id & 1023 as double) as k2\",\n+          \"cast(id & 1023 as int) as k3\",\n+          \"case when id > 100 and id <= 200 then 1 else 0 end as v1\",\n+          \"case when id > 200 and id <= 300 then 1 else 0 end as v2\",\n+          \"case when id > 300 and id <= 400 then 1 else 0 end as v3\",\n+          \"case when id > 400 and id <= 500 then 1 else 0 end as v4\",\n+          \"case when id > 500 and id <= 600 then 1 else 0 end as v5\",\n+          \"case when id > 600 and id <= 700 then 1 else 0 end as v6\",\n+          \"case when id > 700 and id <= 800 then 1 else 0 end as v7\",\n+          \"case when id > 800 and id <= 900 then 1 else 0 end as v8\",\n+          \"case when id > 900 and id <= 1000 then 1 else 0 end as v9\",\n+          \"case when id > 1000 and id <= 1100 then 1 else 0 end as v10\",\n+          \"case when id > 1100 and id <= 1200 then 1 else 0 end as v11\",\n+          \"case when id > 1200 and id <= 1300 then 1 else 0 end as v12\",\n+          \"case when id > 1300 and id <= 1400 then 1 else 0 end as v13\",\n+          \"case when id > 1400 and id <= 1500 then 1 else 0 end as v14\",\n+          \"case when id > 1500 and id <= 1600 then 1 else 0 end as v15\",\n+          \"case when id > 1600 and id <= 1700 then 1 else 0 end as v16\",\n+          \"case when id > 1700 and id <= 1800 then 1 else 0 end as v17\",\n+          \"case when id > 1800 and id <= 1900 then 1 else 0 end as v18\")\n+        .groupBy(\"k1\", \"k2\", \"k3\")\n+        .sum()\n+        .collect()\n+\n+      benchmark.addCase(\"codegen = F\") { iter =>\n+        spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"false\")\n+        f()\n+      }\n \n+      benchmark.addCase(\"codegen = T hugeMethodLimit = 10000\") { iter =>\n+        spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+        spark.conf.set(SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key, \"10000\")\n+        f()\n+      }\n \n-  ignore(\"cube\") {\n-    val N = 5 << 20\n+      benchmark.addCase(\"codegen = T hugeMethodLimit = 1500\") { iter =>\n+        spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, \"true\")\n+        spark.conf.set(SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key, \"1500\")"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`\"false\"` instead of `false.toString`?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:03:24Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "When we use `Seq(true, false).foreach { value =>`, we usually do `s\"$value\"`. But, for this, I think `\"false\"` is the simplest and the best.",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:08:41Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```scala\r\nwithSQLConf(\r\n  SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\",\r\n  SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> \"false\",\r\n  \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> \"false\") {\r\n```",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:05:46Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> false.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> false.toString) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@wangyum . This one is also for indentation. Please note that `withSQLConf(` is beyond the first configuration.",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:16:33Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> false.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> false.toString) {"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Do you mean change\r\n```scala\r\nwithSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\",\r\n  SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> \"false\",\r\n  \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> \"false\") {\r\n  f()\r\n}\r\n```\r\nto \r\n```scala\r\nwithSQLConf(\r\n  SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\",\r\n  SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> \"false\",\r\n  \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> \"false\") {\r\n  f()\r\n}\r\n```\r\n?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:28:40Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> false.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> false.toString) {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yes!",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T01:04:19Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> false.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> false.toString) {"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Fixed",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T01:14:35Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> false.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> false.toString) {"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "ditto.",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:05:58Z",
    "diffHunk": "@@ -73,23 +73,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n         spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n       }\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> false.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> true.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> true.toString) {"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`\"false\"` instead of `false.toString`.",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:06:23Z",
    "diffHunk": "@@ -104,23 +107,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n \n       def f(): Unit = spark.sql(\"select k, k, sum(id) from test group by k, k\").collect()\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "ditto.",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-25T00:06:32Z",
    "diffHunk": "@@ -104,23 +107,26 @@ object AggregateBenchmark extends SqlBasedBenchmark {\n \n       def f(): Unit = spark.sql(\"select k, k, sum(id) from test group by k, k\").collect()\n \n-      benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\n-        f()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> false.toString) {\n+          f()\n+        }\n       }\n \n-      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-        spark.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-        spark.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-        f()\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> true.toString,\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> false.toString,\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> false.toString) {"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Shall we remove this redundant line 148?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-27T05:49:46Z",
    "diffHunk": "@@ -34,621 +34,539 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends SqlBasedBenchmark {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmarkWithCodegen(\"agg w/o group\", N) {\n+        spark.range(N).selectExpr(\"sum(id)\").collect()\n+      }\n     }\n-    /*\n-    agg w/o group:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    agg w/o group wholestage off                30136 / 31885         69.6          14.4       1.0X\n-    agg w/o group wholestage on                   1851 / 1860       1132.9           0.9      16.3X\n-     */\n-  }\n \n-  ignore(\"stat functions\") {\n-    val N = 100L << 20\n+    runBenchmark(\"stat functions\") {\n+      val N = 100L << 20\n \n-    runBenchmark(\"stddev\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n-    }\n+      runBenchmarkWithCodegen(\"stddev\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n+      }\n \n-    runBenchmark(\"kurtosis\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      runBenchmarkWithCodegen(\"kurtosis\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      }\n     }\n \n-    /*\n-    Using ImperativeAggregate (as implemented in Spark 1.6):\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                            Avg Time(ms)    Avg Rate(M/s)  Relative Rate\n-      -------------------------------------------------------------------------------\n-      stddev w/o codegen                      2019.04            10.39         1.00 X\n-      stddev w codegen                        2097.29            10.00         0.96 X\n-      kurtosis w/o codegen                    2108.99             9.94         0.96 X\n-      kurtosis w codegen                      2090.69            10.03         0.97 X\n-\n-      Using DeclarativeAggregate:\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      stddev codegen=false                     5630 / 5776         18.0          55.6       1.0X\n-      stddev codegen=true                      1259 / 1314         83.0          12.0       4.5X\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      kurtosis:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      kurtosis codegen=false                 14847 / 15084          7.0         142.9       1.0X\n-      kurtosis codegen=true                    1652 / 2124         63.0          15.9       9.0X\n-    */\n-  }\n-\n-  ignore(\"aggregate with linear keys\") {\n-    val N = 20 << 22\n+    runBenchmark(\"aggregate with linear keys\") {\n+      val N = 20 << 22\n \n-    val benchmark = new Benchmark(\"Aggregate w keys\", N)\n-    def f(): Unit = {\n-      sparkSession.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n-    }\n+      val benchmark = new Benchmark(\"Aggregate w keys\", N, output = output)\n \n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n+      def f(): Unit = {\n+        spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\") {\n+          f()\n+        }\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(\n+          SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\",\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> \"false\",\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> \"false\") {\n+          f()\n+        }\n+      }\n \n-    benchmark.run()\n+      benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { _ =>\n+        withSQLConf(\n+          SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\",\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> \"true\",\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> \"true\") {\n+          f()\n+        }\n+      }\n \n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Mac OS X 10.11\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n+      benchmark.run()\n+    }\n \n-    Aggregate w keys:                        Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    codegen = F                                   6619 / 6780         12.7          78.9       1.0X\n-    codegen = T hashmap = F                       3935 / 4059         21.3          46.9       1.7X\n-    codegen = T hashmap = T                        897 /  971         93.5          10.7       7.4X\n-    */\n-  }\n+    runBenchmark(\"aggregate with randomized keys\") {\n+      val N = 20 << 22\n \n-  ignore(\"aggregate with randomized keys\") {\n-    val N = 20 << 22\n+      val benchmark = new Benchmark(\"Aggregate w keys\", N, output = output)\n+      spark.range(N).selectExpr(\"id\", \"floor(rand() * 10000) as k\")\n+        .createOrReplaceTempView(\"test\")\n \n-    val benchmark = new Benchmark(\"Aggregate w keys\", N)\n-    sparkSession.range(N).selectExpr(\"id\", \"floor(rand() * 10000) as k\")\n-      .createOrReplaceTempView(\"test\")\n+      def f(): Unit = spark.sql(\"select k, k, sum(id) from test group by k, k\").collect()\n \n-    def f(): Unit = sparkSession.sql(\"select k, k, sum(id) from test group by k, k\").collect()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\") {\n+          f()\n+        }\n+      }\n \n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = false)\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>\n+        withSQLConf(\n+          SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\",\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> \"false\",\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> \"false\") {\n+          f()\n+        }\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { _ =>\n+        withSQLConf(\n+          SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\",\n+          SQLConf.ENABLE_TWOLEVEL_AGG_MAP.key -> \"true\",\n+          \"spark.sql.codegen.aggregate.map.vectorized.enable\" -> \"true\") {\n+          f()\n+        }\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", value = true)\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n+      benchmark.run()\n     }\n \n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27 on Mac OS X 10.11\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n+    runBenchmark(\"aggregate with string key\") {\n+      val N = 20 << 20\n \n-    Aggregate w keys:                        Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    codegen = F                                   7445 / 7517         11.3          88.7       1.0X\n-    codegen = T hashmap = F                       4672 / 4703         18.0          55.7       1.6X\n-    codegen = T hashmap = T                       1764 / 1958         47.6          21.0       4.2X\n-    */\n-  }\n+      val benchmark = new Benchmark(\"Aggregate w string key\", N, output = output)\n \n-  ignore(\"aggregate with string key\") {\n-    val N = 20 << 20\n+      def f(): Unit = spark.range(N).selectExpr(\"id\", \"cast(id & 1023 as string) as k\")\n+        .groupBy(\"k\").count().collect()\n \n-    val benchmark = new Benchmark(\"Aggregate w string key\", N)\n-    def f(): Unit = sparkSession.range(N).selectExpr(\"id\", \"cast(id & 1023 as string) as k\")\n-      .groupBy(\"k\").count().collect()\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`s\"codegen = F\"` -> `\"codegen = F\"`?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-27T05:52:52Z",
    "diffHunk": "@@ -34,621 +34,539 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends SqlBasedBenchmark {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmarkWithCodegen(\"agg w/o group\", N) {\n+        spark.range(N).selectExpr(\"sum(id)\").collect()\n+      }\n     }\n-    /*\n-    agg w/o group:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    agg w/o group wholestage off                30136 / 31885         69.6          14.4       1.0X\n-    agg w/o group wholestage on                   1851 / 1860       1132.9           0.9      16.3X\n-     */\n-  }\n \n-  ignore(\"stat functions\") {\n-    val N = 100L << 20\n+    runBenchmark(\"stat functions\") {\n+      val N = 100L << 20\n \n-    runBenchmark(\"stddev\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n-    }\n+      runBenchmarkWithCodegen(\"stddev\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n+      }\n \n-    runBenchmark(\"kurtosis\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      runBenchmarkWithCodegen(\"kurtosis\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      }\n     }\n \n-    /*\n-    Using ImperativeAggregate (as implemented in Spark 1.6):\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                            Avg Time(ms)    Avg Rate(M/s)  Relative Rate\n-      -------------------------------------------------------------------------------\n-      stddev w/o codegen                      2019.04            10.39         1.00 X\n-      stddev w codegen                        2097.29            10.00         0.96 X\n-      kurtosis w/o codegen                    2108.99             9.94         0.96 X\n-      kurtosis w codegen                      2090.69            10.03         0.97 X\n-\n-      Using DeclarativeAggregate:\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      stddev codegen=false                     5630 / 5776         18.0          55.6       1.0X\n-      stddev codegen=true                      1259 / 1314         83.0          12.0       4.5X\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      kurtosis:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      kurtosis codegen=false                 14847 / 15084          7.0         142.9       1.0X\n-      kurtosis codegen=true                    1652 / 2124         63.0          15.9       9.0X\n-    */\n-  }\n-\n-  ignore(\"aggregate with linear keys\") {\n-    val N = 20 << 22\n+    runBenchmark(\"aggregate with linear keys\") {\n+      val N = 20 << 22\n \n-    val benchmark = new Benchmark(\"Aggregate w keys\", N)\n-    def f(): Unit = {\n-      sparkSession.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n-    }\n+      val benchmark = new Benchmark(\"Aggregate w keys\", N, output = output)\n \n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n+      def f(): Unit = {\n+        spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Thanks @dongjoon-hyun I plan add `EmptyInterpolatedStringChecker` to scalastyle-config.xml to avoid this issue: [SPARK-25553](https://issues.apache.org/jira/browse/SPARK-25553)",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-27T15:00:17Z",
    "diffHunk": "@@ -34,621 +34,539 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends SqlBasedBenchmark {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmarkWithCodegen(\"agg w/o group\", N) {\n+        spark.range(N).selectExpr(\"sum(id)\").collect()\n+      }\n     }\n-    /*\n-    agg w/o group:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    agg w/o group wholestage off                30136 / 31885         69.6          14.4       1.0X\n-    agg w/o group wholestage on                   1851 / 1860       1132.9           0.9      16.3X\n-     */\n-  }\n \n-  ignore(\"stat functions\") {\n-    val N = 100L << 20\n+    runBenchmark(\"stat functions\") {\n+      val N = 100L << 20\n \n-    runBenchmark(\"stddev\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n-    }\n+      runBenchmarkWithCodegen(\"stddev\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n+      }\n \n-    runBenchmark(\"kurtosis\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      runBenchmarkWithCodegen(\"kurtosis\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      }\n     }\n \n-    /*\n-    Using ImperativeAggregate (as implemented in Spark 1.6):\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                            Avg Time(ms)    Avg Rate(M/s)  Relative Rate\n-      -------------------------------------------------------------------------------\n-      stddev w/o codegen                      2019.04            10.39         1.00 X\n-      stddev w codegen                        2097.29            10.00         0.96 X\n-      kurtosis w/o codegen                    2108.99             9.94         0.96 X\n-      kurtosis w codegen                      2090.69            10.03         0.97 X\n-\n-      Using DeclarativeAggregate:\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      stddev codegen=false                     5630 / 5776         18.0          55.6       1.0X\n-      stddev codegen=true                      1259 / 1314         83.0          12.0       4.5X\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      kurtosis:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      kurtosis codegen=false                 14847 / 15084          7.0         142.9       1.0X\n-      kurtosis codegen=true                    1652 / 2124         63.0          15.9       9.0X\n-    */\n-  }\n-\n-  ignore(\"aggregate with linear keys\") {\n-    val N = 20 << 22\n+    runBenchmark(\"aggregate with linear keys\") {\n+      val N = 20 << 22\n \n-    val benchmark = new Benchmark(\"Aggregate w keys\", N)\n-    def f(): Unit = {\n-      sparkSession.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n-    }\n+      val benchmark = new Benchmark(\"Aggregate w keys\", N, output = output)\n \n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n+      def f(): Unit = {\n+        spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>"
  }],
  "prId": 22484
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`s\"codegen = T hashmap = F\"` -> `\"codegen = T hashmap = F\"`\r\n\r\nCould you fix all instances like this?",
    "commit": "6c46ad59c063fa6283fb23046300404767a82248",
    "createdAt": "2018-09-27T05:53:16Z",
    "diffHunk": "@@ -34,621 +34,539 @@ import org.apache.spark.unsafe.map.BytesToBytesMap\n \n /**\n  * Benchmark to measure performance for aggregate primitives.\n- * To run this:\n- *  build/sbt \"sql/test-only *benchmark.AggregateBenchmark\"\n- *\n- * Benchmarks in this file are skipped in normal builds.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/AggregateBenchmark-results.txt\".\n+ * }}}\n  */\n-class AggregateBenchmark extends BenchmarkWithCodegen {\n+object AggregateBenchmark extends SqlBasedBenchmark {\n \n-  ignore(\"aggregate without grouping\") {\n-    val N = 500L << 22\n-    val benchmark = new Benchmark(\"agg without grouping\", N)\n-    runBenchmark(\"agg w/o group\", N) {\n-      sparkSession.range(N).selectExpr(\"sum(id)\").collect()\n+  override def benchmark(): Unit = {\n+    runBenchmark(\"aggregate without grouping\") {\n+      val N = 500L << 22\n+      runBenchmarkWithCodegen(\"agg w/o group\", N) {\n+        spark.range(N).selectExpr(\"sum(id)\").collect()\n+      }\n     }\n-    /*\n-    agg w/o group:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    agg w/o group wholestage off                30136 / 31885         69.6          14.4       1.0X\n-    agg w/o group wholestage on                   1851 / 1860       1132.9           0.9      16.3X\n-     */\n-  }\n \n-  ignore(\"stat functions\") {\n-    val N = 100L << 20\n+    runBenchmark(\"stat functions\") {\n+      val N = 100L << 20\n \n-    runBenchmark(\"stddev\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n-    }\n+      runBenchmarkWithCodegen(\"stddev\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"stddev\").collect()\n+      }\n \n-    runBenchmark(\"kurtosis\", N) {\n-      sparkSession.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      runBenchmarkWithCodegen(\"kurtosis\", N) {\n+        spark.range(N).groupBy().agg(\"id\" -> \"kurtosis\").collect()\n+      }\n     }\n \n-    /*\n-    Using ImperativeAggregate (as implemented in Spark 1.6):\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                            Avg Time(ms)    Avg Rate(M/s)  Relative Rate\n-      -------------------------------------------------------------------------------\n-      stddev w/o codegen                      2019.04            10.39         1.00 X\n-      stddev w codegen                        2097.29            10.00         0.96 X\n-      kurtosis w/o codegen                    2108.99             9.94         0.96 X\n-      kurtosis w codegen                      2090.69            10.03         0.97 X\n-\n-      Using DeclarativeAggregate:\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      stddev:                             Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      stddev codegen=false                     5630 / 5776         18.0          55.6       1.0X\n-      stddev codegen=true                      1259 / 1314         83.0          12.0       4.5X\n-\n-      Intel(R) Core(TM) i7-4558U CPU @ 2.80GHz\n-      kurtosis:                           Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-      -------------------------------------------------------------------------------------------\n-      kurtosis codegen=false                 14847 / 15084          7.0         142.9       1.0X\n-      kurtosis codegen=true                    1652 / 2124         63.0          15.9       9.0X\n-    */\n-  }\n-\n-  ignore(\"aggregate with linear keys\") {\n-    val N = 20 << 22\n+    runBenchmark(\"aggregate with linear keys\") {\n+      val N = 20 << 22\n \n-    val benchmark = new Benchmark(\"Aggregate w keys\", N)\n-    def f(): Unit = {\n-      sparkSession.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n-    }\n+      val benchmark = new Benchmark(\"Aggregate w keys\", N, output = output)\n \n-    benchmark.addCase(s\"codegen = F\", numIters = 2) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n-      f()\n-    }\n+      def f(): Unit = {\n+        spark.range(N).selectExpr(\"(id & 65535) as k\").groupBy(\"k\").sum().collect()\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"false\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"false\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = F\", numIters = 2) { _ =>\n+        withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\") {\n+          f()\n+        }\n+      }\n \n-    benchmark.addCase(s\"codegen = T hashmap = T\", numIters = 5) { iter =>\n-      sparkSession.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.twolevel.enabled\", \"true\")\n-      sparkSession.conf.set(\"spark.sql.codegen.aggregate.map.vectorized.enable\", \"true\")\n-      f()\n-    }\n+      benchmark.addCase(s\"codegen = T hashmap = F\", numIters = 3) { _ =>"
  }],
  "prId": 22484
}]