[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @peter-toth . Could you explain why we need to replace `ExternalAppendOnlyUnsafeRowArray.DefaultInitialSizeOfInMemoryBuffer` with `numSpillThreshold` here? Actually, this is not an obvious refactoring.\r\nIf this is related to `Fix issue in ExternalAppendOnlyUnsafeRowArray creation`, please add some comments here or PR description clearly.",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2018-10-28T22:24:22Z",
    "diffHunk": "@@ -68,9 +100,7 @@ object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n     benchmark.addCase(\"ExternalAppendOnlyUnsafeRowArray\") { _: Int =>\n       var sum = 0L\n       for (_ <- 0L until iterations) {\n-        val array = new ExternalAppendOnlyUnsafeRowArray(\n-          ExternalAppendOnlyUnsafeRowArray.DefaultInitialSizeOfInMemoryBuffer,\n-          numSpillThreshold)\n+        val array = new ExternalAppendOnlyUnsafeRowArray(numSpillThreshold, numSpillThreshold)"
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "Thanks for the feedback @dongjoon-hyun. I added some details to the description.",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2018-10-29T11:35:54Z",
    "diffHunk": "@@ -68,9 +100,7 @@ object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n     benchmark.addCase(\"ExternalAppendOnlyUnsafeRowArray\") { _: Int =>\n       var sum = 0L\n       for (_ <- 0L until iterations) {\n-        val array = new ExternalAppendOnlyUnsafeRowArray(\n-          ExternalAppendOnlyUnsafeRowArray.DefaultInitialSizeOfInMemoryBuffer,\n-          numSpillThreshold)\n+        val array = new ExternalAppendOnlyUnsafeRowArray(numSpillThreshold, numSpillThreshold)"
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "Actually, following my logic in the description, I think \r\n```\r\nval array = new ExternalAppendOnlyUnsafeRowArray(numSpillThreshold, numSpillThreshold)\r\n```\r\nshould be changed to\r\n```\r\nval array = new ExternalAppendOnlyUnsafeRowArray(0, numSpillThreshold)\r\n```\r\nin `testAgainstRawUnsafeExternalSorter` in \"WITH SPILL\" cases so as to compare `ExternalAppendOnlyUnsafeRowArray` to `UnsafeExternalSorter` when it behaves so.\r\n\r\nBut would be great if someone could confirm this idea.",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2018-10-29T14:21:37Z",
    "diffHunk": "@@ -68,9 +100,7 @@ object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n     benchmark.addCase(\"ExternalAppendOnlyUnsafeRowArray\") { _: Int =>\n       var sum = 0L\n       for (_ <- 0L until iterations) {\n-        val array = new ExternalAppendOnlyUnsafeRowArray(\n-          ExternalAppendOnlyUnsafeRowArray.DefaultInitialSizeOfInMemoryBuffer,\n-          numSpillThreshold)\n+        val array = new ExternalAppendOnlyUnsafeRowArray(numSpillThreshold, numSpillThreshold)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "In that case, we need two-step comparision.\r\n1. Refactoring only to ensure no regression.\r\n2. Change that value to check the performance value difference.\r\n\r\nCould you rollback this line and let us finish `Step 1` first?",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2018-10-30T17:55:17Z",
    "diffHunk": "@@ -68,9 +100,7 @@ object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n     benchmark.addCase(\"ExternalAppendOnlyUnsafeRowArray\") { _: Int =>\n       var sum = 0L\n       for (_ <- 0L until iterations) {\n-        val array = new ExternalAppendOnlyUnsafeRowArray(\n-          ExternalAppendOnlyUnsafeRowArray.DefaultInitialSizeOfInMemoryBuffer,\n-          numSpillThreshold)\n+        val array = new ExternalAppendOnlyUnsafeRowArray(numSpillThreshold, numSpillThreshold)"
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "Ok. Reverted the change.",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2018-10-30T20:48:59Z",
    "diffHunk": "@@ -68,9 +100,7 @@ object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n     benchmark.addCase(\"ExternalAppendOnlyUnsafeRowArray\") { _: Int =>\n       var sum = 0L\n       for (_ <- 0L until iterations) {\n-        val array = new ExternalAppendOnlyUnsafeRowArray(\n-          ExternalAppendOnlyUnsafeRowArray.DefaultInitialSizeOfInMemoryBuffer,\n-          numSpillThreshold)\n+        val array = new ExternalAppendOnlyUnsafeRowArray(numSpillThreshold, numSpillThreshold)"
  }],
  "prId": 22617
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "It seems that 2~3 should be the same except `SPARK_GENERATE_BENCHMARK_FILES=1`.\r\nAlso, we need `spark.memory.debugFill` configuration for 1 (spark-submit).",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T07:22:10Z",
    "diffHunk": "@@ -20,24 +20,56 @@ package org.apache.spark.sql.execution\n import scala.collection.mutable.ArrayBuffer\n \n import org.apache.spark.{SparkConf, SparkContext, SparkEnv, TaskContext}\n-import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.benchmark.{Benchmark, BenchmarkBase}\n import org.apache.spark.internal.config\n import org.apache.spark.memory.MemoryTestingUtils\n import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n \n-object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n+/**\n+ * Benchmark ExternalAppendOnlyUnsafeRowArray.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\""
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "Actually I'm removing `spark.memory.debugFill=true` from the configuration of 3 to become similar to 1 (spark-submit). `spark.memory.debugFill` is `false` by default and setting it to `true` adds enormous overhead.\r\nI think I can change it to `+= \\\"-Dspark.memory.debugFill=false\\\"` if that better fits here.",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T08:05:24Z",
    "diffHunk": "@@ -20,24 +20,56 @@ package org.apache.spark.sql.execution\n import scala.collection.mutable.ArrayBuffer\n \n import org.apache.spark.{SparkConf, SparkContext, SparkEnv, TaskContext}\n-import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.benchmark.{Benchmark, BenchmarkBase}\n import org.apache.spark.internal.config\n import org.apache.spark.memory.MemoryTestingUtils\n import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n \n-object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n+/**\n+ * Benchmark ExternalAppendOnlyUnsafeRowArray.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\""
  }],
  "prId": 22617
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "~Let's keep the original sequence; `1000` -> `30 * 1000` -> `100 * 1000`. Increasing order is more intuitive.~\r\nAh, I got it. This is reordered by the calculation. Please forgot about the above comment.\r\n```\r\n>>> 1000 * (1<<18)\r\n262144000\r\n>>> 30 * 1000 * (1<<14)\r\n491520000\r\n>>> 100 * 1000 * (1<<10)\r\n102400000\r\n```",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T07:29:15Z",
    "diffHunk": "@@ -158,80 +176,23 @@ object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n       }\n     }\n \n-    val conf = new SparkConf(false)\n-    // Make the Java serializer write a reset instruction (TC_RESET) after each object to test\n-    // for a bug we had with bytes written past the last object in a batch (SPARK-2792)\n-    conf.set(\"spark.serializer.objectStreamReset\", \"1\")\n-    conf.set(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\")\n-\n-    val sc = new SparkContext(\"local\", \"test\", conf)\n-    val taskContext = MemoryTestingUtils.fakeTaskContext(SparkEnv.get)\n-    TaskContext.setTaskContext(taskContext)\n-    benchmark.run()\n-    sc.stop()\n+    withFakeTaskContext {\n+      benchmark.run()\n+    }\n   }\n \n-  def main(args: Array[String]): Unit = {\n-\n-    // ========================================================================================= //\n-    // WITHOUT SPILL\n-    // ========================================================================================= //\n-\n-    val spillThreshold = 100 * 1000\n-\n-    /*\n-    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n-\n-    Array with 1000 rows:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    ArrayBuffer                                   7821 / 7941         33.5          29.8       1.0X\n-    ExternalAppendOnlyUnsafeRowArray              8798 / 8819         29.8          33.6       0.9X\n-    */\n-    testAgainstRawArrayBuffer(spillThreshold, 1000, 1 << 18)\n-\n-    /*\n-    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n-\n-    Array with 30000 rows:                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    ArrayBuffer                                 19200 / 19206         25.6          39.1       1.0X\n-    ExternalAppendOnlyUnsafeRowArray            19558 / 19562         25.1          39.8       1.0X\n-    */\n-    testAgainstRawArrayBuffer(spillThreshold, 30 * 1000, 1 << 14)\n-\n-    /*\n-    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n-\n-    Array with 100000 rows:                  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    ArrayBuffer                                   5949 / 6028         17.2          58.1       1.0X\n-    ExternalAppendOnlyUnsafeRowArray              6078 / 6138         16.8          59.4       1.0X\n-    */\n-    testAgainstRawArrayBuffer(spillThreshold, 100 * 1000, 1 << 10)\n-\n-    // ========================================================================================= //\n-    // WITH SPILL\n-    // ========================================================================================= //\n-\n-    /*\n-    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n-\n-    Spilling with 1000 rows:                 Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    UnsafeExternalSorter                          9239 / 9470         28.4          35.2       1.0X\n-    ExternalAppendOnlyUnsafeRowArray              8857 / 8909         29.6          33.8       1.0X\n-    */\n-    testAgainstRawUnsafeExternalSorter(100 * 1000, 1000, 1 << 18)\n-\n-    /*\n-    Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n-\n-    Spilling with 10000 rows:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    UnsafeExternalSorter                             4 /    5         39.3          25.5       1.0X\n-    ExternalAppendOnlyUnsafeRowArray                 5 /    6         29.8          33.5       0.8X\n-    */\n-    testAgainstRawUnsafeExternalSorter(\n-      config.SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get, 10 * 1000, 1 << 4)\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    runBenchmark(\"WITHOUT SPILL\") {\n+      val spillThreshold = 100 * 1000\n+      testAgainstRawArrayBuffer(spillThreshold, 100 * 1000, 1 << 10)\n+      testAgainstRawArrayBuffer(spillThreshold, 1000, 1 << 18)\n+      testAgainstRawArrayBuffer(spillThreshold, 30 * 1000, 1 << 14)",
    "line": 190
  }],
  "prId": 22617
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Got it. I was confused with `-=`.",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T08:27:49Z",
    "diffHunk": "@@ -32,9 +32,10 @@ import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n  * {{{\n  *   1. without sbt:\n  *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n- *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   2. build/sbt build/sbt \";project sql;set javaOptions\n+  *  *    in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"\n  *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \";project sql;set javaOptions\n- *        in Test -= \\\"-Dspark.memory.debugFill=true\\\";test:runMain <this class>\"\n+ *        in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"",
    "line": 21
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "sorry, i did it a bit confusing way, but updated now to `+= ...=false` in a new commit",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T08:28:37Z",
    "diffHunk": "@@ -32,9 +32,10 @@ import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n  * {{{\n  *   1. without sbt:\n  *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n- *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   2. build/sbt build/sbt \";project sql;set javaOptions\n+  *  *    in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"\n  *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \";project sql;set javaOptions\n- *        in Test -= \\\"-Dspark.memory.debugFill=true\\\";test:runMain <this class>\"\n+ *        in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"",
    "line": 21
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Ur, in PR description, `runMain` is repeated twice; `test:runMain test:runMain`",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T09:04:38Z",
    "diffHunk": "@@ -32,9 +32,10 @@ import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n  * {{{\n  *   1. without sbt:\n  *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n- *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   2. build/sbt build/sbt \";project sql;set javaOptions\n+  *  *    in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"\n  *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \";project sql;set javaOptions\n- *        in Test -= \\\"-Dspark.memory.debugFill=true\\\";test:runMain <this class>\"\n+ *        in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"",
    "line": 21
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "fixed",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T09:34:31Z",
    "diffHunk": "@@ -32,9 +32,10 @@ import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n  * {{{\n  *   1. without sbt:\n  *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n- *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   2. build/sbt build/sbt \";project sql;set javaOptions\n+  *  *    in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"\n  *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \";project sql;set javaOptions\n- *        in Test -= \\\"-Dspark.memory.debugFill=true\\\";test:runMain <this class>\"\n+ *        in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\"",
    "line": 21
  }],
  "prId": 22617
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Could you fix `  *  *`?",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T11:42:47Z",
    "diffHunk": "@@ -20,24 +20,57 @@ package org.apache.spark.sql.execution\n import scala.collection.mutable.ArrayBuffer\n \n import org.apache.spark.{SparkConf, SparkContext, SparkEnv, TaskContext}\n-import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.benchmark.{Benchmark, BenchmarkBase}\n import org.apache.spark.internal.config\n import org.apache.spark.memory.MemoryTestingUtils\n import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n \n-object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n+/**\n+ * Benchmark ExternalAppendOnlyUnsafeRowArray.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n+ *   2. build/sbt build/sbt \";project sql;set javaOptions\n+  *  *    in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\""
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "done",
    "commit": "b0d829eafe26177aad519224a16df017566e7ef4",
    "createdAt": "2019-01-09T12:02:48Z",
    "diffHunk": "@@ -20,24 +20,57 @@ package org.apache.spark.sql.execution\n import scala.collection.mutable.ArrayBuffer\n \n import org.apache.spark.{SparkConf, SparkContext, SparkEnv, TaskContext}\n-import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.benchmark.{Benchmark, BenchmarkBase}\n import org.apache.spark.internal.config\n import org.apache.spark.memory.MemoryTestingUtils\n import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n import org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n \n-object ExternalAppendOnlyUnsafeRowArrayBenchmark {\n+/**\n+ * Benchmark ExternalAppendOnlyUnsafeRowArray.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt:\n+ *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>\n+ *   2. build/sbt build/sbt \";project sql;set javaOptions\n+  *  *    in Test += \\\"-Dspark.memory.debugFill=false\\\";test:runMain <this class>\""
  }],
  "prId": 22617
}]