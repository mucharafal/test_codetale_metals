[{
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "@holdenk @cloud-fan I'm trying out the DateType conversion and ran into this problem.  The Dataset encoder uses `DateTimeUtils.toJavaDate` and `fromJavaDate` similar to above (which fails), and this forces a `defaultTimeZone()` when working with the data.  So a value `new Date(0)` should be the epoch, but in my timezone it forces it to be the day before and the test here will not pass.  \r\n\r\nWhat are your thoughts on this, should the conversion to Arrow assume the defaultTimeZone()?  is this something that should be fixed first in Spark?  Thanks!",
    "commit": "addd35f49d58227dd59b7d9d3595403cb992c1a8",
    "createdAt": "2017-07-18T00:54:43Z",
    "diffHunk": "@@ -792,6 +793,76 @@ class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n     collectAndValidate(df, json, \"binaryData.json\")\n   }\n \n+  test(\"date type conversion\") {\n+    val json =\n+      s\"\"\"\n+         |{\n+         |  \"schema\" : {\n+         |    \"fields\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"type\" : {\n+         |        \"name\" : \"date\",\n+         |        \"unit\" : \"DAY\"\n+         |      },\n+         |      \"nullable\" : true,\n+         |      \"children\" : [ ],\n+         |      \"typeLayout\" : {\n+         |        \"vectors\" : [ {\n+         |          \"type\" : \"VALIDITY\",\n+         |          \"typeBitWidth\" : 1\n+         |        }, {\n+         |          \"type\" : \"DATA\",\n+         |          \"typeBitWidth\" : 32\n+         |        } ]\n+         |      }\n+         |    } ]\n+         |  },\n+         |  \"batches\" : [ {\n+         |    \"count\" : 4,\n+         |    \"columns\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"count\" : 4,\n+         |      \"VALIDITY\" : [ 1, 1, 1, 1 ],\n+         |      \"DATA\" : [ -1, 0, 16533, 16930 ]\n+         |    } ]\n+         |  } ]\n+         |}\n+       \"\"\".stripMargin\n+\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(-1)  // \"1969-12-31 13:10:15.000 UTC\"\n+    val d2 = new Date(0)  // \"1970-01-01 13:10:15.000 UTC\"\n+    val d3 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d4 = new Date(sdf.parse(\"2016-05-09 12:01:01.000 UTC\").getTime)\n+\n+    // Date is created unaware of timezone, but DateTimeUtils force defaultTimeZone()\n+    assert(DateTimeUtils.toJavaDate(DateTimeUtils.fromJavaDate(d2)).getTime == d2.getTime)"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "cc @ueshin",
    "commit": "addd35f49d58227dd59b7d9d3595403cb992c1a8",
    "createdAt": "2017-07-18T01:23:01Z",
    "diffHunk": "@@ -792,6 +793,76 @@ class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n     collectAndValidate(df, json, \"binaryData.json\")\n   }\n \n+  test(\"date type conversion\") {\n+    val json =\n+      s\"\"\"\n+         |{\n+         |  \"schema\" : {\n+         |    \"fields\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"type\" : {\n+         |        \"name\" : \"date\",\n+         |        \"unit\" : \"DAY\"\n+         |      },\n+         |      \"nullable\" : true,\n+         |      \"children\" : [ ],\n+         |      \"typeLayout\" : {\n+         |        \"vectors\" : [ {\n+         |          \"type\" : \"VALIDITY\",\n+         |          \"typeBitWidth\" : 1\n+         |        }, {\n+         |          \"type\" : \"DATA\",\n+         |          \"typeBitWidth\" : 32\n+         |        } ]\n+         |      }\n+         |    } ]\n+         |  },\n+         |  \"batches\" : [ {\n+         |    \"count\" : 4,\n+         |    \"columns\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"count\" : 4,\n+         |      \"VALIDITY\" : [ 1, 1, 1, 1 ],\n+         |      \"DATA\" : [ -1, 0, 16533, 16930 ]\n+         |    } ]\n+         |  } ]\n+         |}\n+       \"\"\".stripMargin\n+\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(-1)  // \"1969-12-31 13:10:15.000 UTC\"\n+    val d2 = new Date(0)  // \"1970-01-01 13:10:15.000 UTC\"\n+    val d3 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d4 = new Date(sdf.parse(\"2016-05-09 12:01:01.000 UTC\").getTime)\n+\n+    // Date is created unaware of timezone, but DateTimeUtils force defaultTimeZone()\n+    assert(DateTimeUtils.toJavaDate(DateTimeUtils.fromJavaDate(d2)).getTime == d2.getTime)"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "We handle `DateType` value as number of days from `1970-01-01` internally.\r\n\r\nWhen converting from/to `Date` to/from internal value, we assume the `Date` instance contains the timestamp of `00:00:00` time of the day in `TimeZone.getDefault()` timezone, which is the offset of the timezone. e.g. in JST (GMT+09:00):\r\n\r\n```\r\nscala> TimeZone.setDefault(TimeZone.getTimeZone(\"JST\"))\r\n\r\nscala> Date.valueOf(\"1970-01-01\").getTime()\r\nres6: Long = -32400000\r\n```\r\n\r\nwhereas in PST (GMT-08:00):\r\n\r\n```\r\nscala> TimeZone.setDefault(TimeZone.getTimeZone(\"PST\"))\r\n\r\nscala> Date.valueOf(\"1970-01-01\").getTime()\r\nres8: Long = 28800000\r\n```\r\n\r\nWe use `DateTimeUtils.defaultTimeZone()` to adjust the offset.",
    "commit": "addd35f49d58227dd59b7d9d3595403cb992c1a8",
    "createdAt": "2017-07-18T04:18:29Z",
    "diffHunk": "@@ -792,6 +793,76 @@ class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n     collectAndValidate(df, json, \"binaryData.json\")\n   }\n \n+  test(\"date type conversion\") {\n+    val json =\n+      s\"\"\"\n+         |{\n+         |  \"schema\" : {\n+         |    \"fields\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"type\" : {\n+         |        \"name\" : \"date\",\n+         |        \"unit\" : \"DAY\"\n+         |      },\n+         |      \"nullable\" : true,\n+         |      \"children\" : [ ],\n+         |      \"typeLayout\" : {\n+         |        \"vectors\" : [ {\n+         |          \"type\" : \"VALIDITY\",\n+         |          \"typeBitWidth\" : 1\n+         |        }, {\n+         |          \"type\" : \"DATA\",\n+         |          \"typeBitWidth\" : 32\n+         |        } ]\n+         |      }\n+         |    } ]\n+         |  },\n+         |  \"batches\" : [ {\n+         |    \"count\" : 4,\n+         |    \"columns\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"count\" : 4,\n+         |      \"VALIDITY\" : [ 1, 1, 1, 1 ],\n+         |      \"DATA\" : [ -1, 0, 16533, 16930 ]\n+         |    } ]\n+         |  } ]\n+         |}\n+       \"\"\".stripMargin\n+\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(-1)  // \"1969-12-31 13:10:15.000 UTC\"\n+    val d2 = new Date(0)  // \"1970-01-01 13:10:15.000 UTC\"\n+    val d3 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d4 = new Date(sdf.parse(\"2016-05-09 12:01:01.000 UTC\").getTime)\n+\n+    // Date is created unaware of timezone, but DateTimeUtils force defaultTimeZone()\n+    assert(DateTimeUtils.toJavaDate(DateTimeUtils.fromJavaDate(d2)).getTime == d2.getTime)"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "Thanks @ueshin!  I think I got it",
    "commit": "addd35f49d58227dd59b7d9d3595403cb992c1a8",
    "createdAt": "2017-07-18T22:01:46Z",
    "diffHunk": "@@ -792,6 +793,76 @@ class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n     collectAndValidate(df, json, \"binaryData.json\")\n   }\n \n+  test(\"date type conversion\") {\n+    val json =\n+      s\"\"\"\n+         |{\n+         |  \"schema\" : {\n+         |    \"fields\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"type\" : {\n+         |        \"name\" : \"date\",\n+         |        \"unit\" : \"DAY\"\n+         |      },\n+         |      \"nullable\" : true,\n+         |      \"children\" : [ ],\n+         |      \"typeLayout\" : {\n+         |        \"vectors\" : [ {\n+         |          \"type\" : \"VALIDITY\",\n+         |          \"typeBitWidth\" : 1\n+         |        }, {\n+         |          \"type\" : \"DATA\",\n+         |          \"typeBitWidth\" : 32\n+         |        } ]\n+         |      }\n+         |    } ]\n+         |  },\n+         |  \"batches\" : [ {\n+         |    \"count\" : 4,\n+         |    \"columns\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"count\" : 4,\n+         |      \"VALIDITY\" : [ 1, 1, 1, 1 ],\n+         |      \"DATA\" : [ -1, 0, 16533, 16930 ]\n+         |    } ]\n+         |  } ]\n+         |}\n+       \"\"\".stripMargin\n+\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = new Date(-1)  // \"1969-12-31 13:10:15.000 UTC\"\n+    val d2 = new Date(0)  // \"1970-01-01 13:10:15.000 UTC\"\n+    val d3 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d4 = new Date(sdf.parse(\"2016-05-09 12:01:01.000 UTC\").getTime)\n+\n+    // Date is created unaware of timezone, but DateTimeUtils force defaultTimeZone()\n+    assert(DateTimeUtils.toJavaDate(DateTimeUtils.fromJavaDate(d2)).getTime == d2.getTime)"
  }],
  "prId": 18664
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "`d3` and `d4` might be flaky in some timezone.\r\nShould we use `Date.valueOf()`?:\r\n\r\n```scala\r\nval d3 = Date.valueOf(\"2015-04-08\")\r\nval d4 = Date.valueOf(\"3017-07-18\")\r\n```",
    "commit": "addd35f49d58227dd59b7d9d3595403cb992c1a8",
    "createdAt": "2017-07-28T06:04:29Z",
    "diffHunk": "@@ -792,6 +793,104 @@ class ArrowConvertersSuite extends SharedSQLContext with BeforeAndAfterAll {\n     collectAndValidate(df, json, \"binaryData.json\")\n   }\n \n+  test(\"date type conversion\") {\n+    val json =\n+      s\"\"\"\n+         |{\n+         |  \"schema\" : {\n+         |    \"fields\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"type\" : {\n+         |        \"name\" : \"date\",\n+         |        \"unit\" : \"DAY\"\n+         |      },\n+         |      \"nullable\" : true,\n+         |      \"children\" : [ ],\n+         |      \"typeLayout\" : {\n+         |        \"vectors\" : [ {\n+         |          \"type\" : \"VALIDITY\",\n+         |          \"typeBitWidth\" : 1\n+         |        }, {\n+         |          \"type\" : \"DATA\",\n+         |          \"typeBitWidth\" : 32\n+         |        } ]\n+         |      }\n+         |    } ]\n+         |  },\n+         |  \"batches\" : [ {\n+         |    \"count\" : 4,\n+         |    \"columns\" : [ {\n+         |      \"name\" : \"date\",\n+         |      \"count\" : 4,\n+         |      \"VALIDITY\" : [ 1, 1, 1, 1 ],\n+         |      \"DATA\" : [ -1, 0, 16533, 382607 ]\n+         |    } ]\n+         |  } ]\n+         |}\n+       \"\"\".stripMargin\n+\n+    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS z\", Locale.US)\n+    val d1 = DateTimeUtils.toJavaDate(-1)  // \"1969-12-31\"\n+    val d2 = DateTimeUtils.toJavaDate(0)  // \"1970-01-01\"\n+    val d3 = new Date(sdf.parse(\"2015-04-08 13:10:15.000 UTC\").getTime)\n+    val d4 = new Date(sdf.parse(\"3017-07-18 14:55:00.000 UTC\").getTime)"
  }],
  "prId": 18664
}]